Using GATED_RATIO=0.1 from SLURM_ARRAY_TASK_ID=1
OUTPUT_DIR: ./finetuned_checkpoints/MoE-LLaVA-Qwen-1.8B-4e-RePa-Save-Experiment-ratio0p1
Redirecting logs to logs/finetune_repa_moe_nccl_ratio0p1_job11635685_task1.out/.err
Traceback (most recent call last):
Traceback (most recent call last):
  File "/datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA/moellava/train/train_mem.py", line 6, in <module>
  File "/datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA/moellava/train/train_mem.py", line 6, in <module>
    from moellava.train.llama_flash_attn_monkey_patch import replace_llama_attn_with_flash_attn
  File "/datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA/moellava/__init__.py", line 1, in <module>
    from moellava.train.llama_flash_attn_monkey_patch import replace_llama_attn_with_flash_attn
  File "/datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA/moellava/__init__.py", line 1, in <module>
    from .model import LlavaLlamaForCausalLM
  File "/datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA/moellava/model/__init__.py", line 1, in <module>
    from .model import LlavaLlamaForCausalLM
  File "/datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA/moellava/model/__init__.py", line 1, in <module>
    from .language_model.llava_llama import LlavaLlamaForCausalLM, LlavaLlamaConfig
  File "/datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA/moellava/model/language_model/llava_llama.py", line 21, in <module>
    from .language_model.llava_llama import LlavaLlamaForCausalLM, LlavaLlamaConfig
  File "/datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA/moellava/model/language_model/llava_llama.py", line 21, in <module>
    from transformers import AutoConfig, AutoModelForCausalLM, \
  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2302, in __getattr__
    from transformers import AutoConfig, AutoModelForCausalLM, \
  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2302, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2332, in _get_module
    module = self._get_module(self._class_to_module[name])
  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2332, in _get_module
    raise e
  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2330, in _get_module
    raise e
  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2330, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/importlib/__init__.py", line 126, in import_module
        return _bootstrap._gcd_import(name[level:], package, level)return _bootstrap._gcd_import(name[level:], package, level)

  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/site-packages/transformers/models/auto/modeling_auto.py", line 23, in <module>
  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/site-packages/transformers/models/auto/modeling_auto.py", line 23, in <module>
    from .auto_factory import (
  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 43, in <module>
    from .auto_factory import (
  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 43, in <module>
    from ...generation import GenerationMixin
  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2302, in __getattr__
    from ...generation import GenerationMixin
  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2302, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2332, in _get_module
    module = self._get_module(self._class_to_module[name])
  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2332, in _get_module
    raise e
  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2330, in _get_module
    raise e
  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2330, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/site-packages/transformers/generation/utils.py", line 58, in <module>
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/site-packages/transformers/generation/utils.py", line 58, in <module>
        from .candidate_generator import (from .candidate_generator import (

  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/site-packages/transformers/generation/candidate_generator.py", line 29, in <module>
  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/site-packages/transformers/generation/candidate_generator.py", line 29, in <module>
        from sklearn.metrics import roc_curvefrom sklearn.metrics import roc_curve

  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/site-packages/sklearn/__init__.py", line 69, in <module>
  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/site-packages/sklearn/__init__.py", line 69, in <module>
    from . import (  # noqa: F401 E402    
from . import (  # noqa: F401 E402
ImportError: ImportErrorcannot import name '__check_build' from partially initialized module 'sklearn' (most likely due to a circular import) (/home/li309/pct_code/venv/moellava-test2/lib/python3.10/site-packages/sklearn/__init__.py): 
cannot import name '__check_build' from partially initialized module 'sklearn' (most likely due to a circular import) (/home/li309/pct_code/venv/moellava-test2/lib/python3.10/site-packages/sklearn/__init__.py)
E0911 04:26:14.544000 62340 /datasets/work/d61-pct/work/yang_work/venv/moellava-test2/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 62390) of binary: /home/li309/pct_code/venv/moellava-test2/bin/python
Traceback (most recent call last):
  File "/home/li309/pct_code/venv/moellava-test2/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/site-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/li309/pct_code/venv/moellava-test2/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
moellava/train/train_mem.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-09-11_04:26:14
  host      : g031.cm.cluster
  rank      : 7 (local_rank: 1)
  exitcode  : 1 (pid: 62391)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-09-11_04:26:14
  host      : g031.cm.cluster
  rank      : 6 (local_rank: 0)
  exitcode  : 1 (pid: 62390)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: g031: task 3: Exited with exit code 1
You are using a model of type moe_llava_qwen to instantiate a model of type repa_moe_llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type moe_llava_qwen to instantiate a model of type repa_moe_llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type moe_llava_qwen to instantiate a model of type repa_moe_llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type moe_llava_qwen to instantiate a model of type repa_moe_llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type moe_llava_qwen to instantiate a model of type repa_moe_llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type moe_llava_qwen to instantiate a model of type repa_moe_llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type moe_llava_qwen to instantiate a model of type repa_moe_llava_qwen. This is not supported for all configurations of models and can yield errors.
The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
You are using a model of type moe_llava_qwen to instantiate a model of type repa_moe_llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type moe_llava_qwen to instantiate a model of type repa_moe_llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type moe_llava_qwen to instantiate a model of type repa_moe_llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type moe_llava_qwen to instantiate a model of type repa_moe_llava_qwen. This is not supported for all configurations of models and can yield errors.
The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
You are using a model of type moe_llava_qwen to instantiate a model of type repa_moe_llava_qwen. This is not supported for all configurations of models and can yield errors.
The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
You are using a model of type moe_llava_qwen to instantiate a model of type repa_moe_llava_qwen. This is not supported for all configurations of models and can yield errors.
The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
You are using a model of type moe_llava_qwen to instantiate a model of type repa_moe_llava_qwen. This is not supported for all configurations of models and can yield errors.
The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:48<00:48, 48.42s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:48<00:48, 48.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 34.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 36.79s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 34.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 36.79s/it]
Some weights of RePaMoELLaVAQWenForCausalLM were not initialized from the model checkpoint at ./checkpoints/MoE-LLaVA-Qwen-1.8B-4e and are newly initialized: ['transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of RePaMoELLaVAQWenForCausalLM were not initialized from the model checkpoint at ./checkpoints/MoE-LLaVA-Qwen-1.8B-4e and are newly initialized: ['transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:48<00:48, 48.42s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:48<00:48, 48.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 34.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 36.79s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 34.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 36.79s/it]
Some weights of RePaMoELLaVAQWenForCausalLM were not initialized from the model checkpoint at ./checkpoints/MoE-LLaVA-Qwen-1.8B-4e and are newly initialized: ['transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:48<00:48, 48.41s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:48<00:48, 48.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 34.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 36.79s/it]
eed_experts.2.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of RePaMoELLaVAQWenForCausalLM were not initialized from the model checkpoint at ./checkpoints/MoE-LLaVA-Qwen-1.8B-4e and are newly initialized: ['transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 34.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 36.79s/it]
Some weights of RePaMoELLaVAQWenForCausalLM were not initialized from the model checkpoint at ./checkpoints/MoE-LLaVA-Qwen-1.8B-4e and are newly initialized: ['transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of RePaMoELLaVAQWenForCausalLM were not initialized from the model checkpoint at ./checkpoints/MoE-LLaVA-Qwen-1.8B-4e and are newly initialized: ['transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:48<00:48, 48.42s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:48<00:48, 48.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 34.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 34.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 36.79s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 36.79s/it]
Some weights of RePaMoELLaVAQWenForCausalLM were not initialized from the model checkpoint at ./checkpoints/MoE-LLaVA-Qwen-1.8B-4e and are newly initialized: ['transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of RePaMoELLaVAQWenForCausalLM were not initialized from the model checkpoint at ./checkpoints/MoE-LLaVA-Qwen-1.8B-4e and are newly initialized: ['transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:48<00:48, 48.42s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:48<00:48, 48.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 34.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 34.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 36.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 36.79s/it]

Some weights of RePaMoELLaVAQWenForCausalLM were not initialized from the model checkpoint at ./checkpoints/MoE-LLaVA-Qwen-1.8B-4e and are newly initialized: ['transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of RePaMoELLaVAQWenForCausalLM were not initialized from the model checkpoint at ./checkpoints/MoE-LLaVA-Qwen-1.8B-4e and are newly initialized: ['transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:48<00:48, 48.41s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:48<00:48, 48.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 34.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 34.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 36.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 36.79s/it]

Some weights of RePaMoELLaVAQWenForCausalLM were not initialized from the model checkpoint at ./checkpoints/MoE-LLaVA-Qwen-1.8B-4e and are newly initialized: ['transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of RePaMoELLaVAQWenForCausalLM were not initialized from the model checkpoint at ./checkpoints/MoE-LLaVA-Qwen-1.8B-4e and are newly initialized: ['transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:48<00:48, 48.41s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:48<00:48, 48.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 34.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 34.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 36.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 36.79s/it]

Some weights of RePaMoELLaVAQWenForCausalLM were not initialized from the model checkpoint at ./checkpoints/MoE-LLaVA-Qwen-1.8B-4e and are newly initialized: ['transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of RePaMoELLaVAQWenForCausalLM were not initialized from the model checkpoint at ./checkpoints/MoE-LLaVA-Qwen-1.8B-4e and are newly initialized: ['transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'transformer.h.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA/moellava/train/llava_trainer.py:187: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `LLaVATrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
/datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA/moellava/train/llava_trainer.py:187: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `LLaVATrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
/datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA/moellava/train/llava_trainer.py:187: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `LLaVATrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
/datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA/moellava/train/llava_trainer.py:187: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `LLaVATrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
/datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA/moellava/train/llava_trainer.py:187: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `LLaVATrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
/datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA/moellava/train/llava_trainer.py:187: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `LLaVATrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
/datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA/moellava/train/llava_trainer.py:187: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `LLaVATrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
/datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA/moellava/train/llava_trainer.py:187: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `LLaVATrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
/datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA/moellava/train/llava_trainer.py:187: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `LLaVATrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
/datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA/moellava/train/llava_trainer.py:187: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `LLaVATrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
/datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA/moellava/train/llava_trainer.py:187: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `LLaVATrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
/datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA/moellava/train/llava_trainer.py:187: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `LLaVATrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
/datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA/moellava/train/llava_trainer.py:187: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `LLaVATrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
/datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA/moellava/train/llava_trainer.py:187: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `LLaVATrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151643, 'bos_token_id': 151647, 'pad_token_id': 151646}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151643, 'bos_token_id': 151647, 'pad_token_id': 151646}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151643, 'bos_token_id': 151647, 'pad_token_id': 151646}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151643, 'bos_token_id': 151647, 'pad_token_id': 151646}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151643, 'bos_token_id': 151647, 'pad_token_id': 151646}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151643, 'bos_token_id': 151647, 'pad_token_id': 151646}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151643, 'bos_token_id': 151647, 'pad_token_id': 151646}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151643, 'bos_token_id': 151647, 'pad_token_id': 151646}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151643, 'bos_token_id': 151647, 'pad_token_id': 151646}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151643, 'bos_token_id': 151647, 'pad_token_id': 151646}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151643, 'bos_token_id': 151647, 'pad_token_id': 151646}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151643, 'bos_token_id': 151647, 'pad_token_id': 151646}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151643, 'bos_token_id': 151647, 'pad_token_id': 151646}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151643, 'bos_token_id': 151647, 'pad_token_id': 151646}.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 2. Using DeepSpeed's value.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
