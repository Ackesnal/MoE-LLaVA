Loading cuda/12.4.0
  Unloading conflict: cuda/12.8.1
VERSION: 1.6 (NCCL-only)
MASTER_PORT: 60199
MASTER_ADDR: g011
[2025-09-09 07:27:58,234] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-09 07:27:58,234] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-09 07:27:58,234] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-09 07:27:58,234] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-09 07:27:58,234] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-09 07:27:58,234] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-09 07:27:58,234] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-09 07:27:58,234] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-09 07:27:58,235] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-09 07:27:58,235] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-09 07:27:58,234] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-09 07:27:58,234] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-09 07:27:58,234] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-09 07:27:58,235] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-09 07:27:58,235] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-09 07:27:58,234] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-09-09 07:28:15,088] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-09 07:28:15,088] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-09 07:28:15,088] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-09 07:28:15,088] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-09 07:28:15,087] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-09 07:28:15,087] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-09 07:28:15,087] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-09 07:28:15,087] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-09 07:28:15,087] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-09 07:28:15,087] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-09 07:28:15,088] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-09 07:28:15,088] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-09 07:28:15,087] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-09 07:28:15,087] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-09 07:28:15,088] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-09 07:28:15,088] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode

⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode

⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode

⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode

⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode

⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
[2025-09-09 07:28:20,913] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 07:28:20,913] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 07:28:20,913] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 07:28:20,913] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 07:28:20,913] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 07:28:20,913] [INFO] [comm.py:852:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-09-09 07:28:20,913] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 07:28:20,913] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 07:28:20,913] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 07:28:20,913] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 07:28:20,913] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 07:28:20,913] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 07:28:20,913] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 07:28:20,913] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 07:28:20,913] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 07:28:20,913] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 07:28:20,913] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 07:28:21,338] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-09 07:28:21,341] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-09 07:28:21,343] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-09 07:28:21,344] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-09 07:28:21,346] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-09 07:28:21,348] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-09 07:28:21,350] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-09 07:28:21,351] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-09 07:28:21,353] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-09 07:28:21,355] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-09 07:28:21,356] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-09 07:28:21,358] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
Vision encoder and proj init.

Vision encoder and proj init.

Vision encoder and proj init.

Vision encoder and proj init.

Vision encoder and proj init.

Vision encoder and proj init.

Vision encoder and proj init.

Vision encoder and proj init.

RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Setting up RePaMoE fine-tuning mode...
Setting up RePaMoE fine-tuning mode...  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM

  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Setting up RePaMoE fine-tuning mode...Setting up RePaMoE fine-tuning mode...

  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Setting up RePaMoE fine-tuning mode...
Setting up RePaMoE fine-tuning mode...Setting up RePaMoE fine-tuning mode...
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM

  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable

  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable

  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  Set initial gated ratio to 1.0  Set initial gated ratio to 1.0

  Set initial gated ratio to 1.0  Set initial gated ratio to 1.0
  Set initial gated ratio to 1.0
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Total training steps: 5197.0  Total training steps: 5197.0

  Total training steps: 5197.0

  Total training steps: 5197.0  Total training steps: 5197.0

  Stage 1 (gated ratio reduction): 2598.0 steps  Stage 1 (gated ratio reduction): 2598.0 steps

  Stage 2 (post-reparam training): 2599.0 steps  Stage 2 (post-reparam training): 2599.0 steps

  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1

  Stage 1 (gated ratio reduction): 2598.0 steps  Set initial gated ratio to 1.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 1 (gated ratio reduction): 2598.0 steps  Stage 2 (post-reparam training): 2599.0 steps

  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Stage 2 (post-reparam training): 2599.0 steps
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]



  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1  Total training steps: 5197.0

  Stage 1 (gated ratio reduction): 2598.0 steps  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]

  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0  Set initial gated ratio to 1.0

  Total training steps: 5197.0  Total training steps: 5197.0

  Stage 1 (gated ratio reduction): 2598.0 steps  Stage 1 (gated ratio reduction): 2598.0 steps

  Stage 2 (post-reparam training): 2599.0 steps  Stage 2 (post-reparam training): 2599.0 steps

  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1

  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]

Formatting inputs...Skip in lazy mode
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
NCCL version 2.21.5+cuda12.4
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
No existing process group found, creating a new group named: ep_size_1
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
No existing process group found, creating a new group named: ep_size_1
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
{'loss': 0.9637, 'grad_norm': 0.38185951113700867, 'learning_rate': 0.0, 'epoch': 0.0}
{'loss': 1.0459, 'grad_norm': 0.43599504232406616, 'learning_rate': 1.282051282051282e-07, 'epoch': 0.0}
{'loss': 1.0661, 'grad_norm': 0.3767589330673218, 'learning_rate': 2.564102564102564e-07, 'epoch': 0.0}
{'loss': 1.0198, 'grad_norm': 0.40061020851135254, 'learning_rate': 3.846153846153847e-07, 'epoch': 0.0}
{'loss': 1.1773, 'grad_norm': 0.4309060871601105, 'learning_rate': 5.128205128205128e-07, 'epoch': 0.0}
{'loss': 1.1202, 'grad_norm': 0.414167582988739, 'learning_rate': 6.41025641025641e-07, 'epoch': 0.0}
{'loss': 1.047, 'grad_norm': 0.38038820028305054, 'learning_rate': 7.692307692307694e-07, 'epoch': 0.0}
{'loss': 1.1174, 'grad_norm': 0.39153048396110535, 'learning_rate': 8.974358974358975e-07, 'epoch': 0.0}
{'loss': 0.9943, 'grad_norm': 0.39565467834472656, 'learning_rate': 1.0256410256410257e-06, 'epoch': 0.0}
{'loss': 1.027, 'grad_norm': 0.3943064212799072, 'learning_rate': 1.153846153846154e-06, 'epoch': 0.0}
{'loss': 0.9566, 'grad_norm': 0.3699406385421753, 'learning_rate': 1.282051282051282e-06, 'epoch': 0.0}
{'loss': 1.0296, 'grad_norm': 0.3818357288837433, 'learning_rate': 1.4102564102564104e-06, 'epoch': 0.0}
{'loss': 1.1078, 'grad_norm': 0.4001147150993347, 'learning_rate': 1.5384615384615387e-06, 'epoch': 0.0}
{'loss': 0.9892, 'grad_norm': 0.3870723247528076, 'learning_rate': 1.6666666666666667e-06, 'epoch': 0.0}
{'loss': 1.0469, 'grad_norm': 0.3589767515659332, 'learning_rate': 1.794871794871795e-06, 'epoch': 0.0}
{'loss': 1.0165, 'grad_norm': 0.3905545175075531, 'learning_rate': 1.9230769230769234e-06, 'epoch': 0.0}
{'loss': 0.883, 'grad_norm': 0.23324011266231537, 'learning_rate': 2.0512820512820513e-06, 'epoch': 0.0}
{'loss': 0.9777, 'grad_norm': 0.38336411118507385, 'learning_rate': 2.1794871794871797e-06, 'epoch': 0.0}
{'loss': 0.9555, 'grad_norm': 0.3846658170223236, 'learning_rate': 2.307692307692308e-06, 'epoch': 0.0}
{'loss': 0.9932, 'grad_norm': 0.3820459246635437, 'learning_rate': 2.435897435897436e-06, 'epoch': 0.0}
{'loss': 0.9961, 'grad_norm': 0.3754333257675171, 'learning_rate': 2.564102564102564e-06, 'epoch': 0.0}
{'loss': 1.0365, 'grad_norm': 0.4044008255004883, 'learning_rate': 2.6923076923076923e-06, 'epoch': 0.0}
{'loss': 0.8866, 'grad_norm': 0.3805997371673584, 'learning_rate': 2.8205128205128207e-06, 'epoch': 0.0}
{'loss': 1.0387, 'grad_norm': 0.3848766088485718, 'learning_rate': 2.948717948717949e-06, 'epoch': 0.0}
{'loss': 1.0753, 'grad_norm': 0.39014801383018494, 'learning_rate': 3.0769230769230774e-06, 'epoch': 0.0}
{'loss': 1.0286, 'grad_norm': 0.39084485173225403, 'learning_rate': 3.205128205128206e-06, 'epoch': 0.01}
{'loss': 0.9557, 'grad_norm': 0.420537531375885, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.01}
{'loss': 1.0517, 'grad_norm': 0.3629978895187378, 'learning_rate': 3.4615384615384617e-06, 'epoch': 0.01}
{'loss': 0.9597, 'grad_norm': 0.3705620765686035, 'learning_rate': 3.58974358974359e-06, 'epoch': 0.01}
{'loss': 1.0175, 'grad_norm': 0.4024987518787384, 'learning_rate': 3.7179487179487184e-06, 'epoch': 0.01}
{'loss': 0.9452, 'grad_norm': 0.4008093476295471, 'learning_rate': 3.846153846153847e-06, 'epoch': 0.01}
{'loss': 0.978, 'grad_norm': 0.4277590215206146, 'learning_rate': 3.974358974358974e-06, 'epoch': 0.01}
{'loss': 1.0164, 'grad_norm': 0.41670122742652893, 'learning_rate': 4.102564102564103e-06, 'epoch': 0.01}
{'loss': 0.9555, 'grad_norm': 0.38440847396850586, 'learning_rate': 4.230769230769231e-06, 'epoch': 0.01}
{'loss': 0.9813, 'grad_norm': 0.37041419744491577, 'learning_rate': 4.358974358974359e-06, 'epoch': 0.01}
{'loss': 0.9442, 'grad_norm': 0.3786560893058777, 'learning_rate': 4.487179487179488e-06, 'epoch': 0.01}
{'loss': 1.0575, 'grad_norm': 0.39288267493247986, 'learning_rate': 4.615384615384616e-06, 'epoch': 0.01}
{'loss': 0.996, 'grad_norm': 0.38935598731040955, 'learning_rate': 4.743589743589744e-06, 'epoch': 0.01}
{'loss': 1.0444, 'grad_norm': 0.3940633237361908, 'learning_rate': 4.871794871794872e-06, 'epoch': 0.01}
{'loss': 0.9242, 'grad_norm': 0.3794988691806793, 'learning_rate': 5e-06, 'epoch': 0.01}
{'loss': 0.9842, 'grad_norm': 0.4078682065010071, 'learning_rate': 5.128205128205128e-06, 'epoch': 0.01}
{'loss': 0.9471, 'grad_norm': 0.36048465967178345, 'learning_rate': 5.256410256410257e-06, 'epoch': 0.01}
{'loss': 0.9907, 'grad_norm': 0.3690033555030823, 'learning_rate': 5.384615384615385e-06, 'epoch': 0.01}
{'loss': 0.9424, 'grad_norm': 0.366258442401886, 'learning_rate': 5.512820512820514e-06, 'epoch': 0.01}
{'loss': 0.9622, 'grad_norm': 0.38040071725845337, 'learning_rate': 5.641025641025641e-06, 'epoch': 0.01}
{'loss': 0.9593, 'grad_norm': 0.3772548735141754, 'learning_rate': 5.769230769230769e-06, 'epoch': 0.01}
{'loss': 1.0409, 'grad_norm': 0.40139034390449524, 'learning_rate': 5.897435897435898e-06, 'epoch': 0.01}
{'loss': 0.9449, 'grad_norm': 0.4110969603061676, 'learning_rate': 6.025641025641026e-06, 'epoch': 0.01}
{'loss': 1.0383, 'grad_norm': 0.4016667306423187, 'learning_rate': 6.153846153846155e-06, 'epoch': 0.01}
{'loss': 1.0449, 'grad_norm': 0.4236903190612793, 'learning_rate': 6.282051282051282e-06, 'epoch': 0.01}
{'loss': 1.0736, 'grad_norm': 0.4716777205467224, 'learning_rate': 6.410256410256412e-06, 'epoch': 0.01}
{'loss': 1.0851, 'grad_norm': 0.47569867968559265, 'learning_rate': 6.538461538461539e-06, 'epoch': 0.01}
{'loss': 0.925, 'grad_norm': 0.37702566385269165, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.01}
{'loss': 1.0966, 'grad_norm': 0.4612610638141632, 'learning_rate': 6.794871794871796e-06, 'epoch': 0.01}
{'loss': 1.1203, 'grad_norm': 0.47736838459968567, 'learning_rate': 6.923076923076923e-06, 'epoch': 0.01}
{'loss': 1.136, 'grad_norm': 0.4404584765434265, 'learning_rate': 7.051282051282053e-06, 'epoch': 0.01}
{'loss': 1.0715, 'grad_norm': 0.41570547223091125, 'learning_rate': 7.17948717948718e-06, 'epoch': 0.01}
{'loss': 1.0404, 'grad_norm': 0.39780691266059875, 'learning_rate': 7.307692307692308e-06, 'epoch': 0.01}
{'loss': 1.0282, 'grad_norm': 0.4079526364803314, 'learning_rate': 7.435897435897437e-06, 'epoch': 0.01}
{'loss': 1.0318, 'grad_norm': 0.4149922728538513, 'learning_rate': 7.564102564102564e-06, 'epoch': 0.01}
{'loss': 1.1025, 'grad_norm': 0.440075546503067, 'learning_rate': 7.692307692307694e-06, 'epoch': 0.01}
{'loss': 1.0624, 'grad_norm': 0.3920471966266632, 'learning_rate': 7.820512820512822e-06, 'epoch': 0.01}
{'loss': 1.1198, 'grad_norm': 0.47644439339637756, 'learning_rate': 7.948717948717949e-06, 'epoch': 0.01}
{'loss': 0.98, 'grad_norm': 0.40589064359664917, 'learning_rate': 8.076923076923077e-06, 'epoch': 0.01}
{'loss': 0.9394, 'grad_norm': 0.38011661171913147, 'learning_rate': 8.205128205128205e-06, 'epoch': 0.01}
{'loss': 0.9544, 'grad_norm': 0.4123018980026245, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.01}
{'loss': 0.9594, 'grad_norm': 0.4027431607246399, 'learning_rate': 8.461538461538462e-06, 'epoch': 0.01}
{'loss': 1.027, 'grad_norm': 0.4089970290660858, 'learning_rate': 8.58974358974359e-06, 'epoch': 0.01}
{'loss': 1.0073, 'grad_norm': 0.40082672238349915, 'learning_rate': 8.717948717948719e-06, 'epoch': 0.01}
{'loss': 1.102, 'grad_norm': 0.4482477605342865, 'learning_rate': 8.846153846153847e-06, 'epoch': 0.01}
{'loss': 0.9658, 'grad_norm': 0.42207837104797363, 'learning_rate': 8.974358974358976e-06, 'epoch': 0.01}
{'loss': 1.1015, 'grad_norm': 0.4431520402431488, 'learning_rate': 9.102564102564104e-06, 'epoch': 0.01}
{'loss': 1.0406, 'grad_norm': 0.42185381054878235, 'learning_rate': 9.230769230769232e-06, 'epoch': 0.01}
{'loss': 0.9807, 'grad_norm': 0.40780869126319885, 'learning_rate': 9.358974358974359e-06, 'epoch': 0.01}
{'loss': 0.9332, 'grad_norm': 0.42086711525917053, 'learning_rate': 9.487179487179487e-06, 'epoch': 0.01}
{'loss': 1.0036, 'grad_norm': 0.42069172859191895, 'learning_rate': 9.615384615384616e-06, 'epoch': 0.01}
{'loss': 0.9663, 'grad_norm': 0.3964119851589203, 'learning_rate': 9.743589743589744e-06, 'epoch': 0.01}
{'loss': 1.1497, 'grad_norm': 0.4362667500972748, 'learning_rate': 9.871794871794872e-06, 'epoch': 0.02}
{'loss': 1.1052, 'grad_norm': 0.44269102811813354, 'learning_rate': 1e-05, 'epoch': 0.02}
{'loss': 1.0243, 'grad_norm': 0.45758020877838135, 'learning_rate': 1.012820512820513e-05, 'epoch': 0.02}
{'loss': 0.9758, 'grad_norm': 0.3954891860485077, 'learning_rate': 1.0256410256410256e-05, 'epoch': 0.02}
{'loss': 0.9565, 'grad_norm': 0.4013056755065918, 'learning_rate': 1.0384615384615386e-05, 'epoch': 0.02}
{'loss': 0.9969, 'grad_norm': 0.4217711091041565, 'learning_rate': 1.0512820512820514e-05, 'epoch': 0.02}
{'loss': 1.0165, 'grad_norm': 0.42240890860557556, 'learning_rate': 1.0641025641025643e-05, 'epoch': 0.02}
{'loss': 0.9458, 'grad_norm': 0.28005552291870117, 'learning_rate': 1.076923076923077e-05, 'epoch': 0.02}
{'loss': 1.0418, 'grad_norm': 0.4975886046886444, 'learning_rate': 1.0897435897435898e-05, 'epoch': 0.02}
{'loss': 0.9763, 'grad_norm': 0.3979032039642334, 'learning_rate': 1.1025641025641028e-05, 'epoch': 0.02}
{'loss': 1.0138, 'grad_norm': 0.4040750563144684, 'learning_rate': 1.1153846153846154e-05, 'epoch': 0.02}
{'loss': 1.0348, 'grad_norm': 0.4107803702354431, 'learning_rate': 1.1282051282051283e-05, 'epoch': 0.02}
{'loss': 0.9778, 'grad_norm': 0.394627183675766, 'learning_rate': 1.1410256410256411e-05, 'epoch': 0.02}
{'loss': 1.0448, 'grad_norm': 0.40575939416885376, 'learning_rate': 1.1538461538461538e-05, 'epoch': 0.02}
{'loss': 0.9966, 'grad_norm': 0.4146633446216583, 'learning_rate': 1.1666666666666668e-05, 'epoch': 0.02}
{'loss': 1.0715, 'grad_norm': 0.4893724024295807, 'learning_rate': 1.1794871794871796e-05, 'epoch': 0.02}
{'loss': 0.9269, 'grad_norm': 0.4354807138442993, 'learning_rate': 1.1923076923076925e-05, 'epoch': 0.02}
{'loss': 0.9527, 'grad_norm': 0.40623560547828674, 'learning_rate': 1.2051282051282051e-05, 'epoch': 0.02}
{'loss': 0.9816, 'grad_norm': 0.4419151544570923, 'learning_rate': 1.217948717948718e-05, 'epoch': 0.02}
{'loss': 0.974, 'grad_norm': 0.38508862257003784, 'learning_rate': 1.230769230769231e-05, 'epoch': 0.02}
{'loss': 1.0001, 'grad_norm': 0.4196280837059021, 'learning_rate': 1.2435897435897436e-05, 'epoch': 0.02}
{'loss': 0.9573, 'grad_norm': 0.3987467885017395, 'learning_rate': 1.2564102564102565e-05, 'epoch': 0.02}
{'loss': 1.0522, 'grad_norm': 0.4285297095775604, 'learning_rate': 1.2692307692307693e-05, 'epoch': 0.02}
{'loss': 1.118, 'grad_norm': 0.46624308824539185, 'learning_rate': 1.2820512820512823e-05, 'epoch': 0.02}
{'loss': 0.9973, 'grad_norm': 0.4494979977607727, 'learning_rate': 1.294871794871795e-05, 'epoch': 0.02}
{'loss': 1.0021, 'grad_norm': 0.42856401205062866, 'learning_rate': 1.3076923076923078e-05, 'epoch': 0.02}
{'loss': 0.9895, 'grad_norm': 0.41277751326560974, 'learning_rate': 1.3205128205128207e-05, 'epoch': 0.02}
{'loss': 1.0701, 'grad_norm': 0.4244519770145416, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.02}
{'loss': 1.0867, 'grad_norm': 0.45737460255622864, 'learning_rate': 1.3461538461538463e-05, 'epoch': 0.02}
{'loss': 1.0229, 'grad_norm': 0.4107794463634491, 'learning_rate': 1.3589743589743592e-05, 'epoch': 0.02}
{'loss': 0.9189, 'grad_norm': 0.4057696461677551, 'learning_rate': 1.3717948717948718e-05, 'epoch': 0.02}
{'loss': 1.078, 'grad_norm': 0.5061537027359009, 'learning_rate': 1.3846153846153847e-05, 'epoch': 0.02}
{'loss': 0.9723, 'grad_norm': 0.3888603150844574, 'learning_rate': 1.3974358974358975e-05, 'epoch': 0.02}
{'loss': 0.9926, 'grad_norm': 0.4214168190956116, 'learning_rate': 1.4102564102564105e-05, 'epoch': 0.02}
{'loss': 1.0358, 'grad_norm': 0.3952372670173645, 'learning_rate': 1.4230769230769232e-05, 'epoch': 0.02}
{'loss': 0.9279, 'grad_norm': 0.3961476683616638, 'learning_rate': 1.435897435897436e-05, 'epoch': 0.02}
{'loss': 1.0071, 'grad_norm': 0.42399725317955017, 'learning_rate': 1.4487179487179489e-05, 'epoch': 0.02}
{'loss': 1.0903, 'grad_norm': 0.4763885736465454, 'learning_rate': 1.4615384615384615e-05, 'epoch': 0.02}
{'loss': 1.0589, 'grad_norm': 0.4365084767341614, 'learning_rate': 1.4743589743589745e-05, 'epoch': 0.02}
{'loss': 0.9469, 'grad_norm': 0.4164469242095947, 'learning_rate': 1.4871794871794874e-05, 'epoch': 0.02}
{'loss': 1.0744, 'grad_norm': 0.4703492820262909, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.02}
{'loss': 1.0156, 'grad_norm': 0.42588624358177185, 'learning_rate': 1.5128205128205129e-05, 'epoch': 0.02}
{'loss': 1.0964, 'grad_norm': 0.4385073482990265, 'learning_rate': 1.5256410256410257e-05, 'epoch': 0.02}
{'loss': 1.0004, 'grad_norm': 0.41134539246559143, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.02}
{'loss': 0.9412, 'grad_norm': 0.4076603055000305, 'learning_rate': 1.5512820512820516e-05, 'epoch': 0.02}
{'loss': 1.1129, 'grad_norm': 0.5235875844955444, 'learning_rate': 1.5641025641025644e-05, 'epoch': 0.02}
{'loss': 1.0682, 'grad_norm': 0.4234437942504883, 'learning_rate': 1.576923076923077e-05, 'epoch': 0.02}
{'loss': 1.0694, 'grad_norm': 0.4549732208251953, 'learning_rate': 1.5897435897435897e-05, 'epoch': 0.02}
{'loss': 1.0256, 'grad_norm': 0.4347303509712219, 'learning_rate': 1.602564102564103e-05, 'epoch': 0.02}
{'loss': 0.9754, 'grad_norm': 0.408511221408844, 'learning_rate': 1.6153846153846154e-05, 'epoch': 0.02}
{'loss': 0.9557, 'grad_norm': 0.39178746938705444, 'learning_rate': 1.6282051282051282e-05, 'epoch': 0.02}
{'loss': 0.9547, 'grad_norm': 0.40747758746147156, 'learning_rate': 1.641025641025641e-05, 'epoch': 0.02}
{'loss': 1.0805, 'grad_norm': 0.4517393410205841, 'learning_rate': 1.653846153846154e-05, 'epoch': 0.03}
{'loss': 0.9987, 'grad_norm': 0.4284980595111847, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.03}
{'loss': 1.0308, 'grad_norm': 0.4097234010696411, 'learning_rate': 1.6794871794871796e-05, 'epoch': 0.03}
{'loss': 1.1065, 'grad_norm': 0.4505472779273987, 'learning_rate': 1.6923076923076924e-05, 'epoch': 0.03}
{'loss': 0.9821, 'grad_norm': 0.40265554189682007, 'learning_rate': 1.7051282051282053e-05, 'epoch': 0.03}
{'loss': 1.1492, 'grad_norm': 0.4853315055370331, 'learning_rate': 1.717948717948718e-05, 'epoch': 0.03}
{'loss': 0.9501, 'grad_norm': 0.2499159723520279, 'learning_rate': 1.730769230769231e-05, 'epoch': 0.03}
{'loss': 0.9816, 'grad_norm': 0.2781260907649994, 'learning_rate': 1.7435897435897438e-05, 'epoch': 0.03}
{'loss': 1.0583, 'grad_norm': 0.3867362141609192, 'learning_rate': 1.7564102564102566e-05, 'epoch': 0.03}
{'loss': 1.1131, 'grad_norm': 0.4618499279022217, 'learning_rate': 1.7692307692307694e-05, 'epoch': 0.03}
{'loss': 1.0577, 'grad_norm': 0.4259531795978546, 'learning_rate': 1.7820512820512823e-05, 'epoch': 0.03}
{'loss': 0.9674, 'grad_norm': 0.39564818143844604, 'learning_rate': 1.794871794871795e-05, 'epoch': 0.03}
{'loss': 0.9828, 'grad_norm': 0.4165026545524597, 'learning_rate': 1.807692307692308e-05, 'epoch': 0.03}
{'loss': 0.9588, 'grad_norm': 0.4283573627471924, 'learning_rate': 1.8205128205128208e-05, 'epoch': 0.03}
{'loss': 0.9494, 'grad_norm': 0.4313771426677704, 'learning_rate': 1.8333333333333333e-05, 'epoch': 0.03}
{'loss': 0.9083, 'grad_norm': 0.4238349497318268, 'learning_rate': 1.8461538461538465e-05, 'epoch': 0.03}
{'loss': 0.9355, 'grad_norm': 0.40428483486175537, 'learning_rate': 1.8589743589743593e-05, 'epoch': 0.03}
{'loss': 1.0233, 'grad_norm': 0.390483558177948, 'learning_rate': 1.8717948717948718e-05, 'epoch': 0.03}
{'loss': 0.9106, 'grad_norm': 0.4211927354335785, 'learning_rate': 1.8846153846153846e-05, 'epoch': 0.03}
{'loss': 1.0734, 'grad_norm': 0.4159435033798218, 'learning_rate': 1.8974358974358975e-05, 'epoch': 0.03}
{'loss': 1.0653, 'grad_norm': 0.4930610954761505, 'learning_rate': 1.9102564102564106e-05, 'epoch': 0.03}
{'loss': 0.9982, 'grad_norm': 0.4705204963684082, 'learning_rate': 1.923076923076923e-05, 'epoch': 0.03}
{'loss': 0.9629, 'grad_norm': 0.4095376133918762, 'learning_rate': 1.935897435897436e-05, 'epoch': 0.03}
{'loss': 1.0305, 'grad_norm': 0.41133689880371094, 'learning_rate': 1.9487179487179488e-05, 'epoch': 0.03}
{'loss': 1.0953, 'grad_norm': 0.46018555760383606, 'learning_rate': 1.9615384615384617e-05, 'epoch': 0.03}
{'loss': 1.0961, 'grad_norm': 0.4740429222583771, 'learning_rate': 1.9743589743589745e-05, 'epoch': 0.03}
{'loss': 1.0132, 'grad_norm': 0.44559818506240845, 'learning_rate': 1.9871794871794873e-05, 'epoch': 0.03}
{'loss': 1.0045, 'grad_norm': 0.4430030584335327, 'learning_rate': 2e-05, 'epoch': 0.03}
{'loss': 0.9717, 'grad_norm': 0.423671156167984, 'learning_rate': 1.9999998058827844e-05, 'epoch': 0.03}
{'loss': 0.9173, 'grad_norm': 0.4345341920852661, 'learning_rate': 1.9999992235312136e-05, 'epoch': 0.03}
{'loss': 1.1163, 'grad_norm': 0.48007452487945557, 'learning_rate': 1.9999982529455127e-05, 'epoch': 0.03}
{'loss': 0.972, 'grad_norm': 0.398579865694046, 'learning_rate': 1.9999968941260596e-05, 'epoch': 0.03}
{'loss': 0.8986, 'grad_norm': 0.4098132252693176, 'learning_rate': 1.9999951470733808e-05, 'epoch': 0.03}
{'loss': 0.978, 'grad_norm': 0.42200419306755066, 'learning_rate': 1.9999930117881548e-05, 'epoch': 0.03}
{'loss': 1.048, 'grad_norm': 0.4142773747444153, 'learning_rate': 1.9999904882712115e-05, 'epoch': 0.03}
{'loss': 1.0408, 'grad_norm': 0.4042162299156189, 'learning_rate': 1.99998757652353e-05, 'epoch': 0.03}
{'loss': 0.9427, 'grad_norm': 0.4006008505821228, 'learning_rate': 1.9999842765462403e-05, 'epoch': 0.03}
{'loss': 1.0442, 'grad_norm': 0.42829039692878723, 'learning_rate': 1.999980588340624e-05, 'epoch': 0.03}
{'loss': 1.005, 'grad_norm': 0.433671236038208, 'learning_rate': 1.9999765119081132e-05, 'epoch': 0.03}
{'loss': 1.0309, 'grad_norm': 0.2764073610305786, 'learning_rate': 1.9999720472502902e-05, 'epoch': 0.03}
{'loss': 0.9869, 'grad_norm': 0.4073949158191681, 'learning_rate': 1.9999671943688885e-05, 'epoch': 0.03}
{'loss': 0.9673, 'grad_norm': 0.4355604648590088, 'learning_rate': 1.9999619532657915e-05, 'epoch': 0.03}
{'loss': 1.044, 'grad_norm': 0.44728758931159973, 'learning_rate': 1.9999563239430352e-05, 'epoch': 0.03}
{'loss': 0.9808, 'grad_norm': 0.4158967137336731, 'learning_rate': 1.9999503064028043e-05, 'epoch': 0.03}
{'loss': 1.1182, 'grad_norm': 0.5041176676750183, 'learning_rate': 1.999943900647435e-05, 'epoch': 0.03}
{'loss': 1.0003, 'grad_norm': 0.44742199778556824, 'learning_rate': 1.9999371066794146e-05, 'epoch': 0.03}
{'loss': 1.0128, 'grad_norm': 0.40127941966056824, 'learning_rate': 1.9999299245013805e-05, 'epoch': 0.03}
{'loss': 0.9997, 'grad_norm': 0.4186597466468811, 'learning_rate': 1.999922354116121e-05, 'epoch': 0.03}
{'loss': 1.0263, 'grad_norm': 0.4078634977340698, 'learning_rate': 1.999914395526575e-05, 'epoch': 0.03}
{'loss': 1.0761, 'grad_norm': 0.439001202583313, 'learning_rate': 1.9999060487358333e-05, 'epoch': 0.03}
{'loss': 1.0023, 'grad_norm': 0.4045267105102539, 'learning_rate': 1.9998973137471352e-05, 'epoch': 0.03}
{'loss': 0.9957, 'grad_norm': 0.41093963384628296, 'learning_rate': 1.9998881905638727e-05, 'epoch': 0.03}
{'loss': 0.9535, 'grad_norm': 0.41045594215393066, 'learning_rate': 1.9998786791895874e-05, 'epoch': 0.04}
{'loss': 1.0452, 'grad_norm': 0.4281843900680542, 'learning_rate': 1.999868779627972e-05, 'epoch': 0.04}
{'loss': 1.0872, 'grad_norm': 0.4211367666721344, 'learning_rate': 1.9998584918828695e-05, 'epoch': 0.04}
{'loss': 0.9628, 'grad_norm': 0.39116916060447693, 'learning_rate': 1.9998478159582747e-05, 'epoch': 0.04}
{'loss': 0.9483, 'grad_norm': 0.4205601215362549, 'learning_rate': 1.999836751858332e-05, 'epoch': 0.04}
{'loss': 1.0333, 'grad_norm': 0.41604194045066833, 'learning_rate': 1.9998252995873367e-05, 'epoch': 0.04}
{'loss': 0.9501, 'grad_norm': 0.41754624247550964, 'learning_rate': 1.999813459149735e-05, 'epoch': 0.04}
{'loss': 0.9895, 'grad_norm': 0.44338080286979675, 'learning_rate': 1.9998012305501243e-05, 'epoch': 0.04}
{'loss': 0.9757, 'grad_norm': 0.4240995943546295, 'learning_rate': 1.999788613793251e-05, 'epoch': 0.04}
{'loss': 0.9948, 'grad_norm': 0.4137147068977356, 'learning_rate': 1.999775608884015e-05, 'epoch': 0.04}
{'loss': 1.0256, 'grad_norm': 0.4403260052204132, 'learning_rate': 1.9997622158274635e-05, 'epoch': 0.04}
{'loss': 0.9413, 'grad_norm': 0.4077033996582031, 'learning_rate': 1.9997484346287973e-05, 'epoch': 0.04}
{'loss': 1.0359, 'grad_norm': 0.5217321515083313, 'learning_rate': 1.9997342652933668e-05, 'epoch': 0.04}
{'loss': 1.105, 'grad_norm': 0.4544716775417328, 'learning_rate': 1.9997197078266723e-05, 'epoch': 0.04}
{'loss': 1.0353, 'grad_norm': 0.4797815680503845, 'learning_rate': 1.999704762234366e-05, 'epoch': 0.04}
{'loss': 0.8967, 'grad_norm': 0.4462072551250458, 'learning_rate': 1.99968942852225e-05, 'epoch': 0.04}
{'loss': 1.0206, 'grad_norm': 0.43072956800460815, 'learning_rate': 1.9996737066962778e-05, 'epoch': 0.04}
{'loss': 1.0332, 'grad_norm': 0.4064345955848694, 'learning_rate': 1.9996575967625525e-05, 'epoch': 0.04}
{'loss': 0.9433, 'grad_norm': 0.4455844461917877, 'learning_rate': 1.999641098727329e-05, 'epoch': 0.04}
{'loss': 0.9777, 'grad_norm': 0.44217902421951294, 'learning_rate': 1.999624212597013e-05, 'epoch': 0.04}
{'loss': 1.0873, 'grad_norm': 0.45439112186431885, 'learning_rate': 1.9996069383781587e-05, 'epoch': 0.04}
{'loss': 1.1075, 'grad_norm': 0.45886319875717163, 'learning_rate': 1.9995892760774738e-05, 'epoch': 0.04}
{'loss': 1.0167, 'grad_norm': 0.45338064432144165, 'learning_rate': 1.9995712257018153e-05, 'epoch': 0.04}
{'loss': 1.0213, 'grad_norm': 0.45966124534606934, 'learning_rate': 1.9995527872581903e-05, 'epoch': 0.04}
{'loss': 1.0936, 'grad_norm': 0.45380935072898865, 'learning_rate': 1.9995339607537578e-05, 'epoch': 0.04}
{'loss': 0.9287, 'grad_norm': 0.2669084668159485, 'learning_rate': 1.9995147461958267e-05, 'epoch': 0.04}
{'loss': 1.0093, 'grad_norm': 0.4244769215583801, 'learning_rate': 1.999495143591857e-05, 'epoch': 0.04}
{'loss': 1.1006, 'grad_norm': 0.4653173089027405, 'learning_rate': 1.999475152949459e-05, 'epoch': 0.04}
{'loss': 1.045, 'grad_norm': 0.40513524413108826, 'learning_rate': 1.9994547742763935e-05, 'epoch': 0.04}
{'loss': 1.0597, 'grad_norm': 0.43529751896858215, 'learning_rate': 1.9994340075805724e-05, 'epoch': 0.04}
{'loss': 0.9696, 'grad_norm': 0.41434112191200256, 'learning_rate': 1.9994128528700583e-05, 'epoch': 0.04}
{'loss': 0.9211, 'grad_norm': 0.41919809579849243, 'learning_rate': 1.9993913101530635e-05, 'epoch': 0.04}
{'loss': 1.0663, 'grad_norm': 0.42084574699401855, 'learning_rate': 1.9993693794379525e-05, 'epoch': 0.04}
{'loss': 1.0648, 'grad_norm': 0.4349386990070343, 'learning_rate': 1.9993470607332387e-05, 'epoch': 0.04}
{'loss': 0.9585, 'grad_norm': 0.38901588320732117, 'learning_rate': 1.999324354047588e-05, 'epoch': 0.04}
{'loss': 1.0606, 'grad_norm': 0.45851051807403564, 'learning_rate': 1.9993012593898146e-05, 'epoch': 0.04}
{'loss': 1.0689, 'grad_norm': 0.4252517521381378, 'learning_rate': 1.9992777767688857e-05, 'epoch': 0.04}
{'loss': 0.983, 'grad_norm': 0.45545512437820435, 'learning_rate': 1.9992539061939175e-05, 'epoch': 0.04}
{'loss': 0.9506, 'grad_norm': 0.4043044447898865, 'learning_rate': 1.999229647674178e-05, 'epoch': 0.04}
{'loss': 0.9784, 'grad_norm': 0.2881167232990265, 'learning_rate': 1.9992050012190845e-05, 'epoch': 0.04}
{'loss': 1.0682, 'grad_norm': 0.4453083872795105, 'learning_rate': 1.9991799668382058e-05, 'epoch': 0.04}
{'loss': 1.2319, 'grad_norm': 0.47450515627861023, 'learning_rate': 1.9991545445412614e-05, 'epoch': 0.04}
{'loss': 1.0459, 'grad_norm': 0.4841076135635376, 'learning_rate': 1.9991287343381208e-05, 'epoch': 0.04}
{'loss': 1.0702, 'grad_norm': 0.42064163088798523, 'learning_rate': 1.9991025362388044e-05, 'epoch': 0.04}
{'loss': 1.0757, 'grad_norm': 0.4303666353225708, 'learning_rate': 1.9990759502534835e-05, 'epoch': 0.04}
{'loss': 1.0596, 'grad_norm': 0.43741434812545776, 'learning_rate': 1.9990489763924796e-05, 'epoch': 0.04}
{'loss': 1.0783, 'grad_norm': 0.4232644736766815, 'learning_rate': 1.9990216146662648e-05, 'epoch': 0.04}
{'loss': 0.9783, 'grad_norm': 0.4251575171947479, 'learning_rate': 1.9989938650854618e-05, 'epoch': 0.04}
{'loss': 0.9729, 'grad_norm': 0.45000723004341125, 'learning_rate': 1.998965727660844e-05, 'epoch': 0.04}
{'loss': 1.0225, 'grad_norm': 0.4157576560974121, 'learning_rate': 1.9989372024033352e-05, 'epoch': 0.04}
{'loss': 0.9056, 'grad_norm': 0.4214897155761719, 'learning_rate': 1.99890828932401e-05, 'epoch': 0.04}
{'loss': 0.9407, 'grad_norm': 0.4166336953639984, 'learning_rate': 1.9988789884340938e-05, 'epoch': 0.04}
{'loss': 0.9256, 'grad_norm': 0.397151917219162, 'learning_rate': 1.9988492997449615e-05, 'epoch': 0.05}
{'loss': 1.0387, 'grad_norm': 0.4276715815067291, 'learning_rate': 1.9988192232681398e-05, 'epoch': 0.05}
{'loss': 0.959, 'grad_norm': 0.42963898181915283, 'learning_rate': 1.9987887590153055e-05, 'epoch': 0.05}
{'loss': 1.0483, 'grad_norm': 0.42787134647369385, 'learning_rate': 1.9987579069982856e-05, 'epoch': 0.05}
{'loss': 0.9912, 'grad_norm': 0.41383373737335205, 'learning_rate': 1.9987266672290577e-05, 'epoch': 0.05}
{'loss': 1.0246, 'grad_norm': 0.4662209749221802, 'learning_rate': 1.9986950397197503e-05, 'epoch': 0.05}
{'loss': 1.0776, 'grad_norm': 0.4932425022125244, 'learning_rate': 1.9986630244826425e-05, 'epoch': 0.05}
{'loss': 0.9763, 'grad_norm': 0.4109407961368561, 'learning_rate': 1.998630621530164e-05, 'epoch': 0.05}
{'loss': 1.0676, 'grad_norm': 0.4121074974536896, 'learning_rate': 1.998597830874894e-05, 'epoch': 0.05}
{'loss': 0.979, 'grad_norm': 0.41187959909439087, 'learning_rate': 1.9985646525295634e-05, 'epoch': 0.05}
{'loss': 1.0195, 'grad_norm': 0.4152331054210663, 'learning_rate': 1.998531086507053e-05, 'epoch': 0.05}
Error with image file is truncated (16 bytes not processed)
{'loss': 1.0967, 'grad_norm': 0.469465970993042, 'learning_rate': 1.9984971328203945e-05, 'epoch': 0.05}
{'loss': 0.9895, 'grad_norm': 0.4181056320667267, 'learning_rate': 1.9984627914827698e-05, 'epoch': 0.05}
{'loss': 0.9222, 'grad_norm': 0.43991991877555847, 'learning_rate': 1.9984280625075115e-05, 'epoch': 0.05}
{'loss': 0.9688, 'grad_norm': 0.2634037733078003, 'learning_rate': 1.9983929459081022e-05, 'epoch': 0.05}
{'loss': 0.985, 'grad_norm': 0.38841453194618225, 'learning_rate': 1.998357441698176e-05, 'epoch': 0.05}
{'loss': 1.0968, 'grad_norm': 0.42417868971824646, 'learning_rate': 1.998321549891516e-05, 'epoch': 0.05}
{'loss': 1.0712, 'grad_norm': 0.4547957479953766, 'learning_rate': 1.9982852705020572e-05, 'epoch': 0.05}
{'loss': 0.9609, 'grad_norm': 0.3928126394748688, 'learning_rate': 1.9982486035438848e-05, 'epoch': 0.05}
{'loss': 0.9311, 'grad_norm': 0.23645517230033875, 'learning_rate': 1.9982115490312334e-05, 'epoch': 0.05}
{'loss': 1.1273, 'grad_norm': 0.4933551251888275, 'learning_rate': 1.9981741069784894e-05, 'epoch': 0.05}
{'loss': 1.1426, 'grad_norm': 0.44528505206108093, 'learning_rate': 1.9981362774001886e-05, 'epoch': 0.05}
{'loss': 0.9369, 'grad_norm': 0.24873505532741547, 'learning_rate': 1.9980980603110185e-05, 'epoch': 0.05}
{'loss': 0.9429, 'grad_norm': 0.24155782163143158, 'learning_rate': 1.9980594557258158e-05, 'epoch': 0.05}
{'loss': 0.8681, 'grad_norm': 0.22240129113197327, 'learning_rate': 1.9980204636595682e-05, 'epoch': 0.05}
{'loss': 0.9498, 'grad_norm': 0.40954291820526123, 'learning_rate': 1.9979810841274135e-05, 'epoch': 0.05}
{'loss': 1.0986, 'grad_norm': 0.4407125413417816, 'learning_rate': 1.9979413171446403e-05, 'epoch': 0.05}
{'loss': 1.0253, 'grad_norm': 0.39861437678337097, 'learning_rate': 1.9979011627266884e-05, 'epoch': 0.05}
{'loss': 0.9724, 'grad_norm': 0.41232454776763916, 'learning_rate': 1.997860620889146e-05, 'epoch': 0.05}
{'loss': 1.02, 'grad_norm': 0.4480290114879608, 'learning_rate': 1.997819691647753e-05, 'epoch': 0.05}
{'loss': 0.9683, 'grad_norm': 0.3770327568054199, 'learning_rate': 1.9977783750184e-05, 'epoch': 0.05}
{'loss': 1.104, 'grad_norm': 0.4195248782634735, 'learning_rate': 1.9977366710171274e-05, 'epoch': 0.05}
{'loss': 0.9782, 'grad_norm': 0.43625524640083313, 'learning_rate': 1.9976945796601258e-05, 'epoch': 0.05}
{'loss': 1.0545, 'grad_norm': 0.43093183636665344, 'learning_rate': 1.9976521009637366e-05, 'epoch': 0.05}
{'loss': 0.9712, 'grad_norm': 0.4449610114097595, 'learning_rate': 1.997609234944452e-05, 'epoch': 0.05}
{'loss': 0.9587, 'grad_norm': 0.4457460641860962, 'learning_rate': 1.9975659816189137e-05, 'epoch': 0.05}
{'loss': 1.0984, 'grad_norm': 0.45498502254486084, 'learning_rate': 1.997522341003914e-05, 'epoch': 0.05}
{'loss': 0.9212, 'grad_norm': 0.39372411370277405, 'learning_rate': 1.9974783131163957e-05, 'epoch': 0.05}
{'loss': 0.9234, 'grad_norm': 0.4343997538089752, 'learning_rate': 1.9974338979734523e-05, 'epoch': 0.05}
{'loss': 1.0397, 'grad_norm': 0.394474059343338, 'learning_rate': 1.997389095592327e-05, 'epoch': 0.05}
{'loss': 1.0509, 'grad_norm': 0.40972229838371277, 'learning_rate': 1.9973439059904133e-05, 'epoch': 0.05}
{'loss': 1.0196, 'grad_norm': 0.4684310257434845, 'learning_rate': 1.9972983291852565e-05, 'epoch': 0.05}
{'loss': 1.0072, 'grad_norm': 0.43023020029067993, 'learning_rate': 1.9972523651945496e-05, 'epoch': 0.05}
{'loss': 1.0679, 'grad_norm': 0.4325675666332245, 'learning_rate': 1.9972060140361384e-05, 'epoch': 0.05}
{'loss': 0.9656, 'grad_norm': 0.4358471632003784, 'learning_rate': 1.997159275728018e-05, 'epoch': 0.05}
{'loss': 0.9915, 'grad_norm': 0.4413163363933563, 'learning_rate': 1.9971121502883332e-05, 'epoch': 0.05}
{'loss': 1.0011, 'grad_norm': 0.407372385263443, 'learning_rate': 1.9970646377353802e-05, 'epoch': 0.05}
{'loss': 0.9222, 'grad_norm': 0.3633984327316284, 'learning_rate': 1.997016738087605e-05, 'epoch': 0.05}
{'loss': 1.0586, 'grad_norm': 0.4376826584339142, 'learning_rate': 1.9969684513636035e-05, 'epoch': 0.05}
{'loss': 1.0695, 'grad_norm': 0.452862411737442, 'learning_rate': 1.9969197775821227e-05, 'epoch': 0.05}
{'loss': 0.939, 'grad_norm': 0.402976930141449, 'learning_rate': 1.9968707167620593e-05, 'epoch': 0.05}
{'loss': 0.9474, 'grad_norm': 0.39795613288879395, 'learning_rate': 1.9968212689224603e-05, 'epoch': 0.05}
{'loss': 1.0644, 'grad_norm': 0.4334562420845032, 'learning_rate': 1.996771434082523e-05, 'epoch': 0.06}
{'loss': 0.9057, 'grad_norm': 0.28856050968170166, 'learning_rate': 1.9967212122615958e-05, 'epoch': 0.06}
{'loss': 1.1086, 'grad_norm': 0.4391399323940277, 'learning_rate': 1.9966706034791752e-05, 'epoch': 0.06}
{'loss': 0.9311, 'grad_norm': 0.25101959705352783, 'learning_rate': 1.9966196077549106e-05, 'epoch': 0.06}
{'loss': 0.9859, 'grad_norm': 0.4232160449028015, 'learning_rate': 1.996568225108599e-05, 'epoch': 0.06}
{'loss': 1.023, 'grad_norm': 0.42522934079170227, 'learning_rate': 1.99651645556019e-05, 'epoch': 0.06}
{'loss': 0.9797, 'grad_norm': 0.4335065484046936, 'learning_rate': 1.9964642991297817e-05, 'epoch': 0.06}
{'loss': 1.0656, 'grad_norm': 0.4017011821269989, 'learning_rate': 1.996411755837623e-05, 'epoch': 0.06}
{'loss': 1.0219, 'grad_norm': 0.4301506280899048, 'learning_rate': 1.9963588257041137e-05, 'epoch': 0.06}
{'loss': 0.9877, 'grad_norm': 0.4040554463863373, 'learning_rate': 1.996305508749802e-05, 'epoch': 0.06}
{'loss': 1.0179, 'grad_norm': 0.4608830511569977, 'learning_rate': 1.9962518049953887e-05, 'epoch': 0.06}
{'loss': 1.0921, 'grad_norm': 0.4718112349510193, 'learning_rate': 1.9961977144617225e-05, 'epoch': 0.06}
{'loss': 0.958, 'grad_norm': 0.39865776896476746, 'learning_rate': 1.996143237169803e-05, 'epoch': 0.06}
{'loss': 1.0812, 'grad_norm': 0.4377672076225281, 'learning_rate': 1.996088373140781e-05, 'epoch': 0.06}
{'loss': 0.9714, 'grad_norm': 0.37190550565719604, 'learning_rate': 1.9960331223959564e-05, 'epoch': 0.06}
{'loss': 1.033, 'grad_norm': 0.4387699067592621, 'learning_rate': 1.995977484956779e-05, 'epoch': 0.06}
{'loss': 1.0108, 'grad_norm': 0.4283488392829895, 'learning_rate': 1.9959214608448495e-05, 'epoch': 0.06}
{'loss': 1.1275, 'grad_norm': 0.5029289722442627, 'learning_rate': 1.9958650500819183e-05, 'epoch': 0.06}
{'loss': 0.9784, 'grad_norm': 0.3890276849269867, 'learning_rate': 1.995808252689886e-05, 'epoch': 0.06}
{'loss': 1.0378, 'grad_norm': 0.40745142102241516, 'learning_rate': 1.9957510686908034e-05, 'epoch': 0.06}
{'loss': 1.027, 'grad_norm': 0.4229041337966919, 'learning_rate': 1.9956934981068713e-05, 'epoch': 0.06}
{'loss': 0.9943, 'grad_norm': 0.42112115025520325, 'learning_rate': 1.9956355409604402e-05, 'epoch': 0.06}
{'loss': 0.9955, 'grad_norm': 0.3796493411064148, 'learning_rate': 1.9955771972740118e-05, 'epoch': 0.06}
{'loss': 1.0703, 'grad_norm': 0.4449256360530853, 'learning_rate': 1.9955184670702363e-05, 'epoch': 0.06}
{'loss': 1.0918, 'grad_norm': 0.4163622558116913, 'learning_rate': 1.995459350371915e-05, 'epoch': 0.06}
{'loss': 1.1344, 'grad_norm': 0.4637009799480438, 'learning_rate': 1.9953998472019996e-05, 'epoch': 0.06}
{'loss': 0.9051, 'grad_norm': 0.24360474944114685, 'learning_rate': 1.995339957583591e-05, 'epoch': 0.06}
{'loss': 0.9885, 'grad_norm': 0.41315072774887085, 'learning_rate': 1.9952796815399403e-05, 'epoch': 0.06}
{'loss': 0.9901, 'grad_norm': 0.43421006202697754, 'learning_rate': 1.9952190190944484e-05, 'epoch': 0.06}
{'loss': 1.0198, 'grad_norm': 0.4057796895503998, 'learning_rate': 1.9951579702706668e-05, 'epoch': 0.06}
{'loss': 0.9885, 'grad_norm': 0.4237634241580963, 'learning_rate': 1.9950965350922975e-05, 'epoch': 0.06}
{'loss': 0.8897, 'grad_norm': 0.4165235161781311, 'learning_rate': 1.9950347135831907e-05, 'epoch': 0.06}
{'loss': 0.9693, 'grad_norm': 0.37399032711982727, 'learning_rate': 1.994972505767348e-05, 'epoch': 0.06}
{'loss': 1.0923, 'grad_norm': 0.4318678081035614, 'learning_rate': 1.994909911668921e-05, 'epoch': 0.06}
{'loss': 1.024, 'grad_norm': 0.43755924701690674, 'learning_rate': 1.99484693131221e-05, 'epoch': 0.06}
{'loss': 1.0094, 'grad_norm': 0.45122140645980835, 'learning_rate': 1.994783564721667e-05, 'epoch': 0.06}
{'loss': 0.9405, 'grad_norm': 0.39369329810142517, 'learning_rate': 1.9947198119218924e-05, 'epoch': 0.06}
{'loss': 1.0479, 'grad_norm': 0.4436579644680023, 'learning_rate': 1.994655672937638e-05, 'epoch': 0.06}
{'loss': 0.9632, 'grad_norm': 0.2506677210330963, 'learning_rate': 1.9945911477938044e-05, 'epoch': 0.06}
{'loss': 0.9852, 'grad_norm': 0.33520519733428955, 'learning_rate': 1.994526236515442e-05, 'epoch': 0.06}
{'loss': 0.953, 'grad_norm': 0.41371259093284607, 'learning_rate': 1.994460939127753e-05, 'epoch': 0.06}
{'loss': 0.8779, 'grad_norm': 0.39271628856658936, 'learning_rate': 1.9943952556560863e-05, 'epoch': 0.06}
{'loss': 1.0873, 'grad_norm': 0.4389479160308838, 'learning_rate': 1.9943291861259433e-05, 'epoch': 0.06}
{'loss': 1.0031, 'grad_norm': 0.4365622401237488, 'learning_rate': 1.9942627305629747e-05, 'epoch': 0.06}
{'loss': 0.9971, 'grad_norm': 0.4757564663887024, 'learning_rate': 1.9941958889929808e-05, 'epoch': 0.06}
{'loss': 0.9103, 'grad_norm': 0.3967956006526947, 'learning_rate': 1.9941286614419113e-05, 'epoch': 0.06}
{'loss': 0.9768, 'grad_norm': 0.43443039059638977, 'learning_rate': 1.994061047935867e-05, 'epoch': 0.06}
{'loss': 1.0418, 'grad_norm': 0.46018481254577637, 'learning_rate': 1.9939930485010968e-05, 'epoch': 0.06}
{'loss': 0.9772, 'grad_norm': 0.4109247326850891, 'learning_rate': 1.9939246631640014e-05, 'epoch': 0.06}
{'loss': 0.932, 'grad_norm': 0.3721621036529541, 'learning_rate': 1.99385589195113e-05, 'epoch': 0.06}
{'loss': 1.1149, 'grad_norm': 0.4612407684326172, 'learning_rate': 1.9937867348891815e-05, 'epoch': 0.06}
{'loss': 0.9534, 'grad_norm': 0.24314796924591064, 'learning_rate': 1.9937171920050057e-05, 'epoch': 0.06}
{'loss': 0.9625, 'grad_norm': 0.40986743569374084, 'learning_rate': 1.9936472633256012e-05, 'epoch': 0.07}
{'loss': 0.976, 'grad_norm': 0.4476960003376007, 'learning_rate': 1.9935769488781167e-05, 'epoch': 0.07}
{'loss': 1.0497, 'grad_norm': 0.43212804198265076, 'learning_rate': 1.993506248689851e-05, 'epoch': 0.07}
{'loss': 1.1071, 'grad_norm': 0.46411415934562683, 'learning_rate': 1.993435162788252e-05, 'epoch': 0.07}
{'loss': 1.0435, 'grad_norm': 0.4396679699420929, 'learning_rate': 1.993363691200918e-05, 'epoch': 0.07}
{'loss': 1.1808, 'grad_norm': 0.5024749636650085, 'learning_rate': 1.9932918339555965e-05, 'epoch': 0.07}
{'loss': 1.0322, 'grad_norm': 0.4139571189880371, 'learning_rate': 1.9932195910801848e-05, 'epoch': 0.07}
{'loss': 1.0127, 'grad_norm': 0.4160222113132477, 'learning_rate': 1.9931469626027305e-05, 'epoch': 0.07}
{'loss': 1.0651, 'grad_norm': 0.4613135755062103, 'learning_rate': 1.9930739485514304e-05, 'epoch': 0.07}
{'loss': 0.9606, 'grad_norm': 0.4019380807876587, 'learning_rate': 1.9930005489546308e-05, 'epoch': 0.07}
{'loss': 0.9971, 'grad_norm': 0.3072254955768585, 'learning_rate': 1.9929267638408277e-05, 'epoch': 0.07}
{'loss': 0.9739, 'grad_norm': 0.28773558139801025, 'learning_rate': 1.9928525932386678e-05, 'epoch': 0.07}
{'loss': 0.9805, 'grad_norm': 0.41893377900123596, 'learning_rate': 1.9927780371769463e-05, 'epoch': 0.07}
{'loss': 0.9263, 'grad_norm': 0.44928261637687683, 'learning_rate': 1.9927030956846083e-05, 'epoch': 0.07}
{'loss': 1.0375, 'grad_norm': 0.4305599629878998, 'learning_rate': 1.992627768790749e-05, 'epoch': 0.07}
{'loss': 1.0806, 'grad_norm': 0.4585745334625244, 'learning_rate': 1.9925520565246125e-05, 'epoch': 0.07}
{'loss': 1.0142, 'grad_norm': 0.3895997405052185, 'learning_rate': 1.9924759589155932e-05, 'epoch': 0.07}
{'loss': 1.015, 'grad_norm': 0.4119608700275421, 'learning_rate': 1.9923994759932344e-05, 'epoch': 0.07}
{'loss': 1.0153, 'grad_norm': 0.4082050621509552, 'learning_rate': 1.9923226077872296e-05, 'epoch': 0.07}
{'loss': 1.1512, 'grad_norm': 0.4907427132129669, 'learning_rate': 1.9922453543274223e-05, 'epoch': 0.07}
{'loss': 1.0576, 'grad_norm': 0.43629956245422363, 'learning_rate': 1.9921677156438044e-05, 'epoch': 0.07}
{'loss': 1.1003, 'grad_norm': 0.4593692421913147, 'learning_rate': 1.9920896917665178e-05, 'epoch': 0.07}
{'loss': 1.0528, 'grad_norm': 0.43640145659446716, 'learning_rate': 1.992011282725854e-05, 'epoch': 0.07}
{'loss': 1.0018, 'grad_norm': 0.41759011149406433, 'learning_rate': 1.9919324885522548e-05, 'epoch': 0.07}
{'loss': 0.9282, 'grad_norm': 0.3947441279888153, 'learning_rate': 1.99185330927631e-05, 'epoch': 0.07}
{'loss': 1.0525, 'grad_norm': 0.44222375750541687, 'learning_rate': 1.99177374492876e-05, 'epoch': 0.07}
{'loss': 0.9729, 'grad_norm': 0.2568050026893616, 'learning_rate': 1.991693795540494e-05, 'epoch': 0.07}
{'loss': 1.0394, 'grad_norm': 0.4651893377304077, 'learning_rate': 1.9916134611425522e-05, 'epoch': 0.07}
{'loss': 0.9149, 'grad_norm': 0.45849505066871643, 'learning_rate': 1.9915327417661226e-05, 'epoch': 0.07}
{'loss': 0.9347, 'grad_norm': 0.2574819326400757, 'learning_rate': 1.991451637442543e-05, 'epoch': 0.07}
{'loss': 0.9325, 'grad_norm': 0.39986860752105713, 'learning_rate': 1.9913701482033008e-05, 'epoch': 0.07}
{'loss': 0.9927, 'grad_norm': 0.4234355390071869, 'learning_rate': 1.9912882740800336e-05, 'epoch': 0.07}
{'loss': 1.0211, 'grad_norm': 0.41919389367103577, 'learning_rate': 1.9912060151045273e-05, 'epoch': 0.07}
{'loss': 0.9438, 'grad_norm': 0.4042688012123108, 'learning_rate': 1.9911233713087172e-05, 'epoch': 0.07}
{'loss': 0.9678, 'grad_norm': 0.26527073979377747, 'learning_rate': 1.9910403427246895e-05, 'epoch': 0.07}
{'loss': 1.09, 'grad_norm': 0.4160754382610321, 'learning_rate': 1.990956929384678e-05, 'epoch': 0.07}
{'loss': 1.0799, 'grad_norm': 0.43557703495025635, 'learning_rate': 1.990873131321067e-05, 'epoch': 0.07}
{'loss': 1.0171, 'grad_norm': 0.4319925010204315, 'learning_rate': 1.9907889485663897e-05, 'epoch': 0.07}
{'loss': 1.0326, 'grad_norm': 0.42978763580322266, 'learning_rate': 1.9907043811533283e-05, 'epoch': 0.07}
{'loss': 0.9986, 'grad_norm': 0.4219287633895874, 'learning_rate': 1.9906194291147155e-05, 'epoch': 0.07}
{'loss': 0.9908, 'grad_norm': 0.402682363986969, 'learning_rate': 1.9905340924835322e-05, 'epoch': 0.07}
{'loss': 1.1275, 'grad_norm': 0.45586708188056946, 'learning_rate': 1.9904483712929094e-05, 'epoch': 0.07}
{'loss': 1.0619, 'grad_norm': 0.43149954080581665, 'learning_rate': 1.9903622655761267e-05, 'epoch': 0.07}
{'loss': 1.0202, 'grad_norm': 0.4422500431537628, 'learning_rate': 1.990275775366613e-05, 'epoch': 0.07}
{'loss': 1.0034, 'grad_norm': 0.4509417712688446, 'learning_rate': 1.9901889006979473e-05, 'epoch': 0.07}
{'loss': 1.0481, 'grad_norm': 0.4255293607711792, 'learning_rate': 1.990101641603857e-05, 'epoch': 0.07}
{'loss': 0.8918, 'grad_norm': 0.2511894106864929, 'learning_rate': 1.9900139981182193e-05, 'epoch': 0.07}
{'loss': 1.0209, 'grad_norm': 0.443536639213562, 'learning_rate': 1.9899259702750604e-05, 'epoch': 0.07}
{'loss': 1.0257, 'grad_norm': 0.4495393931865692, 'learning_rate': 1.9898375581085555e-05, 'epoch': 0.07}
{'loss': 1.0913, 'grad_norm': 0.4486071765422821, 'learning_rate': 1.9897487616530296e-05, 'epoch': 0.07}
{'loss': 1.0453, 'grad_norm': 0.43129152059555054, 'learning_rate': 1.9896595809429565e-05, 'epoch': 0.07}
{'loss': 1.0771, 'grad_norm': 0.4219355285167694, 'learning_rate': 1.9895700160129593e-05, 'epoch': 0.07}
{'loss': 0.9157, 'grad_norm': 0.417770653963089, 'learning_rate': 1.9894800668978095e-05, 'epoch': 0.08}
{'loss': 0.9713, 'grad_norm': 0.3904341161251068, 'learning_rate': 1.9893897336324292e-05, 'epoch': 0.08}
{'loss': 1.0578, 'grad_norm': 0.5058534145355225, 'learning_rate': 1.9892990162518884e-05, 'epoch': 0.08}
{'loss': 0.9854, 'grad_norm': 0.42553046345710754, 'learning_rate': 1.9892079147914072e-05, 'epoch': 0.08}
{'loss': 1.0412, 'grad_norm': 0.4143429696559906, 'learning_rate': 1.9891164292863537e-05, 'epoch': 0.08}
{'loss': 1.0434, 'grad_norm': 0.3820996880531311, 'learning_rate': 1.9890245597722465e-05, 'epoch': 0.08}
{'loss': 1.048, 'grad_norm': 0.43854233622550964, 'learning_rate': 1.9889323062847516e-05, 'epoch': 0.08}
{'loss': 0.9554, 'grad_norm': 0.44024857878685, 'learning_rate': 1.988839668859686e-05, 'epoch': 0.08}
{'loss': 1.1369, 'grad_norm': 0.4165029525756836, 'learning_rate': 1.988746647533014e-05, 'epoch': 0.08}
{'loss': 1.0662, 'grad_norm': 0.40674853324890137, 'learning_rate': 1.9886532423408495e-05, 'epoch': 0.08}
{'loss': 1.0568, 'grad_norm': 0.46808409690856934, 'learning_rate': 1.9885594533194564e-05, 'epoch': 0.08}
{'loss': 1.0183, 'grad_norm': 0.3935006558895111, 'learning_rate': 1.9884652805052465e-05, 'epoch': 0.08}
{'loss': 1.0557, 'grad_norm': 0.4619766175746918, 'learning_rate': 1.9883707239347804e-05, 'epoch': 0.08}
{'loss': 1.0484, 'grad_norm': 0.4494358003139496, 'learning_rate': 1.988275783644769e-05, 'epoch': 0.08}
{'loss': 1.0542, 'grad_norm': 0.4185766279697418, 'learning_rate': 1.988180459672071e-05, 'epoch': 0.08}
{'loss': 0.9297, 'grad_norm': 0.4048956632614136, 'learning_rate': 1.988084752053695e-05, 'epoch': 0.08}
{'loss': 1.0339, 'grad_norm': 0.43510109186172485, 'learning_rate': 1.9879886608267967e-05, 'epoch': 0.08}
{'loss': 1.0103, 'grad_norm': 0.4191170930862427, 'learning_rate': 1.9878921860286832e-05, 'epoch': 0.08}
{'loss': 0.9275, 'grad_norm': 0.4100727438926697, 'learning_rate': 1.9877953276968088e-05, 'epoch': 0.08}
{'loss': 0.9813, 'grad_norm': 0.38571637868881226, 'learning_rate': 1.9876980858687777e-05, 'epoch': 0.08}
{'loss': 1.048, 'grad_norm': 0.4638240933418274, 'learning_rate': 1.9876004605823417e-05, 'epoch': 0.08}
{'loss': 1.0472, 'grad_norm': 0.41585421562194824, 'learning_rate': 1.987502451875403e-05, 'epoch': 0.08}
{'loss': 0.9328, 'grad_norm': 0.39592838287353516, 'learning_rate': 1.987404059786012e-05, 'epoch': 0.08}
{'loss': 1.0254, 'grad_norm': 0.40682199597358704, 'learning_rate': 1.9873052843523676e-05, 'epoch': 0.08}
{'loss': 1.1187, 'grad_norm': 0.5014071464538574, 'learning_rate': 1.987206125612818e-05, 'epoch': 0.08}
{'loss': 1.0438, 'grad_norm': 0.4793561100959778, 'learning_rate': 1.98710658360586e-05, 'epoch': 0.08}
{'loss': 1.0833, 'grad_norm': 0.45068299770355225, 'learning_rate': 1.987006658370139e-05, 'epoch': 0.08}
{'loss': 1.0398, 'grad_norm': 0.44381460547447205, 'learning_rate': 1.9869063499444495e-05, 'epoch': 0.08}
{'loss': 0.9645, 'grad_norm': 0.40660491585731506, 'learning_rate': 1.9868056583677346e-05, 'epoch': 0.08}
{'loss': 0.9267, 'grad_norm': 0.2646402418613434, 'learning_rate': 1.9867045836790867e-05, 'epoch': 0.08}
{'loss': 1.0538, 'grad_norm': 0.46576035022735596, 'learning_rate': 1.9866031259177463e-05, 'epoch': 0.08}
{'loss': 1.0865, 'grad_norm': 0.5012021660804749, 'learning_rate': 1.9865012851231022e-05, 'epoch': 0.08}
{'loss': 1.0004, 'grad_norm': 0.43833857774734497, 'learning_rate': 1.9863990613346936e-05, 'epoch': 0.08}
{'loss': 1.0094, 'grad_norm': 0.4333195090293884, 'learning_rate': 1.986296454592206e-05, 'epoch': 0.08}
{'loss': 0.9899, 'grad_norm': 0.4278292655944824, 'learning_rate': 1.9861934649354763e-05, 'epoch': 0.08}
{'loss': 0.9465, 'grad_norm': 0.25351083278656006, 'learning_rate': 1.9860900924044873e-05, 'epoch': 0.08}
{'loss': 0.9255, 'grad_norm': 0.4178617596626282, 'learning_rate': 1.9859863370393726e-05, 'epoch': 0.08}
{'loss': 1.0363, 'grad_norm': 0.41905197501182556, 'learning_rate': 1.9858821988804132e-05, 'epoch': 0.08}
{'loss': 1.0246, 'grad_norm': 0.430618017911911, 'learning_rate': 1.9857776779680393e-05, 'epoch': 0.08}
{'loss': 1.0419, 'grad_norm': 0.3915369510650635, 'learning_rate': 1.98567277434283e-05, 'epoch': 0.08}
{'loss': 1.0199, 'grad_norm': 0.40543875098228455, 'learning_rate': 1.9855674880455115e-05, 'epoch': 0.08}
{'loss': 1.0062, 'grad_norm': 0.4402800500392914, 'learning_rate': 1.98546181911696e-05, 'epoch': 0.08}
{'loss': 1.0117, 'grad_norm': 0.4262188971042633, 'learning_rate': 1.9853557675982e-05, 'epoch': 0.08}
{'loss': 1.0567, 'grad_norm': 0.4270086884498596, 'learning_rate': 1.985249333530404e-05, 'epoch': 0.08}
{'loss': 0.9612, 'grad_norm': 0.404145210981369, 'learning_rate': 1.9851425169548938e-05, 'epoch': 0.08}
{'loss': 1.022, 'grad_norm': 0.41343599557876587, 'learning_rate': 1.9850353179131392e-05, 'epoch': 0.08}
{'loss': 1.0174, 'grad_norm': 0.28669437766075134, 'learning_rate': 1.9849277364467585e-05, 'epoch': 0.08}
{'loss': 0.9582, 'grad_norm': 0.43518996238708496, 'learning_rate': 1.984819772597518e-05, 'epoch': 0.08}
{'loss': 1.1075, 'grad_norm': 0.4382844567298889, 'learning_rate': 1.9847114264073336e-05, 'epoch': 0.08}
{'loss': 1.0714, 'grad_norm': 0.512546956539154, 'learning_rate': 1.984602697918269e-05, 'epoch': 0.08}
{'loss': 1.0657, 'grad_norm': 0.41605523228645325, 'learning_rate': 1.9844935871725363e-05, 'epoch': 0.08}
{'loss': 0.972, 'grad_norm': 0.40158283710479736, 'learning_rate': 1.9843840942124956e-05, 'epoch': 0.08}
{'loss': 1.0614, 'grad_norm': 0.41399839520454407, 'learning_rate': 1.9842742190806566e-05, 'epoch': 0.09}
{'loss': 0.9954, 'grad_norm': 0.3942837417125702, 'learning_rate': 1.984163961819676e-05, 'epoch': 0.09}
{'loss': 0.9906, 'grad_norm': 0.42447996139526367, 'learning_rate': 1.9840533224723595e-05, 'epoch': 0.09}
{'loss': 0.9174, 'grad_norm': 0.41388821601867676, 'learning_rate': 1.9839423010816616e-05, 'epoch': 0.09}
{'loss': 1.0539, 'grad_norm': 0.4254336655139923, 'learning_rate': 1.983830897690684e-05, 'epoch': 0.09}
{'loss': 1.1086, 'grad_norm': 0.4993647336959839, 'learning_rate': 1.9837191123426777e-05, 'epoch': 0.09}
{'loss': 1.0985, 'grad_norm': 0.4596238434314728, 'learning_rate': 1.983606945081042e-05, 'epoch': 0.09}
{'loss': 1.1145, 'grad_norm': 0.45379334688186646, 'learning_rate': 1.983494395949323e-05, 'epoch': 0.09}
{'loss': 1.0471, 'grad_norm': 0.4299722909927368, 'learning_rate': 1.983381464991217e-05, 'epoch': 0.09}
{'loss': 0.9993, 'grad_norm': 0.41354435682296753, 'learning_rate': 1.9832681522505676e-05, 'epoch': 0.09}
{'loss': 1.0001, 'grad_norm': 0.4015634059906006, 'learning_rate': 1.9831544577713663e-05, 'epoch': 0.09}
{'loss': 1.0815, 'grad_norm': 0.4656842052936554, 'learning_rate': 1.983040381597754e-05, 'epoch': 0.09}
{'loss': 0.9432, 'grad_norm': 0.24697180092334747, 'learning_rate': 1.982925923774018e-05, 'epoch': 0.09}
{'loss': 0.999, 'grad_norm': 0.24914155900478363, 'learning_rate': 1.9828110843445954e-05, 'epoch': 0.09}
{'loss': 1.0495, 'grad_norm': 0.4357411861419678, 'learning_rate': 1.982695863354071e-05, 'epoch': 0.09}
{'loss': 0.9543, 'grad_norm': 0.42860904335975647, 'learning_rate': 1.9825802608471767e-05, 'epoch': 0.09}
{'loss': 0.9874, 'grad_norm': 0.45560383796691895, 'learning_rate': 1.982464276868794e-05, 'epoch': 0.09}
{'loss': 1.067, 'grad_norm': 0.43515387177467346, 'learning_rate': 1.982347911463952e-05, 'epoch': 0.09}
{'loss': 1.0832, 'grad_norm': 0.4085972309112549, 'learning_rate': 1.9822311646778277e-05, 'epoch': 0.09}
{'loss': 0.9699, 'grad_norm': 0.44780272245407104, 'learning_rate': 1.982114036555746e-05, 'epoch': 0.09}
{'loss': 1.0972, 'grad_norm': 0.475340336561203, 'learning_rate': 1.9819965271431797e-05, 'epoch': 0.09}
{'loss': 1.0517, 'grad_norm': 0.47110190987586975, 'learning_rate': 1.9818786364857506e-05, 'epoch': 0.09}
{'loss': 0.9763, 'grad_norm': 0.3922779858112335, 'learning_rate': 1.9817603646292278e-05, 'epoch': 0.09}
{'loss': 1.0894, 'grad_norm': 0.45736292004585266, 'learning_rate': 1.9816417116195287e-05, 'epoch': 0.09}
{'loss': 0.8934, 'grad_norm': 0.22726944088935852, 'learning_rate': 1.9815226775027182e-05, 'epoch': 0.09}
{'loss': 1.007, 'grad_norm': 0.42491015791893005, 'learning_rate': 1.9814032623250093e-05, 'epoch': 0.09}
{'loss': 0.9798, 'grad_norm': 0.43165963888168335, 'learning_rate': 1.9812834661327632e-05, 'epoch': 0.09}
{'loss': 0.9514, 'grad_norm': 0.4009203016757965, 'learning_rate': 1.9811632889724888e-05, 'epoch': 0.09}
{'loss': 1.0682, 'grad_norm': 0.43691837787628174, 'learning_rate': 1.9810427308908437e-05, 'epoch': 0.09}
{'loss': 1.0941, 'grad_norm': 0.51070636510849, 'learning_rate': 1.9809217919346318e-05, 'epoch': 0.09}
{'loss': 1.1032, 'grad_norm': 0.4725519120693207, 'learning_rate': 1.980800472150806e-05, 'epoch': 0.09}
{'loss': 0.987, 'grad_norm': 0.410941481590271, 'learning_rate': 1.9806787715864674e-05, 'epoch': 0.09}
{'loss': 1.128, 'grad_norm': 0.44887882471084595, 'learning_rate': 1.9805566902888637e-05, 'epoch': 0.09}
{'loss': 0.9852, 'grad_norm': 0.41715699434280396, 'learning_rate': 1.9804342283053916e-05, 'epoch': 0.09}
{'loss': 1.033, 'grad_norm': 0.4185080826282501, 'learning_rate': 1.980311385683594e-05, 'epoch': 0.09}
{'loss': 0.953, 'grad_norm': 0.42760229110717773, 'learning_rate': 1.980188162471164e-05, 'epoch': 0.09}
{'loss': 1.0013, 'grad_norm': 0.4281085133552551, 'learning_rate': 1.98006455871594e-05, 'epoch': 0.09}
{'loss': 1.0665, 'grad_norm': 0.44571778178215027, 'learning_rate': 1.97994057446591e-05, 'epoch': 0.09}
{'loss': 1.0223, 'grad_norm': 0.460951566696167, 'learning_rate': 1.979816209769209e-05, 'epoch': 0.09}
{'loss': 1.0687, 'grad_norm': 0.45202335715293884, 'learning_rate': 1.9796914646741187e-05, 'epoch': 0.09}
{'loss': 0.9324, 'grad_norm': 0.257598340511322, 'learning_rate': 1.9795663392290702e-05, 'epoch': 0.09}
{'loss': 0.996, 'grad_norm': 0.442674458026886, 'learning_rate': 1.9794408334826415e-05, 'epoch': 0.09}
{'loss': 1.1036, 'grad_norm': 0.4495345950126648, 'learning_rate': 1.979314947483558e-05, 'epoch': 0.09}
{'loss': 0.9495, 'grad_norm': 0.4347599148750305, 'learning_rate': 1.9791886812806932e-05, 'epoch': 0.09}
{'loss': 0.9182, 'grad_norm': 0.24093030393123627, 'learning_rate': 1.9790620349230676e-05, 'epoch': 0.09}
{'loss': 1.0193, 'grad_norm': 0.44126686453819275, 'learning_rate': 1.9789350084598504e-05, 'epoch': 0.09}
{'loss': 1.002, 'grad_norm': 0.43242955207824707, 'learning_rate': 1.9788076019403565e-05, 'epoch': 0.09}
{'loss': 1.0639, 'grad_norm': 0.4218723475933075, 'learning_rate': 1.9786798154140507e-05, 'epoch': 0.09}
{'loss': 1.009, 'grad_norm': 0.4546991288661957, 'learning_rate': 1.9785516489305437e-05, 'epoch': 0.09}
{'loss': 1.1237, 'grad_norm': 0.4265231490135193, 'learning_rate': 1.9784231025395936e-05, 'epoch': 0.09}
{'loss': 0.9852, 'grad_norm': 0.4025968909263611, 'learning_rate': 1.9782941762911075e-05, 'epoch': 0.09}
{'loss': 1.1328, 'grad_norm': 0.4320894181728363, 'learning_rate': 1.9781648702351383e-05, 'epoch': 0.09}
{'loss': 1.1086, 'grad_norm': 0.47082585096359253, 'learning_rate': 1.9780351844218874e-05, 'epoch': 0.1}
{'loss': 1.078, 'grad_norm': 0.4244362413883209, 'learning_rate': 1.977905118901703e-05, 'epoch': 0.1}
{'loss': 1.0274, 'grad_norm': 0.42322179675102234, 'learning_rate': 1.977774673725081e-05, 'epoch': 0.1}
{'loss': 0.982, 'grad_norm': 0.4288889169692993, 'learning_rate': 1.977643848942665e-05, 'epoch': 0.1}
{'loss': 0.9876, 'grad_norm': 0.290853887796402, 'learning_rate': 1.977512644605246e-05, 'epoch': 0.1}
{'loss': 1.1075, 'grad_norm': 0.4484245181083679, 'learning_rate': 1.9773810607637612e-05, 'epoch': 0.1}
{'loss': 0.9972, 'grad_norm': 0.421585351228714, 'learning_rate': 1.9772490974692962e-05, 'epoch': 0.1}
{'loss': 0.9888, 'grad_norm': 0.39183902740478516, 'learning_rate': 1.9771167547730844e-05, 'epoch': 0.1}
{'loss': 0.9642, 'grad_norm': 0.42570871114730835, 'learning_rate': 1.976984032726505e-05, 'epoch': 0.1}
{'loss': 1.0139, 'grad_norm': 0.411422461271286, 'learning_rate': 1.976850931381086e-05, 'epoch': 0.1}
{'loss': 1.0619, 'grad_norm': 0.4163241684436798, 'learning_rate': 1.976717450788501e-05, 'epoch': 0.1}
{'loss': 0.9626, 'grad_norm': 0.4096946716308594, 'learning_rate': 1.9765835910005726e-05, 'epoch': 0.1}
{'loss': 1.0124, 'grad_norm': 0.45689672231674194, 'learning_rate': 1.9764493520692685e-05, 'epoch': 0.1}
{'loss': 0.9954, 'grad_norm': 0.3941703736782074, 'learning_rate': 1.9763147340467067e-05, 'epoch': 0.1}
{'loss': 0.9499, 'grad_norm': 0.40238216519355774, 'learning_rate': 1.9761797369851498e-05, 'epoch': 0.1}
{'loss': 0.9582, 'grad_norm': 0.2515791058540344, 'learning_rate': 1.9760443609370074e-05, 'epoch': 0.1}
{'loss': 1.0304, 'grad_norm': 0.4341410994529724, 'learning_rate': 1.975908605954838e-05, 'epoch': 0.1}
{'loss': 1.0549, 'grad_norm': 0.4732714295387268, 'learning_rate': 1.9757724720913466e-05, 'epoch': 0.1}
{'loss': 1.0721, 'grad_norm': 0.4233410954475403, 'learning_rate': 1.9756359593993845e-05, 'epoch': 0.1}
{'loss': 0.9308, 'grad_norm': 0.4034748077392578, 'learning_rate': 1.975499067931951e-05, 'epoch': 0.1}
{'loss': 0.9094, 'grad_norm': 0.4441170394420624, 'learning_rate': 1.975361797742192e-05, 'epoch': 0.1}
{'loss': 1.1042, 'grad_norm': 0.46546363830566406, 'learning_rate': 1.9752241488834002e-05, 'epoch': 0.1}
{'loss': 1.0218, 'grad_norm': 0.41897523403167725, 'learning_rate': 1.975086121409016e-05, 'epoch': 0.1}
{'loss': 1.0836, 'grad_norm': 0.5083447694778442, 'learning_rate': 1.974947715372626e-05, 'epoch': 0.1}
{'loss': 1.0958, 'grad_norm': 0.47642016410827637, 'learning_rate': 1.974808930827965e-05, 'epoch': 0.1}
{'loss': 1.0786, 'grad_norm': 0.44804292917251587, 'learning_rate': 1.9746697678289128e-05, 'epoch': 0.1}
{'loss': 1.0424, 'grad_norm': 0.46047645807266235, 'learning_rate': 1.9745302264294982e-05, 'epoch': 0.1}
{'loss': 0.9824, 'grad_norm': 0.40863236784935, 'learning_rate': 1.9743903066838954e-05, 'epoch': 0.1}
{'loss': 0.9909, 'grad_norm': 0.4413621723651886, 'learning_rate': 1.9742500086464266e-05, 'epoch': 0.1}
{'loss': 1.0659, 'grad_norm': 0.44054561853408813, 'learning_rate': 1.9741093323715597e-05, 'epoch': 0.1}
{'loss': 0.9801, 'grad_norm': 0.4170873761177063, 'learning_rate': 1.9739682779139107e-05, 'epoch': 0.1}
{'loss': 0.9501, 'grad_norm': 0.4039187729358673, 'learning_rate': 1.9738268453282414e-05, 'epoch': 0.1}
{'loss': 1.0687, 'grad_norm': 0.42472344636917114, 'learning_rate': 1.9736850346694608e-05, 'epoch': 0.1}
{'loss': 1.0845, 'grad_norm': 0.4793073832988739, 'learning_rate': 1.973542845992625e-05, 'epoch': 0.1}
{'loss': 0.9776, 'grad_norm': 0.39674443006515503, 'learning_rate': 1.9734002793529362e-05, 'epoch': 0.1}
{'loss': 1.0059, 'grad_norm': 0.44246116280555725, 'learning_rate': 1.9732573348057437e-05, 'epoch': 0.1}
{'loss': 1.0222, 'grad_norm': 0.4024757742881775, 'learning_rate': 1.973114012406544e-05, 'epoch': 0.1}
{'loss': 1.0071, 'grad_norm': 0.4268690347671509, 'learning_rate': 1.9729703122109788e-05, 'epoch': 0.1}
{'loss': 1.0305, 'grad_norm': 0.4096287488937378, 'learning_rate': 1.9728262342748384e-05, 'epoch': 0.1}
{'loss': 0.9557, 'grad_norm': 0.42898085713386536, 'learning_rate': 1.9726817786540584e-05, 'epoch': 0.1}
{'loss': 1.0675, 'grad_norm': 0.5114479660987854, 'learning_rate': 1.9725369454047215e-05, 'epoch': 0.1}
{'loss': 0.9779, 'grad_norm': 0.40684717893600464, 'learning_rate': 1.9723917345830568e-05, 'epoch': 0.1}
{'loss': 1.0849, 'grad_norm': 0.4781489372253418, 'learning_rate': 1.9722461462454405e-05, 'epoch': 0.1}
{'loss': 1.0465, 'grad_norm': 0.41846001148223877, 'learning_rate': 1.9721001804483947e-05, 'epoch': 0.1}
{'loss': 0.9833, 'grad_norm': 0.4106734097003937, 'learning_rate': 1.9719538372485887e-05, 'epoch': 0.1}
{'loss': 1.0386, 'grad_norm': 0.47748425602912903, 'learning_rate': 1.9718071167028376e-05, 'epoch': 0.1}
{'loss': 1.072, 'grad_norm': 0.465389609336853, 'learning_rate': 1.9716600188681038e-05, 'epoch': 0.1}
{'loss': 1.0546, 'grad_norm': 0.4706188142299652, 'learning_rate': 1.971512543801495e-05, 'epoch': 0.1}
{'loss': 1.0584, 'grad_norm': 0.4243098497390747, 'learning_rate': 1.9713646915602663e-05, 'epoch': 0.1}
{'loss': 1.1771, 'grad_norm': 0.44306182861328125, 'learning_rate': 1.9712164622018197e-05, 'epoch': 0.1}
{'loss': 1.0549, 'grad_norm': 0.43091872334480286, 'learning_rate': 1.9710678557837024e-05, 'epoch': 0.1}
{'loss': 0.9972, 'grad_norm': 0.416849285364151, 'learning_rate': 1.9709188723636088e-05, 'epoch': 0.1}
{'loss': 0.9839, 'grad_norm': 0.42687085270881653, 'learning_rate': 1.970769511999379e-05, 'epoch': 0.11}
{'loss': 1.0396, 'grad_norm': 0.42405322194099426, 'learning_rate': 1.9706197747490004e-05, 'epoch': 0.11}
{'loss': 1.0002, 'grad_norm': 0.43546807765960693, 'learning_rate': 1.9704696606706055e-05, 'epoch': 0.11}
Error with image file is truncated (35 bytes not processed)
{'loss': 0.9927, 'grad_norm': 0.4184710383415222, 'learning_rate': 1.9703191698224742e-05, 'epoch': 0.11}
{'loss': 1.0049, 'grad_norm': 0.4049273431301117, 'learning_rate': 1.9701683022630323e-05, 'epoch': 0.11}
{'loss': 0.9895, 'grad_norm': 0.3800562620162964, 'learning_rate': 1.9700170580508514e-05, 'epoch': 0.11}
{'loss': 1.1292, 'grad_norm': 0.44815510511398315, 'learning_rate': 1.9698654372446495e-05, 'epoch': 0.11}
{'loss': 1.0236, 'grad_norm': 0.42317327857017517, 'learning_rate': 1.969713439903292e-05, 'epoch': 0.11}
{'loss': 1.0266, 'grad_norm': 0.42824625968933105, 'learning_rate': 1.9695610660857886e-05, 'epoch': 0.11}
{'loss': 1.023, 'grad_norm': 0.40507811307907104, 'learning_rate': 1.9694083158512965e-05, 'epoch': 0.11}
{'loss': 1.0703, 'grad_norm': 0.4259089231491089, 'learning_rate': 1.9692551892591185e-05, 'epoch': 0.11}
{'loss': 1.0802, 'grad_norm': 0.5011857151985168, 'learning_rate': 1.9691016863687037e-05, 'epoch': 0.11}
{'loss': 1.0457, 'grad_norm': 0.43156737089157104, 'learning_rate': 1.968947807239647e-05, 'epoch': 0.11}
{'loss': 0.9237, 'grad_norm': 0.26308977603912354, 'learning_rate': 1.9687935519316897e-05, 'epoch': 0.11}
{'loss': 1.1751, 'grad_norm': 0.4411312937736511, 'learning_rate': 1.9686389205047186e-05, 'epoch': 0.11}
{'loss': 1.0754, 'grad_norm': 0.47681012749671936, 'learning_rate': 1.9684839130187678e-05, 'epoch': 0.11}
{'loss': 0.9619, 'grad_norm': 0.44267991185188293, 'learning_rate': 1.968328529534016e-05, 'epoch': 0.11}
{'loss': 1.1312, 'grad_norm': 0.5017073154449463, 'learning_rate': 1.9681727701107885e-05, 'epoch': 0.11}
{'loss': 1.0062, 'grad_norm': 0.4279087781906128, 'learning_rate': 1.9680166348095568e-05, 'epoch': 0.11}
{'loss': 1.0478, 'grad_norm': 0.40745824575424194, 'learning_rate': 1.967860123690937e-05, 'epoch': 0.11}
{'loss': 0.9764, 'grad_norm': 0.3953886330127716, 'learning_rate': 1.9677032368156934e-05, 'epoch': 0.11}
{'loss': 1.0491, 'grad_norm': 0.42261558771133423, 'learning_rate': 1.967545974244734e-05, 'epoch': 0.11}
{'loss': 0.8807, 'grad_norm': 0.4006943106651306, 'learning_rate': 1.9673883360391138e-05, 'epoch': 0.11}
{'loss': 1.0797, 'grad_norm': 0.4194743037223816, 'learning_rate': 1.9672303222600333e-05, 'epoch': 0.11}
{'loss': 0.8832, 'grad_norm': 0.24573469161987305, 'learning_rate': 1.967071932968839e-05, 'epoch': 0.11}
{'loss': 1.0121, 'grad_norm': 0.41321030259132385, 'learning_rate': 1.9669131682270232e-05, 'epoch': 0.11}
{'loss': 0.9128, 'grad_norm': 0.40845632553100586, 'learning_rate': 1.9667540280962235e-05, 'epoch': 0.11}
{'loss': 0.9917, 'grad_norm': 0.4351023733615875, 'learning_rate': 1.966594512638224e-05, 'epoch': 0.11}
{'loss': 1.07, 'grad_norm': 0.4542575776576996, 'learning_rate': 1.9664346219149538e-05, 'epoch': 0.11}
{'loss': 1.0105, 'grad_norm': 0.3950604200363159, 'learning_rate': 1.966274355988488e-05, 'epoch': 0.11}
{'loss': 1.0199, 'grad_norm': 0.4450990855693817, 'learning_rate': 1.9661137149210473e-05, 'epoch': 0.11}
{'loss': 1.1263, 'grad_norm': 0.43313220143318176, 'learning_rate': 1.9659526987749987e-05, 'epoch': 0.11}
{'loss': 0.9746, 'grad_norm': 0.43410179018974304, 'learning_rate': 1.9657913076128532e-05, 'epoch': 0.11}
{'loss': 1.0232, 'grad_norm': 0.4647635817527771, 'learning_rate': 1.965629541497269e-05, 'epoch': 0.11}
{'loss': 1.0923, 'grad_norm': 0.4567219614982605, 'learning_rate': 1.9654674004910493e-05, 'epoch': 0.11}
{'loss': 0.9786, 'grad_norm': 0.4270954728126526, 'learning_rate': 1.9653048846571427e-05, 'epoch': 0.11}
{'loss': 1.1734, 'grad_norm': 0.4570803642272949, 'learning_rate': 1.9651419940586437e-05, 'epoch': 0.11}
{'loss': 0.9826, 'grad_norm': 0.445068359375, 'learning_rate': 1.964978728758791e-05, 'epoch': 0.11}
{'loss': 1.0288, 'grad_norm': 0.42847567796707153, 'learning_rate': 1.9648150888209715e-05, 'epoch': 0.11}
{'loss': 1.0869, 'grad_norm': 0.44932687282562256, 'learning_rate': 1.9646510743087144e-05, 'epoch': 0.11}
{'loss': 0.9343, 'grad_norm': 0.42078620195388794, 'learning_rate': 1.964486685285697e-05, 'epoch': 0.11}
{'loss': 1.0787, 'grad_norm': 0.4258333742618561, 'learning_rate': 1.9643219218157395e-05, 'epoch': 0.11}
{'loss': 0.9353, 'grad_norm': 0.3902912735939026, 'learning_rate': 1.9641567839628092e-05, 'epoch': 0.11}
{'loss': 0.9338, 'grad_norm': 0.41378653049468994, 'learning_rate': 1.963991271791019e-05, 'epoch': 0.11}
{'loss': 0.9652, 'grad_norm': 0.40585237741470337, 'learning_rate': 1.9638253853646255e-05, 'epoch': 0.11}
{'loss': 1.1915, 'grad_norm': 0.49474555253982544, 'learning_rate': 1.9636591247480323e-05, 'epoch': 0.11}
{'loss': 0.9626, 'grad_norm': 0.4055960476398468, 'learning_rate': 1.9634924900057867e-05, 'epoch': 0.11}
{'loss': 0.9221, 'grad_norm': 0.3932996094226837, 'learning_rate': 1.963325481202583e-05, 'epoch': 0.11}
{'loss': 0.976, 'grad_norm': 0.42017266154289246, 'learning_rate': 1.963158098403259e-05, 'epoch': 0.11}
{'loss': 0.8999, 'grad_norm': 0.24022594094276428, 'learning_rate': 1.9629903416727987e-05, 'epoch': 0.11}
{'loss': 0.9749, 'grad_norm': 0.4100283980369568, 'learning_rate': 1.962822211076331e-05, 'epoch': 0.11}
{'loss': 0.9766, 'grad_norm': 0.44590824842453003, 'learning_rate': 1.96265370667913e-05, 'epoch': 0.11}
{'loss': 1.0107, 'grad_norm': 0.40991735458374023, 'learning_rate': 1.9624848285466146e-05, 'epoch': 0.12}
{'loss': 1.0393, 'grad_norm': 0.40981096029281616, 'learning_rate': 1.9623155767443498e-05, 'epoch': 0.12}
{'loss': 1.0236, 'grad_norm': 0.40559864044189453, 'learning_rate': 1.9621459513380445e-05, 'epoch': 0.12}
{'loss': 0.9191, 'grad_norm': 0.45821672677993774, 'learning_rate': 1.9619759523935532e-05, 'epoch': 0.12}
{'loss': 1.0534, 'grad_norm': 0.4353579878807068, 'learning_rate': 1.9618055799768757e-05, 'epoch': 0.12}
{'loss': 1.0686, 'grad_norm': 0.3187796473503113, 'learning_rate': 1.961634834154156e-05, 'epoch': 0.12}
{'loss': 1.0728, 'grad_norm': 0.445943146944046, 'learning_rate': 1.9614637149916834e-05, 'epoch': 0.12}
{'loss': 1.0146, 'grad_norm': 0.500149130821228, 'learning_rate': 1.9612922225558924e-05, 'epoch': 0.12}
{'loss': 1.1972, 'grad_norm': 0.49847638607025146, 'learning_rate': 1.961120356913363e-05, 'epoch': 0.12}
{'loss': 0.9842, 'grad_norm': 0.41864070296287537, 'learning_rate': 1.960948118130818e-05, 'epoch': 0.12}
{'loss': 1.059, 'grad_norm': 0.4322701394557953, 'learning_rate': 1.9607755062751273e-05, 'epoch': 0.12}
{'loss': 1.0251, 'grad_norm': 0.40141457319259644, 'learning_rate': 1.9606025214133046e-05, 'epoch': 0.12}
{'loss': 1.0209, 'grad_norm': 0.4319341778755188, 'learning_rate': 1.9604291636125084e-05, 'epoch': 0.12}
{'loss': 1.0675, 'grad_norm': 0.5110517144203186, 'learning_rate': 1.960255432940043e-05, 'epoch': 0.12}
{'loss': 0.9823, 'grad_norm': 0.27854156494140625, 'learning_rate': 1.9600813294633552e-05, 'epoch': 0.12}
{'loss': 0.961, 'grad_norm': 0.3983768820762634, 'learning_rate': 1.9599068532500394e-05, 'epoch': 0.12}
{'loss': 1.068, 'grad_norm': 0.4504496455192566, 'learning_rate': 1.9597320043678322e-05, 'epoch': 0.12}
{'loss': 0.9951, 'grad_norm': 0.41951534152030945, 'learning_rate': 1.9595567828846166e-05, 'epoch': 0.12}
{'loss': 1.0264, 'grad_norm': 0.39714694023132324, 'learning_rate': 1.9593811888684192e-05, 'epoch': 0.12}
{'loss': 1.1111, 'grad_norm': 0.4474373757839203, 'learning_rate': 1.9592052223874115e-05, 'epoch': 0.12}
{'loss': 1.064, 'grad_norm': 0.38774651288986206, 'learning_rate': 1.959028883509911e-05, 'epoch': 0.12}
{'loss': 0.9655, 'grad_norm': 0.42017319798469543, 'learning_rate': 1.9588521723043764e-05, 'epoch': 0.12}
{'loss': 0.9429, 'grad_norm': 0.40965020656585693, 'learning_rate': 1.958675088839415e-05, 'epoch': 0.12}
{'loss': 1.0773, 'grad_norm': 0.49333977699279785, 'learning_rate': 1.9584976331837758e-05, 'epoch': 0.12}
{'loss': 0.9836, 'grad_norm': 0.4201720654964447, 'learning_rate': 1.9583198054063535e-05, 'epoch': 0.12}
{'loss': 0.9255, 'grad_norm': 0.26206716895103455, 'learning_rate': 1.9581416055761865e-05, 'epoch': 0.12}
{'loss': 1.0498, 'grad_norm': 0.4209047853946686, 'learning_rate': 1.9579630337624585e-05, 'epoch': 0.12}
{'loss': 1.0157, 'grad_norm': 0.42156872153282166, 'learning_rate': 1.9577840900344974e-05, 'epoch': 0.12}
{'loss': 0.8907, 'grad_norm': 0.239331915974617, 'learning_rate': 1.9576047744617752e-05, 'epoch': 0.12}
{'loss': 1.1005, 'grad_norm': 0.4688258171081543, 'learning_rate': 1.957425087113908e-05, 'epoch': 0.12}
{'loss': 1.0417, 'grad_norm': 0.3988226056098938, 'learning_rate': 1.9572450280606568e-05, 'epoch': 0.12}
{'loss': 1.0767, 'grad_norm': 0.46621161699295044, 'learning_rate': 1.9570645973719273e-05, 'epoch': 0.12}
{'loss': 1.1488, 'grad_norm': 0.4621041715145111, 'learning_rate': 1.9568837951177677e-05, 'epoch': 0.12}
{'loss': 0.9971, 'grad_norm': 0.2707424461841583, 'learning_rate': 1.9567026213683728e-05, 'epoch': 0.12}
{'loss': 0.9742, 'grad_norm': 0.42564770579338074, 'learning_rate': 1.9565210761940798e-05, 'epoch': 0.12}
{'loss': 1.0087, 'grad_norm': 0.4439622461795807, 'learning_rate': 1.956339159665371e-05, 'epoch': 0.12}
{'loss': 0.8988, 'grad_norm': 0.28157609701156616, 'learning_rate': 1.956156871852873e-05, 'epoch': 0.12}
{'loss': 1.016, 'grad_norm': 0.3897164762020111, 'learning_rate': 1.9559742128273558e-05, 'epoch': 0.12}
{'loss': 1.0549, 'grad_norm': 0.46668562293052673, 'learning_rate': 1.9557911826597337e-05, 'epoch': 0.12}
{'loss': 1.0622, 'grad_norm': 0.4243082106113434, 'learning_rate': 1.9556077814210662e-05, 'epoch': 0.12}
{'loss': 1.0182, 'grad_norm': 0.4331830143928528, 'learning_rate': 1.955424009182555e-05, 'epoch': 0.12}
{'loss': 1.0137, 'grad_norm': 0.44553661346435547, 'learning_rate': 1.955239866015547e-05, 'epoch': 0.12}
{'loss': 0.9677, 'grad_norm': 0.4246639013290405, 'learning_rate': 1.9550553519915335e-05, 'epoch': 0.12}
{'loss': 1.0655, 'grad_norm': 0.3971092700958252, 'learning_rate': 1.954870467182149e-05, 'epoch': 0.12}
{'loss': 0.9504, 'grad_norm': 0.4397343695163727, 'learning_rate': 1.954685211659172e-05, 'epoch': 0.12}
{'loss': 0.9817, 'grad_norm': 0.43859878182411194, 'learning_rate': 1.9544995854945248e-05, 'epoch': 0.12}
{'loss': 1.0512, 'grad_norm': 0.4417305290699005, 'learning_rate': 1.954313588760274e-05, 'epoch': 0.12}
{'loss': 0.9853, 'grad_norm': 0.429555743932724, 'learning_rate': 1.9541272215286304e-05, 'epoch': 0.12}
{'loss': 1.0159, 'grad_norm': 0.3957020342350006, 'learning_rate': 1.9539404838719477e-05, 'epoch': 0.12}
{'loss': 0.9694, 'grad_norm': 0.45100271701812744, 'learning_rate': 1.9537533758627242e-05, 'epoch': 0.12}
{'loss': 1.0855, 'grad_norm': 0.48419442772865295, 'learning_rate': 1.953565897573601e-05, 'epoch': 0.12}
{'loss': 0.9491, 'grad_norm': 0.4337885081768036, 'learning_rate': 1.9533780490773645e-05, 'epoch': 0.12}
{'loss': 0.9077, 'grad_norm': 0.261331170797348, 'learning_rate': 1.9531898304469435e-05, 'epoch': 0.13}
{'loss': 0.9192, 'grad_norm': 0.41652846336364746, 'learning_rate': 1.953001241755411e-05, 'epoch': 0.13}
{'loss': 1.0093, 'grad_norm': 0.3980211019515991, 'learning_rate': 1.952812283075984e-05, 'epoch': 0.13}
{'loss': 0.9819, 'grad_norm': 0.399061918258667, 'learning_rate': 1.952622954482022e-05, 'epoch': 0.13}
{'loss': 1.092, 'grad_norm': 0.5187615156173706, 'learning_rate': 1.9524332560470293e-05, 'epoch': 0.13}
{'loss': 0.9691, 'grad_norm': 0.45060405135154724, 'learning_rate': 1.9522431878446536e-05, 'epoch': 0.13}
{'loss': 1.0769, 'grad_norm': 0.48035845160484314, 'learning_rate': 1.9520527499486856e-05, 'epoch': 0.13}
{'loss': 1.0301, 'grad_norm': 0.4209691286087036, 'learning_rate': 1.95186194243306e-05, 'epoch': 0.13}
{'loss': 0.9912, 'grad_norm': 0.42302700877189636, 'learning_rate': 1.9516707653718546e-05, 'epoch': 0.13}
{'loss': 1.0183, 'grad_norm': 0.42943665385246277, 'learning_rate': 1.9514792188392914e-05, 'epoch': 0.13}
{'loss': 1.0081, 'grad_norm': 0.386167973279953, 'learning_rate': 1.9512873029097347e-05, 'epoch': 0.13}
{'loss': 0.9945, 'grad_norm': 0.41731923818588257, 'learning_rate': 1.9510950176576933e-05, 'epoch': 0.13}
{'loss': 0.9424, 'grad_norm': 0.4861500561237335, 'learning_rate': 1.950902363157819e-05, 'epoch': 0.13}
{'loss': 1.0622, 'grad_norm': 0.4153689742088318, 'learning_rate': 1.950709339484907e-05, 'epoch': 0.13}
{'loss': 0.95, 'grad_norm': 0.3124443590641022, 'learning_rate': 1.9505159467138954e-05, 'epoch': 0.13}
{'loss': 1.078, 'grad_norm': 0.47715845704078674, 'learning_rate': 1.9503221849198655e-05, 'epoch': 0.13}
{'loss': 1.1627, 'grad_norm': 0.4906391203403473, 'learning_rate': 1.9501280541780435e-05, 'epoch': 0.13}
{'loss': 0.9173, 'grad_norm': 0.2379380762577057, 'learning_rate': 1.9499335545637968e-05, 'epoch': 0.13}
{'loss': 1.055, 'grad_norm': 0.4565185606479645, 'learning_rate': 1.949738686152637e-05, 'epoch': 0.13}
{'loss': 0.9155, 'grad_norm': 0.3919207751750946, 'learning_rate': 1.9495434490202188e-05, 'epoch': 0.13}
{'loss': 1.0265, 'grad_norm': 0.40080466866493225, 'learning_rate': 1.94934784324234e-05, 'epoch': 0.13}
{'loss': 1.0317, 'grad_norm': 0.4571654796600342, 'learning_rate': 1.9491518688949417e-05, 'epoch': 0.13}
{'loss': 1.0432, 'grad_norm': 0.4465762972831726, 'learning_rate': 1.9489555260541074e-05, 'epoch': 0.13}
{'loss': 0.9321, 'grad_norm': 0.39135992527008057, 'learning_rate': 1.948758814796064e-05, 'epoch': 0.13}
{'loss': 1.0867, 'grad_norm': 0.4800599217414856, 'learning_rate': 1.9485617351971827e-05, 'epoch': 0.13}
{'loss': 1.0834, 'grad_norm': 0.4697965681552887, 'learning_rate': 1.9483642873339753e-05, 'epoch': 0.13}
{'loss': 0.9526, 'grad_norm': 0.42696526646614075, 'learning_rate': 1.9481664712830987e-05, 'epoch': 0.13}
{'loss': 1.1191, 'grad_norm': 0.4696812331676483, 'learning_rate': 1.9479682871213515e-05, 'epoch': 0.13}
{'loss': 1.1283, 'grad_norm': 0.49150070548057556, 'learning_rate': 1.9477697349256756e-05, 'epoch': 0.13}
{'loss': 0.9807, 'grad_norm': 0.39746373891830444, 'learning_rate': 1.947570814773156e-05, 'epoch': 0.13}
{'loss': 1.0674, 'grad_norm': 0.39927271008491516, 'learning_rate': 1.9473715267410206e-05, 'epoch': 0.13}
{'loss': 0.9927, 'grad_norm': 0.46605804562568665, 'learning_rate': 1.9471718709066392e-05, 'epoch': 0.13}
{'loss': 1.1138, 'grad_norm': 0.44374892115592957, 'learning_rate': 1.9469718473475256e-05, 'epoch': 0.13}
{'loss': 0.9519, 'grad_norm': 0.3963271975517273, 'learning_rate': 1.9467714561413358e-05, 'epoch': 0.13}
{'loss': 0.9155, 'grad_norm': 0.24979141354560852, 'learning_rate': 1.9465706973658683e-05, 'epoch': 0.13}
{'loss': 0.9638, 'grad_norm': 0.39319199323654175, 'learning_rate': 1.9463695710990648e-05, 'epoch': 0.13}
{'loss': 1.0789, 'grad_norm': 0.46228212118148804, 'learning_rate': 1.946168077419009e-05, 'epoch': 0.13}
{'loss': 0.9886, 'grad_norm': 0.38015714287757874, 'learning_rate': 1.9459662164039283e-05, 'epoch': 0.13}
{'loss': 0.9349, 'grad_norm': 0.4346690773963928, 'learning_rate': 1.9457639881321917e-05, 'epoch': 0.13}
{'loss': 0.9883, 'grad_norm': 0.389577180147171, 'learning_rate': 1.9455613926823115e-05, 'epoch': 0.13}
{'loss': 1.0824, 'grad_norm': 0.4633167088031769, 'learning_rate': 1.945358430132942e-05, 'epoch': 0.13}
{'loss': 1.0372, 'grad_norm': 0.4321710467338562, 'learning_rate': 1.9451551005628803e-05, 'epoch': 0.13}
{'loss': 1.037, 'grad_norm': 0.462429404258728, 'learning_rate': 1.9449514040510654e-05, 'epoch': 0.13}
{'loss': 0.9741, 'grad_norm': 0.394031286239624, 'learning_rate': 1.9447473406765803e-05, 'epoch': 0.13}
{'loss': 1.0318, 'grad_norm': 0.4265391528606415, 'learning_rate': 1.9445429105186487e-05, 'epoch': 0.13}
{'loss': 0.9759, 'grad_norm': 0.408690482378006, 'learning_rate': 1.9443381136566382e-05, 'epoch': 0.13}
{'loss': 1.1274, 'grad_norm': 0.48943793773651123, 'learning_rate': 1.9441329501700568e-05, 'epoch': 0.13}
{'loss': 0.9036, 'grad_norm': 0.41112086176872253, 'learning_rate': 1.943927420138557e-05, 'epoch': 0.13}
{'loss': 1.0141, 'grad_norm': 0.40800997614860535, 'learning_rate': 1.9437215236419322e-05, 'epoch': 0.13}
{'loss': 0.9729, 'grad_norm': 0.4290589988231659, 'learning_rate': 1.9435152607601187e-05, 'epoch': 0.13}
{'loss': 0.9152, 'grad_norm': 0.3904605805873871, 'learning_rate': 1.943308631573195e-05, 'epoch': 0.13}
{'loss': 1.0721, 'grad_norm': 0.4788742959499359, 'learning_rate': 1.9431016361613816e-05, 'epoch': 0.13}
{'loss': 0.9892, 'grad_norm': 0.42830783128738403, 'learning_rate': 1.9428942746050406e-05, 'epoch': 0.14}
{'loss': 0.9817, 'grad_norm': 0.4347919821739197, 'learning_rate': 1.9426865469846773e-05, 'epoch': 0.14}
{'loss': 1.0103, 'grad_norm': 0.4379032254219055, 'learning_rate': 1.9424784533809393e-05, 'epoch': 0.14}
{'loss': 0.9145, 'grad_norm': 0.4156939685344696, 'learning_rate': 1.942269993874615e-05, 'epoch': 0.14}
{'loss': 1.0103, 'grad_norm': 0.4166417717933655, 'learning_rate': 1.9420611685466358e-05, 'epoch': 0.14}
{'loss': 1.0712, 'grad_norm': 0.4922912120819092, 'learning_rate': 1.9418519774780748e-05, 'epoch': 0.14}
{'loss': 0.9805, 'grad_norm': 0.398639440536499, 'learning_rate': 1.9416424207501474e-05, 'epoch': 0.14}
{'loss': 1.0286, 'grad_norm': 0.4733237326145172, 'learning_rate': 1.9414324984442102e-05, 'epoch': 0.14}
{'loss': 0.9875, 'grad_norm': 0.3970688581466675, 'learning_rate': 1.9412222106417632e-05, 'epoch': 0.14}
{'loss': 1.0463, 'grad_norm': 0.4245131313800812, 'learning_rate': 1.9410115574244462e-05, 'epoch': 0.14}
{'loss': 0.9568, 'grad_norm': 0.40340012311935425, 'learning_rate': 1.9408005388740433e-05, 'epoch': 0.14}
{'loss': 1.0766, 'grad_norm': 0.48864489793777466, 'learning_rate': 1.9405891550724778e-05, 'epoch': 0.14}
{'loss': 1.0173, 'grad_norm': 0.45226362347602844, 'learning_rate': 1.940377406101817e-05, 'epoch': 0.14}
{'loss': 1.0949, 'grad_norm': 0.4677705466747284, 'learning_rate': 1.9401652920442694e-05, 'epoch': 0.14}
{'loss': 1.0394, 'grad_norm': 0.4399515390396118, 'learning_rate': 1.9399528129821842e-05, 'epoch': 0.14}
{'loss': 1.0619, 'grad_norm': 0.4223293662071228, 'learning_rate': 1.939739968998054e-05, 'epoch': 0.14}
{'loss': 1.1163, 'grad_norm': 0.48128601908683777, 'learning_rate': 1.939526760174511e-05, 'epoch': 0.14}
{'loss': 1.0121, 'grad_norm': 0.4235583245754242, 'learning_rate': 1.939313186594331e-05, 'epoch': 0.14}
{'loss': 0.959, 'grad_norm': 0.4164108633995056, 'learning_rate': 1.9390992483404308e-05, 'epoch': 0.14}
{'loss': 0.9894, 'grad_norm': 0.41540291905403137, 'learning_rate': 1.938884945495868e-05, 'epoch': 0.14}
{'loss': 1.0798, 'grad_norm': 0.40656858682632446, 'learning_rate': 1.9386702781438425e-05, 'epoch': 0.14}
{'loss': 1.1251, 'grad_norm': 0.44992634654045105, 'learning_rate': 1.938455246367696e-05, 'epoch': 0.14}
{'loss': 1.0338, 'grad_norm': 0.4237435758113861, 'learning_rate': 1.9382398502509107e-05, 'epoch': 0.14}
{'loss': 1.0398, 'grad_norm': 0.42079830169677734, 'learning_rate': 1.938024089877111e-05, 'epoch': 0.14}
{'loss': 0.9148, 'grad_norm': 0.2533835768699646, 'learning_rate': 1.9378079653300624e-05, 'epoch': 0.14}
{'loss': 1.0619, 'grad_norm': 0.500986635684967, 'learning_rate': 1.9375914766936723e-05, 'epoch': 0.14}
{'loss': 0.9779, 'grad_norm': 0.3842742145061493, 'learning_rate': 1.9373746240519884e-05, 'epoch': 0.14}
{'loss': 1.037, 'grad_norm': 0.4348176419734955, 'learning_rate': 1.937157407489201e-05, 'epoch': 0.14}
{'loss': 1.0592, 'grad_norm': 0.41472601890563965, 'learning_rate': 1.9369398270896403e-05, 'epoch': 0.14}
{'loss': 0.975, 'grad_norm': 0.3942280113697052, 'learning_rate': 1.936721882937779e-05, 'epoch': 0.14}
{'loss': 1.0855, 'grad_norm': 0.4503380358219147, 'learning_rate': 1.9365035751182307e-05, 'epoch': 0.14}
{'loss': 1.0888, 'grad_norm': 0.4543353319168091, 'learning_rate': 1.93628490371575e-05, 'epoch': 0.14}
{'loss': 0.9956, 'grad_norm': 0.42975878715515137, 'learning_rate': 1.9360658688152322e-05, 'epoch': 0.14}
{'loss': 0.9605, 'grad_norm': 0.4012552797794342, 'learning_rate': 1.9358464705017143e-05, 'epoch': 0.14}
{'loss': 1.1772, 'grad_norm': 0.4279763102531433, 'learning_rate': 1.9356267088603745e-05, 'epoch': 0.14}
{'loss': 1.0158, 'grad_norm': 0.4361991584300995, 'learning_rate': 1.9354065839765316e-05, 'epoch': 0.14}
{'loss': 1.0009, 'grad_norm': 0.409451961517334, 'learning_rate': 1.9351860959356462e-05, 'epoch': 0.14}
{'loss': 1.0176, 'grad_norm': 0.3983542025089264, 'learning_rate': 1.9349652448233187e-05, 'epoch': 0.14}
{'loss': 0.9618, 'grad_norm': 0.4084448218345642, 'learning_rate': 1.934744030725291e-05, 'epoch': 0.14}
{'loss': 0.8975, 'grad_norm': 0.4494015574455261, 'learning_rate': 1.934522453727447e-05, 'epoch': 0.14}
{'loss': 1.0235, 'grad_norm': 0.43511712551116943, 'learning_rate': 1.93430051391581e-05, 'epoch': 0.14}
{'loss': 1.0808, 'grad_norm': 0.4281351566314697, 'learning_rate': 1.934078211376544e-05, 'epoch': 0.14}
{'loss': 1.0666, 'grad_norm': 0.47694632411003113, 'learning_rate': 1.9338555461959554e-05, 'epoch': 0.14}
{'loss': 0.9827, 'grad_norm': 0.43647417426109314, 'learning_rate': 1.93363251846049e-05, 'epoch': 0.14}
{'loss': 1.0266, 'grad_norm': 0.45488616824150085, 'learning_rate': 1.9334091282567352e-05, 'epoch': 0.14}
{'loss': 1.144, 'grad_norm': 0.49694880843162537, 'learning_rate': 1.9331853756714185e-05, 'epoch': 0.14}
{'loss': 1.1063, 'grad_norm': 0.5444958806037903, 'learning_rate': 1.9329612607914088e-05, 'epoch': 0.14}
{'loss': 1.073, 'grad_norm': 0.4752940535545349, 'learning_rate': 1.9327367837037142e-05, 'epoch': 0.14}
{'loss': 0.917, 'grad_norm': 0.22506393492221832, 'learning_rate': 1.9325119444954855e-05, 'epoch': 0.14}
{'loss': 1.0038, 'grad_norm': 0.40709608793258667, 'learning_rate': 1.9322867432540126e-05, 'epoch': 0.14}
{'loss': 1.0379, 'grad_norm': 0.461460143327713, 'learning_rate': 1.9320611800667268e-05, 'epoch': 0.14}
{'loss': 0.9752, 'grad_norm': 0.41498899459838867, 'learning_rate': 1.9318352550211986e-05, 'epoch': 0.14}
{'loss': 0.9555, 'grad_norm': 0.43809953331947327, 'learning_rate': 1.9316089682051403e-05, 'epoch': 0.15}
{'loss': 1.1062, 'grad_norm': 0.44839945435523987, 'learning_rate': 1.9313823197064042e-05, 'epoch': 0.15}
{'loss': 1.0861, 'grad_norm': 0.44916605949401855, 'learning_rate': 1.9311553096129835e-05, 'epoch': 0.15}
{'loss': 1.0737, 'grad_norm': 0.4430954158306122, 'learning_rate': 1.9309279380130112e-05, 'epoch': 0.15}
{'loss': 0.9134, 'grad_norm': 0.4165725111961365, 'learning_rate': 1.93070020499476e-05, 'epoch': 0.15}
{'loss': 0.8674, 'grad_norm': 0.24086779356002808, 'learning_rate': 1.930472110646645e-05, 'epoch': 0.15}
{'loss': 1.0476, 'grad_norm': 0.44267645478248596, 'learning_rate': 1.9302436550572187e-05, 'epoch': 0.15}
{'loss': 0.9872, 'grad_norm': 0.4210651218891144, 'learning_rate': 1.930014838315177e-05, 'epoch': 0.15}
{'loss': 0.9888, 'grad_norm': 0.42094606161117554, 'learning_rate': 1.9297856605093534e-05, 'epoch': 0.15}
{'loss': 1.0636, 'grad_norm': 0.45261645317077637, 'learning_rate': 1.9295561217287226e-05, 'epoch': 0.15}
{'loss': 1.0234, 'grad_norm': 0.3956705629825592, 'learning_rate': 1.9293262220624002e-05, 'epoch': 0.15}
{'loss': 1.0303, 'grad_norm': 0.41047653555870056, 'learning_rate': 1.9290959615996407e-05, 'epoch': 0.15}
{'loss': 0.9864, 'grad_norm': 0.4414278566837311, 'learning_rate': 1.9288653404298392e-05, 'epoch': 0.15}
{'loss': 0.9843, 'grad_norm': 0.45291048288345337, 'learning_rate': 1.9286343586425307e-05, 'epoch': 0.15}
{'loss': 0.9811, 'grad_norm': 0.41461658477783203, 'learning_rate': 1.9284030163273907e-05, 'epoch': 0.15}
{'loss': 0.9867, 'grad_norm': 0.4133249521255493, 'learning_rate': 1.9281713135742333e-05, 'epoch': 0.15}
{'loss': 0.9618, 'grad_norm': 0.42149215936660767, 'learning_rate': 1.9279392504730147e-05, 'epoch': 0.15}
{'loss': 1.1054, 'grad_norm': 0.46468880772590637, 'learning_rate': 1.9277068271138287e-05, 'epoch': 0.15}
{'loss': 1.056, 'grad_norm': 0.4523586630821228, 'learning_rate': 1.9274740435869107e-05, 'epoch': 0.15}
{'loss': 1.1039, 'grad_norm': 0.44189026951789856, 'learning_rate': 1.927240899982635e-05, 'epoch': 0.15}
{'loss': 1.1229, 'grad_norm': 0.469061940908432, 'learning_rate': 1.9270073963915162e-05, 'epoch': 0.15}
{'loss': 0.9443, 'grad_norm': 0.4134552776813507, 'learning_rate': 1.9267735329042086e-05, 'epoch': 0.15}
{'loss': 0.9646, 'grad_norm': 0.42597413063049316, 'learning_rate': 1.9265393096115056e-05, 'epoch': 0.15}
{'loss': 1.0251, 'grad_norm': 0.41674432158470154, 'learning_rate': 1.926304726604341e-05, 'epoch': 0.15}
{'loss': 0.8944, 'grad_norm': 0.4498351514339447, 'learning_rate': 1.9260697839737875e-05, 'epoch': 0.15}
{'loss': 1.0328, 'grad_norm': 0.40824583172798157, 'learning_rate': 1.925834481811059e-05, 'epoch': 0.15}
{'loss': 0.9963, 'grad_norm': 0.4311281740665436, 'learning_rate': 1.9255988202075065e-05, 'epoch': 0.15}
{'loss': 1.0022, 'grad_norm': 0.4245704412460327, 'learning_rate': 1.925362799254623e-05, 'epoch': 0.15}
{'loss': 0.9902, 'grad_norm': 0.4396072328090668, 'learning_rate': 1.9251264190440398e-05, 'epoch': 0.15}
{'loss': 1.0685, 'grad_norm': 0.4195954501628876, 'learning_rate': 1.9248896796675277e-05, 'epoch': 0.15}
{'loss': 1.0433, 'grad_norm': 0.412283331155777, 'learning_rate': 1.924652581216997e-05, 'epoch': 0.15}
{'loss': 1.033, 'grad_norm': 0.46074527502059937, 'learning_rate': 1.9244151237844975e-05, 'epoch': 0.15}
{'loss': 0.9517, 'grad_norm': 0.42702704668045044, 'learning_rate': 1.9241773074622182e-05, 'epoch': 0.15}
{'loss': 1.1324, 'grad_norm': 0.48582276701927185, 'learning_rate': 1.923939132342488e-05, 'epoch': 0.15}
{'loss': 0.9758, 'grad_norm': 0.4188452959060669, 'learning_rate': 1.923700598517775e-05, 'epoch': 0.15}
{'loss': 1.0624, 'grad_norm': 0.423112690448761, 'learning_rate': 1.923461706080685e-05, 'epoch': 0.15}
{'loss': 1.0244, 'grad_norm': 0.3990744650363922, 'learning_rate': 1.923222455123965e-05, 'epoch': 0.15}
{'loss': 1.1186, 'grad_norm': 0.46380504965782166, 'learning_rate': 1.9229828457405005e-05, 'epoch': 0.15}
{'loss': 1.0276, 'grad_norm': 0.4372921288013458, 'learning_rate': 1.9227428780233162e-05, 'epoch': 0.15}
{'loss': 1.0406, 'grad_norm': 0.49695101380348206, 'learning_rate': 1.922502552065576e-05, 'epoch': 0.15}
{'loss': 1.0486, 'grad_norm': 0.450868159532547, 'learning_rate': 1.922261867960582e-05, 'epoch': 0.15}
{'loss': 1.1168, 'grad_norm': 0.4889692962169647, 'learning_rate': 1.9220208258017763e-05, 'epoch': 0.15}
{'loss': 1.0164, 'grad_norm': 0.4143182933330536, 'learning_rate': 1.92177942568274e-05, 'epoch': 0.15}
{'loss': 1.038, 'grad_norm': 0.4245927929878235, 'learning_rate': 1.921537667697193e-05, 'epoch': 0.15}
{'loss': 0.9896, 'grad_norm': 0.435945600271225, 'learning_rate': 1.9212955519389938e-05, 'epoch': 0.15}
{'loss': 1.009, 'grad_norm': 0.3962176442146301, 'learning_rate': 1.9210530785021405e-05, 'epoch': 0.15}
{'loss': 1.0456, 'grad_norm': 0.43677252531051636, 'learning_rate': 1.9208102474807692e-05, 'epoch': 0.15}
{'loss': 1.0283, 'grad_norm': 0.41504544019699097, 'learning_rate': 1.920567058969155e-05, 'epoch': 0.15}
{'loss': 1.0429, 'grad_norm': 0.4254271388053894, 'learning_rate': 1.920323513061713e-05, 'epoch': 0.15}
{'loss': 1.0337, 'grad_norm': 0.43368959426879883, 'learning_rate': 1.9200796098529956e-05, 'epoch': 0.15}
{'loss': 0.9817, 'grad_norm': 0.4204685091972351, 'learning_rate': 1.919835349437694e-05, 'epoch': 0.15}
{'loss': 1.0791, 'grad_norm': 0.4985591769218445, 'learning_rate': 1.9195907319106394e-05, 'epoch': 0.15}
{'loss': 1.0239, 'grad_norm': 0.4164031147956848, 'learning_rate': 1.9193457573667996e-05, 'epoch': 0.16}
{'loss': 1.0659, 'grad_norm': 0.4712819755077362, 'learning_rate': 1.919100425901283e-05, 'epoch': 0.16}
{'loss': 0.9664, 'grad_norm': 0.4014354944229126, 'learning_rate': 1.9188547376093355e-05, 'epoch': 0.16}
{'loss': 1.0009, 'grad_norm': 0.4287031888961792, 'learning_rate': 1.918608692586342e-05, 'epoch': 0.16}
{'loss': 1.0274, 'grad_norm': 0.4228079915046692, 'learning_rate': 1.918362290927825e-05, 'epoch': 0.16}
{'loss': 0.9842, 'grad_norm': 0.3901379406452179, 'learning_rate': 1.9181155327294468e-05, 'epoch': 0.16}
{'loss': 1.0653, 'grad_norm': 0.43719732761383057, 'learning_rate': 1.9178684180870072e-05, 'epoch': 0.16}
{'loss': 1.0067, 'grad_norm': 0.46068117022514343, 'learning_rate': 1.9176209470964446e-05, 'epoch': 0.16}
{'loss': 1.0755, 'grad_norm': 0.4686548709869385, 'learning_rate': 1.9173731198538354e-05, 'epoch': 0.16}
{'loss': 1.0788, 'grad_norm': 0.48463714122772217, 'learning_rate': 1.9171249364553956e-05, 'epoch': 0.16}
{'loss': 1.0993, 'grad_norm': 0.40658703446388245, 'learning_rate': 1.9168763969974773e-05, 'epoch': 0.16}
{'loss': 0.9135, 'grad_norm': 0.39092394709587097, 'learning_rate': 1.916627501576573e-05, 'epoch': 0.16}
{'loss': 1.0887, 'grad_norm': 0.43095022439956665, 'learning_rate': 1.916378250289312e-05, 'epoch': 0.16}
{'loss': 0.9682, 'grad_norm': 0.4377581775188446, 'learning_rate': 1.9161286432324628e-05, 'epoch': 0.16}
{'loss': 1.114, 'grad_norm': 0.4769675135612488, 'learning_rate': 1.9158786805029307e-05, 'epoch': 0.16}
{'loss': 1.0011, 'grad_norm': 0.4143090844154358, 'learning_rate': 1.9156283621977603e-05, 'epoch': 0.16}
{'loss': 1.0312, 'grad_norm': 0.4749368131160736, 'learning_rate': 1.9153776884141336e-05, 'epoch': 0.16}
{'loss': 1.0235, 'grad_norm': 0.4268469214439392, 'learning_rate': 1.915126659249371e-05, 'epoch': 0.16}
{'loss': 0.9646, 'grad_norm': 0.43632856011390686, 'learning_rate': 1.9148752748009304e-05, 'epoch': 0.16}
{'loss': 0.9185, 'grad_norm': 0.24580802023410797, 'learning_rate': 1.914623535166408e-05, 'epoch': 0.16}
{'loss': 0.9973, 'grad_norm': 0.4495789110660553, 'learning_rate': 1.9143714404435382e-05, 'epoch': 0.16}
{'loss': 0.8851, 'grad_norm': 0.423715740442276, 'learning_rate': 1.9141189907301922e-05, 'epoch': 0.16}
{'loss': 1.0317, 'grad_norm': 0.4062504768371582, 'learning_rate': 1.9138661861243802e-05, 'epoch': 0.16}
{'loss': 0.9539, 'grad_norm': 0.43422731757164, 'learning_rate': 1.913613026724249e-05, 'epoch': 0.16}
{'loss': 1.0815, 'grad_norm': 0.42658060789108276, 'learning_rate': 1.9133595126280848e-05, 'epoch': 0.16}
{'loss': 1.0079, 'grad_norm': 0.40442395210266113, 'learning_rate': 1.9131056439343095e-05, 'epoch': 0.16}
{'loss': 1.0418, 'grad_norm': 0.4297802448272705, 'learning_rate': 1.9128514207414838e-05, 'epoch': 0.16}
{'loss': 0.9196, 'grad_norm': 0.42239683866500854, 'learning_rate': 1.9125968431483068e-05, 'epoch': 0.16}
{'loss': 1.0509, 'grad_norm': 0.39774611592292786, 'learning_rate': 1.9123419112536132e-05, 'epoch': 0.16}
{'loss': 1.0671, 'grad_norm': 0.46488720178604126, 'learning_rate': 1.912086625156377e-05, 'epoch': 0.16}
{'loss': 0.9689, 'grad_norm': 0.42759284377098083, 'learning_rate': 1.911830984955709e-05, 'epoch': 0.16}
{'loss': 0.9118, 'grad_norm': 0.41879284381866455, 'learning_rate': 1.911574990750857e-05, 'epoch': 0.16}
{'loss': 0.9943, 'grad_norm': 0.43096834421157837, 'learning_rate': 1.9113186426412073e-05, 'epoch': 0.16}
{'loss': 1.093, 'grad_norm': 0.4829327464103699, 'learning_rate': 1.9110619407262828e-05, 'epoch': 0.16}
{'loss': 1.012, 'grad_norm': 0.41574570536613464, 'learning_rate': 1.9108048851057447e-05, 'epoch': 0.16}
{'loss': 1.0087, 'grad_norm': 0.41477862000465393, 'learning_rate': 1.9105474758793897e-05, 'epoch': 0.16}
{'loss': 1.0149, 'grad_norm': 0.4478239119052887, 'learning_rate': 1.9102897131471536e-05, 'epoch': 0.16}
{'loss': 0.9992, 'grad_norm': 0.41297537088394165, 'learning_rate': 1.9100315970091088e-05, 'epoch': 0.16}
{'loss': 0.9812, 'grad_norm': 0.42709916830062866, 'learning_rate': 1.9097731275654645e-05, 'epoch': 0.16}
{'loss': 1.0427, 'grad_norm': 0.42797961831092834, 'learning_rate': 1.909514304916568e-05, 'epoch': 0.16}
{'loss': 1.0156, 'grad_norm': 0.46242156624794006, 'learning_rate': 1.9092551291629026e-05, 'epoch': 0.16}
{'loss': 1.1352, 'grad_norm': 0.48539218306541443, 'learning_rate': 1.9089956004050893e-05, 'epoch': 0.16}
{'loss': 1.109, 'grad_norm': 0.4561465084552765, 'learning_rate': 1.908735718743887e-05, 'epoch': 0.16}
{'loss': 0.9947, 'grad_norm': 0.430496484041214, 'learning_rate': 1.908475484280189e-05, 'epoch': 0.16}
{'loss': 1.1244, 'grad_norm': 0.4723265767097473, 'learning_rate': 1.908214897115029e-05, 'epoch': 0.16}
{'loss': 0.9364, 'grad_norm': 0.4012184739112854, 'learning_rate': 1.907953957349575e-05, 'epoch': 0.16}
{'loss': 1.0045, 'grad_norm': 0.44553226232528687, 'learning_rate': 1.907692665085133e-05, 'epoch': 0.16}
{'loss': 1.0259, 'grad_norm': 0.4591110348701477, 'learning_rate': 1.9074310204231457e-05, 'epoch': 0.16}
{'loss': 1.0858, 'grad_norm': 0.3888438940048218, 'learning_rate': 1.9071690234651923e-05, 'epoch': 0.16}
{'loss': 1.0958, 'grad_norm': 0.5067487359046936, 'learning_rate': 1.9069066743129893e-05, 'epoch': 0.16}
{'loss': 0.9638, 'grad_norm': 0.428187757730484, 'learning_rate': 1.90664397306839e-05, 'epoch': 0.16}
{'loss': 0.9636, 'grad_norm': 0.4171636402606964, 'learning_rate': 1.9063809198333832e-05, 'epoch': 0.16}
{'loss': 0.9473, 'grad_norm': 0.26629284024238586, 'learning_rate': 1.9061175147100957e-05, 'epoch': 0.17}
{'loss': 1.1204, 'grad_norm': 0.5016010403633118, 'learning_rate': 1.905853757800791e-05, 'epoch': 0.17}
{'loss': 0.9686, 'grad_norm': 0.415155827999115, 'learning_rate': 1.9055896492078675e-05, 'epoch': 0.17}
{'loss': 1.0787, 'grad_norm': 0.425174355506897, 'learning_rate': 1.905325189033862e-05, 'epoch': 0.17}
{'loss': 1.0209, 'grad_norm': 0.4192298948764801, 'learning_rate': 1.905060377381447e-05, 'epoch': 0.17}
{'loss': 0.9794, 'grad_norm': 0.441267728805542, 'learning_rate': 1.904795214353431e-05, 'epoch': 0.17}
{'loss': 1.0919, 'grad_norm': 0.4342976212501526, 'learning_rate': 1.90452970005276e-05, 'epoch': 0.17}
{'loss': 1.0705, 'grad_norm': 0.4092099964618683, 'learning_rate': 1.9042638345825155e-05, 'epoch': 0.17}
{'loss': 1.0607, 'grad_norm': 0.43971601128578186, 'learning_rate': 1.9039976180459158e-05, 'epoch': 0.17}
{'loss': 1.1566, 'grad_norm': 0.4391929805278778, 'learning_rate': 1.9037310505463153e-05, 'epoch': 0.17}
{'loss': 1.022, 'grad_norm': 0.4056164026260376, 'learning_rate': 1.9034641321872043e-05, 'epoch': 0.17}
{'loss': 1.0753, 'grad_norm': 0.45410624146461487, 'learning_rate': 1.9031968630722104e-05, 'epoch': 0.17}
{'loss': 0.9444, 'grad_norm': 0.41781583428382874, 'learning_rate': 1.902929243305096e-05, 'epoch': 0.17}
{'loss': 0.9803, 'grad_norm': 0.4075067639350891, 'learning_rate': 1.902661272989761e-05, 'epoch': 0.17}
{'loss': 1.0756, 'grad_norm': 0.41884374618530273, 'learning_rate': 1.9023929522302394e-05, 'epoch': 0.17}
{'loss': 0.9657, 'grad_norm': 0.42955273389816284, 'learning_rate': 1.9021242811307044e-05, 'epoch': 0.17}
{'loss': 0.9313, 'grad_norm': 0.3979054093360901, 'learning_rate': 1.901855259795462e-05, 'epoch': 0.17}
{'loss': 0.9879, 'grad_norm': 0.39046621322631836, 'learning_rate': 1.9015858883289556e-05, 'epoch': 0.17}
{'loss': 1.166, 'grad_norm': 0.450359046459198, 'learning_rate': 1.9013161668357655e-05, 'epoch': 0.17}
{'loss': 0.942, 'grad_norm': 0.44365212321281433, 'learning_rate': 1.901046095420606e-05, 'epoch': 0.17}
{'loss': 1.0737, 'grad_norm': 0.45967623591423035, 'learning_rate': 1.9007756741883284e-05, 'epoch': 0.17}
{'loss': 1.0203, 'grad_norm': 0.4264701306819916, 'learning_rate': 1.9005049032439193e-05, 'epoch': 0.17}
{'loss': 0.9274, 'grad_norm': 0.2579658329486847, 'learning_rate': 1.9002337826925012e-05, 'epoch': 0.17}
{'loss': 1.0046, 'grad_norm': 0.4077712595462799, 'learning_rate': 1.899962312639333e-05, 'epoch': 0.17}
{'loss': 1.0378, 'grad_norm': 0.4159297049045563, 'learning_rate': 1.8996904931898085e-05, 'epoch': 0.17}
{'loss': 1.181, 'grad_norm': 0.5069621205329895, 'learning_rate': 1.899418324449457e-05, 'epoch': 0.17}
{'loss': 0.9851, 'grad_norm': 0.42326784133911133, 'learning_rate': 1.8991458065239444e-05, 'epoch': 0.17}
{'loss': 1.0026, 'grad_norm': 0.42887404561042786, 'learning_rate': 1.8988729395190712e-05, 'epoch': 0.17}
{'loss': 1.022, 'grad_norm': 0.49095866084098816, 'learning_rate': 1.8985997235407735e-05, 'epoch': 0.17}
{'loss': 1.0359, 'grad_norm': 0.46802079677581787, 'learning_rate': 1.898326158695124e-05, 'epoch': 0.17}
{'loss': 1.1387, 'grad_norm': 0.4464552700519562, 'learning_rate': 1.8980522450883287e-05, 'epoch': 0.17}
{'loss': 1.034, 'grad_norm': 0.40201258659362793, 'learning_rate': 1.8977779828267314e-05, 'epoch': 0.17}
{'loss': 0.9565, 'grad_norm': 0.40903440117836, 'learning_rate': 1.8975033720168094e-05, 'epoch': 0.17}
{'loss': 0.9131, 'grad_norm': 0.22622594237327576, 'learning_rate': 1.897228412765177e-05, 'epoch': 0.17}
{'loss': 1.0711, 'grad_norm': 0.4214245080947876, 'learning_rate': 1.896953105178582e-05, 'epoch': 0.17}
{'loss': 0.9483, 'grad_norm': 0.25238895416259766, 'learning_rate': 1.8966774493639084e-05, 'epoch': 0.17}
{'loss': 1.1326, 'grad_norm': 0.4538538157939911, 'learning_rate': 1.896401445428176e-05, 'epoch': 0.17}
{'loss': 0.9487, 'grad_norm': 0.2604513168334961, 'learning_rate': 1.896125093478538e-05, 'epoch': 0.17}
{'loss': 1.0223, 'grad_norm': 0.3994816541671753, 'learning_rate': 1.895848393622284e-05, 'epoch': 0.17}
{'loss': 1.0553, 'grad_norm': 0.44109177589416504, 'learning_rate': 1.895571345966839e-05, 'epoch': 0.17}
{'loss': 0.9858, 'grad_norm': 0.39236003160476685, 'learning_rate': 1.8952939506197622e-05, 'epoch': 0.17}
{'loss': 0.955, 'grad_norm': 0.41206982731819153, 'learning_rate': 1.8950162076887477e-05, 'epoch': 0.17}
{'loss': 1.1307, 'grad_norm': 0.4896000921726227, 'learning_rate': 1.894738117281625e-05, 'epoch': 0.17}
{'loss': 1.0084, 'grad_norm': 0.4134019911289215, 'learning_rate': 1.8944596795063584e-05, 'epoch': 0.17}
{'loss': 0.9064, 'grad_norm': 0.4142581820487976, 'learning_rate': 1.894180894471047e-05, 'epoch': 0.17}
{'loss': 0.8441, 'grad_norm': 0.22805726528167725, 'learning_rate': 1.8939017622839253e-05, 'epoch': 0.17}
{'loss': 0.9299, 'grad_norm': 0.41155433654785156, 'learning_rate': 1.8936222830533613e-05, 'epoch': 0.17}
{'loss': 1.1422, 'grad_norm': 0.49774450063705444, 'learning_rate': 1.8933424568878586e-05, 'epoch': 0.17}
{'loss': 1.0476, 'grad_norm': 0.43744635581970215, 'learning_rate': 1.8930622838960555e-05, 'epoch': 0.17}
{'loss': 0.9776, 'grad_norm': 0.42920631170272827, 'learning_rate': 1.8927817641867244e-05, 'epoch': 0.17}
{'loss': 1.0757, 'grad_norm': 0.5010355114936829, 'learning_rate': 1.8925008978687737e-05, 'epoch': 0.17}
{'loss': 0.9979, 'grad_norm': 0.42466339468955994, 'learning_rate': 1.8922196850512446e-05, 'epoch': 0.17}
{'loss': 1.044, 'grad_norm': 0.4415116310119629, 'learning_rate': 1.8919381258433135e-05, 'epoch': 0.18}
{'loss': 1.0325, 'grad_norm': 0.46484100818634033, 'learning_rate': 1.8916562203542916e-05, 'epoch': 0.18}
{'loss': 0.8521, 'grad_norm': 0.43831294775009155, 'learning_rate': 1.8913739686936244e-05, 'epoch': 0.18}
{'loss': 0.9649, 'grad_norm': 0.4294406771659851, 'learning_rate': 1.8910913709708918e-05, 'epoch': 0.18}
{'loss': 1.0149, 'grad_norm': 0.4232719838619232, 'learning_rate': 1.8908084272958077e-05, 'epoch': 0.18}
{'loss': 0.9716, 'grad_norm': 0.2691432535648346, 'learning_rate': 1.8905251377782206e-05, 'epoch': 0.18}
{'loss': 1.0321, 'grad_norm': 0.43358078598976135, 'learning_rate': 1.8902415025281136e-05, 'epoch': 0.18}
{'loss': 1.0605, 'grad_norm': 0.4584644138813019, 'learning_rate': 1.889957521655603e-05, 'epoch': 0.18}
{'loss': 1.0864, 'grad_norm': 0.4791721999645233, 'learning_rate': 1.8896731952709408e-05, 'epoch': 0.18}
{'loss': 1.0554, 'grad_norm': 0.470089316368103, 'learning_rate': 1.8893885234845117e-05, 'epoch': 0.18}
{'loss': 1.0697, 'grad_norm': 0.445188969373703, 'learning_rate': 1.8891035064068354e-05, 'epoch': 0.18}
{'loss': 1.0485, 'grad_norm': 0.4670998752117157, 'learning_rate': 1.888818144148565e-05, 'epoch': 0.18}
{'loss': 0.9954, 'grad_norm': 0.38905200362205505, 'learning_rate': 1.888532436820488e-05, 'epoch': 0.18}
{'loss': 0.9648, 'grad_norm': 0.3778417408466339, 'learning_rate': 1.8882463845335263e-05, 'epoch': 0.18}
{'loss': 0.9736, 'grad_norm': 0.4122978150844574, 'learning_rate': 1.8879599873987343e-05, 'epoch': 0.18}
{'loss': 1.1058, 'grad_norm': 0.48416203260421753, 'learning_rate': 1.8876732455273022e-05, 'epoch': 0.18}
{'loss': 1.0088, 'grad_norm': 0.41082680225372314, 'learning_rate': 1.8873861590305527e-05, 'epoch': 0.18}
{'loss': 1.0044, 'grad_norm': 0.47347790002822876, 'learning_rate': 1.8870987280199428e-05, 'epoch': 0.18}
{'loss': 1.0575, 'grad_norm': 0.39474719762802124, 'learning_rate': 1.886810952607063e-05, 'epoch': 0.18}
{'loss': 1.0291, 'grad_norm': 0.4421970844268799, 'learning_rate': 1.8865228329036372e-05, 'epoch': 0.18}
{'loss': 0.9189, 'grad_norm': 0.24875977635383606, 'learning_rate': 1.886234369021524e-05, 'epoch': 0.18}
Error with image file is truncated (33 bytes not processed)
{'loss': 1.1181, 'grad_norm': 0.4484672248363495, 'learning_rate': 1.885945561072715e-05, 'epoch': 0.18}
{'loss': 1.0334, 'grad_norm': 0.40318194031715393, 'learning_rate': 1.885656409169335e-05, 'epoch': 0.18}
{'loss': 1.0323, 'grad_norm': 0.4359821379184723, 'learning_rate': 1.885366913423643e-05, 'epoch': 0.18}
{'loss': 1.0048, 'grad_norm': 0.3920655846595764, 'learning_rate': 1.8850770739480312e-05, 'epoch': 0.18}
{'loss': 1.05, 'grad_norm': 0.43009960651397705, 'learning_rate': 1.8847868908550252e-05, 'epoch': 0.18}
{'loss': 0.9831, 'grad_norm': 0.44027313590049744, 'learning_rate': 1.8844963642572837e-05, 'epoch': 0.18}
{'loss': 1.0006, 'grad_norm': 0.43206480145454407, 'learning_rate': 1.8842054942676e-05, 'epoch': 0.18}
{'loss': 1.038, 'grad_norm': 0.43244269490242004, 'learning_rate': 1.8839142809988987e-05, 'epoch': 0.18}
{'loss': 1.0463, 'grad_norm': 0.44091227650642395, 'learning_rate': 1.88362272456424e-05, 'epoch': 0.18}
{'loss': 0.8686, 'grad_norm': 0.25658419728279114, 'learning_rate': 1.8833308250768153e-05, 'epoch': 0.18}
{'loss': 1.0229, 'grad_norm': 0.4239063858985901, 'learning_rate': 1.8830385826499507e-05, 'epoch': 0.18}
{'loss': 0.995, 'grad_norm': 0.4627702832221985, 'learning_rate': 1.882745997397104e-05, 'epoch': 0.18}
{'loss': 0.9832, 'grad_norm': 0.4251687824726105, 'learning_rate': 1.8824530694318675e-05, 'epoch': 0.18}
{'loss': 1.0153, 'grad_norm': 0.3970535695552826, 'learning_rate': 1.882159798867966e-05, 'epoch': 0.18}
{'loss': 1.0481, 'grad_norm': 0.4439062476158142, 'learning_rate': 1.8818661858192562e-05, 'epoch': 0.18}
{'loss': 0.9723, 'grad_norm': 0.40596386790275574, 'learning_rate': 1.88157223039973e-05, 'epoch': 0.18}
{'loss': 0.9539, 'grad_norm': 0.40460506081581116, 'learning_rate': 1.8812779327235106e-05, 'epoch': 0.18}
{'loss': 1.1545, 'grad_norm': 0.44784989953041077, 'learning_rate': 1.880983292904854e-05, 'epoch': 0.18}
{'loss': 0.9738, 'grad_norm': 0.4268665313720703, 'learning_rate': 1.88068831105815e-05, 'epoch': 0.18}
{'loss': 0.9452, 'grad_norm': 0.367939829826355, 'learning_rate': 1.8803929872979214e-05, 'epoch': 0.18}
{'loss': 1.1031, 'grad_norm': 0.4651985466480255, 'learning_rate': 1.8800973217388215e-05, 'epoch': 0.18}
{'loss': 1.0638, 'grad_norm': 0.4207567274570465, 'learning_rate': 1.879801314495639e-05, 'epoch': 0.18}
{'loss': 1.1504, 'grad_norm': 0.47143179178237915, 'learning_rate': 1.879504965683294e-05, 'epoch': 0.18}
{'loss': 0.9457, 'grad_norm': 0.4034232795238495, 'learning_rate': 1.8792082754168385e-05, 'epoch': 0.18}
{'loss': 0.9234, 'grad_norm': 0.41445159912109375, 'learning_rate': 1.878911243811459e-05, 'epoch': 0.18}
{'loss': 1.0074, 'grad_norm': 0.44983595609664917, 'learning_rate': 1.8786138709824726e-05, 'epoch': 0.18}
{'loss': 0.9852, 'grad_norm': 0.28440162539482117, 'learning_rate': 1.8783161570453295e-05, 'epoch': 0.18}
{'loss': 1.041, 'grad_norm': 0.42564550042152405, 'learning_rate': 1.878018102115614e-05, 'epoch': 0.18}
{'loss': 0.9632, 'grad_norm': 0.39999017119407654, 'learning_rate': 1.8777197063090394e-05, 'epoch': 0.18}
{'loss': 1.0961, 'grad_norm': 0.49496203660964966, 'learning_rate': 1.877420969741454e-05, 'epoch': 0.18}
{'loss': 0.9662, 'grad_norm': 0.4358596205711365, 'learning_rate': 1.877121892528838e-05, 'epoch': 0.18}
{'loss': 1.0541, 'grad_norm': 0.4082021415233612, 'learning_rate': 1.876822474787303e-05, 'epoch': 0.19}
{'loss': 1.0362, 'grad_norm': 0.44142821431159973, 'learning_rate': 1.8765227166330933e-05, 'epoch': 0.19}
{'loss': 1.0307, 'grad_norm': 0.4289230406284332, 'learning_rate': 1.8762226181825857e-05, 'epoch': 0.19}
{'loss': 0.9904, 'grad_norm': 0.4577694237232208, 'learning_rate': 1.875922179552288e-05, 'epoch': 0.19}
{'loss': 1.1071, 'grad_norm': 0.4463246464729309, 'learning_rate': 1.875621400858842e-05, 'epoch': 0.19}
{'loss': 0.9896, 'grad_norm': 0.38573211431503296, 'learning_rate': 1.875320282219019e-05, 'epoch': 0.19}
{'loss': 1.0569, 'grad_norm': 0.42325615882873535, 'learning_rate': 1.8750188237497247e-05, 'epoch': 0.19}
{'loss': 1.1349, 'grad_norm': 0.48519453406333923, 'learning_rate': 1.874717025567995e-05, 'epoch': 0.19}
{'loss': 1.0892, 'grad_norm': 0.4468896985054016, 'learning_rate': 1.874414887790999e-05, 'epoch': 0.19}
{'loss': 1.0357, 'grad_norm': 0.41477254033088684, 'learning_rate': 1.8741124105360363e-05, 'epoch': 0.19}
{'loss': 0.9723, 'grad_norm': 0.4193262457847595, 'learning_rate': 1.873809593920539e-05, 'epoch': 0.19}
{'loss': 1.0014, 'grad_norm': 0.42457568645477295, 'learning_rate': 1.8735064380620717e-05, 'epoch': 0.19}
{'loss': 1.0508, 'grad_norm': 0.4152970612049103, 'learning_rate': 1.873202943078329e-05, 'epoch': 0.19}
{'loss': 1.0609, 'grad_norm': 0.4303061068058014, 'learning_rate': 1.8728991090871387e-05, 'epoch': 0.19}
{'loss': 1.1231, 'grad_norm': 0.4706880450248718, 'learning_rate': 1.8725949362064596e-05, 'epoch': 0.19}
{'loss': 0.9966, 'grad_norm': 0.4619007110595703, 'learning_rate': 1.8722904245543817e-05, 'epoch': 0.19}
{'loss': 0.9354, 'grad_norm': 0.431431382894516, 'learning_rate': 1.871985574249127e-05, 'epoch': 0.19}
{'loss': 1.1468, 'grad_norm': 0.5155208110809326, 'learning_rate': 1.8716803854090495e-05, 'epoch': 0.19}
{'loss': 1.0882, 'grad_norm': 0.44469019770622253, 'learning_rate': 1.8713748581526334e-05, 'epoch': 0.19}
{'loss': 0.975, 'grad_norm': 0.39743098616600037, 'learning_rate': 1.871068992598495e-05, 'epoch': 0.19}
{'loss': 0.9701, 'grad_norm': 0.39506059885025024, 'learning_rate': 1.8707627888653816e-05, 'epoch': 0.19}
{'loss': 0.9922, 'grad_norm': 0.3689383566379547, 'learning_rate': 1.8704562470721728e-05, 'epoch': 0.19}
{'loss': 0.9259, 'grad_norm': 0.4065455198287964, 'learning_rate': 1.870149367337878e-05, 'epoch': 0.19}
{'loss': 0.9336, 'grad_norm': 0.4484361708164215, 'learning_rate': 1.8698421497816386e-05, 'epoch': 0.19}
{'loss': 0.9868, 'grad_norm': 0.3884940445423126, 'learning_rate': 1.869534594522727e-05, 'epoch': 0.19}
{'loss': 0.9643, 'grad_norm': 0.40497931838035583, 'learning_rate': 1.8692267016805473e-05, 'epoch': 0.19}
{'loss': 0.8749, 'grad_norm': 0.36987558007240295, 'learning_rate': 1.8689184713746333e-05, 'epoch': 0.19}
{'loss': 0.9922, 'grad_norm': 0.41119033098220825, 'learning_rate': 1.868609903724651e-05, 'epoch': 0.19}
{'loss': 0.904, 'grad_norm': 0.22501437366008759, 'learning_rate': 1.8683009988503972e-05, 'epoch': 0.19}
{'loss': 0.9652, 'grad_norm': 0.40924084186553955, 'learning_rate': 1.867991756871799e-05, 'epoch': 0.19}
{'loss': 1.0285, 'grad_norm': 0.41929593682289124, 'learning_rate': 1.867682177908915e-05, 'epoch': 0.19}
{'loss': 1.0276, 'grad_norm': 0.40978506207466125, 'learning_rate': 1.867372262081934e-05, 'epoch': 0.19}
{'loss': 0.9847, 'grad_norm': 0.418265700340271, 'learning_rate': 1.8670620095111766e-05, 'epoch': 0.19}
{'loss': 1.1473, 'grad_norm': 0.4571625590324402, 'learning_rate': 1.8667514203170934e-05, 'epoch': 0.19}
{'loss': 0.9842, 'grad_norm': 0.4416535198688507, 'learning_rate': 1.8664404946202658e-05, 'epoch': 0.19}
{'loss': 0.9713, 'grad_norm': 0.4068574607372284, 'learning_rate': 1.8661292325414058e-05, 'epoch': 0.19}
{'loss': 0.9695, 'grad_norm': 0.4190167188644409, 'learning_rate': 1.865817634201356e-05, 'epoch': 0.19}
{'loss': 1.0541, 'grad_norm': 0.457674115896225, 'learning_rate': 1.8655056997210893e-05, 'epoch': 0.19}
{'loss': 1.074, 'grad_norm': 0.5011122822761536, 'learning_rate': 1.8651934292217097e-05, 'epoch': 0.19}
{'loss': 1.0486, 'grad_norm': 0.4295715391635895, 'learning_rate': 1.864880822824452e-05, 'epoch': 0.19}
{'loss': 0.9732, 'grad_norm': 0.4395752549171448, 'learning_rate': 1.8645678806506795e-05, 'epoch': 0.19}
{'loss': 1.0414, 'grad_norm': 0.43063482642173767, 'learning_rate': 1.864254602821888e-05, 'epoch': 0.19}
{'loss': 1.0527, 'grad_norm': 0.41767656803131104, 'learning_rate': 1.8639409894597026e-05, 'epoch': 0.19}
{'loss': 1.0318, 'grad_norm': 0.39558783173561096, 'learning_rate': 1.8636270406858786e-05, 'epoch': 0.19}
{'loss': 0.9272, 'grad_norm': 0.3983551859855652, 'learning_rate': 1.8633127566223023e-05, 'epoch': 0.19}
{'loss': 0.95, 'grad_norm': 0.4032686948776245, 'learning_rate': 1.862998137390989e-05, 'epoch': 0.19}
{'loss': 0.9773, 'grad_norm': 0.4534725546836853, 'learning_rate': 1.8626831831140845e-05, 'epoch': 0.19}
{'loss': 1.0885, 'grad_norm': 0.45824554562568665, 'learning_rate': 1.8623678939138652e-05, 'epoch': 0.19}
{'loss': 1.0344, 'grad_norm': 0.39371219277381897, 'learning_rate': 1.8620522699127374e-05, 'epoch': 0.19}
{'loss': 1.0431, 'grad_norm': 0.4713997542858124, 'learning_rate': 1.8617363112332376e-05, 'epoch': 0.19}
{'loss': 1.0226, 'grad_norm': 0.43097370862960815, 'learning_rate': 1.8614200179980307e-05, 'epoch': 0.19}
{'loss': 1.1012, 'grad_norm': 0.4452691376209259, 'learning_rate': 1.8611033903299136e-05, 'epoch': 0.19}
{'loss': 0.9638, 'grad_norm': 0.40561872720718384, 'learning_rate': 1.8607864283518116e-05, 'epoch': 0.2}
{'loss': 1.0388, 'grad_norm': 0.4441082775592804, 'learning_rate': 1.8604691321867804e-05, 'epoch': 0.2}
{'loss': 1.1422, 'grad_norm': 0.49566158652305603, 'learning_rate': 1.8601515019580053e-05, 'epoch': 0.2}
{'loss': 1.0021, 'grad_norm': 0.4143878221511841, 'learning_rate': 1.8598335377888012e-05, 'epoch': 0.2}
{'loss': 0.8956, 'grad_norm': 0.24582292139530182, 'learning_rate': 1.8595152398026128e-05, 'epoch': 0.2}
{'loss': 0.9317, 'grad_norm': 0.25302040576934814, 'learning_rate': 1.8591966081230142e-05, 'epoch': 0.2}
{'loss': 1.0466, 'grad_norm': 0.41506367921829224, 'learning_rate': 1.8588776428737095e-05, 'epoch': 0.2}
{'loss': 1.0959, 'grad_norm': 0.451498419046402, 'learning_rate': 1.858558344178532e-05, 'epoch': 0.2}
{'loss': 0.9997, 'grad_norm': 0.4283047318458557, 'learning_rate': 1.8582387121614437e-05, 'epoch': 0.2}
{'loss': 1.1941, 'grad_norm': 0.46680745482444763, 'learning_rate': 1.857918746946538e-05, 'epoch': 0.2}
{'loss': 0.9922, 'grad_norm': 0.419423907995224, 'learning_rate': 1.8575984486580353e-05, 'epoch': 0.2}
{'loss': 1.0253, 'grad_norm': 0.41150739789009094, 'learning_rate': 1.857277817420287e-05, 'epoch': 0.2}
{'loss': 1.0899, 'grad_norm': 0.434358149766922, 'learning_rate': 1.8569568533577727e-05, 'epoch': 0.2}
{'loss': 1.1074, 'grad_norm': 0.4803498089313507, 'learning_rate': 1.8566355565951023e-05, 'epoch': 0.2}
{'loss': 1.0789, 'grad_norm': 0.44682806730270386, 'learning_rate': 1.8563139272570142e-05, 'epoch': 0.2}
{'loss': 1.1265, 'grad_norm': 0.489723265171051, 'learning_rate': 1.8559919654683756e-05, 'epoch': 0.2}
{'loss': 1.1147, 'grad_norm': 0.45304417610168457, 'learning_rate': 1.8556696713541833e-05, 'epoch': 0.2}
{'loss': 0.9861, 'grad_norm': 0.4083940088748932, 'learning_rate': 1.855347045039563e-05, 'epoch': 0.2}
{'loss': 1.0629, 'grad_norm': 0.414276659488678, 'learning_rate': 1.8550240866497697e-05, 'epoch': 0.2}
{'loss': 1.0815, 'grad_norm': 0.46520787477493286, 'learning_rate': 1.854700796310186e-05, 'epoch': 0.2}
{'loss': 1.1229, 'grad_norm': 0.4897776246070862, 'learning_rate': 1.8543771741463254e-05, 'epoch': 0.2}
{'loss': 1.0501, 'grad_norm': 0.41212955117225647, 'learning_rate': 1.8540532202838286e-05, 'epoch': 0.2}
{'loss': 0.9418, 'grad_norm': 0.38985639810562134, 'learning_rate': 1.8537289348484658e-05, 'epoch': 0.2}
{'loss': 0.9864, 'grad_norm': 0.29118722677230835, 'learning_rate': 1.8534043179661357e-05, 'epoch': 0.2}
{'loss': 1.1842, 'grad_norm': 0.4753832519054413, 'learning_rate': 1.8530793697628658e-05, 'epoch': 0.2}
{'loss': 1.0449, 'grad_norm': 0.43037939071655273, 'learning_rate': 1.8527540903648122e-05, 'epoch': 0.2}
{'loss': 1.0117, 'grad_norm': 0.4382888674736023, 'learning_rate': 1.8524284798982595e-05, 'epoch': 0.2}
{'loss': 0.9772, 'grad_norm': 0.4098171591758728, 'learning_rate': 1.852102538489621e-05, 'epoch': 0.2}
{'loss': 0.9957, 'grad_norm': 0.27814507484436035, 'learning_rate': 1.8517762662654383e-05, 'epoch': 0.2}
{'loss': 1.0517, 'grad_norm': 0.44073131680488586, 'learning_rate': 1.851449663352381e-05, 'epoch': 0.2}
{'loss': 0.9671, 'grad_norm': 0.41873863339424133, 'learning_rate': 1.851122729877249e-05, 'epoch': 0.2}
{'loss': 1.0205, 'grad_norm': 0.43365713953971863, 'learning_rate': 1.8507954659669677e-05, 'epoch': 0.2}
{'loss': 0.963, 'grad_norm': 0.4252971112728119, 'learning_rate': 1.850467871748593e-05, 'epoch': 0.2}
{'loss': 1.0514, 'grad_norm': 0.39460837841033936, 'learning_rate': 1.850139947349308e-05, 'epoch': 0.2}
{'loss': 1.0913, 'grad_norm': 0.47664695978164673, 'learning_rate': 1.8498116928964244e-05, 'epoch': 0.2}
{'loss': 0.9861, 'grad_norm': 0.39932486414909363, 'learning_rate': 1.849483108517381e-05, 'epoch': 0.2}
{'loss': 0.9712, 'grad_norm': 0.41304051876068115, 'learning_rate': 1.849154194339747e-05, 'epoch': 0.2}
{'loss': 0.9856, 'grad_norm': 0.391628623008728, 'learning_rate': 1.8488249504912173e-05, 'epoch': 0.2}
{'loss': 0.9588, 'grad_norm': 0.3878483474254608, 'learning_rate': 1.8484953770996163e-05, 'epoch': 0.2}
{'loss': 0.9842, 'grad_norm': 0.4198869466781616, 'learning_rate': 1.848165474292895e-05, 'epoch': 0.2}
{'loss': 1.0671, 'grad_norm': 0.4310864210128784, 'learning_rate': 1.8478352421991334e-05, 'epoch': 0.2}
{'loss': 0.9652, 'grad_norm': 0.41374677419662476, 'learning_rate': 1.847504680946539e-05, 'epoch': 0.2}
{'loss': 0.9702, 'grad_norm': 0.40860211849212646, 'learning_rate': 1.847173790663447e-05, 'epoch': 0.2}
{'loss': 1.0557, 'grad_norm': 0.3929577171802521, 'learning_rate': 1.8468425714783206e-05, 'epoch': 0.2}
{'loss': 0.993, 'grad_norm': 0.4160633981227875, 'learning_rate': 1.84651102351975e-05, 'epoch': 0.2}
{'loss': 1.0972, 'grad_norm': 0.48192963004112244, 'learning_rate': 1.846179146916454e-05, 'epoch': 0.2}
{'loss': 0.9458, 'grad_norm': 0.2595595419406891, 'learning_rate': 1.8458469417972783e-05, 'epoch': 0.2}
{'loss': 1.0653, 'grad_norm': 0.43971771001815796, 'learning_rate': 1.8455144082911965e-05, 'epoch': 0.2}
{'loss': 0.987, 'grad_norm': 0.4263770878314972, 'learning_rate': 1.8451815465273097e-05, 'epoch': 0.2}
{'loss': 0.9931, 'grad_norm': 0.4095625877380371, 'learning_rate': 1.8448483566348456e-05, 'epoch': 0.2}
{'loss': 1.0924, 'grad_norm': 0.4535960853099823, 'learning_rate': 1.8445148387431605e-05, 'epoch': 0.2}
{'loss': 1.0264, 'grad_norm': 0.39146459102630615, 'learning_rate': 1.8441809929817382e-05, 'epoch': 0.2}
{'loss': 0.947, 'grad_norm': 0.4032754898071289, 'learning_rate': 1.8438468194801876e-05, 'epoch': 0.21}
{'loss': 0.9766, 'grad_norm': 0.4607398509979248, 'learning_rate': 1.8435123183682475e-05, 'epoch': 0.21}
{'loss': 0.9934, 'grad_norm': 0.38549137115478516, 'learning_rate': 1.8431774897757824e-05, 'epoch': 0.21}
{'loss': 1.0918, 'grad_norm': 0.4178251028060913, 'learning_rate': 1.8428423338327847e-05, 'epoch': 0.21}
{'loss': 0.991, 'grad_norm': 0.4136204421520233, 'learning_rate': 1.8425068506693727e-05, 'epoch': 0.21}
{'loss': 0.9913, 'grad_norm': 0.43384236097335815, 'learning_rate': 1.842171040415793e-05, 'epoch': 0.21}
{'loss': 1.0485, 'grad_norm': 0.43201926350593567, 'learning_rate': 1.8418349032024185e-05, 'epoch': 0.21}
{'loss': 0.9913, 'grad_norm': 0.4078894853591919, 'learning_rate': 1.8414984391597492e-05, 'epoch': 0.21}
{'loss': 1.0566, 'grad_norm': 0.4665803611278534, 'learning_rate': 1.8411616484184126e-05, 'epoch': 0.21}
{'loss': 0.9384, 'grad_norm': 0.4144570529460907, 'learning_rate': 1.8408245311091618e-05, 'epoch': 0.21}
{'loss': 1.0389, 'grad_norm': 0.4489086866378784, 'learning_rate': 1.8404870873628774e-05, 'epoch': 0.21}
{'loss': 1.0407, 'grad_norm': 0.4322929382324219, 'learning_rate': 1.8401493173105675e-05, 'epoch': 0.21}
{'loss': 0.9967, 'grad_norm': 0.4026798605918884, 'learning_rate': 1.8398112210833648e-05, 'epoch': 0.21}
{'loss': 0.9778, 'grad_norm': 0.3939124047756195, 'learning_rate': 1.8394727988125308e-05, 'epoch': 0.21}
{'loss': 0.9322, 'grad_norm': 0.39590418338775635, 'learning_rate': 1.8391340506294524e-05, 'epoch': 0.21}
{'loss': 0.9868, 'grad_norm': 0.41983190178871155, 'learning_rate': 1.8387949766656434e-05, 'epoch': 0.21}
{'loss': 0.9736, 'grad_norm': 0.43455156683921814, 'learning_rate': 1.8384555770527438e-05, 'epoch': 0.21}
{'loss': 0.998, 'grad_norm': 0.41227447986602783, 'learning_rate': 1.8381158519225204e-05, 'epoch': 0.21}
{'loss': 1.0191, 'grad_norm': 0.4274974763393402, 'learning_rate': 1.8377758014068662e-05, 'epoch': 0.21}
{'loss': 0.9916, 'grad_norm': 0.3196960985660553, 'learning_rate': 1.8374354256378e-05, 'epoch': 0.21}
{'loss': 1.0893, 'grad_norm': 0.49858394265174866, 'learning_rate': 1.837094724747468e-05, 'epoch': 0.21}
{'loss': 1.0191, 'grad_norm': 0.4570574164390564, 'learning_rate': 1.8367536988681422e-05, 'epoch': 0.21}
{'loss': 1.0887, 'grad_norm': 0.4863601624965668, 'learning_rate': 1.83641234813222e-05, 'epoch': 0.21}
{'loss': 0.8964, 'grad_norm': 0.4486962556838989, 'learning_rate': 1.8360706726722253e-05, 'epoch': 0.21}
{'loss': 1.0308, 'grad_norm': 0.4398791790008545, 'learning_rate': 1.835728672620809e-05, 'epoch': 0.21}
{'loss': 1.0533, 'grad_norm': 0.4551565945148468, 'learning_rate': 1.8353863481107473e-05, 'epoch': 0.21}
{'loss': 0.9783, 'grad_norm': 0.46737611293792725, 'learning_rate': 1.835043699274942e-05, 'epoch': 0.21}
{'loss': 0.9974, 'grad_norm': 0.4905904233455658, 'learning_rate': 1.8347007262464206e-05, 'epoch': 0.21}
Error with image file is truncated (60 bytes not processed)
{'loss': 0.9154, 'grad_norm': 0.4364282786846161, 'learning_rate': 1.8343574291583385e-05, 'epoch': 0.21}
{'loss': 0.9911, 'grad_norm': 0.40504685044288635, 'learning_rate': 1.8340138081439743e-05, 'epoch': 0.21}
{'loss': 1.037, 'grad_norm': 0.4519375264644623, 'learning_rate': 1.833669863336734e-05, 'epoch': 0.21}
{'loss': 1.074, 'grad_norm': 0.4419800639152527, 'learning_rate': 1.833325594870148e-05, 'epoch': 0.21}
{'loss': 1.0293, 'grad_norm': 0.4252164363861084, 'learning_rate': 1.8329810028778747e-05, 'epoch': 0.21}
{'loss': 1.0777, 'grad_norm': 0.4817918837070465, 'learning_rate': 1.8326360874936952e-05, 'epoch': 0.21}
{'loss': 1.0112, 'grad_norm': 0.4355113208293915, 'learning_rate': 1.8322908488515182e-05, 'epoch': 0.21}
{'loss': 1.0707, 'grad_norm': 0.4415231943130493, 'learning_rate': 1.8319452870853772e-05, 'epoch': 0.21}
{'loss': 1.0466, 'grad_norm': 0.42759567499160767, 'learning_rate': 1.8315994023294306e-05, 'epoch': 0.21}
{'loss': 1.1296, 'grad_norm': 0.44391584396362305, 'learning_rate': 1.8312531947179634e-05, 'epoch': 0.21}
{'loss': 1.0247, 'grad_norm': 0.4174475371837616, 'learning_rate': 1.8309066643853854e-05, 'epoch': 0.21}
{'loss': 1.0737, 'grad_norm': 0.4244873523712158, 'learning_rate': 1.8305598114662312e-05, 'epoch': 0.21}
{'loss': 1.0307, 'grad_norm': 0.45821326971054077, 'learning_rate': 1.830212636095161e-05, 'epoch': 0.21}
{'loss': 0.8998, 'grad_norm': 0.2582100033760071, 'learning_rate': 1.8298651384069605e-05, 'epoch': 0.21}
{'loss': 1.0352, 'grad_norm': 0.4551178216934204, 'learning_rate': 1.8295173185365405e-05, 'epoch': 0.21}
{'loss': 1.0694, 'grad_norm': 0.47094348073005676, 'learning_rate': 1.829169176618936e-05, 'epoch': 0.21}
{'loss': 1.0528, 'grad_norm': 0.43810880184173584, 'learning_rate': 1.828820712789308e-05, 'epoch': 0.21}
{'loss': 1.0901, 'grad_norm': 0.45715659856796265, 'learning_rate': 1.828471927182942e-05, 'epoch': 0.21}
{'loss': 0.966, 'grad_norm': 0.43265554308891296, 'learning_rate': 1.828122819935249e-05, 'epoch': 0.21}
{'loss': 0.9278, 'grad_norm': 0.40952372550964355, 'learning_rate': 1.8277733911817642e-05, 'epoch': 0.21}
{'loss': 1.0336, 'grad_norm': 0.43592020869255066, 'learning_rate': 1.8274236410581478e-05, 'epoch': 0.21}
{'loss': 0.9791, 'grad_norm': 0.4754442572593689, 'learning_rate': 1.827073569700185e-05, 'epoch': 0.21}
{'loss': 0.9866, 'grad_norm': 0.4295155704021454, 'learning_rate': 1.8267231772437854e-05, 'epoch': 0.21}
{'loss': 0.968, 'grad_norm': 0.386636346578598, 'learning_rate': 1.8263724638249834e-05, 'epoch': 0.21}
{'loss': 1.0061, 'grad_norm': 0.42816945910453796, 'learning_rate': 1.8260214295799382e-05, 'epoch': 0.22}
{'loss': 1.0003, 'grad_norm': 0.26138943433761597, 'learning_rate': 1.825670074644933e-05, 'epoch': 0.22}
{'loss': 1.0364, 'grad_norm': 0.4312153160572052, 'learning_rate': 1.8253183991563768e-05, 'epoch': 0.22}
{'loss': 1.0274, 'grad_norm': 0.4492298662662506, 'learning_rate': 1.824966403250801e-05, 'epoch': 0.22}
{'loss': 0.9127, 'grad_norm': 0.22923558950424194, 'learning_rate': 1.8246140870648633e-05, 'epoch': 0.22}
{'loss': 0.987, 'grad_norm': 0.4222768247127533, 'learning_rate': 1.8242614507353446e-05, 'epoch': 0.22}
{'loss': 1.0379, 'grad_norm': 0.4876191318035126, 'learning_rate': 1.8239084943991507e-05, 'epoch': 0.22}
{'loss': 1.0085, 'grad_norm': 0.4535054564476013, 'learning_rate': 1.823555218193311e-05, 'epoch': 0.22}
{'loss': 0.968, 'grad_norm': 0.24957279860973358, 'learning_rate': 1.8232016222549797e-05, 'epoch': 0.22}
{'loss': 1.1009, 'grad_norm': 0.481854647397995, 'learning_rate': 1.8228477067214352e-05, 'epoch': 0.22}
{'loss': 1.1135, 'grad_norm': 0.4832991659641266, 'learning_rate': 1.8224934717300794e-05, 'epoch': 0.22}
{'loss': 0.9256, 'grad_norm': 0.4182541072368622, 'learning_rate': 1.8221389174184385e-05, 'epoch': 0.22}
{'loss': 1.041, 'grad_norm': 0.3949047029018402, 'learning_rate': 1.8217840439241633e-05, 'epoch': 0.22}
{'loss': 0.9459, 'grad_norm': 0.43836650252342224, 'learning_rate': 1.8214288513850267e-05, 'epoch': 0.22}
{'loss': 0.9535, 'grad_norm': 0.3699791133403778, 'learning_rate': 1.8210733399389277e-05, 'epoch': 0.22}
{'loss': 1.0226, 'grad_norm': 0.414223849773407, 'learning_rate': 1.820717509723888e-05, 'epoch': 0.22}
{'loss': 1.0285, 'grad_norm': 0.4342537820339203, 'learning_rate': 1.8203613608780525e-05, 'epoch': 0.22}
{'loss': 0.9196, 'grad_norm': 0.41206443309783936, 'learning_rate': 1.8200048935396908e-05, 'epoch': 0.22}
{'loss': 1.0397, 'grad_norm': 0.4457535743713379, 'learning_rate': 1.819648107847196e-05, 'epoch': 0.22}
{'loss': 1.0153, 'grad_norm': 0.42624521255493164, 'learning_rate': 1.8192910039390844e-05, 'epoch': 0.22}
{'loss': 0.949, 'grad_norm': 0.36745840311050415, 'learning_rate': 1.8189335819539963e-05, 'epoch': 0.22}
{'loss': 1.0444, 'grad_norm': 0.4411306381225586, 'learning_rate': 1.8185758420306947e-05, 'epoch': 0.22}
{'loss': 0.9572, 'grad_norm': 0.30296045541763306, 'learning_rate': 1.818217784308067e-05, 'epoch': 0.22}
{'loss': 0.9569, 'grad_norm': 0.46880102157592773, 'learning_rate': 1.817859408925123e-05, 'epoch': 0.22}
{'loss': 0.9602, 'grad_norm': 0.4343229830265045, 'learning_rate': 1.817500716020997e-05, 'epoch': 0.22}
{'loss': 0.9895, 'grad_norm': 0.4575192928314209, 'learning_rate': 1.8171417057349457e-05, 'epoch': 0.22}
{'loss': 1.0842, 'grad_norm': 0.42674291133880615, 'learning_rate': 1.816782378206349e-05, 'epoch': 0.22}
{'loss': 1.0452, 'grad_norm': 0.4721362888813019, 'learning_rate': 1.8164227335747108e-05, 'epoch': 0.22}
{'loss': 1.0764, 'grad_norm': 0.45544615387916565, 'learning_rate': 1.8160627719796568e-05, 'epoch': 0.22}
{'loss': 1.0264, 'grad_norm': 0.41527485847473145, 'learning_rate': 1.815702493560937e-05, 'epoch': 0.22}
{'loss': 1.0182, 'grad_norm': 0.4299886226654053, 'learning_rate': 1.8153418984584238e-05, 'epoch': 0.22}
{'loss': 0.9819, 'grad_norm': 0.445029079914093, 'learning_rate': 1.8149809868121125e-05, 'epoch': 0.22}
{'loss': 1.0781, 'grad_norm': 0.4709028899669647, 'learning_rate': 1.8146197587621217e-05, 'epoch': 0.22}
{'loss': 1.0825, 'grad_norm': 0.4153733253479004, 'learning_rate': 1.814258214448692e-05, 'epoch': 0.22}
{'loss': 0.9672, 'grad_norm': 0.40072402358055115, 'learning_rate': 1.8138963540121878e-05, 'epoch': 0.22}
{'loss': 1.0198, 'grad_norm': 0.426738440990448, 'learning_rate': 1.813534177593096e-05, 'epoch': 0.22}
{'loss': 0.9327, 'grad_norm': 0.4063494801521301, 'learning_rate': 1.8131716853320254e-05, 'epoch': 0.22}
{'loss': 1.1, 'grad_norm': 0.4897335469722748, 'learning_rate': 1.8128088773697086e-05, 'epoch': 0.22}
{'loss': 0.9262, 'grad_norm': 0.4169764220714569, 'learning_rate': 1.8124457538469996e-05, 'epoch': 0.22}
{'loss': 1.0945, 'grad_norm': 0.49908778071403503, 'learning_rate': 1.8120823149048753e-05, 'epoch': 0.22}
{'loss': 1.0134, 'grad_norm': 0.27713483572006226, 'learning_rate': 1.811718560684436e-05, 'epoch': 0.22}
{'loss': 1.0262, 'grad_norm': 0.3978288769721985, 'learning_rate': 1.8113544913269025e-05, 'epoch': 0.22}
{'loss': 0.9814, 'grad_norm': 0.42449209094047546, 'learning_rate': 1.8109901069736202e-05, 'epoch': 0.22}
{'loss': 1.0905, 'grad_norm': 0.5020301342010498, 'learning_rate': 1.8106254077660552e-05, 'epoch': 0.22}
{'loss': 1.027, 'grad_norm': 0.4279331564903259, 'learning_rate': 1.810260393845796e-05, 'epoch': 0.22}
{'loss': 1.0161, 'grad_norm': 0.4144127368927002, 'learning_rate': 1.809895065354554e-05, 'epoch': 0.22}
{'loss': 1.0482, 'grad_norm': 0.4521058201789856, 'learning_rate': 1.8095294224341622e-05, 'epoch': 0.22}
{'loss': 1.04, 'grad_norm': 0.466935396194458, 'learning_rate': 1.8091634652265755e-05, 'epoch': 0.22}
{'loss': 1.0189, 'grad_norm': 0.4187766909599304, 'learning_rate': 1.8087971938738715e-05, 'epoch': 0.22}
{'loss': 0.9704, 'grad_norm': 0.42382505536079407, 'learning_rate': 1.808430608518249e-05, 'epoch': 0.22}
{'loss': 1.0076, 'grad_norm': 0.4083283841609955, 'learning_rate': 1.808063709302029e-05, 'epoch': 0.22}
{'loss': 1.0237, 'grad_norm': 0.4022526144981384, 'learning_rate': 1.807696496367655e-05, 'epoch': 0.22}
{'loss': 0.9695, 'grad_norm': 0.4199587404727936, 'learning_rate': 1.8073289698576913e-05, 'epoch': 0.23}
{'loss': 1.0446, 'grad_norm': 0.4273192584514618, 'learning_rate': 1.8069611299148236e-05, 'epoch': 0.23}
{'loss': 0.865, 'grad_norm': 0.23862697184085846, 'learning_rate': 1.8065929766818617e-05, 'epoch': 0.23}
{'loss': 0.9838, 'grad_norm': 0.41678768396377563, 'learning_rate': 1.806224510301734e-05, 'epoch': 0.23}
{'loss': 1.0553, 'grad_norm': 0.4794909358024597, 'learning_rate': 1.8058557309174926e-05, 'epoch': 0.23}
{'loss': 1.0089, 'grad_norm': 0.4227304756641388, 'learning_rate': 1.8054866386723096e-05, 'epoch': 0.23}
{'loss': 1.0374, 'grad_norm': 0.4342135488986969, 'learning_rate': 1.80511723370948e-05, 'epoch': 0.23}
{'loss': 0.9506, 'grad_norm': 0.38256165385246277, 'learning_rate': 1.804747516172419e-05, 'epoch': 0.23}
{'loss': 0.9886, 'grad_norm': 0.40115654468536377, 'learning_rate': 1.8043774862046644e-05, 'epoch': 0.23}
{'loss': 0.9391, 'grad_norm': 0.4529995620250702, 'learning_rate': 1.804007143949874e-05, 'epoch': 0.23}
{'loss': 0.932, 'grad_norm': 0.4156434237957001, 'learning_rate': 1.8036364895518272e-05, 'epoch': 0.23}
{'loss': 1.0363, 'grad_norm': 0.45274418592453003, 'learning_rate': 1.8032655231544253e-05, 'epoch': 0.23}
{'loss': 1.0543, 'grad_norm': 0.507453203201294, 'learning_rate': 1.8028942449016903e-05, 'epoch': 0.23}
{'loss': 0.9745, 'grad_norm': 0.4382113814353943, 'learning_rate': 1.8025226549377647e-05, 'epoch': 0.23}
{'loss': 0.9248, 'grad_norm': 0.4115496873855591, 'learning_rate': 1.8021507534069133e-05, 'epoch': 0.23}
{'loss': 1.0359, 'grad_norm': 0.4743254780769348, 'learning_rate': 1.8017785404535198e-05, 'epoch': 0.23}
{'loss': 0.9716, 'grad_norm': 0.4470934569835663, 'learning_rate': 1.8014060162220916e-05, 'epoch': 0.23}
{'loss': 1.0833, 'grad_norm': 0.4282122552394867, 'learning_rate': 1.801033180857254e-05, 'epoch': 0.23}
{'loss': 1.0147, 'grad_norm': 0.5524799227714539, 'learning_rate': 1.8006600345037558e-05, 'epoch': 0.23}
{'loss': 0.9758, 'grad_norm': 0.44078418612480164, 'learning_rate': 1.8002865773064644e-05, 'epoch': 0.23}
{'loss': 1.0024, 'grad_norm': 0.4253000319004059, 'learning_rate': 1.799912809410369e-05, 'epoch': 0.23}
{'loss': 1.0039, 'grad_norm': 0.43953949213027954, 'learning_rate': 1.799538730960579e-05, 'epoch': 0.23}
{'loss': 0.9737, 'grad_norm': 0.397036612033844, 'learning_rate': 1.799164342102325e-05, 'epoch': 0.23}
{'loss': 0.917, 'grad_norm': 0.26162102818489075, 'learning_rate': 1.7987896429809573e-05, 'epoch': 0.23}
{'loss': 0.9008, 'grad_norm': 0.447477251291275, 'learning_rate': 1.798414633741947e-05, 'epoch': 0.23}
{'loss': 1.0241, 'grad_norm': 0.43219879269599915, 'learning_rate': 1.7980393145308857e-05, 'epoch': 0.23}
{'loss': 1.0564, 'grad_norm': 0.5038734078407288, 'learning_rate': 1.797663685493485e-05, 'epoch': 0.23}
{'loss': 1.0882, 'grad_norm': 0.4776918888092041, 'learning_rate': 1.7972877467755777e-05, 'epoch': 0.23}
{'loss': 1.0342, 'grad_norm': 0.4167887270450592, 'learning_rate': 1.7969114985231152e-05, 'epoch': 0.23}
{'loss': 0.9762, 'grad_norm': 0.43237683176994324, 'learning_rate': 1.796534940882171e-05, 'epoch': 0.23}
{'loss': 1.0866, 'grad_norm': 0.42617735266685486, 'learning_rate': 1.7961580739989365e-05, 'epoch': 0.23}
{'loss': 1.0872, 'grad_norm': 0.43477997183799744, 'learning_rate': 1.795780898019726e-05, 'epoch': 0.23}
{'loss': 1.0285, 'grad_norm': 0.45929449796676636, 'learning_rate': 1.795403413090971e-05, 'epoch': 0.23}
{'loss': 0.9841, 'grad_norm': 0.43494006991386414, 'learning_rate': 1.7950256193592243e-05, 'epoch': 0.23}
{'loss': 0.9566, 'grad_norm': 0.42436447739601135, 'learning_rate': 1.794647516971159e-05, 'epoch': 0.23}
{'loss': 0.9748, 'grad_norm': 0.43535834550857544, 'learning_rate': 1.7942691060735666e-05, 'epoch': 0.23}
{'loss': 0.9781, 'grad_norm': 0.4922892153263092, 'learning_rate': 1.79389038681336e-05, 'epoch': 0.23}
{'loss': 0.9421, 'grad_norm': 0.4219880700111389, 'learning_rate': 1.7935113593375707e-05, 'epoch': 0.23}
{'loss': 1.0613, 'grad_norm': 0.3990215063095093, 'learning_rate': 1.7931320237933503e-05, 'epoch': 0.23}
{'loss': 1.0221, 'grad_norm': 0.45335277915000916, 'learning_rate': 1.79275238032797e-05, 'epoch': 0.23}
{'loss': 1.0746, 'grad_norm': 0.473766565322876, 'learning_rate': 1.7923724290888205e-05, 'epoch': 0.23}
{'loss': 1.0724, 'grad_norm': 0.4799131751060486, 'learning_rate': 1.791992170223412e-05, 'epoch': 0.23}
{'loss': 1.0119, 'grad_norm': 0.42608389258384705, 'learning_rate': 1.791611603879374e-05, 'epoch': 0.23}
{'loss': 1.0335, 'grad_norm': 0.4473757743835449, 'learning_rate': 1.791230730204455e-05, 'epoch': 0.23}
{'loss': 1.0808, 'grad_norm': 0.41714271903038025, 'learning_rate': 1.7908495493465236e-05, 'epoch': 0.23}
{'loss': 0.967, 'grad_norm': 0.41813868284225464, 'learning_rate': 1.7904680614535675e-05, 'epoch': 0.23}
{'loss': 0.96, 'grad_norm': 0.40076521039009094, 'learning_rate': 1.7900862666736935e-05, 'epoch': 0.23}
{'loss': 1.0309, 'grad_norm': 0.4575834572315216, 'learning_rate': 1.789704165155127e-05, 'epoch': 0.23}
{'loss': 1.0459, 'grad_norm': 0.4327254891395569, 'learning_rate': 1.7893217570462134e-05, 'epoch': 0.23}
{'loss': 1.0466, 'grad_norm': 0.42593228816986084, 'learning_rate': 1.7889390424954168e-05, 'epoch': 0.23}
{'loss': 1.0362, 'grad_norm': 0.42639076709747314, 'learning_rate': 1.78855602165132e-05, 'epoch': 0.23}
{'loss': 0.9652, 'grad_norm': 0.45374181866645813, 'learning_rate': 1.7881726946626244e-05, 'epoch': 0.23}
{'loss': 1.0069, 'grad_norm': 0.3924316465854645, 'learning_rate': 1.787789061678151e-05, 'epoch': 0.24}
{'loss': 0.987, 'grad_norm': 0.42137861251831055, 'learning_rate': 1.78740512284684e-05, 'epoch': 0.24}
{'loss': 0.9565, 'grad_norm': 0.40218180418014526, 'learning_rate': 1.787020878317749e-05, 'epoch': 0.24}
{'loss': 0.9884, 'grad_norm': 0.42767953872680664, 'learning_rate': 1.7866363282400555e-05, 'epoch': 0.24}
{'loss': 1.1311, 'grad_norm': 0.4650877118110657, 'learning_rate': 1.7862514727630543e-05, 'epoch': 0.24}
{'loss': 1.1178, 'grad_norm': 0.44652310013771057, 'learning_rate': 1.7858663120361597e-05, 'epoch': 0.24}
{'loss': 0.9659, 'grad_norm': 0.393817663192749, 'learning_rate': 1.785480846208905e-05, 'epoch': 0.24}
{'loss': 0.9671, 'grad_norm': 0.40983256697654724, 'learning_rate': 1.7850950754309405e-05, 'epoch': 0.24}
{'loss': 1.0386, 'grad_norm': 0.43576475977897644, 'learning_rate': 1.7847089998520365e-05, 'epoch': 0.24}
{'loss': 0.9643, 'grad_norm': 0.38125941157341003, 'learning_rate': 1.7843226196220803e-05, 'epoch': 0.24}
{'loss': 1.0315, 'grad_norm': 0.40340152382850647, 'learning_rate': 1.783935934891078e-05, 'epoch': 0.24}
{'loss': 0.967, 'grad_norm': 0.41759899258613586, 'learning_rate': 1.7835489458091544e-05, 'epoch': 0.24}
{'loss': 1.1331, 'grad_norm': 0.44216811656951904, 'learning_rate': 1.7831616525265515e-05, 'epoch': 0.24}
{'loss': 1.1139, 'grad_norm': 0.4664892256259918, 'learning_rate': 1.7827740551936296e-05, 'epoch': 0.24}
{'loss': 1.09, 'grad_norm': 0.41891467571258545, 'learning_rate': 1.7823861539608686e-05, 'epoch': 0.24}
{'loss': 0.9663, 'grad_norm': 0.4189101457595825, 'learning_rate': 1.7819979489788638e-05, 'epoch': 0.24}
{'loss': 1.0605, 'grad_norm': 0.4525635242462158, 'learning_rate': 1.7816094403983298e-05, 'epoch': 0.24}
{'loss': 0.9888, 'grad_norm': 0.43342292308807373, 'learning_rate': 1.7812206283701002e-05, 'epoch': 0.24}
{'loss': 0.9558, 'grad_norm': 0.4218040406703949, 'learning_rate': 1.7808315130451244e-05, 'epoch': 0.24}
{'loss': 1.0496, 'grad_norm': 0.4175841212272644, 'learning_rate': 1.78044209457447e-05, 'epoch': 0.24}
{'loss': 1.0569, 'grad_norm': 0.4797418713569641, 'learning_rate': 1.7800523731093232e-05, 'epoch': 0.24}
WARNING: tokenization mismatch: 0 vs. 1275. (ignored)
{'loss': 1.0076, 'grad_norm': 0.4160287380218506, 'learning_rate': 1.7796623488009875e-05, 'epoch': 0.24}
{'loss': 0.9984, 'grad_norm': 0.40502288937568665, 'learning_rate': 1.7792720218008826e-05, 'epoch': 0.24}
{'loss': 1.0633, 'grad_norm': 0.4616585075855255, 'learning_rate': 1.7788813922605488e-05, 'epoch': 0.24}
{'loss': 1.0002, 'grad_norm': 0.4327195882797241, 'learning_rate': 1.7784904603316402e-05, 'epoch': 0.24}
{'loss': 0.9956, 'grad_norm': 0.42308714985847473, 'learning_rate': 1.7780992261659305e-05, 'epoch': 0.24}
{'loss': 0.957, 'grad_norm': 0.4320797622203827, 'learning_rate': 1.777707689915311e-05, 'epoch': 0.24}
{'loss': 1.0684, 'grad_norm': 0.43490612506866455, 'learning_rate': 1.777315851731789e-05, 'epoch': 0.24}
{'loss': 1.0287, 'grad_norm': 0.42475026845932007, 'learning_rate': 1.7769237117674893e-05, 'epoch': 0.24}
{'loss': 1.0058, 'grad_norm': 0.43532443046569824, 'learning_rate': 1.7765312701746543e-05, 'epoch': 0.24}
{'loss': 1.0256, 'grad_norm': 0.4120900630950928, 'learning_rate': 1.7761385271056436e-05, 'epoch': 0.24}
{'loss': 0.9408, 'grad_norm': 0.266816109418869, 'learning_rate': 1.7757454827129338e-05, 'epoch': 0.24}
{'loss': 0.9977, 'grad_norm': 0.46236658096313477, 'learning_rate': 1.7753521371491174e-05, 'epoch': 0.24}
{'loss': 1.0821, 'grad_norm': 0.4932697117328644, 'learning_rate': 1.7749584905669057e-05, 'epoch': 0.24}
{'loss': 1.046, 'grad_norm': 0.46591633558273315, 'learning_rate': 1.774564543119125e-05, 'epoch': 0.24}
{'loss': 1.1304, 'grad_norm': 0.4643985629081726, 'learning_rate': 1.7741702949587196e-05, 'epoch': 0.24}
{'loss': 1.1156, 'grad_norm': 0.439439058303833, 'learning_rate': 1.7737757462387507e-05, 'epoch': 0.24}
{'loss': 0.9682, 'grad_norm': 0.4146701693534851, 'learning_rate': 1.7733808971123946e-05, 'epoch': 0.24}
{'loss': 0.9608, 'grad_norm': 0.4438614845275879, 'learning_rate': 1.7729857477329463e-05, 'epoch': 0.24}
{'loss': 0.9706, 'grad_norm': 0.39360547065734863, 'learning_rate': 1.7725902982538162e-05, 'epoch': 0.24}
{'loss': 0.9362, 'grad_norm': 0.43721768260002136, 'learning_rate': 1.772194548828531e-05, 'epoch': 0.24}
{'loss': 0.9311, 'grad_norm': 0.25563541054725647, 'learning_rate': 1.7717984996107346e-05, 'epoch': 0.24}
{'loss': 0.9767, 'grad_norm': 0.4025355577468872, 'learning_rate': 1.771402150754187e-05, 'epoch': 0.24}
{'loss': 0.978, 'grad_norm': 0.43069109320640564, 'learning_rate': 1.7710055024127637e-05, 'epoch': 0.24}
{'loss': 1.0482, 'grad_norm': 0.407428503036499, 'learning_rate': 1.7706085547404582e-05, 'epoch': 0.24}
{'loss': 1.0093, 'grad_norm': 0.4288380742073059, 'learning_rate': 1.770211307891379e-05, 'epoch': 0.24}
{'loss': 0.9742, 'grad_norm': 0.4361400008201599, 'learning_rate': 1.769813762019751e-05, 'epoch': 0.24}
{'loss': 1.0934, 'grad_norm': 0.47468045353889465, 'learning_rate': 1.769415917279915e-05, 'epoch': 0.24}
{'loss': 1.0281, 'grad_norm': 0.44525909423828125, 'learning_rate': 1.7690177738263284e-05, 'epoch': 0.24}
{'loss': 1.0647, 'grad_norm': 0.40503036975860596, 'learning_rate': 1.7686193318135635e-05, 'epoch': 0.24}
{'loss': 0.9988, 'grad_norm': 0.404752641916275, 'learning_rate': 1.76822059139631e-05, 'epoch': 0.24}
{'loss': 1.0064, 'grad_norm': 0.42745545506477356, 'learning_rate': 1.7678215527293724e-05, 'epoch': 0.24}
{'loss': 1.0733, 'grad_norm': 0.43008872866630554, 'learning_rate': 1.767422215967671e-05, 'epoch': 0.25}
{'loss': 1.1192, 'grad_norm': 0.4389473497867584, 'learning_rate': 1.767022581266242e-05, 'epoch': 0.25}
{'loss': 1.0157, 'grad_norm': 0.5149773359298706, 'learning_rate': 1.766622648780238e-05, 'epoch': 0.25}
{'loss': 1.1045, 'grad_norm': 0.45430901646614075, 'learning_rate': 1.766222418664926e-05, 'epoch': 0.25}
{'loss': 0.9936, 'grad_norm': 0.3753800094127655, 'learning_rate': 1.765821891075689e-05, 'epoch': 0.25}
{'loss': 1.0569, 'grad_norm': 0.40501973032951355, 'learning_rate': 1.7654210661680263e-05, 'epoch': 0.25}
{'loss': 0.979, 'grad_norm': 0.29001057147979736, 'learning_rate': 1.765019944097551e-05, 'epoch': 0.25}
{'loss': 0.886, 'grad_norm': 0.2524016797542572, 'learning_rate': 1.7646185250199936e-05, 'epoch': 0.25}
{'loss': 1.1713, 'grad_norm': 0.513990044593811, 'learning_rate': 1.7642168090911976e-05, 'epoch': 0.25}
{'loss': 0.9736, 'grad_norm': 0.4231596887111664, 'learning_rate': 1.763814796467124e-05, 'epoch': 0.25}
{'loss': 1.0577, 'grad_norm': 0.4415467083454132, 'learning_rate': 1.763412487303847e-05, 'epoch': 0.25}
{'loss': 1.0762, 'grad_norm': 0.4812415540218353, 'learning_rate': 1.7630098817575578e-05, 'epoch': 0.25}
{'loss': 1.0019, 'grad_norm': 0.437515527009964, 'learning_rate': 1.762606979984561e-05, 'epoch': 0.25}
{'loss': 1.0178, 'grad_norm': 0.5015124082565308, 'learning_rate': 1.7622037821412775e-05, 'epoch': 0.25}
{'loss': 0.9976, 'grad_norm': 0.42662736773490906, 'learning_rate': 1.7618002883842426e-05, 'epoch': 0.25}
{'loss': 1.1285, 'grad_norm': 0.4272167384624481, 'learning_rate': 1.7613964988701057e-05, 'epoch': 0.25}
{'loss': 1.0236, 'grad_norm': 0.525726854801178, 'learning_rate': 1.7609924137556326e-05, 'epoch': 0.25}
{'loss': 1.037, 'grad_norm': 0.41879409551620483, 'learning_rate': 1.7605880331977022e-05, 'epoch': 0.25}
{'loss': 1.0897, 'grad_norm': 0.42580708861351013, 'learning_rate': 1.76018335735331e-05, 'epoch': 0.25}
{'loss': 0.9616, 'grad_norm': 0.4352656602859497, 'learning_rate': 1.7597783863795644e-05, 'epoch': 0.25}
{'loss': 1.0463, 'grad_norm': 0.4051126539707184, 'learning_rate': 1.7593731204336895e-05, 'epoch': 0.25}
{'loss': 1.0499, 'grad_norm': 0.40218278765678406, 'learning_rate': 1.7589675596730233e-05, 'epoch': 0.25}
{'loss': 1.0417, 'grad_norm': 0.41326797008514404, 'learning_rate': 1.758561704255018e-05, 'epoch': 0.25}
{'loss': 1.07, 'grad_norm': 0.4555625319480896, 'learning_rate': 1.7581555543372413e-05, 'epoch': 0.25}
{'loss': 1.0183, 'grad_norm': 0.4449767768383026, 'learning_rate': 1.7577491100773744e-05, 'epoch': 0.25}
{'loss': 0.9773, 'grad_norm': 0.4316007196903229, 'learning_rate': 1.7573423716332128e-05, 'epoch': 0.25}
{'loss': 1.0465, 'grad_norm': 0.44090113043785095, 'learning_rate': 1.7569353391626665e-05, 'epoch': 0.25}
{'loss': 0.9192, 'grad_norm': 0.24413147568702698, 'learning_rate': 1.7565280128237595e-05, 'epoch': 0.25}
{'loss': 1.0016, 'grad_norm': 0.4549088180065155, 'learning_rate': 1.75612039277463e-05, 'epoch': 0.25}
{'loss': 0.9657, 'grad_norm': 0.39873722195625305, 'learning_rate': 1.75571247917353e-05, 'epoch': 0.25}
{'loss': 0.9907, 'grad_norm': 0.2845399081707001, 'learning_rate': 1.7553042721788255e-05, 'epoch': 0.25}
{'loss': 1.1618, 'grad_norm': 0.5076539516448975, 'learning_rate': 1.754895771948997e-05, 'epoch': 0.25}
{'loss': 1.054, 'grad_norm': 0.47569355368614197, 'learning_rate': 1.754486978642637e-05, 'epoch': 0.25}
{'loss': 1.1141, 'grad_norm': 0.48538511991500854, 'learning_rate': 1.7540778924184553e-05, 'epoch': 0.25}
{'loss': 1.0276, 'grad_norm': 0.4290929436683655, 'learning_rate': 1.7536685134352717e-05, 'epoch': 0.25}
{'loss': 1.1678, 'grad_norm': 0.5078533887863159, 'learning_rate': 1.7532588418520215e-05, 'epoch': 0.25}
{'loss': 0.9821, 'grad_norm': 0.4516637325286865, 'learning_rate': 1.7528488778277535e-05, 'epoch': 0.25}
{'loss': 1.0339, 'grad_norm': 0.4545394480228424, 'learning_rate': 1.75243862152163e-05, 'epoch': 0.25}
{'loss': 1.0466, 'grad_norm': 0.43859297037124634, 'learning_rate': 1.752028073092926e-05, 'epoch': 0.25}
{'loss': 1.0265, 'grad_norm': 0.42347970604896545, 'learning_rate': 1.7516172327010314e-05, 'epoch': 0.25}
{'loss': 1.0379, 'grad_norm': 0.43073004484176636, 'learning_rate': 1.751206100505448e-05, 'epoch': 0.25}
{'loss': 0.9733, 'grad_norm': 0.44351014494895935, 'learning_rate': 1.7507946766657914e-05, 'epoch': 0.25}
{'loss': 0.9714, 'grad_norm': 0.4214725196361542, 'learning_rate': 1.7503829613417905e-05, 'epoch': 0.25}
{'loss': 1.004, 'grad_norm': 0.4188901484012604, 'learning_rate': 1.749970954693288e-05, 'epoch': 0.25}
{'loss': 0.9918, 'grad_norm': 0.46022677421569824, 'learning_rate': 1.7495586568802384e-05, 'epoch': 0.25}
{'loss': 0.9782, 'grad_norm': 0.4513126313686371, 'learning_rate': 1.7491460680627105e-05, 'epoch': 0.25}
{'loss': 1.0632, 'grad_norm': 0.4594583809375763, 'learning_rate': 1.7487331884008845e-05, 'epoch': 0.25}
{'loss': 1.0835, 'grad_norm': 0.49427008628845215, 'learning_rate': 1.7483200180550554e-05, 'epoch': 0.25}
{'loss': 0.9301, 'grad_norm': 0.24850454926490784, 'learning_rate': 1.74790655718563e-05, 'epoch': 0.25}
{'loss': 1.0512, 'grad_norm': 0.4772276282310486, 'learning_rate': 1.747492805953128e-05, 'epoch': 0.25}
{'loss': 1.0096, 'grad_norm': 0.44225943088531494, 'learning_rate': 1.7470787645181818e-05, 'epoch': 0.25}
{'loss': 0.9595, 'grad_norm': 0.3985014855861664, 'learning_rate': 1.7466644330415362e-05, 'epoch': 0.25}
{'loss': 1.1362, 'grad_norm': 0.4854014813899994, 'learning_rate': 1.7462498116840496e-05, 'epoch': 0.26}
{'loss': 0.9608, 'grad_norm': 0.44399335980415344, 'learning_rate': 1.745834900606692e-05, 'epoch': 0.26}
{'loss': 1.0684, 'grad_norm': 0.44417819380760193, 'learning_rate': 1.7454196999705458e-05, 'epoch': 0.26}
{'loss': 1.0805, 'grad_norm': 0.4624986946582794, 'learning_rate': 1.7450042099368066e-05, 'epoch': 0.26}
Error with image file is truncated (36 bytes not processed)
{'loss': 1.0623, 'grad_norm': 0.4865308701992035, 'learning_rate': 1.7445884306667823e-05, 'epoch': 0.26}
{'loss': 1.0198, 'grad_norm': 0.41855672001838684, 'learning_rate': 1.7441723623218917e-05, 'epoch': 0.26}
{'loss': 0.9709, 'grad_norm': 0.41897329688072205, 'learning_rate': 1.7437560050636678e-05, 'epoch': 0.26}
{'loss': 1.0604, 'grad_norm': 0.4455561935901642, 'learning_rate': 1.7433393590537543e-05, 'epoch': 0.26}
{'loss': 1.0915, 'grad_norm': 0.46068906784057617, 'learning_rate': 1.7429224244539077e-05, 'epoch': 0.26}
{'loss': 1.1272, 'grad_norm': 0.4683946371078491, 'learning_rate': 1.7425052014259965e-05, 'epoch': 0.26}
{'loss': 1.0531, 'grad_norm': 0.48143115639686584, 'learning_rate': 1.7420876901320006e-05, 'epoch': 0.26}
{'loss': 1.0915, 'grad_norm': 0.4305688142776489, 'learning_rate': 1.7416698907340128e-05, 'epoch': 0.26}
{'loss': 0.9487, 'grad_norm': 0.46881768107414246, 'learning_rate': 1.741251803394237e-05, 'epoch': 0.26}
{'loss': 0.9789, 'grad_norm': 0.38558998703956604, 'learning_rate': 1.740833428274989e-05, 'epoch': 0.26}
{'loss': 1.0423, 'grad_norm': 0.3977837562561035, 'learning_rate': 1.7404147655386966e-05, 'epoch': 0.26}
{'loss': 1.0487, 'grad_norm': 0.4485200047492981, 'learning_rate': 1.739995815347899e-05, 'epoch': 0.26}
{'loss': 1.0032, 'grad_norm': 0.41946539282798767, 'learning_rate': 1.739576577865247e-05, 'epoch': 0.26}
{'loss': 1.0072, 'grad_norm': 0.44465741515159607, 'learning_rate': 1.739157053253503e-05, 'epoch': 0.26}
{'loss': 1.0554, 'grad_norm': 0.38747724890708923, 'learning_rate': 1.738737241675541e-05, 'epoch': 0.26}
{'loss': 1.082, 'grad_norm': 0.4532823860645294, 'learning_rate': 1.7383171432943466e-05, 'epoch': 0.26}
{'loss': 1.0982, 'grad_norm': 0.43298131227493286, 'learning_rate': 1.737896758273016e-05, 'epoch': 0.26}
{'loss': 0.9567, 'grad_norm': 0.4653150141239166, 'learning_rate': 1.7374760867747574e-05, 'epoch': 0.26}
{'loss': 1.0181, 'grad_norm': 0.4462037980556488, 'learning_rate': 1.7370551289628895e-05, 'epoch': 0.26}
{'loss': 1.0105, 'grad_norm': 0.44123178720474243, 'learning_rate': 1.7366338850008432e-05, 'epoch': 0.26}
{'loss': 1.0912, 'grad_norm': 0.4514414668083191, 'learning_rate': 1.73621235505216e-05, 'epoch': 0.26}
{'loss': 1.1259, 'grad_norm': 0.45868030190467834, 'learning_rate': 1.7357905392804918e-05, 'epoch': 0.26}
{'loss': 1.0129, 'grad_norm': 0.31878694891929626, 'learning_rate': 1.735368437849602e-05, 'epoch': 0.26}
{'loss': 1.0246, 'grad_norm': 0.3926967680454254, 'learning_rate': 1.7349460509233654e-05, 'epoch': 0.26}
{'loss': 1.1421, 'grad_norm': 0.4558257460594177, 'learning_rate': 1.734523378665767e-05, 'epoch': 0.26}
{'loss': 0.9614, 'grad_norm': 0.4166697859764099, 'learning_rate': 1.7341004212409026e-05, 'epoch': 0.26}
{'loss': 1.0495, 'grad_norm': 0.41404739022254944, 'learning_rate': 1.7336771788129785e-05, 'epoch': 0.26}
{'loss': 0.9838, 'grad_norm': 0.43878933787345886, 'learning_rate': 1.7332536515463126e-05, 'epoch': 0.26}
{'loss': 1.0879, 'grad_norm': 0.45034223794937134, 'learning_rate': 1.7328298396053324e-05, 'epoch': 0.26}
{'loss': 1.0748, 'grad_norm': 0.4437313675880432, 'learning_rate': 1.7324057431545768e-05, 'epoch': 0.26}
{'loss': 1.0717, 'grad_norm': 0.4402080178260803, 'learning_rate': 1.7319813623586935e-05, 'epoch': 0.26}
{'loss': 1.054, 'grad_norm': 0.4481343626976013, 'learning_rate': 1.7315566973824433e-05, 'epoch': 0.26}
{'loss': 1.0559, 'grad_norm': 0.38009360432624817, 'learning_rate': 1.7311317483906946e-05, 'epoch': 0.26}
Error with image file is truncated (43 bytes not processed)
{'loss': 1.0528, 'grad_norm': 0.4492673873901367, 'learning_rate': 1.730706515548427e-05, 'epoch': 0.26}
{'loss': 1.1056, 'grad_norm': 0.45134973526000977, 'learning_rate': 1.730280999020732e-05, 'epoch': 0.26}
{'loss': 0.969, 'grad_norm': 0.4258098304271698, 'learning_rate': 1.729855198972808e-05, 'epoch': 0.26}
{'loss': 0.9911, 'grad_norm': 0.3899032175540924, 'learning_rate': 1.729429115569967e-05, 'epoch': 0.26}
{'loss': 1.0185, 'grad_norm': 0.3936571180820465, 'learning_rate': 1.729002748977628e-05, 'epoch': 0.26}
{'loss': 1.0854, 'grad_norm': 0.4648132026195526, 'learning_rate': 1.7285760993613215e-05, 'epoch': 0.26}
{'loss': 1.072, 'grad_norm': 0.46487584710121155, 'learning_rate': 1.7281491668866874e-05, 'epoch': 0.26}
{'loss': 1.0825, 'grad_norm': 0.4606074392795563, 'learning_rate': 1.727721951719476e-05, 'epoch': 0.26}
{'loss': 1.0357, 'grad_norm': 0.43805667757987976, 'learning_rate': 1.7272944540255468e-05, 'epoch': 0.26}
{'loss': 1.0157, 'grad_norm': 0.4107469618320465, 'learning_rate': 1.726866673970869e-05, 'epoch': 0.26}
{'loss': 1.0395, 'grad_norm': 0.47963032126426697, 'learning_rate': 1.7264386117215216e-05, 'epoch': 0.26}
{'loss': 0.9658, 'grad_norm': 0.4369615614414215, 'learning_rate': 1.7260102674436933e-05, 'epoch': 0.26}
{'loss': 0.8975, 'grad_norm': 0.2428438663482666, 'learning_rate': 1.7255816413036818e-05, 'epoch': 0.26}
{'loss': 1.0629, 'grad_norm': 0.4172666370868683, 'learning_rate': 1.7251527334678946e-05, 'epoch': 0.26}
{'loss': 1.025, 'grad_norm': 0.42690756916999817, 'learning_rate': 1.7247235441028486e-05, 'epoch': 0.26}
{'loss': 1.0596, 'grad_norm': 0.4376925528049469, 'learning_rate': 1.7242940733751696e-05, 'epoch': 0.27}
{'loss': 0.9562, 'grad_norm': 0.42078131437301636, 'learning_rate': 1.7238643214515934e-05, 'epoch': 0.27}
{'loss': 1.0375, 'grad_norm': 0.42265328764915466, 'learning_rate': 1.7234342884989642e-05, 'epoch': 0.27}
{'loss': 1.0758, 'grad_norm': 0.5003891587257385, 'learning_rate': 1.7230039746842352e-05, 'epoch': 0.27}
{'loss': 1.0499, 'grad_norm': 0.4431037902832031, 'learning_rate': 1.7225733801744698e-05, 'epoch': 0.27}
{'loss': 1.0432, 'grad_norm': 0.4795449674129486, 'learning_rate': 1.7221425051368394e-05, 'epoch': 0.27}
{'loss': 1.087, 'grad_norm': 0.4382227957248688, 'learning_rate': 1.7217113497386245e-05, 'epoch': 0.27}
{'loss': 1.0055, 'grad_norm': 0.4196619689464569, 'learning_rate': 1.721279914147214e-05, 'epoch': 0.27}
{'loss': 1.1233, 'grad_norm': 0.438607394695282, 'learning_rate': 1.7208481985301065e-05, 'epoch': 0.27}
{'loss': 1.0039, 'grad_norm': 0.3862221837043762, 'learning_rate': 1.7204162030549093e-05, 'epoch': 0.27}
{'loss': 0.9357, 'grad_norm': 0.42977795004844666, 'learning_rate': 1.7199839278893368e-05, 'epoch': 0.27}
{'loss': 1.0035, 'grad_norm': 0.4376884400844574, 'learning_rate': 1.719551373201214e-05, 'epoch': 0.27}
{'loss': 1.0802, 'grad_norm': 0.4275762438774109, 'learning_rate': 1.7191185391584736e-05, 'epoch': 0.27}
{'loss': 0.9629, 'grad_norm': 0.3882366418838501, 'learning_rate': 1.7186854259291558e-05, 'epoch': 0.27}
{'loss': 0.994, 'grad_norm': 0.39196106791496277, 'learning_rate': 1.7182520336814105e-05, 'epoch': 0.27}
{'loss': 1.0692, 'grad_norm': 0.46950003504753113, 'learning_rate': 1.717818362583496e-05, 'epoch': 0.27}
{'loss': 1.099, 'grad_norm': 0.4724297523498535, 'learning_rate': 1.7173844128037777e-05, 'epoch': 0.27}
{'loss': 0.9637, 'grad_norm': 0.37174364924430847, 'learning_rate': 1.71695018451073e-05, 'epoch': 0.27}
{'loss': 1.1345, 'grad_norm': 0.41962242126464844, 'learning_rate': 1.7165156778729355e-05, 'epoch': 0.27}
{'loss': 1.0482, 'grad_norm': 0.4293353855609894, 'learning_rate': 1.7160808930590845e-05, 'epoch': 0.27}
{'loss': 0.9972, 'grad_norm': 0.4482598602771759, 'learning_rate': 1.7156458302379753e-05, 'epoch': 0.27}
{'loss': 0.9914, 'grad_norm': 0.41031742095947266, 'learning_rate': 1.7152104895785147e-05, 'epoch': 0.27}
{'loss': 1.0768, 'grad_norm': 0.40667298436164856, 'learning_rate': 1.7147748712497162e-05, 'epoch': 0.27}
{'loss': 1.1288, 'grad_norm': 0.48038187623023987, 'learning_rate': 1.7143389754207026e-05, 'epoch': 0.27}
{'loss': 1.015, 'grad_norm': 0.42953595519065857, 'learning_rate': 1.713902802260703e-05, 'epoch': 0.27}
{'loss': 1.0712, 'grad_norm': 0.42758721113204956, 'learning_rate': 1.7134663519390557e-05, 'epoch': 0.27}
{'loss': 1.0476, 'grad_norm': 0.39539411664009094, 'learning_rate': 1.7130296246252048e-05, 'epoch': 0.27}
{'loss': 1.0624, 'grad_norm': 0.5035436153411865, 'learning_rate': 1.7125926204887034e-05, 'epoch': 0.27}
{'loss': 1.0737, 'grad_norm': 0.439603716135025, 'learning_rate': 1.712155339699211e-05, 'epoch': 0.27}
{'loss': 0.9072, 'grad_norm': 0.2453775852918625, 'learning_rate': 1.7117177824264962e-05, 'epoch': 0.27}
{'loss': 0.9898, 'grad_norm': 0.410807341337204, 'learning_rate': 1.7112799488404327e-05, 'epoch': 0.27}
{'loss': 1.0801, 'grad_norm': 0.43699249625205994, 'learning_rate': 1.7108418391110033e-05, 'epoch': 0.27}
{'loss': 0.9854, 'grad_norm': 0.4101366102695465, 'learning_rate': 1.7104034534082968e-05, 'epoch': 0.27}
{'loss': 0.9992, 'grad_norm': 0.3931832015514374, 'learning_rate': 1.7099647919025096e-05, 'epoch': 0.27}
{'loss': 1.0503, 'grad_norm': 0.4596954584121704, 'learning_rate': 1.7095258547639456e-05, 'epoch': 0.27}
{'loss': 1.0823, 'grad_norm': 0.4391418993473053, 'learning_rate': 1.709086642163015e-05, 'epoch': 0.27}
{'loss': 1.0454, 'grad_norm': 0.4027055501937866, 'learning_rate': 1.7086471542702355e-05, 'epoch': 0.27}
{'loss': 0.9304, 'grad_norm': 0.42006731033325195, 'learning_rate': 1.708207391256231e-05, 'epoch': 0.27}
{'loss': 0.9872, 'grad_norm': 0.46694445610046387, 'learning_rate': 1.707767353291733e-05, 'epoch': 0.27}
{'loss': 1.0446, 'grad_norm': 0.48597827553749084, 'learning_rate': 1.7073270405475796e-05, 'epoch': 0.27}
{'loss': 1.0175, 'grad_norm': 0.4267268776893616, 'learning_rate': 1.7068864531947147e-05, 'epoch': 0.27}
{'loss': 1.0329, 'grad_norm': 0.42165669798851013, 'learning_rate': 1.70644559140419e-05, 'epoch': 0.27}
{'loss': 1.0523, 'grad_norm': 0.4292728900909424, 'learning_rate': 1.706004455347163e-05, 'epoch': 0.27}
{'loss': 0.9833, 'grad_norm': 0.4394533634185791, 'learning_rate': 1.705563045194898e-05, 'epoch': 0.27}
{'loss': 0.9674, 'grad_norm': 0.42068609595298767, 'learning_rate': 1.7051213611187657e-05, 'epoch': 0.27}
{'loss': 1.0841, 'grad_norm': 0.42379987239837646, 'learning_rate': 1.704679403290243e-05, 'epoch': 0.27}
{'loss': 1.1407, 'grad_norm': 0.45854759216308594, 'learning_rate': 1.7042371718809132e-05, 'epoch': 0.27}
{'loss': 0.9703, 'grad_norm': 0.4452997148036957, 'learning_rate': 1.7037946670624652e-05, 'epoch': 0.27}
{'loss': 0.9781, 'grad_norm': 0.4145844876766205, 'learning_rate': 1.7033518890066956e-05, 'epoch': 0.27}
{'loss': 1.0264, 'grad_norm': 0.4154185652732849, 'learning_rate': 1.7029088378855055e-05, 'epoch': 0.27}
{'loss': 0.9461, 'grad_norm': 0.430411159992218, 'learning_rate': 1.7024655138709025e-05, 'epoch': 0.27}
{'loss': 0.9123, 'grad_norm': 0.24142979085445404, 'learning_rate': 1.7020219171350004e-05, 'epoch': 0.27}
{'loss': 0.9911, 'grad_norm': 0.47499531507492065, 'learning_rate': 1.7015780478500187e-05, 'epoch': 0.28}
{'loss': 1.024, 'grad_norm': 0.4163101017475128, 'learning_rate': 1.701133906188283e-05, 'epoch': 0.28}
{'loss': 1.1268, 'grad_norm': 0.48178258538246155, 'learning_rate': 1.700689492322224e-05, 'epoch': 0.28}
{'loss': 1.0522, 'grad_norm': 0.4778950810432434, 'learning_rate': 1.700244806424379e-05, 'epoch': 0.28}
{'loss': 1.0596, 'grad_norm': 0.4162549674510956, 'learning_rate': 1.6997998486673893e-05, 'epoch': 0.28}
{'loss': 1.0385, 'grad_norm': 0.4578520655632019, 'learning_rate': 1.699354619224004e-05, 'epoch': 0.28}
{'loss': 1.0458, 'grad_norm': 0.41456976532936096, 'learning_rate': 1.698909118267076e-05, 'epoch': 0.28}
{'loss': 0.9228, 'grad_norm': 0.3796272575855255, 'learning_rate': 1.6984633459695646e-05, 'epoch': 0.28}
{'loss': 1.0651, 'grad_norm': 0.4246966242790222, 'learning_rate': 1.6980173025045328e-05, 'epoch': 0.28}
{'loss': 1.0791, 'grad_norm': 0.4561558961868286, 'learning_rate': 1.697570988045151e-05, 'epoch': 0.28}
{'loss': 0.9312, 'grad_norm': 0.42773276567459106, 'learning_rate': 1.6971244027646937e-05, 'epoch': 0.28}
{'loss': 1.0428, 'grad_norm': 0.4741027355194092, 'learning_rate': 1.69667754683654e-05, 'epoch': 0.28}
{'loss': 0.9851, 'grad_norm': 0.4404248297214508, 'learning_rate': 1.6962304204341758e-05, 'epoch': 0.28}
{'loss': 1.0126, 'grad_norm': 0.3787381947040558, 'learning_rate': 1.6957830237311904e-05, 'epoch': 0.28}
{'loss': 0.9736, 'grad_norm': 0.4168356657028198, 'learning_rate': 1.6953353569012784e-05, 'epoch': 0.28}
{'loss': 0.9931, 'grad_norm': 0.4377800226211548, 'learning_rate': 1.6948874201182402e-05, 'epoch': 0.28}
{'loss': 0.9684, 'grad_norm': 0.4174991846084595, 'learning_rate': 1.6944392135559798e-05, 'epoch': 0.28}
{'loss': 1.045, 'grad_norm': 0.4750854969024658, 'learning_rate': 1.6939907373885062e-05, 'epoch': 0.28}
{'loss': 0.989, 'grad_norm': 0.43266361951828003, 'learning_rate': 1.6935419917899335e-05, 'epoch': 0.28}
{'loss': 1.0157, 'grad_norm': 0.3926953077316284, 'learning_rate': 1.6930929769344807e-05, 'epoch': 0.28}
{'loss': 0.9807, 'grad_norm': 0.42805466055870056, 'learning_rate': 1.69264369299647e-05, 'epoch': 0.28}
{'loss': 1.0088, 'grad_norm': 0.3973177671432495, 'learning_rate': 1.692194140150329e-05, 'epoch': 0.28}
{'loss': 1.0055, 'grad_norm': 0.27198439836502075, 'learning_rate': 1.69174431857059e-05, 'epoch': 0.28}
{'loss': 0.974, 'grad_norm': 0.39665108919143677, 'learning_rate': 1.6912942284318898e-05, 'epoch': 0.28}
{'loss': 1.0582, 'grad_norm': 0.4759269654750824, 'learning_rate': 1.6908438699089674e-05, 'epoch': 0.28}
{'loss': 0.9562, 'grad_norm': 0.429019033908844, 'learning_rate': 1.690393243176668e-05, 'epoch': 0.28}
{'loss': 0.9586, 'grad_norm': 0.4394007921218872, 'learning_rate': 1.6899423484099413e-05, 'epoch': 0.28}
{'loss': 1.1427, 'grad_norm': 0.4770066440105438, 'learning_rate': 1.6894911857838394e-05, 'epoch': 0.28}
{'loss': 1.0092, 'grad_norm': 0.4679993987083435, 'learning_rate': 1.689039755473519e-05, 'epoch': 0.28}
{'loss': 1.0138, 'grad_norm': 0.41634315252304077, 'learning_rate': 1.6885880576542417e-05, 'epoch': 0.28}
{'loss': 1.0867, 'grad_norm': 0.4295995533466339, 'learning_rate': 1.6881360925013712e-05, 'epoch': 0.28}
{'loss': 1.0677, 'grad_norm': 0.43975552916526794, 'learning_rate': 1.6876838601903765e-05, 'epoch': 0.28}
{'loss': 1.0495, 'grad_norm': 0.45214948058128357, 'learning_rate': 1.6872313608968296e-05, 'epoch': 0.28}
{'loss': 1.0819, 'grad_norm': 0.4861489236354828, 'learning_rate': 1.6867785947964065e-05, 'epoch': 0.28}
{'loss': 0.9603, 'grad_norm': 0.37879443168640137, 'learning_rate': 1.6863255620648866e-05, 'epoch': 0.28}
{'loss': 1.1231, 'grad_norm': 0.45756080746650696, 'learning_rate': 1.685872262878152e-05, 'epoch': 0.28}
{'loss': 1.0242, 'grad_norm': 0.38603267073631287, 'learning_rate': 1.6854186974121903e-05, 'epoch': 0.28}
{'loss': 1.0762, 'grad_norm': 0.45287004113197327, 'learning_rate': 1.68496486584309e-05, 'epoch': 0.28}
{'loss': 1.0181, 'grad_norm': 0.4288581609725952, 'learning_rate': 1.6845107683470453e-05, 'epoch': 0.28}
{'loss': 1.0622, 'grad_norm': 0.4459324777126312, 'learning_rate': 1.6840564051003517e-05, 'epoch': 0.28}
{'loss': 0.9882, 'grad_norm': 0.4331737160682678, 'learning_rate': 1.6836017762794087e-05, 'epoch': 0.28}
{'loss': 1.0702, 'grad_norm': 0.4072001874446869, 'learning_rate': 1.6831468820607192e-05, 'epoch': 0.28}
{'loss': 0.9593, 'grad_norm': 0.4156810939311981, 'learning_rate': 1.6826917226208886e-05, 'epoch': 0.28}
{'loss': 0.9823, 'grad_norm': 0.41543713212013245, 'learning_rate': 1.6822362981366257e-05, 'epoch': 0.28}
{'loss': 0.9284, 'grad_norm': 0.42789196968078613, 'learning_rate': 1.6817806087847417e-05, 'epoch': 0.28}
{'loss': 0.9879, 'grad_norm': 0.4126017689704895, 'learning_rate': 1.681324654742151e-05, 'epoch': 0.28}
{'loss': 0.966, 'grad_norm': 0.4256538450717926, 'learning_rate': 1.6808684361858706e-05, 'epoch': 0.28}
{'loss': 1.0152, 'grad_norm': 0.44187092781066895, 'learning_rate': 1.6804119532930202e-05, 'epoch': 0.28}
{'loss': 0.9188, 'grad_norm': 0.43409982323646545, 'learning_rate': 1.6799552062408225e-05, 'epoch': 0.28}
{'loss': 0.9336, 'grad_norm': 0.26355499029159546, 'learning_rate': 1.6794981952066018e-05, 'epoch': 0.28}
{'loss': 1.0788, 'grad_norm': 0.3990192711353302, 'learning_rate': 1.6790409203677863e-05, 'epoch': 0.28}
{'loss': 1.0043, 'grad_norm': 0.4186786115169525, 'learning_rate': 1.6785833819019052e-05, 'epoch': 0.28}
{'loss': 1.0355, 'grad_norm': 0.4239789545536041, 'learning_rate': 1.678125579986591e-05, 'epoch': 0.29}
{'loss': 0.9333, 'grad_norm': 0.406876802444458, 'learning_rate': 1.677667514799578e-05, 'epoch': 0.29}
{'loss': 1.0446, 'grad_norm': 0.4591776430606842, 'learning_rate': 1.6772091865187032e-05, 'epoch': 0.29}
{'loss': 1.0388, 'grad_norm': 0.4910547733306885, 'learning_rate': 1.676750595321905e-05, 'epoch': 0.29}
{'loss': 1.0709, 'grad_norm': 0.4943801164627075, 'learning_rate': 1.6762917413872246e-05, 'epoch': 0.29}
{'loss': 0.9841, 'grad_norm': 0.4947410225868225, 'learning_rate': 1.675832624892805e-05, 'epoch': 0.29}
{'loss': 1.0088, 'grad_norm': 0.44062161445617676, 'learning_rate': 1.6753732460168907e-05, 'epoch': 0.29}
{'loss': 1.0027, 'grad_norm': 0.44195714592933655, 'learning_rate': 1.674913604937828e-05, 'epoch': 0.29}
{'loss': 0.9971, 'grad_norm': 0.4318220317363739, 'learning_rate': 1.6744537018340662e-05, 'epoch': 0.29}
{'loss': 1.0104, 'grad_norm': 0.4264690577983856, 'learning_rate': 1.6739935368841555e-05, 'epoch': 0.29}
{'loss': 1.0117, 'grad_norm': 0.45902618765830994, 'learning_rate': 1.6735331102667475e-05, 'epoch': 0.29}
{'loss': 0.9581, 'grad_norm': 0.4323001205921173, 'learning_rate': 1.6730724221605955e-05, 'epoch': 0.29}
{'loss': 1.1046, 'grad_norm': 0.5068395137786865, 'learning_rate': 1.6726114727445547e-05, 'epoch': 0.29}
{'loss': 1.081, 'grad_norm': 0.44374150037765503, 'learning_rate': 1.6721502621975813e-05, 'epoch': 0.29}
{'loss': 1.0158, 'grad_norm': 0.41357338428497314, 'learning_rate': 1.6716887906987332e-05, 'epoch': 0.29}
{'loss': 1.0157, 'grad_norm': 0.43338289856910706, 'learning_rate': 1.6712270584271703e-05, 'epoch': 0.29}
{'loss': 1.0033, 'grad_norm': 0.44933274388313293, 'learning_rate': 1.670765065562152e-05, 'epoch': 0.29}
{'loss': 1.0954, 'grad_norm': 0.4396898150444031, 'learning_rate': 1.67030281228304e-05, 'epoch': 0.29}
{'loss': 1.018, 'grad_norm': 0.4220634996891022, 'learning_rate': 1.6698402987692968e-05, 'epoch': 0.29}
{'loss': 1.0353, 'grad_norm': 0.39296954870224, 'learning_rate': 1.6693775252004866e-05, 'epoch': 0.29}
{'loss': 0.9787, 'grad_norm': 0.42695119976997375, 'learning_rate': 1.668914491756274e-05, 'epoch': 0.29}
{'loss': 0.9876, 'grad_norm': 0.4225848317146301, 'learning_rate': 1.668451198616424e-05, 'epoch': 0.29}
{'loss': 0.9415, 'grad_norm': 0.2796708047389984, 'learning_rate': 1.6679876459608033e-05, 'epoch': 0.29}
{'loss': 1.0174, 'grad_norm': 0.4414021968841553, 'learning_rate': 1.667523833969379e-05, 'epoch': 0.29}
{'loss': 1.0797, 'grad_norm': 0.42598894238471985, 'learning_rate': 1.667059762822219e-05, 'epoch': 0.29}
{'loss': 1.0459, 'grad_norm': 0.433926522731781, 'learning_rate': 1.666595432699491e-05, 'epoch': 0.29}
{'loss': 0.9793, 'grad_norm': 0.40480780601501465, 'learning_rate': 1.6661308437814652e-05, 'epoch': 0.29}
{'loss': 1.0389, 'grad_norm': 0.4212232828140259, 'learning_rate': 1.6656659962485097e-05, 'epoch': 0.29}
{'loss': 0.925, 'grad_norm': 0.4137618839740753, 'learning_rate': 1.6652008902810952e-05, 'epoch': 0.29}
{'loss': 1.0891, 'grad_norm': 0.46834516525268555, 'learning_rate': 1.6647355260597915e-05, 'epoch': 0.29}
{'loss': 1.1793, 'grad_norm': 0.517321765422821, 'learning_rate': 1.664269903765269e-05, 'epoch': 0.29}
{'loss': 1.0411, 'grad_norm': 0.42497310042381287, 'learning_rate': 1.6638040235782983e-05, 'epoch': 0.29}
Error with image file is truncated
{'loss': 1.0524, 'grad_norm': 0.4066113829612732, 'learning_rate': 1.6633378856797505e-05, 'epoch': 0.29}
{'loss': 1.0111, 'grad_norm': 0.4365909993648529, 'learning_rate': 1.662871490250596e-05, 'epoch': 0.29}
{'loss': 1.0317, 'grad_norm': 0.4181416928768158, 'learning_rate': 1.662404837471905e-05, 'epoch': 0.29}
{'loss': 1.039, 'grad_norm': 0.42981821298599243, 'learning_rate': 1.66193792752485e-05, 'epoch': 0.29}
{'loss': 1.0249, 'grad_norm': 0.44344156980514526, 'learning_rate': 1.6614707605906995e-05, 'epoch': 0.29}
{'loss': 1.1168, 'grad_norm': 0.4567437171936035, 'learning_rate': 1.661003336850825e-05, 'epoch': 0.29}
{'loss': 1.1018, 'grad_norm': 0.48982325196266174, 'learning_rate': 1.660535656486696e-05, 'epoch': 0.29}
{'loss': 1.0859, 'grad_norm': 0.42427700757980347, 'learning_rate': 1.660067719679882e-05, 'epoch': 0.29}
{'loss': 1.1049, 'grad_norm': 0.4934181272983551, 'learning_rate': 1.6595995266120528e-05, 'epoch': 0.29}
{'loss': 1.0549, 'grad_norm': 0.465993732213974, 'learning_rate': 1.6591310774649766e-05, 'epoch': 0.29}
Error with image file is truncated (67 bytes not processed)
{'loss': 1.0459, 'grad_norm': 0.44070684909820557, 'learning_rate': 1.6586623724205216e-05, 'epoch': 0.29}
{'loss': 1.0413, 'grad_norm': 0.43301233649253845, 'learning_rate': 1.6581934116606554e-05, 'epoch': 0.29}
{'loss': 0.969, 'grad_norm': 0.39448121190071106, 'learning_rate': 1.657724195367444e-05, 'epoch': 0.29}
{'loss': 1.0236, 'grad_norm': 0.4171454906463623, 'learning_rate': 1.657254723723054e-05, 'epoch': 0.29}
{'loss': 1.0017, 'grad_norm': 0.40359100699424744, 'learning_rate': 1.6567849969097505e-05, 'epoch': 0.29}
{'loss': 0.9708, 'grad_norm': 0.30729857087135315, 'learning_rate': 1.6563150151098973e-05, 'epoch': 0.29}
{'loss': 1.0401, 'grad_norm': 0.4272715151309967, 'learning_rate': 1.6558447785059577e-05, 'epoch': 0.29}
{'loss': 1.0262, 'grad_norm': 0.44520315527915955, 'learning_rate': 1.655374287280494e-05, 'epoch': 0.29}
{'loss': 1.0681, 'grad_norm': 0.46338585019111633, 'learning_rate': 1.6549035416161662e-05, 'epoch': 0.29}
{'loss': 1.0798, 'grad_norm': 0.42607390880584717, 'learning_rate': 1.654432541695735e-05, 'epoch': 0.29}
{'loss': 1.0084, 'grad_norm': 0.42059317231178284, 'learning_rate': 1.653961287702058e-05, 'epoch': 0.3}
{'loss': 0.9423, 'grad_norm': 0.4005863666534424, 'learning_rate': 1.653489779818093e-05, 'epoch': 0.3}
{'loss': 0.9548, 'grad_norm': 0.38431575894355774, 'learning_rate': 1.6530180182268946e-05, 'epoch': 0.3}
{'loss': 0.9889, 'grad_norm': 0.4340653419494629, 'learning_rate': 1.652546003111618e-05, 'epoch': 0.3}
{'loss': 1.0527, 'grad_norm': 0.4604439437389374, 'learning_rate': 1.652073734655515e-05, 'epoch': 0.3}
{'loss': 0.943, 'grad_norm': 0.41371381282806396, 'learning_rate': 1.6516012130419366e-05, 'epoch': 0.3}
{'loss': 1.0354, 'grad_norm': 0.4371994137763977, 'learning_rate': 1.6511284384543317e-05, 'epoch': 0.3}
{'loss': 0.9911, 'grad_norm': 0.4465002715587616, 'learning_rate': 1.6506554110762483e-05, 'epoch': 0.3}
{'loss': 0.9824, 'grad_norm': 0.41578683257102966, 'learning_rate': 1.650182131091332e-05, 'epoch': 0.3}
{'loss': 1.0476, 'grad_norm': 0.48029616475105286, 'learning_rate': 1.6497085986833252e-05, 'epoch': 0.3}
{'loss': 1.0142, 'grad_norm': 0.46656399965286255, 'learning_rate': 1.6492348140360704e-05, 'epoch': 0.3}
{'loss': 1.0323, 'grad_norm': 0.49216005206108093, 'learning_rate': 1.6487607773335074e-05, 'epoch': 0.3}
{'loss': 1.0866, 'grad_norm': 0.4331783354282379, 'learning_rate': 1.648286488759673e-05, 'epoch': 0.3}
{'loss': 1.0643, 'grad_norm': 0.455029159784317, 'learning_rate': 1.6478119484987026e-05, 'epoch': 0.3}
{'loss': 0.9255, 'grad_norm': 0.25496941804885864, 'learning_rate': 1.6473371567348287e-05, 'epoch': 0.3}
{'loss': 0.9644, 'grad_norm': 0.38882482051849365, 'learning_rate': 1.6468621136523823e-05, 'epoch': 0.3}
{'loss': 1.0578, 'grad_norm': 0.4399973452091217, 'learning_rate': 1.646386819435791e-05, 'epoch': 0.3}
{'loss': 1.0242, 'grad_norm': 0.49234890937805176, 'learning_rate': 1.6459112742695807e-05, 'epoch': 0.3}
{'loss': 1.0557, 'grad_norm': 0.43994009494781494, 'learning_rate': 1.6454354783383748e-05, 'epoch': 0.3}
{'loss': 1.0626, 'grad_norm': 0.431553453207016, 'learning_rate': 1.644959431826893e-05, 'epoch': 0.3}
{'loss': 0.9594, 'grad_norm': 0.4039565324783325, 'learning_rate': 1.6444831349199528e-05, 'epoch': 0.3}
{'loss': 1.044, 'grad_norm': 0.39512133598327637, 'learning_rate': 1.6440065878024697e-05, 'epoch': 0.3}
{'loss': 0.9956, 'grad_norm': 0.47582876682281494, 'learning_rate': 1.6435297906594553e-05, 'epoch': 0.3}
{'loss': 0.9721, 'grad_norm': 0.4330857992172241, 'learning_rate': 1.643052743676019e-05, 'epoch': 0.3}
{'loss': 0.9807, 'grad_norm': 0.27416110038757324, 'learning_rate': 1.6425754470373667e-05, 'epoch': 0.3}
{'loss': 0.9619, 'grad_norm': 0.440208375453949, 'learning_rate': 1.642097900928801e-05, 'epoch': 0.3}
{'loss': 0.9932, 'grad_norm': 0.4573775827884674, 'learning_rate': 1.6416201055357225e-05, 'epoch': 0.3}
{'loss': 1.0834, 'grad_norm': 0.41479113698005676, 'learning_rate': 1.641142061043627e-05, 'epoch': 0.3}
{'loss': 0.9702, 'grad_norm': 0.44132956862449646, 'learning_rate': 1.640663767638108e-05, 'epoch': 0.3}
{'loss': 0.9686, 'grad_norm': 0.4175494611263275, 'learning_rate': 1.6401852255048564e-05, 'epoch': 0.3}
{'loss': 0.9283, 'grad_norm': 0.24960316717624664, 'learning_rate': 1.6397064348296578e-05, 'epoch': 0.3}
{'loss': 1.066, 'grad_norm': 0.5048753619194031, 'learning_rate': 1.6392273957983955e-05, 'epoch': 0.3}
{'loss': 1.0236, 'grad_norm': 0.42064526677131653, 'learning_rate': 1.638748108597049e-05, 'epoch': 0.3}
{'loss': 0.9882, 'grad_norm': 0.4213697910308838, 'learning_rate': 1.6382685734116934e-05, 'epoch': 0.3}
{'loss': 1.0102, 'grad_norm': 0.4365812838077545, 'learning_rate': 1.6377887904285018e-05, 'epoch': 0.3}
{'loss': 1.0921, 'grad_norm': 0.4608592092990875, 'learning_rate': 1.637308759833742e-05, 'epoch': 0.3}
{'loss': 1.0342, 'grad_norm': 0.3983089029788971, 'learning_rate': 1.6368284818137787e-05, 'epoch': 0.3}
{'loss': 1.072, 'grad_norm': 0.4743174910545349, 'learning_rate': 1.636347956555072e-05, 'epoch': 0.3}
{'loss': 0.9892, 'grad_norm': 0.38822421431541443, 'learning_rate': 1.635867184244178e-05, 'epoch': 0.3}
{'loss': 1.0773, 'grad_norm': 0.45362526178359985, 'learning_rate': 1.63538616506775e-05, 'epoch': 0.3}
{'loss': 1.0224, 'grad_norm': 0.44165468215942383, 'learning_rate': 1.6349048992125358e-05, 'epoch': 0.3}
{'loss': 1.0826, 'grad_norm': 0.41201984882354736, 'learning_rate': 1.634423386865379e-05, 'epoch': 0.3}
{'loss': 1.1455, 'grad_norm': 0.4920102655887604, 'learning_rate': 1.6339416282132196e-05, 'epoch': 0.3}
{'loss': 1.0466, 'grad_norm': 0.43287765979766846, 'learning_rate': 1.633459623443093e-05, 'epoch': 0.3}
{'loss': 1.0714, 'grad_norm': 0.4187476933002472, 'learning_rate': 1.6329773727421297e-05, 'epoch': 0.3}
{'loss': 1.0091, 'grad_norm': 0.4302724003791809, 'learning_rate': 1.6324948762975567e-05, 'epoch': 0.3}
{'loss': 0.9961, 'grad_norm': 0.4239247441291809, 'learning_rate': 1.632012134296695e-05, 'epoch': 0.3}
{'loss': 0.9803, 'grad_norm': 0.2554345428943634, 'learning_rate': 1.6315291469269617e-05, 'epoch': 0.3}
{'loss': 0.8887, 'grad_norm': 0.24138520658016205, 'learning_rate': 1.63104591437587e-05, 'epoch': 0.3}
{'loss': 1.0533, 'grad_norm': 0.42164701223373413, 'learning_rate': 1.6305624368310265e-05, 'epoch': 0.3}
{'loss': 1.0387, 'grad_norm': 0.44484958052635193, 'learning_rate': 1.630078714480134e-05, 'epoch': 0.3}
{'loss': 1.0619, 'grad_norm': 0.48353323340415955, 'learning_rate': 1.6295947475109904e-05, 'epoch': 0.3}
{'loss': 1.1052, 'grad_norm': 0.4593219757080078, 'learning_rate': 1.629110536111488e-05, 'epoch': 0.31}
{'loss': 1.0026, 'grad_norm': 0.4166294038295746, 'learning_rate': 1.628626080469615e-05, 'epoch': 0.31}
{'loss': 1.09, 'grad_norm': 0.4706440269947052, 'learning_rate': 1.628141380773453e-05, 'epoch': 0.31}
{'loss': 0.9473, 'grad_norm': 0.40396010875701904, 'learning_rate': 1.6276564372111797e-05, 'epoch': 0.31}
{'loss': 1.0459, 'grad_norm': 0.41455763578414917, 'learning_rate': 1.6271712499710663e-05, 'epoch': 0.31}
{'loss': 0.8868, 'grad_norm': 0.2637462019920349, 'learning_rate': 1.62668581924148e-05, 'epoch': 0.31}
{'loss': 1.0253, 'grad_norm': 0.4021539092063904, 'learning_rate': 1.6262001452108807e-05, 'epoch': 0.31}
{'loss': 1.033, 'grad_norm': 0.4401889443397522, 'learning_rate': 1.6257142280678247e-05, 'epoch': 0.31}
{'loss': 0.9917, 'grad_norm': 0.4041580855846405, 'learning_rate': 1.6252280680009613e-05, 'epoch': 0.31}
{'loss': 0.9367, 'grad_norm': 0.3948456346988678, 'learning_rate': 1.6247416651990343e-05, 'epoch': 0.31}
{'loss': 1.1423, 'grad_norm': 0.42564716935157776, 'learning_rate': 1.624255019850883e-05, 'epoch': 0.31}
{'loss': 1.0517, 'grad_norm': 0.43170902132987976, 'learning_rate': 1.6237681321454387e-05, 'epoch': 0.31}
{'loss': 1.0168, 'grad_norm': 0.4155731797218323, 'learning_rate': 1.623281002271729e-05, 'epoch': 0.31}
{'loss': 0.9691, 'grad_norm': 0.3950933516025543, 'learning_rate': 1.6227936304188738e-05, 'epoch': 0.31}
{'loss': 1.0147, 'grad_norm': 0.4346992075443268, 'learning_rate': 1.622306016776088e-05, 'epoch': 0.31}
{'loss': 1.0578, 'grad_norm': 0.4202408492565155, 'learning_rate': 1.6218181615326795e-05, 'epoch': 0.31}
{'loss': 0.9626, 'grad_norm': 0.4233476519584656, 'learning_rate': 1.6213300648780515e-05, 'epoch': 0.31}
{'loss': 1.0553, 'grad_norm': 0.470536470413208, 'learning_rate': 1.620841727001699e-05, 'epoch': 0.31}
{'loss': 1.1006, 'grad_norm': 0.4974822402000427, 'learning_rate': 1.6203531480932114e-05, 'epoch': 0.31}
{'loss': 1.0043, 'grad_norm': 0.3989841639995575, 'learning_rate': 1.619864328342273e-05, 'epoch': 0.31}
{'loss': 0.9686, 'grad_norm': 0.42186105251312256, 'learning_rate': 1.6193752679386593e-05, 'epoch': 0.31}
{'loss': 1.0314, 'grad_norm': 0.40544891357421875, 'learning_rate': 1.6188859670722414e-05, 'epoch': 0.31}
{'loss': 0.974, 'grad_norm': 0.39773520827293396, 'learning_rate': 1.6183964259329817e-05, 'epoch': 0.31}
{'loss': 1.0318, 'grad_norm': 0.4531603157520294, 'learning_rate': 1.6179066447109376e-05, 'epoch': 0.31}
{'loss': 1.0296, 'grad_norm': 0.4164937138557434, 'learning_rate': 1.6174166235962588e-05, 'epoch': 0.31}
{'loss': 1.0348, 'grad_norm': 0.4111036956310272, 'learning_rate': 1.6169263627791886e-05, 'epoch': 0.31}
{'loss': 0.9312, 'grad_norm': 0.25485217571258545, 'learning_rate': 1.616435862450063e-05, 'epoch': 0.31}
{'loss': 0.9324, 'grad_norm': 0.26271122694015503, 'learning_rate': 1.615945122799311e-05, 'epoch': 0.31}
{'loss': 1.0469, 'grad_norm': 0.4801282584667206, 'learning_rate': 1.6154541440174547e-05, 'epoch': 0.31}
{'loss': 0.8999, 'grad_norm': 0.44192561507225037, 'learning_rate': 1.614962926295109e-05, 'epoch': 0.31}
{'loss': 1.1437, 'grad_norm': 0.4911990463733673, 'learning_rate': 1.6144714698229814e-05, 'epoch': 0.31}
{'loss': 0.9975, 'grad_norm': 0.3836365342140198, 'learning_rate': 1.6139797747918725e-05, 'epoch': 0.31}
{'loss': 0.999, 'grad_norm': 0.4007246792316437, 'learning_rate': 1.613487841392675e-05, 'epoch': 0.31}
{'loss': 1.0263, 'grad_norm': 0.4110104739665985, 'learning_rate': 1.612995669816375e-05, 'epoch': 0.31}
{'loss': 1.0732, 'grad_norm': 0.3960816264152527, 'learning_rate': 1.6125032602540492e-05, 'epoch': 0.31}
{'loss': 1.0176, 'grad_norm': 0.41497355699539185, 'learning_rate': 1.6120106128968686e-05, 'epoch': 0.31}
{'loss': 0.9244, 'grad_norm': 0.415305495262146, 'learning_rate': 1.6115177279360965e-05, 'epoch': 0.31}
{'loss': 1.1185, 'grad_norm': 0.5106865167617798, 'learning_rate': 1.611024605563087e-05, 'epoch': 0.31}
{'loss': 1.061, 'grad_norm': 0.4101778566837311, 'learning_rate': 1.610531245969287e-05, 'epoch': 0.31}
{'loss': 1.031, 'grad_norm': 0.4428039491176605, 'learning_rate': 1.6100376493462368e-05, 'epoch': 0.31}
{'loss': 0.8846, 'grad_norm': 0.2469087690114975, 'learning_rate': 1.6095438158855668e-05, 'epoch': 0.31}
{'loss': 1.0115, 'grad_norm': 0.4378473162651062, 'learning_rate': 1.609049745779e-05, 'epoch': 0.31}
{'loss': 1.021, 'grad_norm': 0.40992602705955505, 'learning_rate': 1.6085554392183517e-05, 'epoch': 0.31}
{'loss': 1.0625, 'grad_norm': 0.441308856010437, 'learning_rate': 1.608060896395529e-05, 'epoch': 0.31}
{'loss': 0.934, 'grad_norm': 0.41147151589393616, 'learning_rate': 1.60756611750253e-05, 'epoch': 0.31}
{'loss': 1.0774, 'grad_norm': 0.42494910955429077, 'learning_rate': 1.6070711027314446e-05, 'epoch': 0.31}
{'loss': 0.9701, 'grad_norm': 0.2707558572292328, 'learning_rate': 1.606575852274456e-05, 'epoch': 0.31}
{'loss': 1.0388, 'grad_norm': 0.40554681420326233, 'learning_rate': 1.6060803663238357e-05, 'epoch': 0.31}
{'loss': 1.0117, 'grad_norm': 0.4195457398891449, 'learning_rate': 1.6055846450719498e-05, 'epoch': 0.31}
{'loss': 1.0266, 'grad_norm': 0.3898734152317047, 'learning_rate': 1.6050886887112535e-05, 'epoch': 0.31}
{'loss': 0.9373, 'grad_norm': 0.4276600480079651, 'learning_rate': 1.6045924974342945e-05, 'epoch': 0.31}
{'loss': 1.0623, 'grad_norm': 0.4079105257987976, 'learning_rate': 1.604096071433711e-05, 'epoch': 0.31}
{'loss': 1.0094, 'grad_norm': 0.42978435754776, 'learning_rate': 1.6035994109022333e-05, 'epoch': 0.32}
{'loss': 0.9625, 'grad_norm': 0.4367498755455017, 'learning_rate': 1.6031025160326814e-05, 'epoch': 0.32}
{'loss': 0.966, 'grad_norm': 0.44039052724838257, 'learning_rate': 1.6026053870179678e-05, 'epoch': 0.32}
{'loss': 1.0261, 'grad_norm': 0.4421292543411255, 'learning_rate': 1.6021080240510943e-05, 'epoch': 0.32}
{'loss': 1.1332, 'grad_norm': 0.4753265082836151, 'learning_rate': 1.601610427325155e-05, 'epoch': 0.32}
{'loss': 1.08, 'grad_norm': 0.43626728653907776, 'learning_rate': 1.6011125970333333e-05, 'epoch': 0.32}
{'loss': 1.0029, 'grad_norm': 0.47572728991508484, 'learning_rate': 1.600614533368905e-05, 'epoch': 0.32}
{'loss': 1.0054, 'grad_norm': 0.41297149658203125, 'learning_rate': 1.6001162365252348e-05, 'epoch': 0.32}
{'loss': 0.9544, 'grad_norm': 0.45665642619132996, 'learning_rate': 1.5996177066957787e-05, 'epoch': 0.32}
{'loss': 1.1354, 'grad_norm': 0.46813666820526123, 'learning_rate': 1.5991189440740838e-05, 'epoch': 0.32}
{'loss': 0.999, 'grad_norm': 0.4134538173675537, 'learning_rate': 1.5986199488537867e-05, 'epoch': 0.32}
{'loss': 1.0512, 'grad_norm': 0.4384496808052063, 'learning_rate': 1.598120721228614e-05, 'epoch': 0.32}
{'loss': 0.9959, 'grad_norm': 0.4495435655117035, 'learning_rate': 1.5976212613923836e-05, 'epoch': 0.32}
{'loss': 0.9678, 'grad_norm': 0.4116518199443817, 'learning_rate': 1.5971215695390026e-05, 'epoch': 0.32}
{'loss': 1.0097, 'grad_norm': 0.41784369945526123, 'learning_rate': 1.5966216458624692e-05, 'epoch': 0.32}
{'loss': 1.0524, 'grad_norm': 0.4319138824939728, 'learning_rate': 1.5961214905568705e-05, 'epoch': 0.32}
{'loss': 1.012, 'grad_norm': 0.4538758099079132, 'learning_rate': 1.595621103816384e-05, 'epoch': 0.32}
{'loss': 0.9457, 'grad_norm': 0.39428073167800903, 'learning_rate': 1.5951204858352772e-05, 'epoch': 0.32}
{'loss': 1.0386, 'grad_norm': 0.4050900936126709, 'learning_rate': 1.594619636807907e-05, 'epoch': 0.32}
{'loss': 0.9933, 'grad_norm': 0.43587616086006165, 'learning_rate': 1.5941185569287206e-05, 'epoch': 0.32}
{'loss': 0.9596, 'grad_norm': 0.439001202583313, 'learning_rate': 1.5936172463922542e-05, 'epoch': 0.32}
{'loss': 1.0201, 'grad_norm': 0.42311200499534607, 'learning_rate': 1.593115705393134e-05, 'epoch': 0.32}
{'loss': 1.032, 'grad_norm': 0.41978001594543457, 'learning_rate': 1.5926139341260755e-05, 'epoch': 0.32}
{'loss': 1.0037, 'grad_norm': 0.40556082129478455, 'learning_rate': 1.5921119327858835e-05, 'epoch': 0.32}
{'loss': 1.0093, 'grad_norm': 0.4456964135169983, 'learning_rate': 1.5916097015674518e-05, 'epoch': 0.32}
{'loss': 1.013, 'grad_norm': 0.42571499943733215, 'learning_rate': 1.5911072406657646e-05, 'epoch': 0.32}
{'loss': 0.9282, 'grad_norm': 0.4261445105075836, 'learning_rate': 1.5906045502758943e-05, 'epoch': 0.32}
{'loss': 1.1313, 'grad_norm': 0.4332026541233063, 'learning_rate': 1.590101630593002e-05, 'epoch': 0.32}
{'loss': 1.0646, 'grad_norm': 0.4540698230266571, 'learning_rate': 1.5895984818123392e-05, 'epoch': 0.32}
{'loss': 1.0755, 'grad_norm': 0.49676188826560974, 'learning_rate': 1.5890951041292453e-05, 'epoch': 0.32}
{'loss': 1.047, 'grad_norm': 0.48713749647140503, 'learning_rate': 1.588591497739149e-05, 'epoch': 0.32}
{'loss': 1.0457, 'grad_norm': 0.4527392089366913, 'learning_rate': 1.5880876628375668e-05, 'epoch': 0.32}
{'loss': 0.9373, 'grad_norm': 0.4038808345794678, 'learning_rate': 1.587583599620106e-05, 'epoch': 0.32}
{'loss': 1.0462, 'grad_norm': 0.4623388946056366, 'learning_rate': 1.5870793082824604e-05, 'epoch': 0.32}
{'loss': 0.9052, 'grad_norm': 0.25617194175720215, 'learning_rate': 1.5865747890204138e-05, 'epoch': 0.32}
{'loss': 1.0331, 'grad_norm': 0.4074138104915619, 'learning_rate': 1.5860700420298377e-05, 'epoch': 0.32}
{'loss': 0.9621, 'grad_norm': 0.42323070764541626, 'learning_rate': 1.5855650675066924e-05, 'epoch': 0.32}
{'loss': 1.0533, 'grad_norm': 0.4854068458080292, 'learning_rate': 1.5850598656470265e-05, 'epoch': 0.32}
{'loss': 1.1053, 'grad_norm': 0.49655500054359436, 'learning_rate': 1.584554436646976e-05, 'epoch': 0.32}
{'loss': 1.0757, 'grad_norm': 0.48140275478363037, 'learning_rate': 1.5840487807027665e-05, 'epoch': 0.32}
{'loss': 1.057, 'grad_norm': 0.413520872592926, 'learning_rate': 1.5835428980107113e-05, 'epoch': 0.32}
{'loss': 0.9831, 'grad_norm': 0.430196076631546, 'learning_rate': 1.583036788767211e-05, 'epoch': 0.32}
{'loss': 0.9548, 'grad_norm': 0.41069984436035156, 'learning_rate': 1.5825304531687548e-05, 'epoch': 0.32}
{'loss': 0.8794, 'grad_norm': 0.4310050308704376, 'learning_rate': 1.5820238914119195e-05, 'epoch': 0.32}
{'loss': 1.1231, 'grad_norm': 0.45489266514778137, 'learning_rate': 1.5815171036933697e-05, 'epoch': 0.32}
{'loss': 1.0329, 'grad_norm': 0.4292629659175873, 'learning_rate': 1.5810100902098582e-05, 'epoch': 0.32}
{'loss': 1.0427, 'grad_norm': 0.4138636291027069, 'learning_rate': 1.580502851158225e-05, 'epoch': 0.32}
{'loss': 1.0007, 'grad_norm': 0.4255460202693939, 'learning_rate': 1.5799953867353975e-05, 'epoch': 0.32}
{'loss': 1.0478, 'grad_norm': 0.5127013921737671, 'learning_rate': 1.579487697138391e-05, 'epoch': 0.32}
{'loss': 1.0687, 'grad_norm': 0.42352294921875, 'learning_rate': 1.5789797825643086e-05, 'epoch': 0.32}
{'loss': 1.1789, 'grad_norm': 0.46970853209495544, 'learning_rate': 1.5784716432103394e-05, 'epoch': 0.32}
{'loss': 1.0028, 'grad_norm': 0.4285084009170532, 'learning_rate': 1.5779632792737608e-05, 'epoch': 0.32}
{'loss': 0.9724, 'grad_norm': 0.3812647759914398, 'learning_rate': 1.5774546909519376e-05, 'epoch': 0.33}
{'loss': 1.1043, 'grad_norm': 0.5303966403007507, 'learning_rate': 1.5769458784423206e-05, 'epoch': 0.33}
{'loss': 1.1159, 'grad_norm': 0.43784019351005554, 'learning_rate': 1.5764368419424488e-05, 'epoch': 0.33}
{'loss': 1.1409, 'grad_norm': 0.44984936714172363, 'learning_rate': 1.575927581649948e-05, 'epoch': 0.33}
{'loss': 0.9747, 'grad_norm': 0.41309091448783875, 'learning_rate': 1.5754180977625303e-05, 'epoch': 0.33}
{'loss': 1.0767, 'grad_norm': 0.4277910590171814, 'learning_rate': 1.574908390477995e-05, 'epoch': 0.33}
{'loss': 1.0065, 'grad_norm': 0.40064096450805664, 'learning_rate': 1.5743984599942273e-05, 'epoch': 0.33}
{'loss': 0.9524, 'grad_norm': 0.4179857075214386, 'learning_rate': 1.5738883065092005e-05, 'epoch': 0.33}
{'loss': 0.9572, 'grad_norm': 0.4145874083042145, 'learning_rate': 1.5733779302209735e-05, 'epoch': 0.33}
{'loss': 1.0116, 'grad_norm': 0.41744464635849, 'learning_rate': 1.572867331327692e-05, 'epoch': 0.33}
{'loss': 1.0195, 'grad_norm': 0.4249858856201172, 'learning_rate': 1.5723565100275884e-05, 'epoch': 0.33}
{'loss': 0.9378, 'grad_norm': 0.4119136929512024, 'learning_rate': 1.5718454665189806e-05, 'epoch': 0.33}
{'loss': 0.957, 'grad_norm': 0.4181884229183197, 'learning_rate': 1.5713342010002733e-05, 'epoch': 0.33}
{'loss': 0.9335, 'grad_norm': 0.4101114273071289, 'learning_rate': 1.5708227136699578e-05, 'epoch': 0.33}
{'loss': 0.9513, 'grad_norm': 0.4256703853607178, 'learning_rate': 1.5703110047266105e-05, 'epoch': 0.33}
{'loss': 0.9727, 'grad_norm': 0.4186581075191498, 'learning_rate': 1.569799074368895e-05, 'epoch': 0.33}
{'loss': 0.9213, 'grad_norm': 0.44966980814933777, 'learning_rate': 1.5692869227955603e-05, 'epoch': 0.33}
{'loss': 1.0087, 'grad_norm': 0.4509178102016449, 'learning_rate': 1.5687745502054407e-05, 'epoch': 0.33}
{'loss': 1.0439, 'grad_norm': 0.44208022952079773, 'learning_rate': 1.5682619567974575e-05, 'epoch': 0.33}
{'loss': 1.0413, 'grad_norm': 0.4721869230270386, 'learning_rate': 1.567749142770617e-05, 'epoch': 0.33}
{'loss': 0.9982, 'grad_norm': 0.4491443336009979, 'learning_rate': 1.5672361083240106e-05, 'epoch': 0.33}
{'loss': 0.9462, 'grad_norm': 0.27182862162590027, 'learning_rate': 1.5667228536568167e-05, 'epoch': 0.33}
{'loss': 1.0171, 'grad_norm': 0.4733544886112213, 'learning_rate': 1.566209378968298e-05, 'epoch': 0.33}
{'loss': 1.0637, 'grad_norm': 0.44701361656188965, 'learning_rate': 1.565695684457803e-05, 'epoch': 0.33}
{'loss': 1.0666, 'grad_norm': 0.46477627754211426, 'learning_rate': 1.5651817703247666e-05, 'epoch': 0.33}
{'loss': 1.0416, 'grad_norm': 0.4986080229282379, 'learning_rate': 1.5646676367687067e-05, 'epoch': 0.33}
{'loss': 0.9293, 'grad_norm': 0.2748734951019287, 'learning_rate': 1.564153283989228e-05, 'epoch': 0.33}
{'loss': 0.9439, 'grad_norm': 0.4483301639556885, 'learning_rate': 1.5636387121860207e-05, 'epoch': 0.33}
{'loss': 1.0271, 'grad_norm': 0.5647858381271362, 'learning_rate': 1.5631239215588578e-05, 'epoch': 0.33}
{'loss': 1.107, 'grad_norm': 0.4625740945339203, 'learning_rate': 1.5626089123076004e-05, 'epoch': 0.33}
{'loss': 1.0478, 'grad_norm': 0.42365729808807373, 'learning_rate': 1.5620936846321917e-05, 'epoch': 0.33}
{'loss': 0.9509, 'grad_norm': 0.41974836587905884, 'learning_rate': 1.561578238732661e-05, 'epoch': 0.33}
{'loss': 1.0832, 'grad_norm': 0.4344641864299774, 'learning_rate': 1.561062574809123e-05, 'epoch': 0.33}
{'loss': 0.9881, 'grad_norm': 0.39066585898399353, 'learning_rate': 1.5605466930617747e-05, 'epoch': 0.33}
{'loss': 1.0658, 'grad_norm': 0.4012625813484192, 'learning_rate': 1.5600305936909005e-05, 'epoch': 0.33}
{'loss': 1.0426, 'grad_norm': 0.4587806463241577, 'learning_rate': 1.559514276896867e-05, 'epoch': 0.33}
{'loss': 0.9745, 'grad_norm': 0.4050823748111725, 'learning_rate': 1.558997742880127e-05, 'epoch': 0.33}
{'loss': 0.9672, 'grad_norm': 0.39969512820243835, 'learning_rate': 1.5584809918412158e-05, 'epoch': 0.33}
{'loss': 0.9615, 'grad_norm': 0.2560713589191437, 'learning_rate': 1.557964023980755e-05, 'epoch': 0.33}
{'loss': 0.9858, 'grad_norm': 0.40910786390304565, 'learning_rate': 1.5574468394994486e-05, 'epoch': 0.33}
{'loss': 0.928, 'grad_norm': 0.4039526879787445, 'learning_rate': 1.5569294385980856e-05, 'epoch': 0.33}
{'loss': 1.0264, 'grad_norm': 0.42399874329566956, 'learning_rate': 1.556411821477539e-05, 'epoch': 0.33}
{'loss': 0.9705, 'grad_norm': 0.47699683904647827, 'learning_rate': 1.5558939883387657e-05, 'epoch': 0.33}
{'loss': 1.1387, 'grad_norm': 0.4702886641025543, 'learning_rate': 1.5553759393828058e-05, 'epoch': 0.33}
{'loss': 1.0316, 'grad_norm': 0.43443071842193604, 'learning_rate': 1.554857674810784e-05, 'epoch': 0.33}
{'loss': 0.891, 'grad_norm': 0.4452109932899475, 'learning_rate': 1.554339194823909e-05, 'epoch': 0.33}
{'loss': 0.9899, 'grad_norm': 0.4585001468658447, 'learning_rate': 1.553820499623472e-05, 'epoch': 0.33}
{'loss': 1.0036, 'grad_norm': 0.4098977744579315, 'learning_rate': 1.553301589410848e-05, 'epoch': 0.33}
{'loss': 0.9995, 'grad_norm': 0.4567459523677826, 'learning_rate': 1.5527824643874968e-05, 'epoch': 0.33}
{'loss': 1.0168, 'grad_norm': 0.42356958985328674, 'learning_rate': 1.5522631247549598e-05, 'epoch': 0.33}
{'loss': 1.1578, 'grad_norm': 0.45461928844451904, 'learning_rate': 1.5517435707148628e-05, 'epoch': 0.33}
{'loss': 1.0502, 'grad_norm': 0.44576403498649597, 'learning_rate': 1.5512238024689144e-05, 'epoch': 0.33}
{'loss': 1.0133, 'grad_norm': 0.4149377942085266, 'learning_rate': 1.550703820218907e-05, 'epoch': 0.34}
{'loss': 1.0158, 'grad_norm': 0.4357748329639435, 'learning_rate': 1.550183624166715e-05, 'epoch': 0.34}
{'loss': 0.9997, 'grad_norm': 0.4322369396686554, 'learning_rate': 1.549663214514297e-05, 'epoch': 0.34}
{'loss': 1.1404, 'grad_norm': 0.4681517779827118, 'learning_rate': 1.5491425914636934e-05, 'epoch': 0.34}
{'loss': 1.0967, 'grad_norm': 0.48183688521385193, 'learning_rate': 1.5486217552170283e-05, 'epoch': 0.34}
{'loss': 1.067, 'grad_norm': 0.4245777428150177, 'learning_rate': 1.548100705976508e-05, 'epoch': 0.34}
{'loss': 1.0616, 'grad_norm': 0.4518350064754486, 'learning_rate': 1.5475794439444226e-05, 'epoch': 0.34}
{'loss': 0.9803, 'grad_norm': 0.434172123670578, 'learning_rate': 1.5470579693231432e-05, 'epoch': 0.34}
{'loss': 1.1285, 'grad_norm': 0.4806438088417053, 'learning_rate': 1.5465362823151245e-05, 'epoch': 0.34}
{'loss': 1.003, 'grad_norm': 0.4550974667072296, 'learning_rate': 1.5460143831229026e-05, 'epoch': 0.34}
{'loss': 1.1012, 'grad_norm': 0.44540300965309143, 'learning_rate': 1.545492271949098e-05, 'epoch': 0.34}
{'loss': 0.9465, 'grad_norm': 0.37967389822006226, 'learning_rate': 1.544969948996411e-05, 'epoch': 0.34}
{'loss': 1.0559, 'grad_norm': 0.4643033444881439, 'learning_rate': 1.544447414467626e-05, 'epoch': 0.34}
{'loss': 1.0636, 'grad_norm': 0.44079622626304626, 'learning_rate': 1.5439246685656093e-05, 'epoch': 0.34}
{'loss': 1.0422, 'grad_norm': 0.4291499853134155, 'learning_rate': 1.5434017114933082e-05, 'epoch': 0.34}
{'loss': 1.0096, 'grad_norm': 0.41637519001960754, 'learning_rate': 1.5428785434537527e-05, 'epoch': 0.34}
{'loss': 1.0365, 'grad_norm': 0.44571250677108765, 'learning_rate': 1.542355164650055e-05, 'epoch': 0.34}
{'loss': 1.1154, 'grad_norm': 0.45378297567367554, 'learning_rate': 1.541831575285408e-05, 'epoch': 0.34}
{'loss': 1.0928, 'grad_norm': 0.4405560791492462, 'learning_rate': 1.541307775563088e-05, 'epoch': 0.34}
{'loss': 1.0919, 'grad_norm': 0.44386395812034607, 'learning_rate': 1.540783765686452e-05, 'epoch': 0.34}
{'loss': 1.1192, 'grad_norm': 0.4854709208011627, 'learning_rate': 1.540259545858938e-05, 'epoch': 0.34}
{'loss': 0.9724, 'grad_norm': 0.432636559009552, 'learning_rate': 1.539735116284067e-05, 'epoch': 0.34}
{'loss': 0.9926, 'grad_norm': 0.43070465326309204, 'learning_rate': 1.53921047716544e-05, 'epoch': 0.34}
{'loss': 1.0339, 'grad_norm': 0.4308185875415802, 'learning_rate': 1.53868562870674e-05, 'epoch': 0.34}
{'loss': 0.9088, 'grad_norm': 0.2546979784965515, 'learning_rate': 1.5381605711117318e-05, 'epoch': 0.34}
{'loss': 1.0648, 'grad_norm': 0.441186785697937, 'learning_rate': 1.5376353045842604e-05, 'epoch': 0.34}
{'loss': 1.0858, 'grad_norm': 0.42428091168403625, 'learning_rate': 1.5371098293282526e-05, 'epoch': 0.34}
{'loss': 0.9658, 'grad_norm': 0.4221990406513214, 'learning_rate': 1.5365841455477158e-05, 'epoch': 0.34}
{'loss': 1.052, 'grad_norm': 0.4992999732494354, 'learning_rate': 1.5360582534467382e-05, 'epoch': 0.34}
{'loss': 0.9012, 'grad_norm': 0.235018789768219, 'learning_rate': 1.5355321532294897e-05, 'epoch': 0.34}
{'loss': 0.9729, 'grad_norm': 0.4606828987598419, 'learning_rate': 1.5350058451002204e-05, 'epoch': 0.34}
{'loss': 0.9122, 'grad_norm': 0.41071221232414246, 'learning_rate': 1.5344793292632614e-05, 'epoch': 0.34}
{'loss': 0.9773, 'grad_norm': 0.4177075922489166, 'learning_rate': 1.533952605923024e-05, 'epoch': 0.34}
{'loss': 1.008, 'grad_norm': 0.41031771898269653, 'learning_rate': 1.5334256752840007e-05, 'epoch': 0.34}
{'loss': 1.0378, 'grad_norm': 0.4439443051815033, 'learning_rate': 1.532898537550764e-05, 'epoch': 0.34}
{'loss': 0.9276, 'grad_norm': 0.3981892764568329, 'learning_rate': 1.532371192927966e-05, 'epoch': 0.34}
{'loss': 1.0485, 'grad_norm': 0.3970130980014801, 'learning_rate': 1.5318436416203412e-05, 'epoch': 0.34}
{'loss': 0.991, 'grad_norm': 0.4441272020339966, 'learning_rate': 1.531315883832703e-05, 'epoch': 0.34}
{'loss': 0.9624, 'grad_norm': 0.25093498826026917, 'learning_rate': 1.530787919769945e-05, 'epoch': 0.34}
{'loss': 0.9392, 'grad_norm': 0.40346667170524597, 'learning_rate': 1.5302597496370408e-05, 'epoch': 0.34}
{'loss': 1.0199, 'grad_norm': 0.4213832914829254, 'learning_rate': 1.5297313736390447e-05, 'epoch': 0.34}
{'loss': 1.005, 'grad_norm': 0.4298538267612457, 'learning_rate': 1.5292027919810898e-05, 'epoch': 0.34}
{'loss': 0.9705, 'grad_norm': 0.39821016788482666, 'learning_rate': 1.52867400486839e-05, 'epoch': 0.34}
{'loss': 1.0954, 'grad_norm': 0.4521302878856659, 'learning_rate': 1.528145012506239e-05, 'epoch': 0.34}
{'loss': 0.8867, 'grad_norm': 0.2432200163602829, 'learning_rate': 1.5276158151000096e-05, 'epoch': 0.34}
{'loss': 0.9924, 'grad_norm': 0.4158892035484314, 'learning_rate': 1.5270864128551542e-05, 'epoch': 0.34}
{'loss': 1.029, 'grad_norm': 0.40744540095329285, 'learning_rate': 1.5265568059772053e-05, 'epoch': 0.34}
{'loss': 1.0226, 'grad_norm': 0.4244551956653595, 'learning_rate': 1.5260269946717746e-05, 'epoch': 0.34}
{'loss': 1.0115, 'grad_norm': 0.44613519310951233, 'learning_rate': 1.5254969791445526e-05, 'epoch': 0.34}
{'loss': 1.0198, 'grad_norm': 0.4586961269378662, 'learning_rate': 1.5249667596013102e-05, 'epoch': 0.34}
{'loss': 1.0741, 'grad_norm': 0.43312668800354004, 'learning_rate': 1.5244363362478967e-05, 'epoch': 0.34}
{'loss': 1.0287, 'grad_norm': 0.414786159992218, 'learning_rate': 1.5239057092902404e-05, 'epoch': 0.34}
{'loss': 1.0566, 'grad_norm': 0.45115014910697937, 'learning_rate': 1.523374878934349e-05, 'epoch': 0.35}
{'loss': 1.0198, 'grad_norm': 0.42279472947120667, 'learning_rate': 1.5228438453863095e-05, 'epoch': 0.35}
{'loss': 1.0302, 'grad_norm': 0.4232756793498993, 'learning_rate': 1.522312608852287e-05, 'epoch': 0.35}
{'loss': 1.0193, 'grad_norm': 0.40052518248558044, 'learning_rate': 1.5217811695385263e-05, 'epoch': 0.35}
{'loss': 0.9598, 'grad_norm': 0.27368250489234924, 'learning_rate': 1.52124952765135e-05, 'epoch': 0.35}
{'loss': 0.9182, 'grad_norm': 0.23497775197029114, 'learning_rate': 1.5207176833971598e-05, 'epoch': 0.35}
{'loss': 1.0453, 'grad_norm': 0.42953506112098694, 'learning_rate': 1.520185636982436e-05, 'epoch': 0.35}
{'loss': 1.0006, 'grad_norm': 0.4391983449459076, 'learning_rate': 1.5196533886137376e-05, 'epoch': 0.35}
{'loss': 1.0778, 'grad_norm': 0.48191404342651367, 'learning_rate': 1.5191209384977014e-05, 'epoch': 0.35}
{'loss': 0.8904, 'grad_norm': 0.24453075230121613, 'learning_rate': 1.5185882868410431e-05, 'epoch': 0.35}
{'loss': 1.0323, 'grad_norm': 0.4765017032623291, 'learning_rate': 1.5180554338505564e-05, 'epoch': 0.35}
{'loss': 1.0217, 'grad_norm': 0.38509851694107056, 'learning_rate': 1.517522379733113e-05, 'epoch': 0.35}
{'loss': 0.9802, 'grad_norm': 0.4008128046989441, 'learning_rate': 1.5169891246956629e-05, 'epoch': 0.35}
{'loss': 0.9501, 'grad_norm': 0.41381433606147766, 'learning_rate': 1.5164556689452346e-05, 'epoch': 0.35}
{'loss': 1.0071, 'grad_norm': 0.4382934272289276, 'learning_rate': 1.5159220126889329e-05, 'epoch': 0.35}
{'loss': 0.9702, 'grad_norm': 0.41603925824165344, 'learning_rate': 1.5153881561339426e-05, 'epoch': 0.35}
{'loss': 1.0144, 'grad_norm': 0.39269140362739563, 'learning_rate': 1.5148540994875242e-05, 'epoch': 0.35}
{'loss': 0.8912, 'grad_norm': 0.24312011897563934, 'learning_rate': 1.5143198429570181e-05, 'epoch': 0.35}
{'loss': 1.0269, 'grad_norm': 0.4280400574207306, 'learning_rate': 1.5137853867498403e-05, 'epoch': 0.35}
{'loss': 1.0698, 'grad_norm': 0.42024871706962585, 'learning_rate': 1.5132507310734847e-05, 'epoch': 0.35}
{'loss': 1.0398, 'grad_norm': 0.4650917649269104, 'learning_rate': 1.5127158761355241e-05, 'epoch': 0.35}
{'loss': 1.0609, 'grad_norm': 0.4494483172893524, 'learning_rate': 1.512180822143607e-05, 'epoch': 0.35}
{'loss': 0.9734, 'grad_norm': 0.4268556535243988, 'learning_rate': 1.5116455693054594e-05, 'epoch': 0.35}
{'loss': 1.0083, 'grad_norm': 0.41465720534324646, 'learning_rate': 1.5111101178288858e-05, 'epoch': 0.35}
{'loss': 0.9789, 'grad_norm': 0.45807594060897827, 'learning_rate': 1.510574467921766e-05, 'epoch': 0.35}
{'loss': 1.1223, 'grad_norm': 0.48308640718460083, 'learning_rate': 1.5100386197920585e-05, 'epoch': 0.35}
{'loss': 0.9532, 'grad_norm': 0.4116663336753845, 'learning_rate': 1.5095025736477977e-05, 'epoch': 0.35}
{'loss': 0.9244, 'grad_norm': 0.4380902945995331, 'learning_rate': 1.5089663296970952e-05, 'epoch': 0.35}
{'loss': 0.9901, 'grad_norm': 0.44333600997924805, 'learning_rate': 1.5084298881481388e-05, 'epoch': 0.35}
{'loss': 1.0988, 'grad_norm': 0.45162180066108704, 'learning_rate': 1.5078932492091942e-05, 'epoch': 0.35}
Error with image file is truncated (73 bytes not processed)
{'loss': 0.9773, 'grad_norm': 0.5377882719039917, 'learning_rate': 1.5073564130886032e-05, 'epoch': 0.35}
{'loss': 0.9815, 'grad_norm': 0.394106924533844, 'learning_rate': 1.506819379994784e-05, 'epoch': 0.35}
{'loss': 1.0129, 'grad_norm': 0.41387900710105896, 'learning_rate': 1.5062821501362308e-05, 'epoch': 0.35}
{'loss': 1.01, 'grad_norm': 0.4550520181655884, 'learning_rate': 1.5057447237215152e-05, 'epoch': 0.35}
{'loss': 0.9916, 'grad_norm': 0.4134367108345032, 'learning_rate': 1.5052071009592846e-05, 'epoch': 0.35}
{'loss': 1.0074, 'grad_norm': 0.400838166475296, 'learning_rate': 1.5046692820582625e-05, 'epoch': 0.35}
{'loss': 1.1102, 'grad_norm': 0.45519697666168213, 'learning_rate': 1.504131267227249e-05, 'epoch': 0.35}
{'loss': 1.1318, 'grad_norm': 0.44354334473609924, 'learning_rate': 1.5035930566751198e-05, 'epoch': 0.35}
{'loss': 0.9691, 'grad_norm': 0.3988267481327057, 'learning_rate': 1.5030546506108268e-05, 'epoch': 0.35}
{'loss': 0.9912, 'grad_norm': 0.42641571164131165, 'learning_rate': 1.5025160492433976e-05, 'epoch': 0.35}
{'loss': 1.1088, 'grad_norm': 0.4743439555168152, 'learning_rate': 1.501977252781936e-05, 'epoch': 0.35}
{'loss': 0.8671, 'grad_norm': 0.41914522647857666, 'learning_rate': 1.5014382614356213e-05, 'epoch': 0.35}
{'loss': 1.0288, 'grad_norm': 0.48235028982162476, 'learning_rate': 1.5008990754137088e-05, 'epoch': 0.35}
{'loss': 1.0246, 'grad_norm': 0.42301449179649353, 'learning_rate': 1.5003596949255284e-05, 'epoch': 0.35}
{'loss': 0.9632, 'grad_norm': 0.4020312428474426, 'learning_rate': 1.4998201201804867e-05, 'epoch': 0.35}
{'loss': 0.9961, 'grad_norm': 0.42378759384155273, 'learning_rate': 1.499280351388065e-05, 'epoch': 0.35}
{'loss': 0.9635, 'grad_norm': 0.41752496361732483, 'learning_rate': 1.49874038875782e-05, 'epoch': 0.35}
{'loss': 0.9951, 'grad_norm': 0.4402585029602051, 'learning_rate': 1.498200232499384e-05, 'epoch': 0.35}
{'loss': 0.9895, 'grad_norm': 0.45034223794937134, 'learning_rate': 1.4976598828224643e-05, 'epoch': 0.35}
{'loss': 0.9866, 'grad_norm': 0.43731260299682617, 'learning_rate': 1.497119339936843e-05, 'epoch': 0.35}
{'loss': 0.9891, 'grad_norm': 0.4136456549167633, 'learning_rate': 1.4965786040523779e-05, 'epoch': 0.35}
{'loss': 0.9377, 'grad_norm': 0.3122047483921051, 'learning_rate': 1.496037675379001e-05, 'epoch': 0.35}
{'loss': 0.96, 'grad_norm': 0.46514299511909485, 'learning_rate': 1.4954965541267192e-05, 'epoch': 0.36}
{'loss': 0.9647, 'grad_norm': 0.43056008219718933, 'learning_rate': 1.494955240505615e-05, 'epoch': 0.36}
{'loss': 0.9468, 'grad_norm': 0.409444123506546, 'learning_rate': 1.494413734725844e-05, 'epoch': 0.36}
{'loss': 1.0211, 'grad_norm': 0.4207460880279541, 'learning_rate': 1.4938720369976385e-05, 'epoch': 0.36}
{'loss': 1.0879, 'grad_norm': 0.4570085108280182, 'learning_rate': 1.4933301475313036e-05, 'epoch': 0.36}
{'loss': 0.9788, 'grad_norm': 0.4058309495449066, 'learning_rate': 1.4927880665372197e-05, 'epoch': 0.36}
{'loss': 1.0112, 'grad_norm': 0.42468753457069397, 'learning_rate': 1.4922457942258411e-05, 'epoch': 0.36}
{'loss': 1.1619, 'grad_norm': 0.5347744226455688, 'learning_rate': 1.4917033308076967e-05, 'epoch': 0.36}
{'loss': 1.0471, 'grad_norm': 0.4287535846233368, 'learning_rate': 1.4911606764933892e-05, 'epoch': 0.36}
{'loss': 1.0628, 'grad_norm': 0.45339933037757874, 'learning_rate': 1.490617831493596e-05, 'epoch': 0.36}
{'loss': 1.0727, 'grad_norm': 0.44778406620025635, 'learning_rate': 1.4900747960190682e-05, 'epoch': 0.36}
{'loss': 0.9347, 'grad_norm': 0.4334333539009094, 'learning_rate': 1.489531570280631e-05, 'epoch': 0.36}
{'loss': 0.9656, 'grad_norm': 0.42460307478904724, 'learning_rate': 1.488988154489183e-05, 'epoch': 0.36}
{'loss': 1.1284, 'grad_norm': 0.49188658595085144, 'learning_rate': 1.4884445488556972e-05, 'epoch': 0.36}
{'loss': 0.9735, 'grad_norm': 0.28813493251800537, 'learning_rate': 1.4879007535912198e-05, 'epoch': 0.36}
{'loss': 1.0999, 'grad_norm': 0.42245104908943176, 'learning_rate': 1.4873567689068708e-05, 'epoch': 0.36}
{'loss': 0.9604, 'grad_norm': 0.3885961174964905, 'learning_rate': 1.4868125950138442e-05, 'epoch': 0.36}
{'loss': 1.0334, 'grad_norm': 0.38492101430892944, 'learning_rate': 1.4862682321234064e-05, 'epoch': 0.36}
{'loss': 0.9513, 'grad_norm': 0.41412097215652466, 'learning_rate': 1.4857236804468983e-05, 'epoch': 0.36}
{'loss': 0.9747, 'grad_norm': 0.4088265895843506, 'learning_rate': 1.4851789401957338e-05, 'epoch': 0.36}
{'loss': 1.034, 'grad_norm': 0.43044838309288025, 'learning_rate': 1.4846340115813993e-05, 'epoch': 0.36}
{'loss': 1.1188, 'grad_norm': 0.4578317701816559, 'learning_rate': 1.484088894815455e-05, 'epoch': 0.36}
{'loss': 0.9099, 'grad_norm': 0.27404218912124634, 'learning_rate': 1.4835435901095341e-05, 'epoch': 0.36}
{'loss': 1.0861, 'grad_norm': 0.41420412063598633, 'learning_rate': 1.4829980976753426e-05, 'epoch': 0.36}
{'loss': 0.9767, 'grad_norm': 0.4340921938419342, 'learning_rate': 1.4824524177246597e-05, 'epoch': 0.36}
{'loss': 1.0587, 'grad_norm': 0.4316404163837433, 'learning_rate': 1.4819065504693365e-05, 'epoch': 0.36}
{'loss': 1.0432, 'grad_norm': 0.41589221358299255, 'learning_rate': 1.4813604961212984e-05, 'epoch': 0.36}
{'loss': 1.0664, 'grad_norm': 0.46835359930992126, 'learning_rate': 1.4808142548925417e-05, 'epoch': 0.36}
{'loss': 1.0691, 'grad_norm': 0.4638248085975647, 'learning_rate': 1.4802678269951365e-05, 'epoch': 0.36}
{'loss': 0.9219, 'grad_norm': 0.41361239552497864, 'learning_rate': 1.4797212126412243e-05, 'epoch': 0.36}
{'loss': 1.0998, 'grad_norm': 0.4386131763458252, 'learning_rate': 1.4791744120430202e-05, 'epoch': 0.36}
{'loss': 1.0937, 'grad_norm': 0.46470463275909424, 'learning_rate': 1.4786274254128112e-05, 'epoch': 0.36}
{'loss': 1.0372, 'grad_norm': 0.46155139803886414, 'learning_rate': 1.4780802529629559e-05, 'epoch': 0.36}
{'loss': 1.1208, 'grad_norm': 0.44807112216949463, 'learning_rate': 1.4775328949058856e-05, 'epoch': 0.36}
{'loss': 0.9123, 'grad_norm': 0.4127090573310852, 'learning_rate': 1.4769853514541037e-05, 'epoch': 0.36}
{'loss': 1.0364, 'grad_norm': 0.4600914716720581, 'learning_rate': 1.4764376228201848e-05, 'epoch': 0.36}
{'loss': 1.0482, 'grad_norm': 0.4069674611091614, 'learning_rate': 1.475889709216777e-05, 'epoch': 0.36}
{'loss': 1.0807, 'grad_norm': 0.4678286612033844, 'learning_rate': 1.4753416108565985e-05, 'epoch': 0.36}
{'loss': 1.1151, 'grad_norm': 0.48613765835762024, 'learning_rate': 1.47479332795244e-05, 'epoch': 0.36}
{'loss': 0.9631, 'grad_norm': 0.26809316873550415, 'learning_rate': 1.4742448607171644e-05, 'epoch': 0.36}
{'loss': 1.0251, 'grad_norm': 0.432822048664093, 'learning_rate': 1.473696209363705e-05, 'epoch': 0.36}
{'loss': 1.0301, 'grad_norm': 0.46507665514945984, 'learning_rate': 1.4731473741050673e-05, 'epoch': 0.36}
{'loss': 0.9509, 'grad_norm': 0.4241046607494354, 'learning_rate': 1.4725983551543279e-05, 'epoch': 0.36}
{'loss': 1.0433, 'grad_norm': 0.4015037417411804, 'learning_rate': 1.472049152724635e-05, 'epoch': 0.36}
{'loss': 1.0221, 'grad_norm': 0.42335695028305054, 'learning_rate': 1.471499767029208e-05, 'epoch': 0.36}
{'loss': 0.9438, 'grad_norm': 0.42734572291374207, 'learning_rate': 1.470950198281337e-05, 'epoch': 0.36}
{'loss': 0.9521, 'grad_norm': 0.44414445757865906, 'learning_rate': 1.470400446694384e-05, 'epoch': 0.36}
{'loss': 1.0636, 'grad_norm': 0.48129791021347046, 'learning_rate': 1.4698505124817811e-05, 'epoch': 0.36}
{'loss': 0.992, 'grad_norm': 0.30213040113449097, 'learning_rate': 1.4693003958570318e-05, 'epoch': 0.36}
{'loss': 1.0461, 'grad_norm': 0.4044854938983917, 'learning_rate': 1.4687500970337103e-05, 'epoch': 0.36}
{'loss': 1.0753, 'grad_norm': 0.4141198992729187, 'learning_rate': 1.4681996162254618e-05, 'epoch': 0.36}
{'loss': 1.0621, 'grad_norm': 0.4201701581478119, 'learning_rate': 1.4676489536460015e-05, 'epoch': 0.36}
{'loss': 0.9928, 'grad_norm': 0.41218245029449463, 'learning_rate': 1.467098109509116e-05, 'epoch': 0.37}
{'loss': 1.0796, 'grad_norm': 0.47263991832733154, 'learning_rate': 1.4665470840286614e-05, 'epoch': 0.37}
{'loss': 1.1047, 'grad_norm': 0.4200536608695984, 'learning_rate': 1.4659958774185654e-05, 'epoch': 0.37}
{'loss': 1.1393, 'grad_norm': 0.4475201368331909, 'learning_rate': 1.4654444898928249e-05, 'epoch': 0.37}
{'loss': 1.0573, 'grad_norm': 0.4181520938873291, 'learning_rate': 1.4648929216655077e-05, 'epoch': 0.37}
{'loss': 1.0163, 'grad_norm': 0.40546920895576477, 'learning_rate': 1.4643411729507517e-05, 'epoch': 0.37}
{'loss': 1.1569, 'grad_norm': 0.5015528798103333, 'learning_rate': 1.4637892439627644e-05, 'epoch': 0.37}
{'loss': 1.0927, 'grad_norm': 0.4586334228515625, 'learning_rate': 1.4632371349158241e-05, 'epoch': 0.37}
{'loss': 1.0098, 'grad_norm': 0.42733708024024963, 'learning_rate': 1.4626848460242782e-05, 'epoch': 0.37}
{'loss': 0.9879, 'grad_norm': 0.4116649627685547, 'learning_rate': 1.4621323775025444e-05, 'epoch': 0.37}
Error with image file is truncated (10 bytes not processed)
{'loss': 1.0496, 'grad_norm': 0.4406522512435913, 'learning_rate': 1.4615797295651099e-05, 'epoch': 0.37}
{'loss': 1.0576, 'grad_norm': 0.45148178935050964, 'learning_rate': 1.4610269024265317e-05, 'epoch': 0.37}
{'loss': 0.9244, 'grad_norm': 0.2364613115787506, 'learning_rate': 1.4604738963014365e-05, 'epoch': 0.37}
{'loss': 1.0296, 'grad_norm': 0.3930482566356659, 'learning_rate': 1.4599207114045202e-05, 'epoch': 0.37}
{'loss': 0.9978, 'grad_norm': 0.379557341337204, 'learning_rate': 1.4593673479505482e-05, 'epoch': 0.37}
{'loss': 1.0652, 'grad_norm': 0.40535029768943787, 'learning_rate': 1.4588138061543551e-05, 'epoch': 0.37}
{'loss': 0.9425, 'grad_norm': 0.2668183743953705, 'learning_rate': 1.458260086230845e-05, 'epoch': 0.37}
{'loss': 1.0689, 'grad_norm': 0.4882260859012604, 'learning_rate': 1.4577061883949912e-05, 'epoch': 0.37}
{'loss': 1.0431, 'grad_norm': 0.431265652179718, 'learning_rate': 1.4571521128618358e-05, 'epoch': 0.37}
{'loss': 0.9867, 'grad_norm': 0.4106060266494751, 'learning_rate': 1.4565978598464895e-05, 'epoch': 0.37}
{'loss': 0.9989, 'grad_norm': 0.437595933675766, 'learning_rate': 1.4560434295641338e-05, 'epoch': 0.37}
{'loss': 1.0336, 'grad_norm': 0.44394880533218384, 'learning_rate': 1.455488822230016e-05, 'epoch': 0.37}
{'loss': 0.97, 'grad_norm': 0.38570138812065125, 'learning_rate': 1.4549340380594545e-05, 'epoch': 0.37}
{'loss': 1.0481, 'grad_norm': 0.44608888030052185, 'learning_rate': 1.454379077267836e-05, 'epoch': 0.37}
{'loss': 0.9982, 'grad_norm': 0.4216432273387909, 'learning_rate': 1.4538239400706147e-05, 'epoch': 0.37}
{'loss': 0.9446, 'grad_norm': 0.25247886776924133, 'learning_rate': 1.4532686266833143e-05, 'epoch': 0.37}
{'loss': 0.9704, 'grad_norm': 0.4401332139968872, 'learning_rate': 1.4527131373215265e-05, 'epoch': 0.37}
{'loss': 0.9108, 'grad_norm': 0.24152731895446777, 'learning_rate': 1.4521574722009115e-05, 'epoch': 0.37}
{'loss': 1.0123, 'grad_norm': 0.4446905553340912, 'learning_rate': 1.4516016315371974e-05, 'epoch': 0.37}
{'loss': 0.9766, 'grad_norm': 0.41531383991241455, 'learning_rate': 1.4510456155461807e-05, 'epoch': 0.37}
{'loss': 1.0046, 'grad_norm': 0.432354599237442, 'learning_rate': 1.4504894244437264e-05, 'epoch': 0.37}
{'loss': 1.0491, 'grad_norm': 0.43969401717185974, 'learning_rate': 1.4499330584457667e-05, 'epoch': 0.37}
{'loss': 1.1088, 'grad_norm': 0.4305489957332611, 'learning_rate': 1.4493765177683017e-05, 'epoch': 0.37}
{'loss': 1.0268, 'grad_norm': 0.4225224554538727, 'learning_rate': 1.4488198026274007e-05, 'epoch': 0.37}
{'loss': 0.9972, 'grad_norm': 0.4312281012535095, 'learning_rate': 1.4482629132391985e-05, 'epoch': 0.37}
{'loss': 0.9275, 'grad_norm': 0.2623137831687927, 'learning_rate': 1.4477058498198993e-05, 'epoch': 0.37}
{'loss': 1.0463, 'grad_norm': 0.4631807804107666, 'learning_rate': 1.4471486125857743e-05, 'epoch': 0.37}
{'loss': 1.0228, 'grad_norm': 0.4018787145614624, 'learning_rate': 1.446591201753162e-05, 'epoch': 0.37}
{'loss': 1.0248, 'grad_norm': 0.460712194442749, 'learning_rate': 1.4460336175384688e-05, 'epoch': 0.37}
{'loss': 1.0255, 'grad_norm': 0.3992355465888977, 'learning_rate': 1.4454758601581675e-05, 'epoch': 0.37}
{'loss': 0.9393, 'grad_norm': 0.419616162776947, 'learning_rate': 1.4449179298287999e-05, 'epoch': 0.37}
{'loss': 0.9258, 'grad_norm': 0.41302254796028137, 'learning_rate': 1.4443598267669723e-05, 'epoch': 0.37}
{'loss': 1.1602, 'grad_norm': 0.49832218885421753, 'learning_rate': 1.4438015511893602e-05, 'epoch': 0.37}
{'loss': 1.0492, 'grad_norm': 0.4771825671195984, 'learning_rate': 1.4432431033127056e-05, 'epoch': 0.37}
{'loss': 0.9082, 'grad_norm': 0.40010327100753784, 'learning_rate': 1.442684483353817e-05, 'epoch': 0.37}
{'loss': 0.9859, 'grad_norm': 0.43942466378211975, 'learning_rate': 1.4421256915295697e-05, 'epoch': 0.37}
{'loss': 1.1106, 'grad_norm': 0.45046913623809814, 'learning_rate': 1.4415667280569064e-05, 'epoch': 0.37}
{'loss': 0.995, 'grad_norm': 0.439582496881485, 'learning_rate': 1.4410075931528356e-05, 'epoch': 0.37}
{'loss': 1.0591, 'grad_norm': 0.4293566346168518, 'learning_rate': 1.4404482870344322e-05, 'epoch': 0.37}
{'loss': 0.9728, 'grad_norm': 0.418404757976532, 'learning_rate': 1.4398888099188396e-05, 'epoch': 0.37}
{'loss': 1.0379, 'grad_norm': 0.44434019923210144, 'learning_rate': 1.4393291620232646e-05, 'epoch': 0.37}
{'loss': 1.0184, 'grad_norm': 0.43298330903053284, 'learning_rate': 1.4387693435649826e-05, 'epoch': 0.37}
{'loss': 0.9339, 'grad_norm': 0.4515424966812134, 'learning_rate': 1.4382093547613338e-05, 'epoch': 0.38}
{'loss': 1.0506, 'grad_norm': 0.4474427402019501, 'learning_rate': 1.4376491958297263e-05, 'epoch': 0.38}
{'loss': 1.0062, 'grad_norm': 0.4270933270454407, 'learning_rate': 1.4370888669876317e-05, 'epoch': 0.38}
{'loss': 1.006, 'grad_norm': 0.4696318805217743, 'learning_rate': 1.4365283684525895e-05, 'epoch': 0.38}
{'loss': 1.1106, 'grad_norm': 0.49524566531181335, 'learning_rate': 1.4359677004422045e-05, 'epoch': 0.38}
{'loss': 1.0713, 'grad_norm': 0.4375026822090149, 'learning_rate': 1.4354068631741476e-05, 'epoch': 0.38}
{'loss': 1.123, 'grad_norm': 0.47066494822502136, 'learning_rate': 1.4348458568661548e-05, 'epoch': 0.38}
{'loss': 1.0429, 'grad_norm': 0.43371084332466125, 'learning_rate': 1.434284681736028e-05, 'epoch': 0.38}
{'loss': 1.0526, 'grad_norm': 0.44227832555770874, 'learning_rate': 1.4337233380016354e-05, 'epoch': 0.38}
{'loss': 0.9913, 'grad_norm': 0.3947139084339142, 'learning_rate': 1.433161825880909e-05, 'epoch': 0.38}
{'loss': 0.9266, 'grad_norm': 0.2415163218975067, 'learning_rate': 1.432600145591848e-05, 'epoch': 0.38}
{'loss': 1.0296, 'grad_norm': 0.4203041195869446, 'learning_rate': 1.4320382973525151e-05, 'epoch': 0.38}
{'loss': 1.0952, 'grad_norm': 0.4438357651233673, 'learning_rate': 1.43147628138104e-05, 'epoch': 0.38}
{'loss': 1.0859, 'grad_norm': 0.44089654088020325, 'learning_rate': 1.4309140978956161e-05, 'epoch': 0.38}
{'loss': 1.0106, 'grad_norm': 0.45013532042503357, 'learning_rate': 1.430351747114503e-05, 'epoch': 0.38}
{'loss': 0.8882, 'grad_norm': 0.29146963357925415, 'learning_rate': 1.429789229256024e-05, 'epoch': 0.38}
{'loss': 0.855, 'grad_norm': 0.24896544218063354, 'learning_rate': 1.429226544538568e-05, 'epoch': 0.38}
{'loss': 1.0548, 'grad_norm': 0.41392606496810913, 'learning_rate': 1.4286636931805887e-05, 'epoch': 0.38}
{'loss': 1.0128, 'grad_norm': 0.3826866149902344, 'learning_rate': 1.4281006754006045e-05, 'epoch': 0.38}
{'loss': 0.9233, 'grad_norm': 0.36497801542282104, 'learning_rate': 1.427537491417198e-05, 'epoch': 0.38}
{'loss': 1.097, 'grad_norm': 0.45189806818962097, 'learning_rate': 1.426974141449017e-05, 'epoch': 0.38}
{'loss': 1.0103, 'grad_norm': 0.42347317934036255, 'learning_rate': 1.4264106257147732e-05, 'epoch': 0.38}
{'loss': 1.1559, 'grad_norm': 0.44486621022224426, 'learning_rate': 1.4258469444332423e-05, 'epoch': 0.38}
{'loss': 0.9804, 'grad_norm': 0.4043509066104889, 'learning_rate': 1.4252830978232658e-05, 'epoch': 0.38}
{'loss': 1.1045, 'grad_norm': 0.48056042194366455, 'learning_rate': 1.4247190861037474e-05, 'epoch': 0.38}
{'loss': 0.9642, 'grad_norm': 0.40387535095214844, 'learning_rate': 1.4241549094936567e-05, 'epoch': 0.38}
{'loss': 1.0769, 'grad_norm': 0.47297602891921997, 'learning_rate': 1.4235905682120255e-05, 'epoch': 0.38}
{'loss': 0.9329, 'grad_norm': 0.41351455450057983, 'learning_rate': 1.4230260624779512e-05, 'epoch': 0.38}
{'loss': 0.9944, 'grad_norm': 0.389311820268631, 'learning_rate': 1.4224613925105947e-05, 'epoch': 0.38}
{'loss': 0.9888, 'grad_norm': 0.4083751440048218, 'learning_rate': 1.4218965585291792e-05, 'epoch': 0.38}
{'loss': 1.0001, 'grad_norm': 0.4439537227153778, 'learning_rate': 1.4213315607529939e-05, 'epoch': 0.38}
{'loss': 1.091, 'grad_norm': 0.43712493777275085, 'learning_rate': 1.4207663994013896e-05, 'epoch': 0.38}
{'loss': 1.1065, 'grad_norm': 0.47246021032333374, 'learning_rate': 1.4202010746937815e-05, 'epoch': 0.38}
{'loss': 1.1202, 'grad_norm': 0.4676264524459839, 'learning_rate': 1.4196355868496485e-05, 'epoch': 0.38}
{'loss': 1.0126, 'grad_norm': 0.4535331726074219, 'learning_rate': 1.4190699360885323e-05, 'epoch': 0.38}
{'loss': 1.0277, 'grad_norm': 0.4114263653755188, 'learning_rate': 1.4185041226300376e-05, 'epoch': 0.38}
{'loss': 0.9905, 'grad_norm': 0.4510006904602051, 'learning_rate': 1.4179381466938332e-05, 'epoch': 0.38}
{'loss': 1.1201, 'grad_norm': 0.4414333403110504, 'learning_rate': 1.4173720084996501e-05, 'epoch': 0.38}
{'loss': 1.1186, 'grad_norm': 0.45408281683921814, 'learning_rate': 1.4168057082672828e-05, 'epoch': 0.38}
{'loss': 0.9823, 'grad_norm': 0.4031577408313751, 'learning_rate': 1.4162392462165884e-05, 'epoch': 0.38}
{'loss': 1.0641, 'grad_norm': 0.4452672004699707, 'learning_rate': 1.4156726225674874e-05, 'epoch': 0.38}
{'loss': 1.0819, 'grad_norm': 0.43960216641426086, 'learning_rate': 1.415105837539962e-05, 'epoch': 0.38}
{'loss': 1.0706, 'grad_norm': 0.4134422242641449, 'learning_rate': 1.414538891354058e-05, 'epoch': 0.38}
{'loss': 1.1301, 'grad_norm': 0.4362705945968628, 'learning_rate': 1.4139717842298835e-05, 'epoch': 0.38}
{'loss': 0.9824, 'grad_norm': 0.4025520086288452, 'learning_rate': 1.4134045163876086e-05, 'epoch': 0.38}
{'loss': 1.0201, 'grad_norm': 0.3883660137653351, 'learning_rate': 1.4128370880474667e-05, 'epoch': 0.38}
{'loss': 1.1108, 'grad_norm': 0.453819215297699, 'learning_rate': 1.412269499429753e-05, 'epoch': 0.38}
{'loss': 1.0103, 'grad_norm': 0.44724732637405396, 'learning_rate': 1.4117017507548244e-05, 'epoch': 0.38}
{'loss': 0.8865, 'grad_norm': 0.411839097738266, 'learning_rate': 1.4111338422431013e-05, 'epoch': 0.38}
{'loss': 1.1249, 'grad_norm': 0.4356051981449127, 'learning_rate': 1.4105657741150648e-05, 'epoch': 0.38}
{'loss': 1.0937, 'grad_norm': 0.41989070177078247, 'learning_rate': 1.4099975465912584e-05, 'epoch': 0.38}
{'loss': 1.0045, 'grad_norm': 0.42397674918174744, 'learning_rate': 1.4094291598922877e-05, 'epoch': 0.38}
{'loss': 0.9793, 'grad_norm': 0.39764320850372314, 'learning_rate': 1.40886061423882e-05, 'epoch': 0.39}
{'loss': 0.94, 'grad_norm': 0.3922428488731384, 'learning_rate': 1.4082919098515846e-05, 'epoch': 0.39}
{'loss': 1.0783, 'grad_norm': 0.46527108550071716, 'learning_rate': 1.407723046951372e-05, 'epoch': 0.39}
{'loss': 1.1028, 'grad_norm': 0.43535423278808594, 'learning_rate': 1.4071540257590341e-05, 'epoch': 0.39}
{'loss': 0.9653, 'grad_norm': 0.39076897501945496, 'learning_rate': 1.4065848464954848e-05, 'epoch': 0.39}
{'loss': 0.9112, 'grad_norm': 0.26765573024749756, 'learning_rate': 1.4060155093816988e-05, 'epoch': 0.39}
{'loss': 0.9943, 'grad_norm': 0.4344504773616791, 'learning_rate': 1.4054460146387124e-05, 'epoch': 0.39}
{'loss': 0.9373, 'grad_norm': 0.44948717951774597, 'learning_rate': 1.4048763624876233e-05, 'epoch': 0.39}
{'loss': 0.9376, 'grad_norm': 0.4360678195953369, 'learning_rate': 1.4043065531495904e-05, 'epoch': 0.39}
{'loss': 1.0366, 'grad_norm': 0.4118284285068512, 'learning_rate': 1.4037365868458325e-05, 'epoch': 0.39}
{'loss': 0.9443, 'grad_norm': 0.2744523286819458, 'learning_rate': 1.4031664637976305e-05, 'epoch': 0.39}
{'loss': 1.0289, 'grad_norm': 0.4244089722633362, 'learning_rate': 1.402596184226326e-05, 'epoch': 0.39}
{'loss': 0.9708, 'grad_norm': 0.43447959423065186, 'learning_rate': 1.4020257483533208e-05, 'epoch': 0.39}
{'loss': 1.0092, 'grad_norm': 0.41238856315612793, 'learning_rate': 1.401455156400078e-05, 'epoch': 0.39}
{'loss': 1.0994, 'grad_norm': 0.45782744884490967, 'learning_rate': 1.400884408588121e-05, 'epoch': 0.39}
{'loss': 1.0069, 'grad_norm': 0.38444042205810547, 'learning_rate': 1.400313505139034e-05, 'epoch': 0.39}
{'loss': 1.0063, 'grad_norm': 0.4133279025554657, 'learning_rate': 1.3997424462744607e-05, 'epoch': 0.39}
{'loss': 0.9783, 'grad_norm': 0.44337472319602966, 'learning_rate': 1.3991712322161065e-05, 'epoch': 0.39}
{'loss': 1.0485, 'grad_norm': 0.43038105964660645, 'learning_rate': 1.3985998631857359e-05, 'epoch': 0.39}
{'loss': 0.9061, 'grad_norm': 0.24368304014205933, 'learning_rate': 1.398028339405174e-05, 'epoch': 0.39}
{'loss': 0.9595, 'grad_norm': 0.4752396047115326, 'learning_rate': 1.3974566610963068e-05, 'epoch': 0.39}
{'loss': 0.9574, 'grad_norm': 0.43479281663894653, 'learning_rate': 1.3968848284810785e-05, 'epoch': 0.39}
{'loss': 1.1422, 'grad_norm': 0.49809902906417847, 'learning_rate': 1.3963128417814951e-05, 'epoch': 0.39}
{'loss': 1.11, 'grad_norm': 0.44919145107269287, 'learning_rate': 1.3957407012196204e-05, 'epoch': 0.39}
{'loss': 1.0154, 'grad_norm': 0.43419548869132996, 'learning_rate': 1.3951684070175802e-05, 'epoch': 0.39}
{'loss': 0.9773, 'grad_norm': 0.40055105090141296, 'learning_rate': 1.3945959593975582e-05, 'epoch': 0.39}
{'loss': 0.9702, 'grad_norm': 0.4202180802822113, 'learning_rate': 1.3940233585817984e-05, 'epoch': 0.39}
{'loss': 1.0023, 'grad_norm': 0.4320472776889801, 'learning_rate': 1.3934506047926042e-05, 'epoch': 0.39}
{'loss': 1.1061, 'grad_norm': 0.47494393587112427, 'learning_rate': 1.3928776982523384e-05, 'epoch': 0.39}
{'loss': 0.9076, 'grad_norm': 0.3886021673679352, 'learning_rate': 1.3923046391834229e-05, 'epoch': 0.39}
{'loss': 0.9393, 'grad_norm': 0.2448084056377411, 'learning_rate': 1.3917314278083391e-05, 'epoch': 0.39}
{'loss': 0.9876, 'grad_norm': 0.4502960741519928, 'learning_rate': 1.3911580643496272e-05, 'epoch': 0.39}
{'loss': 1.0139, 'grad_norm': 0.43574395775794983, 'learning_rate': 1.3905845490298867e-05, 'epoch': 0.39}
{'loss': 0.9626, 'grad_norm': 0.2947717607021332, 'learning_rate': 1.390010882071776e-05, 'epoch': 0.39}
{'loss': 0.9787, 'grad_norm': 0.4154437780380249, 'learning_rate': 1.3894370636980128e-05, 'epoch': 0.39}
{'loss': 1.0832, 'grad_norm': 0.46615925431251526, 'learning_rate': 1.3888630941313728e-05, 'epoch': 0.39}
{'loss': 0.9206, 'grad_norm': 0.27875980734825134, 'learning_rate': 1.3882889735946901e-05, 'epoch': 0.39}
{'loss': 1.053, 'grad_norm': 0.42549407482147217, 'learning_rate': 1.3877147023108592e-05, 'epoch': 0.39}
{'loss': 0.8882, 'grad_norm': 0.25470659136772156, 'learning_rate': 1.3871402805028314e-05, 'epoch': 0.39}
{'loss': 1.0197, 'grad_norm': 0.4425196051597595, 'learning_rate': 1.3865657083936167e-05, 'epoch': 0.39}
{'loss': 1.087, 'grad_norm': 0.42859792709350586, 'learning_rate': 1.3859909862062844e-05, 'epoch': 0.39}
{'loss': 0.923, 'grad_norm': 0.4464331269264221, 'learning_rate': 1.385416114163961e-05, 'epoch': 0.39}
{'loss': 1.0672, 'grad_norm': 0.5020057559013367, 'learning_rate': 1.3848410924898321e-05, 'epoch': 0.39}
{'loss': 1.1305, 'grad_norm': 0.4559352695941925, 'learning_rate': 1.3842659214071406e-05, 'epoch': 0.39}
{'loss': 1.0238, 'grad_norm': 0.41107216477394104, 'learning_rate': 1.3836906011391878e-05, 'epoch': 0.39}
{'loss': 0.9479, 'grad_norm': 0.3979359269142151, 'learning_rate': 1.3831151319093323e-05, 'epoch': 0.39}
{'loss': 1.0649, 'grad_norm': 0.437030553817749, 'learning_rate': 1.382539513940992e-05, 'epoch': 0.39}
{'loss': 1.0175, 'grad_norm': 0.41902440786361694, 'learning_rate': 1.3819637474576411e-05, 'epoch': 0.39}
{'loss': 1.0057, 'grad_norm': 0.398820698261261, 'learning_rate': 1.381387832682812e-05, 'epoch': 0.39}
{'loss': 0.993, 'grad_norm': 0.42660582065582275, 'learning_rate': 1.380811769840095e-05, 'epoch': 0.39}
{'loss': 1.0072, 'grad_norm': 0.45502951741218567, 'learning_rate': 1.3802355591531366e-05, 'epoch': 0.39}
{'loss': 0.9578, 'grad_norm': 0.2464004009962082, 'learning_rate': 1.3796592008456427e-05, 'epoch': 0.39}
{'loss': 1.0523, 'grad_norm': 0.4748417139053345, 'learning_rate': 1.3790826951413747e-05, 'epoch': 0.4}
{'loss': 1.001, 'grad_norm': 0.43530747294425964, 'learning_rate': 1.3785060422641526e-05, 'epoch': 0.4}
{'loss': 0.9357, 'grad_norm': 0.43092045187950134, 'learning_rate': 1.3779292424378521e-05, 'epoch': 0.4}
{'loss': 0.9944, 'grad_norm': 0.4178406000137329, 'learning_rate': 1.3773522958864076e-05, 'epoch': 0.4}
{'loss': 1.0851, 'grad_norm': 0.45619598031044006, 'learning_rate': 1.3767752028338091e-05, 'epoch': 0.4}
{'loss': 0.9619, 'grad_norm': 0.4349154829978943, 'learning_rate': 1.376197963504104e-05, 'epoch': 0.4}
{'loss': 0.9354, 'grad_norm': 0.4399247169494629, 'learning_rate': 1.3756205781213965e-05, 'epoch': 0.4}
{'loss': 1.0, 'grad_norm': 0.4403943419456482, 'learning_rate': 1.375043046909848e-05, 'epoch': 0.4}
{'loss': 0.9239, 'grad_norm': 0.43205055594444275, 'learning_rate': 1.3744653700936752e-05, 'epoch': 0.4}
{'loss': 1.1744, 'grad_norm': 0.46349823474884033, 'learning_rate': 1.3738875478971526e-05, 'epoch': 0.4}
{'loss': 0.9823, 'grad_norm': 0.4483254551887512, 'learning_rate': 1.3733095805446107e-05, 'epoch': 0.4}
{'loss': 1.0993, 'grad_norm': 0.4965886175632477, 'learning_rate': 1.372731468260436e-05, 'epoch': 0.4}
{'loss': 0.9841, 'grad_norm': 0.40979263186454773, 'learning_rate': 1.372153211269072e-05, 'epoch': 0.4}
{'loss': 1.0535, 'grad_norm': 0.47021225094795227, 'learning_rate': 1.3715748097950176e-05, 'epoch': 0.4}
{'loss': 1.0993, 'grad_norm': 0.46924853324890137, 'learning_rate': 1.3709962640628284e-05, 'epoch': 0.4}
{'loss': 0.9416, 'grad_norm': 0.41753309965133667, 'learning_rate': 1.3704175742971158e-05, 'epoch': 0.4}
{'loss': 0.9227, 'grad_norm': 0.4015128016471863, 'learning_rate': 1.369838740722547e-05, 'epoch': 0.4}
{'loss': 1.039, 'grad_norm': 0.4910239279270172, 'learning_rate': 1.3692597635638452e-05, 'epoch': 0.4}
{'loss': 0.9735, 'grad_norm': 0.41990259289741516, 'learning_rate': 1.368680643045789e-05, 'epoch': 0.4}
{'loss': 0.9533, 'grad_norm': 0.45504599809646606, 'learning_rate': 1.3681013793932132e-05, 'epoch': 0.4}
{'loss': 0.9939, 'grad_norm': 0.41803106665611267, 'learning_rate': 1.3675219728310076e-05, 'epoch': 0.4}
{'loss': 1.0582, 'grad_norm': 0.428829163312912, 'learning_rate': 1.3669424235841185e-05, 'epoch': 0.4}
{'loss': 1.0041, 'grad_norm': 0.4198402464389801, 'learning_rate': 1.3663627318775459e-05, 'epoch': 0.4}
{'loss': 1.0569, 'grad_norm': 0.44498249888420105, 'learning_rate': 1.3657828979363468e-05, 'epoch': 0.4}
{'loss': 1.051, 'grad_norm': 0.3224560618400574, 'learning_rate': 1.3652029219856324e-05, 'epoch': 0.4}
{'loss': 0.9935, 'grad_norm': 0.4423999786376953, 'learning_rate': 1.3646228042505694e-05, 'epoch': 0.4}
{'loss': 1.0792, 'grad_norm': 0.4763401448726654, 'learning_rate': 1.3640425449563793e-05, 'epoch': 0.4}
{'loss': 1.0097, 'grad_norm': 0.4248906672000885, 'learning_rate': 1.3634621443283389e-05, 'epoch': 0.4}
{'loss': 1.0638, 'grad_norm': 0.4749751091003418, 'learning_rate': 1.36288160259178e-05, 'epoch': 0.4}
{'loss': 1.0645, 'grad_norm': 0.41584792733192444, 'learning_rate': 1.3623009199720882e-05, 'epoch': 0.4}
{'loss': 1.1213, 'grad_norm': 0.42903387546539307, 'learning_rate': 1.3617200966947053e-05, 'epoch': 0.4}
{'loss': 0.9871, 'grad_norm': 0.44036972522735596, 'learning_rate': 1.3611391329851262e-05, 'epoch': 0.4}
{'loss': 1.0805, 'grad_norm': 0.441734254360199, 'learning_rate': 1.3605580290689013e-05, 'epoch': 0.4}
{'loss': 0.9954, 'grad_norm': 0.4034305810928345, 'learning_rate': 1.3599767851716353e-05, 'epoch': 0.4}
{'loss': 1.0241, 'grad_norm': 0.4070691764354706, 'learning_rate': 1.3593954015189867e-05, 'epoch': 0.4}
{'loss': 0.9368, 'grad_norm': 0.4611506164073944, 'learning_rate': 1.3588138783366692e-05, 'epoch': 0.4}
{'loss': 1.0775, 'grad_norm': 0.43420448899269104, 'learning_rate': 1.3582322158504495e-05, 'epoch': 0.4}
{'loss': 1.0915, 'grad_norm': 0.443698912858963, 'learning_rate': 1.3576504142861496e-05, 'epoch': 0.4}
{'loss': 0.9341, 'grad_norm': 0.406284898519516, 'learning_rate': 1.3570684738696444e-05, 'epoch': 0.4}
{'loss': 1.1249, 'grad_norm': 0.4580717384815216, 'learning_rate': 1.3564863948268631e-05, 'epoch': 0.4}
{'loss': 0.9686, 'grad_norm': 0.39650076627731323, 'learning_rate': 1.3559041773837898e-05, 'epoch': 0.4}
{'loss': 1.0767, 'grad_norm': 0.45479220151901245, 'learning_rate': 1.3553218217664603e-05, 'epoch': 0.4}
{'loss': 0.914, 'grad_norm': 0.40288639068603516, 'learning_rate': 1.3547393282009656e-05, 'epoch': 0.4}
{'loss': 1.1255, 'grad_norm': 0.47006043791770935, 'learning_rate': 1.3541566969134496e-05, 'epoch': 0.4}
{'loss': 1.0077, 'grad_norm': 0.443735271692276, 'learning_rate': 1.3535739281301102e-05, 'epoch': 0.4}
{'loss': 0.9371, 'grad_norm': 0.39135733246803284, 'learning_rate': 1.3529910220771975e-05, 'epoch': 0.4}
{'loss': 0.9692, 'grad_norm': 0.43876147270202637, 'learning_rate': 1.3524079789810163e-05, 'epoch': 0.4}
{'loss': 1.0799, 'grad_norm': 0.42445722222328186, 'learning_rate': 1.3518247990679241e-05, 'epoch': 0.4}
{'loss': 0.9478, 'grad_norm': 0.4451741576194763, 'learning_rate': 1.3512414825643312e-05, 'epoch': 0.4}
{'loss': 1.0099, 'grad_norm': 0.426982581615448, 'learning_rate': 1.3506580296967011e-05, 'epoch': 0.4}
{'loss': 1.0425, 'grad_norm': 0.43714606761932373, 'learning_rate': 1.3500744406915505e-05, 'epoch': 0.4}
{'loss': 0.9559, 'grad_norm': 0.4376620054244995, 'learning_rate': 1.3494907157754485e-05, 'epoch': 0.4}
{'loss': 1.0527, 'grad_norm': 0.4189918041229248, 'learning_rate': 1.348906855175017e-05, 'epoch': 0.41}
{'loss': 0.9155, 'grad_norm': 0.3868243396282196, 'learning_rate': 1.3483228591169315e-05, 'epoch': 0.41}
{'loss': 0.916, 'grad_norm': 0.4214196801185608, 'learning_rate': 1.347738727827919e-05, 'epoch': 0.41}
{'loss': 1.0649, 'grad_norm': 0.4501473903656006, 'learning_rate': 1.3471544615347591e-05, 'epoch': 0.41}
{'loss': 1.0936, 'grad_norm': 0.46983373165130615, 'learning_rate': 1.3465700604642847e-05, 'epoch': 0.41}
{'loss': 0.8916, 'grad_norm': 0.24134694039821625, 'learning_rate': 1.34598552484338e-05, 'epoch': 0.41}
{'loss': 1.0122, 'grad_norm': 0.45506325364112854, 'learning_rate': 1.3454008548989816e-05, 'epoch': 0.41}
{'loss': 1.1239, 'grad_norm': 0.46001264452934265, 'learning_rate': 1.3448160508580789e-05, 'epoch': 0.41}
{'loss': 0.8929, 'grad_norm': 0.26525232195854187, 'learning_rate': 1.3442311129477133e-05, 'epoch': 0.41}
{'loss': 1.0137, 'grad_norm': 0.4375482499599457, 'learning_rate': 1.343646041394977e-05, 'epoch': 0.41}
{'loss': 1.1251, 'grad_norm': 0.4379447400569916, 'learning_rate': 1.3430608364270156e-05, 'epoch': 0.41}
{'loss': 1.0284, 'grad_norm': 0.4175895154476166, 'learning_rate': 1.3424754982710256e-05, 'epoch': 0.41}
{'loss': 1.1325, 'grad_norm': 0.47463172674179077, 'learning_rate': 1.3418900271542552e-05, 'epoch': 0.41}
{'loss': 0.9818, 'grad_norm': 0.4319048821926117, 'learning_rate': 1.3413044233040045e-05, 'epoch': 0.41}
{'loss': 0.9493, 'grad_norm': 0.3927172124385834, 'learning_rate': 1.3407186869476253e-05, 'epoch': 0.41}
{'loss': 1.0011, 'grad_norm': 0.40680527687072754, 'learning_rate': 1.3401328183125208e-05, 'epoch': 0.41}
{'loss': 0.9839, 'grad_norm': 0.4491249620914459, 'learning_rate': 1.339546817626145e-05, 'epoch': 0.41}
{'loss': 1.0744, 'grad_norm': 0.4405438303947449, 'learning_rate': 1.3389606851160037e-05, 'epoch': 0.41}
{'loss': 1.0044, 'grad_norm': 0.400689035654068, 'learning_rate': 1.3383744210096537e-05, 'epoch': 0.41}
{'loss': 1.0425, 'grad_norm': 0.4168822169303894, 'learning_rate': 1.3377880255347026e-05, 'epoch': 0.41}
{'loss': 0.9311, 'grad_norm': 0.4336118996143341, 'learning_rate': 1.3372014989188098e-05, 'epoch': 0.41}
{'loss': 1.0661, 'grad_norm': 0.4539406895637512, 'learning_rate': 1.3366148413896851e-05, 'epoch': 0.41}
{'loss': 1.2534, 'grad_norm': 0.51210618019104, 'learning_rate': 1.3360280531750886e-05, 'epoch': 0.41}
{'loss': 1.1567, 'grad_norm': 0.4331015348434448, 'learning_rate': 1.3354411345028324e-05, 'epoch': 0.41}
{'loss': 1.0364, 'grad_norm': 0.4022444188594818, 'learning_rate': 1.3348540856007782e-05, 'epoch': 0.41}
{'loss': 1.0909, 'grad_norm': 0.4609307050704956, 'learning_rate': 1.3342669066968385e-05, 'epoch': 0.41}
{'loss': 1.0087, 'grad_norm': 0.3917200267314911, 'learning_rate': 1.3336795980189763e-05, 'epoch': 0.41}
{'loss': 1.0726, 'grad_norm': 0.4448240399360657, 'learning_rate': 1.3330921597952056e-05, 'epoch': 0.41}
{'loss': 1.0828, 'grad_norm': 0.4026748239994049, 'learning_rate': 1.3325045922535896e-05, 'epoch': 0.41}
{'loss': 0.965, 'grad_norm': 0.43123310804367065, 'learning_rate': 1.3319168956222423e-05, 'epoch': 0.41}
{'loss': 1.0215, 'grad_norm': 0.4261434078216553, 'learning_rate': 1.331329070129328e-05, 'epoch': 0.41}
{'loss': 1.0486, 'grad_norm': 0.4199660122394562, 'learning_rate': 1.3307411160030608e-05, 'epoch': 0.41}
{'loss': 1.1282, 'grad_norm': 0.4350520968437195, 'learning_rate': 1.3301530334717046e-05, 'epoch': 0.41}
{'loss': 1.0229, 'grad_norm': 0.3905341327190399, 'learning_rate': 1.3295648227635729e-05, 'epoch': 0.41}
{'loss': 1.082, 'grad_norm': 0.4141634404659271, 'learning_rate': 1.32897648410703e-05, 'epoch': 0.41}
{'loss': 0.9771, 'grad_norm': 0.40817323327064514, 'learning_rate': 1.328388017730489e-05, 'epoch': 0.41}
{'loss': 0.9736, 'grad_norm': 0.27353933453559875, 'learning_rate': 1.327799423862413e-05, 'epoch': 0.41}
{'loss': 1.0151, 'grad_norm': 0.40814653038978577, 'learning_rate': 1.3272107027313142e-05, 'epoch': 0.41}
{'loss': 1.0255, 'grad_norm': 0.4124704599380493, 'learning_rate': 1.3266218545657541e-05, 'epoch': 0.41}
{'loss': 1.0438, 'grad_norm': 0.447244256734848, 'learning_rate': 1.326032879594344e-05, 'epoch': 0.41}
{'loss': 0.9812, 'grad_norm': 0.4222315549850464, 'learning_rate': 1.3254437780457448e-05, 'epoch': 0.41}
{'loss': 1.057, 'grad_norm': 0.4364326596260071, 'learning_rate': 1.3248545501486654e-05, 'epoch': 0.41}
{'loss': 0.9595, 'grad_norm': 0.4231280982494354, 'learning_rate': 1.3242651961318646e-05, 'epoch': 0.41}
{'loss': 1.1164, 'grad_norm': 0.44164684414863586, 'learning_rate': 1.32367571622415e-05, 'epoch': 0.41}
{'loss': 1.0369, 'grad_norm': 0.42867517471313477, 'learning_rate': 1.3230861106543777e-05, 'epoch': 0.41}
{'loss': 1.0, 'grad_norm': 0.42409011721611023, 'learning_rate': 1.3224963796514532e-05, 'epoch': 0.41}
{'loss': 1.0318, 'grad_norm': 0.43788471817970276, 'learning_rate': 1.32190652344433e-05, 'epoch': 0.41}
{'loss': 0.9903, 'grad_norm': 0.3933958113193512, 'learning_rate': 1.3213165422620111e-05, 'epoch': 0.41}
{'loss': 0.9748, 'grad_norm': 0.2636933922767639, 'learning_rate': 1.3207264363335472e-05, 'epoch': 0.41}
{'loss': 0.9535, 'grad_norm': 0.44849029183387756, 'learning_rate': 1.3201362058880375e-05, 'epoch': 0.41}
{'loss': 1.034, 'grad_norm': 0.42332059144973755, 'learning_rate': 1.3195458511546307e-05, 'epoch': 0.41}
{'loss': 1.0496, 'grad_norm': 0.4275941848754883, 'learning_rate': 1.3189553723625217e-05, 'epoch': 0.41}
{'loss': 1.032, 'grad_norm': 0.4018867015838623, 'learning_rate': 1.318364769740955e-05, 'epoch': 0.42}
{'loss': 1.091, 'grad_norm': 0.4697065055370331, 'learning_rate': 1.3177740435192235e-05, 'epoch': 0.42}
{'loss': 1.1604, 'grad_norm': 0.5164402723312378, 'learning_rate': 1.3171831939266668e-05, 'epoch': 0.42}
{'loss': 0.9588, 'grad_norm': 0.38475778698921204, 'learning_rate': 1.3165922211926734e-05, 'epoch': 0.42}
{'loss': 1.0315, 'grad_norm': 0.4511030614376068, 'learning_rate': 1.3160011255466791e-05, 'epoch': 0.42}
{'loss': 1.0023, 'grad_norm': 0.44850581884384155, 'learning_rate': 1.3154099072181677e-05, 'epoch': 0.42}
{'loss': 0.9729, 'grad_norm': 0.4090794026851654, 'learning_rate': 1.3148185664366704e-05, 'epoch': 0.42}
{'loss': 0.942, 'grad_norm': 0.27249765396118164, 'learning_rate': 1.314227103431766e-05, 'epoch': 0.42}
{'loss': 1.0514, 'grad_norm': 0.4686215817928314, 'learning_rate': 1.3136355184330809e-05, 'epoch': 0.42}
{'loss': 0.9831, 'grad_norm': 0.42245668172836304, 'learning_rate': 1.3130438116702888e-05, 'epoch': 0.42}
{'loss': 0.9497, 'grad_norm': 0.4240807592868805, 'learning_rate': 1.3124519833731106e-05, 'epoch': 0.42}
{'loss': 1.1274, 'grad_norm': 0.45540645718574524, 'learning_rate': 1.3118600337713146e-05, 'epoch': 0.42}
{'loss': 1.1309, 'grad_norm': 0.4528723359107971, 'learning_rate': 1.3112679630947156e-05, 'epoch': 0.42}
{'loss': 1.0844, 'grad_norm': 0.5268611311912537, 'learning_rate': 1.310675771573176e-05, 'epoch': 0.42}
{'loss': 0.9688, 'grad_norm': 0.40559151768684387, 'learning_rate': 1.310083459436605e-05, 'epoch': 0.42}
{'loss': 0.9825, 'grad_norm': 0.4155178666114807, 'learning_rate': 1.3094910269149587e-05, 'epoch': 0.42}
{'loss': 1.0025, 'grad_norm': 0.4105319082736969, 'learning_rate': 1.3088984742382395e-05, 'epoch': 0.42}
{'loss': 1.0359, 'grad_norm': 0.4245613217353821, 'learning_rate': 1.3083058016364972e-05, 'epoch': 0.42}
{'loss': 0.9255, 'grad_norm': 0.42044633626937866, 'learning_rate': 1.3077130093398274e-05, 'epoch': 0.42}
{'loss': 1.1407, 'grad_norm': 0.4702780842781067, 'learning_rate': 1.3071200975783725e-05, 'epoch': 0.42}
{'loss': 0.9886, 'grad_norm': 0.42774057388305664, 'learning_rate': 1.3065270665823206e-05, 'epoch': 0.42}
{'loss': 1.1009, 'grad_norm': 0.4841838479042053, 'learning_rate': 1.3059339165819082e-05, 'epoch': 0.42}
{'loss': 1.0158, 'grad_norm': 0.4206780791282654, 'learning_rate': 1.3053406478074155e-05, 'epoch': 0.42}
{'loss': 0.97, 'grad_norm': 0.42885929346084595, 'learning_rate': 1.3047472604891701e-05, 'epoch': 0.42}
{'loss': 1.1305, 'grad_norm': 0.4620645046234131, 'learning_rate': 1.3041537548575455e-05, 'epoch': 0.42}
{'loss': 1.0777, 'grad_norm': 0.4490601122379303, 'learning_rate': 1.303560131142961e-05, 'epoch': 0.42}
{'loss': 0.9366, 'grad_norm': 0.43189483880996704, 'learning_rate': 1.3029663895758814e-05, 'epoch': 0.42}
{'loss': 1.0762, 'grad_norm': 0.46937984228134155, 'learning_rate': 1.3023725303868183e-05, 'epoch': 0.42}
{'loss': 1.0433, 'grad_norm': 0.4533061683177948, 'learning_rate': 1.3017785538063277e-05, 'epoch': 0.42}
{'loss': 1.1129, 'grad_norm': 0.4777233898639679, 'learning_rate': 1.3011844600650121e-05, 'epoch': 0.42}
{'loss': 1.0574, 'grad_norm': 0.39596936106681824, 'learning_rate': 1.300590249393519e-05, 'epoch': 0.42}
{'loss': 1.1267, 'grad_norm': 0.4596911668777466, 'learning_rate': 1.2999959220225416e-05, 'epoch': 0.42}
{'loss': 0.9044, 'grad_norm': 0.2502739727497101, 'learning_rate': 1.299401478182818e-05, 'epoch': 0.42}
{'loss': 0.992, 'grad_norm': 0.41355183720588684, 'learning_rate': 1.2988069181051314e-05, 'epoch': 0.42}
{'loss': 0.9996, 'grad_norm': 0.42427703738212585, 'learning_rate': 1.2982122420203114e-05, 'epoch': 0.42}
{'loss': 1.0065, 'grad_norm': 0.4108099937438965, 'learning_rate': 1.2976174501592313e-05, 'epoch': 0.42}
{'loss': 1.0678, 'grad_norm': 0.4111255705356598, 'learning_rate': 1.2970225427528098e-05, 'epoch': 0.42}
{'loss': 1.0207, 'grad_norm': 0.3884811997413635, 'learning_rate': 1.2964275200320104e-05, 'epoch': 0.42}
{'loss': 1.0187, 'grad_norm': 0.41422125697135925, 'learning_rate': 1.2958323822278413e-05, 'epoch': 0.42}
{'loss': 1.0582, 'grad_norm': 0.4951283633708954, 'learning_rate': 1.2952371295713558e-05, 'epoch': 0.42}
{'loss': 1.0317, 'grad_norm': 0.42912420630455017, 'learning_rate': 1.2946417622936512e-05, 'epoch': 0.42}
{'loss': 0.9964, 'grad_norm': 0.4075678884983063, 'learning_rate': 1.2940462806258696e-05, 'epoch': 0.42}
{'loss': 0.9417, 'grad_norm': 0.4192237854003906, 'learning_rate': 1.2934506847991976e-05, 'epoch': 0.42}
{'loss': 1.0452, 'grad_norm': 0.42750778794288635, 'learning_rate': 1.2928549750448661e-05, 'epoch': 0.42}
{'loss': 0.9466, 'grad_norm': 0.4064163863658905, 'learning_rate': 1.2922591515941498e-05, 'epoch': 0.42}
{'loss': 1.0536, 'grad_norm': 0.4260520040988922, 'learning_rate': 1.2916632146783683e-05, 'epoch': 0.42}
{'loss': 0.903, 'grad_norm': 0.4213380515575409, 'learning_rate': 1.2910671645288841e-05, 'epoch': 0.42}
{'loss': 1.0907, 'grad_norm': 0.4749300181865692, 'learning_rate': 1.2904710013771054e-05, 'epoch': 0.42}
{'loss': 1.0365, 'grad_norm': 0.48516973853111267, 'learning_rate': 1.2898747254544826e-05, 'epoch': 0.42}
{'loss': 1.0778, 'grad_norm': 0.4740178883075714, 'learning_rate': 1.2892783369925105e-05, 'epoch': 0.42}
{'loss': 1.0273, 'grad_norm': 0.43887344002723694, 'learning_rate': 1.2886818362227283e-05, 'epoch': 0.42}
{'loss': 1.1365, 'grad_norm': 0.501573920249939, 'learning_rate': 1.2880852233767174e-05, 'epoch': 0.42}
{'loss': 1.0167, 'grad_norm': 0.44565415382385254, 'learning_rate': 1.2874884986861038e-05, 'epoch': 0.43}
{'loss': 0.9618, 'grad_norm': 0.4318504333496094, 'learning_rate': 1.2868916623825561e-05, 'epoch': 0.43}
{'loss': 1.1018, 'grad_norm': 0.4739326536655426, 'learning_rate': 1.2862947146977876e-05, 'epoch': 0.43}
{'loss': 1.033, 'grad_norm': 0.4097159504890442, 'learning_rate': 1.2856976558635532e-05, 'epoch': 0.43}
{'loss': 0.9908, 'grad_norm': 0.4257715344429016, 'learning_rate': 1.2851004861116519e-05, 'epoch': 0.43}
{'loss': 1.08, 'grad_norm': 0.4452497661113739, 'learning_rate': 1.2845032056739257e-05, 'epoch': 0.43}
{'loss': 1.0567, 'grad_norm': 0.446890264749527, 'learning_rate': 1.2839058147822595e-05, 'epoch': 0.43}
{'loss': 1.0368, 'grad_norm': 0.4001668393611908, 'learning_rate': 1.2833083136685803e-05, 'epoch': 0.43}
{'loss': 1.0331, 'grad_norm': 0.44015002250671387, 'learning_rate': 1.2827107025648595e-05, 'epoch': 0.43}
{'loss': 1.0934, 'grad_norm': 0.49411094188690186, 'learning_rate': 1.2821129817031099e-05, 'epoch': 0.43}
{'loss': 1.0928, 'grad_norm': 0.43956807255744934, 'learning_rate': 1.2815151513153874e-05, 'epoch': 0.43}
{'loss': 1.0073, 'grad_norm': 0.4242836833000183, 'learning_rate': 1.2809172116337903e-05, 'epoch': 0.43}
{'loss': 1.014, 'grad_norm': 0.44774770736694336, 'learning_rate': 1.2803191628904594e-05, 'epoch': 0.43}
{'loss': 1.0003, 'grad_norm': 0.4164429306983948, 'learning_rate': 1.2797210053175779e-05, 'epoch': 0.43}
{'loss': 1.0023, 'grad_norm': 0.43178948760032654, 'learning_rate': 1.2791227391473706e-05, 'epoch': 0.43}
{'loss': 1.0051, 'grad_norm': 0.4578019082546234, 'learning_rate': 1.2785243646121059e-05, 'epoch': 0.43}
{'loss': 1.1098, 'grad_norm': 0.4661584794521332, 'learning_rate': 1.277925881944093e-05, 'epoch': 0.43}
{'loss': 1.0931, 'grad_norm': 0.44139742851257324, 'learning_rate': 1.2773272913756833e-05, 'epoch': 0.43}
{'loss': 0.9705, 'grad_norm': 0.43949317932128906, 'learning_rate': 1.2767285931392705e-05, 'epoch': 0.43}
{'loss': 1.0919, 'grad_norm': 0.45407190918922424, 'learning_rate': 1.27612978746729e-05, 'epoch': 0.43}
{'loss': 1.0777, 'grad_norm': 0.4384159743785858, 'learning_rate': 1.2755308745922182e-05, 'epoch': 0.43}
{'loss': 1.0457, 'grad_norm': 0.5067992210388184, 'learning_rate': 1.2749318547465742e-05, 'epoch': 0.43}
{'loss': 0.9784, 'grad_norm': 0.439767062664032, 'learning_rate': 1.2743327281629181e-05, 'epoch': 0.43}
{'loss': 0.971, 'grad_norm': 0.26290401816368103, 'learning_rate': 1.2737334950738512e-05, 'epoch': 0.43}
{'loss': 0.9264, 'grad_norm': 0.24834929406642914, 'learning_rate': 1.273134155712017e-05, 'epoch': 0.43}
{'loss': 1.0487, 'grad_norm': 0.40401628613471985, 'learning_rate': 1.272534710310099e-05, 'epoch': 0.43}
{'loss': 0.9471, 'grad_norm': 0.4313505291938782, 'learning_rate': 1.2719351591008228e-05, 'epoch': 0.43}
{'loss': 1.0095, 'grad_norm': 0.43440836668014526, 'learning_rate': 1.2713355023169547e-05, 'epoch': 0.43}
{'loss': 1.0237, 'grad_norm': 0.40684205293655396, 'learning_rate': 1.2707357401913022e-05, 'epoch': 0.43}
{'loss': 1.0957, 'grad_norm': 0.44570159912109375, 'learning_rate': 1.270135872956714e-05, 'epoch': 0.43}
{'loss': 0.9156, 'grad_norm': 0.4608537256717682, 'learning_rate': 1.2695359008460785e-05, 'epoch': 0.43}
{'loss': 1.0137, 'grad_norm': 0.4540080726146698, 'learning_rate': 1.2689358240923264e-05, 'epoch': 0.43}
{'loss': 0.9932, 'grad_norm': 0.4068751931190491, 'learning_rate': 1.2683356429284273e-05, 'epoch': 0.43}
{'loss': 1.0295, 'grad_norm': 0.4268519878387451, 'learning_rate': 1.2677353575873926e-05, 'epoch': 0.43}
{'loss': 1.0024, 'grad_norm': 0.2963651716709137, 'learning_rate': 1.2671349683022736e-05, 'epoch': 0.43}
{'loss': 0.9856, 'grad_norm': 0.25425970554351807, 'learning_rate': 1.2665344753061622e-05, 'epoch': 0.43}
{'loss': 1.066, 'grad_norm': 0.4681539535522461, 'learning_rate': 1.2659338788321904e-05, 'epoch': 0.43}
{'loss': 1.072, 'grad_norm': 0.46110767126083374, 'learning_rate': 1.2653331791135308e-05, 'epoch': 0.43}
{'loss': 0.9895, 'grad_norm': 0.44621792435646057, 'learning_rate': 1.2647323763833952e-05, 'epoch': 0.43}
{'loss': 1.0194, 'grad_norm': 0.4355728030204773, 'learning_rate': 1.264131470875036e-05, 'epoch': 0.43}
{'loss': 1.0715, 'grad_norm': 0.45779696106910706, 'learning_rate': 1.2635304628217452e-05, 'epoch': 0.43}
{'loss': 0.9048, 'grad_norm': 0.22996263206005096, 'learning_rate': 1.2629293524568555e-05, 'epoch': 0.43}
{'loss': 0.9253, 'grad_norm': 0.41439542174339294, 'learning_rate': 1.2623281400137383e-05, 'epoch': 0.43}
{'loss': 1.1135, 'grad_norm': 0.48256030678749084, 'learning_rate': 1.2617268257258051e-05, 'epoch': 0.43}
{'loss': 1.0711, 'grad_norm': 0.4446081221103668, 'learning_rate': 1.2611254098265063e-05, 'epoch': 0.43}
{'loss': 1.0777, 'grad_norm': 0.42622140049934387, 'learning_rate': 1.2605238925493326e-05, 'epoch': 0.43}
{'loss': 0.9511, 'grad_norm': 0.432170033454895, 'learning_rate': 1.2599222741278136e-05, 'epoch': 0.43}
{'loss': 1.0579, 'grad_norm': 0.43682608008384705, 'learning_rate': 1.2593205547955185e-05, 'epoch': 0.43}
{'loss': 1.0361, 'grad_norm': 0.46518829464912415, 'learning_rate': 1.2587187347860554e-05, 'epoch': 0.43}
{'loss': 0.9765, 'grad_norm': 0.4037747085094452, 'learning_rate': 1.2581168143330716e-05, 'epoch': 0.43}
{'loss': 0.969, 'grad_norm': 0.4255843758583069, 'learning_rate': 1.2575147936702531e-05, 'epoch': 0.43}
{'loss': 1.0044, 'grad_norm': 0.44116100668907166, 'learning_rate': 1.2569126730313255e-05, 'epoch': 0.43}
{'loss': 1.0262, 'grad_norm': 0.29195064306259155, 'learning_rate': 1.2563104526500523e-05, 'epoch': 0.44}
{'loss': 0.9265, 'grad_norm': 0.46175551414489746, 'learning_rate': 1.2557081327602361e-05, 'epoch': 0.44}
{'loss': 0.9566, 'grad_norm': 0.4384417235851288, 'learning_rate': 1.2551057135957187e-05, 'epoch': 0.44}
{'loss': 1.1034, 'grad_norm': 0.49100854992866516, 'learning_rate': 1.2545031953903796e-05, 'epoch': 0.44}
{'loss': 1.1035, 'grad_norm': 0.44638368487358093, 'learning_rate': 1.2539005783781374e-05, 'epoch': 0.44}
{'loss': 1.0466, 'grad_norm': 0.434510201215744, 'learning_rate': 1.2532978627929486e-05, 'epoch': 0.44}
{'loss': 0.9945, 'grad_norm': 0.4669163227081299, 'learning_rate': 1.2526950488688083e-05, 'epoch': 0.44}
{'loss': 1.0394, 'grad_norm': 0.46139660477638245, 'learning_rate': 1.2520921368397492e-05, 'epoch': 0.44}
{'loss': 1.0215, 'grad_norm': 0.4539259374141693, 'learning_rate': 1.2514891269398429e-05, 'epoch': 0.44}
{'loss': 0.9334, 'grad_norm': 0.2514707148075104, 'learning_rate': 1.2508860194031986e-05, 'epoch': 0.44}
{'loss': 0.9995, 'grad_norm': 0.3773189187049866, 'learning_rate': 1.2502828144639629e-05, 'epoch': 0.44}
{'loss': 1.0762, 'grad_norm': 0.4684590697288513, 'learning_rate': 1.2496795123563218e-05, 'epoch': 0.44}
{'loss': 0.9138, 'grad_norm': 0.41261816024780273, 'learning_rate': 1.249076113314497e-05, 'epoch': 0.44}
{'loss': 1.1025, 'grad_norm': 0.4749282896518707, 'learning_rate': 1.248472617572749e-05, 'epoch': 0.44}
{'loss': 0.9879, 'grad_norm': 0.43953272700309753, 'learning_rate': 1.2478690253653756e-05, 'epoch': 0.44}
{'loss': 0.9036, 'grad_norm': 0.43943002820014954, 'learning_rate': 1.2472653369267122e-05, 'epoch': 0.44}
{'loss': 0.966, 'grad_norm': 0.42119139432907104, 'learning_rate': 1.2466615524911316e-05, 'epoch': 0.44}
{'loss': 0.9926, 'grad_norm': 0.4334671199321747, 'learning_rate': 1.2460576722930432e-05, 'epoch': 0.44}
{'loss': 1.15, 'grad_norm': 0.4760322868824005, 'learning_rate': 1.2454536965668949e-05, 'epoch': 0.44}
{'loss': 1.0058, 'grad_norm': 0.4581000804901123, 'learning_rate': 1.24484962554717e-05, 'epoch': 0.44}
{'loss': 1.0501, 'grad_norm': 0.4098671078681946, 'learning_rate': 1.24424545946839e-05, 'epoch': 0.44}
{'loss': 0.9803, 'grad_norm': 0.4096073508262634, 'learning_rate': 1.2436411985651131e-05, 'epoch': 0.44}
{'loss': 1.0353, 'grad_norm': 0.4707501530647278, 'learning_rate': 1.2430368430719342e-05, 'epoch': 0.44}
{'loss': 0.9929, 'grad_norm': 0.4642447531223297, 'learning_rate': 1.242432393223485e-05, 'epoch': 0.44}
{'loss': 1.012, 'grad_norm': 0.47170132398605347, 'learning_rate': 1.2418278492544328e-05, 'epoch': 0.44}
{'loss': 1.0485, 'grad_norm': 0.42842039465904236, 'learning_rate': 1.2412232113994841e-05, 'epoch': 0.44}
{'loss': 1.0778, 'grad_norm': 0.4229982793331146, 'learning_rate': 1.2406184798933786e-05, 'epoch': 0.44}
{'loss': 1.0964, 'grad_norm': 0.4364567697048187, 'learning_rate': 1.2400136549708945e-05, 'epoch': 0.44}
{'loss': 0.939, 'grad_norm': 0.42065122723579407, 'learning_rate': 1.239408736866846e-05, 'epoch': 0.44}
{'loss': 0.9489, 'grad_norm': 0.4001956880092621, 'learning_rate': 1.2388037258160823e-05, 'epoch': 0.44}
{'loss': 1.0668, 'grad_norm': 0.4363615810871124, 'learning_rate': 1.23819862205349e-05, 'epoch': 0.44}
{'loss': 1.0967, 'grad_norm': 0.4372742176055908, 'learning_rate': 1.2375934258139917e-05, 'epoch': 0.44}
{'loss': 1.1266, 'grad_norm': 0.46044623851776123, 'learning_rate': 1.2369881373325448e-05, 'epoch': 0.44}
{'loss': 0.9434, 'grad_norm': 0.27244070172309875, 'learning_rate': 1.236382756844143e-05, 'epoch': 0.44}
{'loss': 0.9747, 'grad_norm': 0.4120483100414276, 'learning_rate': 1.2357772845838159e-05, 'epoch': 0.44}
{'loss': 0.9605, 'grad_norm': 0.4186158776283264, 'learning_rate': 1.2351717207866292e-05, 'epoch': 0.44}
{'loss': 1.0301, 'grad_norm': 0.42355358600616455, 'learning_rate': 1.2345660656876832e-05, 'epoch': 0.44}
{'loss': 1.0198, 'grad_norm': 0.43404141068458557, 'learning_rate': 1.233960319522114e-05, 'epoch': 0.44}
{'loss': 0.9683, 'grad_norm': 0.40022528171539307, 'learning_rate': 1.2333544825250938e-05, 'epoch': 0.44}
{'loss': 1.0653, 'grad_norm': 0.4409535527229309, 'learning_rate': 1.2327485549318285e-05, 'epoch': 0.44}
{'loss': 0.9845, 'grad_norm': 0.4201459288597107, 'learning_rate': 1.2321425369775601e-05, 'epoch': 0.44}
{'loss': 0.972, 'grad_norm': 0.39841777086257935, 'learning_rate': 1.2315364288975665e-05, 'epoch': 0.44}
{'loss': 1.0191, 'grad_norm': 0.4073410928249359, 'learning_rate': 1.2309302309271587e-05, 'epoch': 0.44}
{'loss': 1.0091, 'grad_norm': 0.43366140127182007, 'learning_rate': 1.2303239433016842e-05, 'epoch': 0.44}
{'loss': 0.949, 'grad_norm': 0.2510904371738434, 'learning_rate': 1.2297175662565248e-05, 'epoch': 0.44}
{'loss': 1.017, 'grad_norm': 0.4457920789718628, 'learning_rate': 1.229111100027097e-05, 'epoch': 0.44}
{'loss': 0.9196, 'grad_norm': 0.24306431412696838, 'learning_rate': 1.228504544848851e-05, 'epoch': 0.44}
{'loss': 0.902, 'grad_norm': 0.403533399105072, 'learning_rate': 1.2278979009572736e-05, 'epoch': 0.44}
{'loss': 0.987, 'grad_norm': 0.42759525775909424, 'learning_rate': 1.2272911685878841e-05, 'epoch': 0.44}
{'loss': 1.0163, 'grad_norm': 0.45204365253448486, 'learning_rate': 1.2266843479762372e-05, 'epoch': 0.44}
{'loss': 1.0361, 'grad_norm': 0.42032814025878906, 'learning_rate': 1.2260774393579209e-05, 'epoch': 0.44}
{'loss': 1.1045, 'grad_norm': 0.5001955628395081, 'learning_rate': 1.2254704429685593e-05, 'epoch': 0.44}
{'loss': 0.9688, 'grad_norm': 0.4072572588920593, 'learning_rate': 1.2248633590438084e-05, 'epoch': 0.45}
{'loss': 1.0815, 'grad_norm': 0.48537352681159973, 'learning_rate': 1.2242561878193589e-05, 'epoch': 0.45}
{'loss': 0.9956, 'grad_norm': 0.3404296338558197, 'learning_rate': 1.2236489295309362e-05, 'epoch': 0.45}
{'loss': 1.0182, 'grad_norm': 0.48419368267059326, 'learning_rate': 1.2230415844142984e-05, 'epoch': 0.45}
{'loss': 1.0795, 'grad_norm': 0.45509546995162964, 'learning_rate': 1.2224341527052378e-05, 'epoch': 0.45}
{'loss': 1.1062, 'grad_norm': 0.5203363299369812, 'learning_rate': 1.2218266346395811e-05, 'epoch': 0.45}
{'loss': 1.0017, 'grad_norm': 0.4370267987251282, 'learning_rate': 1.221219030453187e-05, 'epoch': 0.45}
{'loss': 1.0296, 'grad_norm': 0.38924017548561096, 'learning_rate': 1.220611340381948e-05, 'epoch': 0.45}
{'loss': 1.0108, 'grad_norm': 0.45145532488822937, 'learning_rate': 1.2200035646617912e-05, 'epoch': 0.45}
{'loss': 1.0398, 'grad_norm': 0.44342729449272156, 'learning_rate': 1.2193957035286757e-05, 'epoch': 0.45}
{'loss': 1.1344, 'grad_norm': 0.5003684163093567, 'learning_rate': 1.2187877572185937e-05, 'epoch': 0.45}
{'loss': 1.134, 'grad_norm': 0.4799371063709259, 'learning_rate': 1.2181797259675713e-05, 'epoch': 0.45}
{'loss': 1.077, 'grad_norm': 0.45863133668899536, 'learning_rate': 1.2175716100116677e-05, 'epoch': 0.45}
{'loss': 1.0474, 'grad_norm': 0.445137619972229, 'learning_rate': 1.2169634095869736e-05, 'epoch': 0.45}
{'loss': 0.9811, 'grad_norm': 0.44328755140304565, 'learning_rate': 1.2163551249296132e-05, 'epoch': 0.45}
{'loss': 0.977, 'grad_norm': 0.4214860498905182, 'learning_rate': 1.2157467562757443e-05, 'epoch': 0.45}
{'loss': 1.0304, 'grad_norm': 0.4507114887237549, 'learning_rate': 1.2151383038615563e-05, 'epoch': 0.45}
{'loss': 1.0303, 'grad_norm': 0.4287372827529907, 'learning_rate': 1.214529767923271e-05, 'epoch': 0.45}
{'loss': 0.9864, 'grad_norm': 0.39783215522766113, 'learning_rate': 1.2139211486971436e-05, 'epoch': 0.45}
{'loss': 1.0371, 'grad_norm': 0.4122197926044464, 'learning_rate': 1.213312446419461e-05, 'epoch': 0.45}
{'loss': 0.9687, 'grad_norm': 0.43551012873649597, 'learning_rate': 1.2127036613265418e-05, 'epoch': 0.45}
{'loss': 0.8866, 'grad_norm': 0.260123610496521, 'learning_rate': 1.2120947936547375e-05, 'epoch': 0.45}
{'loss': 0.9535, 'grad_norm': 0.24967654049396515, 'learning_rate': 1.2114858436404322e-05, 'epoch': 0.45}
{'loss': 1.017, 'grad_norm': 0.4487397074699402, 'learning_rate': 1.2108768115200405e-05, 'epoch': 0.45}
{'loss': 1.0205, 'grad_norm': 0.4552896022796631, 'learning_rate': 1.2102676975300095e-05, 'epoch': 0.45}
{'loss': 0.9183, 'grad_norm': 0.4184796214103699, 'learning_rate': 1.209658501906819e-05, 'epoch': 0.45}
{'loss': 1.0448, 'grad_norm': 0.4305534064769745, 'learning_rate': 1.2090492248869795e-05, 'epoch': 0.45}
{'loss': 0.9957, 'grad_norm': 0.3935151994228363, 'learning_rate': 1.2084398667070325e-05, 'epoch': 0.45}
{'loss': 1.0926, 'grad_norm': 0.4757595360279083, 'learning_rate': 1.2078304276035527e-05, 'epoch': 0.45}
{'loss': 0.9989, 'grad_norm': 0.4419671595096588, 'learning_rate': 1.2072209078131451e-05, 'epoch': 0.45}
{'loss': 1.0022, 'grad_norm': 0.43229416012763977, 'learning_rate': 1.2066113075724461e-05, 'epoch': 0.45}
{'loss': 1.0539, 'grad_norm': 0.4513963758945465, 'learning_rate': 1.206001627118124e-05, 'epoch': 0.45}
{'loss': 1.0131, 'grad_norm': 0.44324907660484314, 'learning_rate': 1.2053918666868776e-05, 'epoch': 0.45}
{'loss': 0.986, 'grad_norm': 0.41854244470596313, 'learning_rate': 1.2047820265154362e-05, 'epoch': 0.45}
{'loss': 1.0249, 'grad_norm': 0.4613044857978821, 'learning_rate': 1.2041721068405614e-05, 'epoch': 0.45}
Error with image file is truncated (91 bytes not processed)
{'loss': 1.0541, 'grad_norm': 0.4796290695667267, 'learning_rate': 1.203562107899045e-05, 'epoch': 0.45}
{'loss': 1.1321, 'grad_norm': 0.48183852434158325, 'learning_rate': 1.2029520299277095e-05, 'epoch': 0.45}
{'loss': 0.9925, 'grad_norm': 0.42410340905189514, 'learning_rate': 1.2023418731634078e-05, 'epoch': 0.45}
{'loss': 1.0328, 'grad_norm': 0.491499125957489, 'learning_rate': 1.2017316378430244e-05, 'epoch': 0.45}
{'loss': 0.9403, 'grad_norm': 0.41734403371810913, 'learning_rate': 1.2011213242034733e-05, 'epoch': 0.45}
{'loss': 1.2077, 'grad_norm': 0.5222870707511902, 'learning_rate': 1.2005109324816992e-05, 'epoch': 0.45}
{'loss': 1.0003, 'grad_norm': 0.4117860794067383, 'learning_rate': 1.1999004629146775e-05, 'epoch': 0.45}
{'loss': 0.9375, 'grad_norm': 0.4181382954120636, 'learning_rate': 1.1992899157394133e-05, 'epoch': 0.45}
{'loss': 1.0323, 'grad_norm': 0.4670645594596863, 'learning_rate': 1.1986792911929418e-05, 'epoch': 0.45}
{'loss': 1.0047, 'grad_norm': 0.46582528948783875, 'learning_rate': 1.198068589512329e-05, 'epoch': 0.45}
{'loss': 0.9966, 'grad_norm': 0.3024443984031677, 'learning_rate': 1.1974578109346702e-05, 'epoch': 0.45}
{'loss': 1.1464, 'grad_norm': 0.47925734519958496, 'learning_rate': 1.1968469556970905e-05, 'epoch': 0.45}
{'loss': 1.0647, 'grad_norm': 0.42872580885887146, 'learning_rate': 1.1962360240367445e-05, 'epoch': 0.45}
{'loss': 1.0085, 'grad_norm': 0.4899022579193115, 'learning_rate': 1.1956250161908179e-05, 'epoch': 0.45}
{'loss': 1.0908, 'grad_norm': 0.5147382020950317, 'learning_rate': 1.195013932396524e-05, 'epoch': 0.45}
Error with image file is truncated (8 bytes not processed)
{'loss': 1.0465, 'grad_norm': 0.41555893421173096, 'learning_rate': 1.1944027728911072e-05, 'epoch': 0.45}
{'loss': 1.0428, 'grad_norm': 0.44905751943588257, 'learning_rate': 1.1937915379118406e-05, 'epoch': 0.45}
{'loss': 1.1266, 'grad_norm': 0.45689451694488525, 'learning_rate': 1.1931802276960265e-05, 'epoch': 0.46}
{'loss': 1.1188, 'grad_norm': 0.44401440024375916, 'learning_rate': 1.1925688424809965e-05, 'epoch': 0.46}
{'loss': 1.0234, 'grad_norm': 0.4074060618877411, 'learning_rate': 1.1919573825041115e-05, 'epoch': 0.46}
{'loss': 1.0705, 'grad_norm': 0.3946613073348999, 'learning_rate': 1.1913458480027614e-05, 'epoch': 0.46}
{'loss': 0.9718, 'grad_norm': 0.4282202422618866, 'learning_rate': 1.1907342392143646e-05, 'epoch': 0.46}
{'loss': 0.9102, 'grad_norm': 0.4232378304004669, 'learning_rate': 1.1901225563763694e-05, 'epoch': 0.46}
{'loss': 0.9657, 'grad_norm': 0.3843148648738861, 'learning_rate': 1.1895107997262516e-05, 'epoch': 0.46}
{'loss': 0.9653, 'grad_norm': 0.42213174700737, 'learning_rate': 1.1888989695015166e-05, 'epoch': 0.46}
{'loss': 1.0033, 'grad_norm': 0.4181826114654541, 'learning_rate': 1.1882870659396968e-05, 'epoch': 0.46}
{'loss': 1.1165, 'grad_norm': 0.47542572021484375, 'learning_rate': 1.1876750892783558e-05, 'epoch': 0.46}
{'loss': 1.1261, 'grad_norm': 0.47117146849632263, 'learning_rate': 1.1870630397550831e-05, 'epoch': 0.46}
{'loss': 1.1258, 'grad_norm': 0.47722670435905457, 'learning_rate': 1.1864509176074974e-05, 'epoch': 0.46}
{'loss': 1.0601, 'grad_norm': 0.4001045823097229, 'learning_rate': 1.185838723073246e-05, 'epoch': 0.46}
{'loss': 0.9951, 'grad_norm': 0.41857945919036865, 'learning_rate': 1.1852264563900038e-05, 'epoch': 0.46}
{'loss': 1.0117, 'grad_norm': 0.41918373107910156, 'learning_rate': 1.1846141177954733e-05, 'epoch': 0.46}
{'loss': 1.0909, 'grad_norm': 0.4362858235836029, 'learning_rate': 1.1840017075273861e-05, 'epoch': 0.46}
{'loss': 0.969, 'grad_norm': 0.4096861481666565, 'learning_rate': 1.1833892258235008e-05, 'epoch': 0.46}
{'loss': 1.1128, 'grad_norm': 0.4744690954685211, 'learning_rate': 1.1827766729216035e-05, 'epoch': 0.46}
{'loss': 0.9469, 'grad_norm': 0.3987724781036377, 'learning_rate': 1.1821640490595086e-05, 'epoch': 0.46}
{'loss': 0.9544, 'grad_norm': 0.40736284852027893, 'learning_rate': 1.181551354475058e-05, 'epoch': 0.46}
{'loss': 1.0567, 'grad_norm': 0.44942402839660645, 'learning_rate': 1.1809385894061206e-05, 'epoch': 0.46}
{'loss': 1.0687, 'grad_norm': 0.4502096474170685, 'learning_rate': 1.1803257540905926e-05, 'epoch': 0.46}
{'loss': 0.9783, 'grad_norm': 0.4249180555343628, 'learning_rate': 1.1797128487663982e-05, 'epoch': 0.46}
{'loss': 1.0044, 'grad_norm': 0.41363075375556946, 'learning_rate': 1.1790998736714882e-05, 'epoch': 0.46}
{'loss': 1.0204, 'grad_norm': 0.4582787752151489, 'learning_rate': 1.1784868290438404e-05, 'epoch': 0.46}
{'loss': 1.0306, 'grad_norm': 0.44857755303382874, 'learning_rate': 1.1778737151214606e-05, 'epoch': 0.46}
{'loss': 0.9788, 'grad_norm': 0.4256173372268677, 'learning_rate': 1.17726053214238e-05, 'epoch': 0.46}
{'loss': 1.0576, 'grad_norm': 0.42096999287605286, 'learning_rate': 1.1766472803446577e-05, 'epoch': 0.46}
{'loss': 0.9997, 'grad_norm': 0.4548388123512268, 'learning_rate': 1.1760339599663788e-05, 'epoch': 0.46}
{'loss': 1.0395, 'grad_norm': 0.4473324418067932, 'learning_rate': 1.1754205712456556e-05, 'epoch': 0.46}
{'loss': 1.0056, 'grad_norm': 0.5025755763053894, 'learning_rate': 1.1748071144206266e-05, 'epoch': 0.46}
{'loss': 0.9556, 'grad_norm': 0.24934539198875427, 'learning_rate': 1.1741935897294572e-05, 'epoch': 0.46}
{'loss': 1.0179, 'grad_norm': 0.44168785214424133, 'learning_rate': 1.1735799974103388e-05, 'epoch': 0.46}
{'loss': 1.0049, 'grad_norm': 0.4651457965373993, 'learning_rate': 1.1729663377014888e-05, 'epoch': 0.46}
{'loss': 1.1011, 'grad_norm': 0.495522677898407, 'learning_rate': 1.172352610841151e-05, 'epoch': 0.46}
{'loss': 1.0846, 'grad_norm': 0.4504257142543793, 'learning_rate': 1.1717388170675954e-05, 'epoch': 0.46}
{'loss': 1.0055, 'grad_norm': 0.41874411702156067, 'learning_rate': 1.1711249566191179e-05, 'epoch': 0.46}
{'loss': 0.9517, 'grad_norm': 0.4446381628513336, 'learning_rate': 1.17051102973404e-05, 'epoch': 0.46}
{'loss': 1.1103, 'grad_norm': 0.44160759449005127, 'learning_rate': 1.1698970366507096e-05, 'epoch': 0.46}
{'loss': 0.9767, 'grad_norm': 0.41333168745040894, 'learning_rate': 1.1692829776074999e-05, 'epoch': 0.46}
{'loss': 1.0301, 'grad_norm': 0.4118618071079254, 'learning_rate': 1.1686688528428099e-05, 'epoch': 0.46}
{'loss': 1.0676, 'grad_norm': 0.437033474445343, 'learning_rate': 1.1680546625950635e-05, 'epoch': 0.46}
{'loss': 1.0464, 'grad_norm': 0.43934527039527893, 'learning_rate': 1.167440407102711e-05, 'epoch': 0.46}
{'loss': 0.9247, 'grad_norm': 0.4111267924308777, 'learning_rate': 1.1668260866042271e-05, 'epoch': 0.46}
{'loss': 1.0109, 'grad_norm': 0.4245956540107727, 'learning_rate': 1.1662117013381126e-05, 'epoch': 0.46}
{'loss': 1.0384, 'grad_norm': 0.4419403076171875, 'learning_rate': 1.1655972515428928e-05, 'epoch': 0.46}
{'loss': 0.9757, 'grad_norm': 0.4382050633430481, 'learning_rate': 1.1649827374571182e-05, 'epoch': 0.46}
{'loss': 0.9632, 'grad_norm': 0.4157417416572571, 'learning_rate': 1.1643681593193642e-05, 'epoch': 0.46}
{'loss': 1.0461, 'grad_norm': 0.4203926920890808, 'learning_rate': 1.1637535173682318e-05, 'epoch': 0.46}
{'loss': 1.0213, 'grad_norm': 0.2753521800041199, 'learning_rate': 1.1631388118423457e-05, 'epoch': 0.46}
{'loss': 0.9386, 'grad_norm': 0.24064010381698608, 'learning_rate': 1.1625240429803553e-05, 'epoch': 0.46}
{'loss': 1.0776, 'grad_norm': 0.45401856303215027, 'learning_rate': 1.1619092110209361e-05, 'epoch': 0.46}
{'loss': 1.0087, 'grad_norm': 0.5006648302078247, 'learning_rate': 1.1612943162027863e-05, 'epoch': 0.47}
{'loss': 0.9998, 'grad_norm': 0.44317755103111267, 'learning_rate': 1.1606793587646295e-05, 'epoch': 0.47}
{'loss': 1.1027, 'grad_norm': 0.5040636658668518, 'learning_rate': 1.160064338945213e-05, 'epoch': 0.47}
{'loss': 0.914, 'grad_norm': 0.23121541738510132, 'learning_rate': 1.1594492569833093e-05, 'epoch': 0.47}
{'loss': 1.0847, 'grad_norm': 0.45951029658317566, 'learning_rate': 1.1588341131177137e-05, 'epoch': 0.47}
{'loss': 1.0826, 'grad_norm': 0.49344179034233093, 'learning_rate': 1.1582189075872467e-05, 'epoch': 0.47}
{'loss': 0.9959, 'grad_norm': 0.4014744758605957, 'learning_rate': 1.1576036406307523e-05, 'epoch': 0.47}
WARNING: tokenization mismatch: 0 vs. 55. (ignored)
{'loss': 1.0782, 'grad_norm': 0.4217154085636139, 'learning_rate': 1.156988312487098e-05, 'epoch': 0.47}
{'loss': 1.084, 'grad_norm': 0.48263615369796753, 'learning_rate': 1.1563729233951757e-05, 'epoch': 0.47}
{'loss': 1.011, 'grad_norm': 0.43848657608032227, 'learning_rate': 1.1557574735939003e-05, 'epoch': 0.47}
{'loss': 1.0043, 'grad_norm': 0.4281012713909149, 'learning_rate': 1.1551419633222107e-05, 'epoch': 0.47}
{'loss': 1.0118, 'grad_norm': 0.4132850170135498, 'learning_rate': 1.1545263928190692e-05, 'epoch': 0.47}
{'loss': 1.0337, 'grad_norm': 0.4331057667732239, 'learning_rate': 1.1539107623234618e-05, 'epoch': 0.47}
{'loss': 1.1014, 'grad_norm': 0.482548326253891, 'learning_rate': 1.153295072074397e-05, 'epoch': 0.47}
{'loss': 1.0493, 'grad_norm': 0.5184208750724792, 'learning_rate': 1.1526793223109072e-05, 'epoch': 0.47}
{'loss': 1.0744, 'grad_norm': 0.4652497172355652, 'learning_rate': 1.1520635132720475e-05, 'epoch': 0.47}
{'loss': 1.0104, 'grad_norm': 0.4331558346748352, 'learning_rate': 1.1514476451968961e-05, 'epoch': 0.47}
{'loss': 0.9438, 'grad_norm': 0.42507752776145935, 'learning_rate': 1.1508317183245545e-05, 'epoch': 0.47}
{'loss': 1.0537, 'grad_norm': 0.43913716077804565, 'learning_rate': 1.1502157328941466e-05, 'epoch': 0.47}
{'loss': 1.0869, 'grad_norm': 0.4307376444339752, 'learning_rate': 1.149599689144819e-05, 'epoch': 0.47}
{'loss': 0.9936, 'grad_norm': 0.4097871780395508, 'learning_rate': 1.1489835873157414e-05, 'epoch': 0.47}
{'loss': 1.0068, 'grad_norm': 0.4344162344932556, 'learning_rate': 1.1483674276461053e-05, 'epoch': 0.47}
{'loss': 0.9778, 'grad_norm': 0.4474991261959076, 'learning_rate': 1.1477512103751254e-05, 'epoch': 0.47}
{'loss': 1.0315, 'grad_norm': 0.49704045057296753, 'learning_rate': 1.1471349357420384e-05, 'epoch': 0.47}
{'loss': 1.0394, 'grad_norm': 0.4428555369377136, 'learning_rate': 1.1465186039861033e-05, 'epoch': 0.47}
{'loss': 0.9895, 'grad_norm': 0.474855899810791, 'learning_rate': 1.1459022153466016e-05, 'epoch': 0.47}
{'loss': 1.0044, 'grad_norm': 0.4502562880516052, 'learning_rate': 1.1452857700628362e-05, 'epoch': 0.47}
{'loss': 1.0961, 'grad_norm': 0.48621147871017456, 'learning_rate': 1.1446692683741326e-05, 'epoch': 0.47}
{'loss': 1.0256, 'grad_norm': 0.4648367762565613, 'learning_rate': 1.1440527105198377e-05, 'epoch': 0.47}
{'loss': 0.9948, 'grad_norm': 0.44909656047821045, 'learning_rate': 1.143436096739321e-05, 'epoch': 0.47}
{'loss': 1.1497, 'grad_norm': 0.4887714982032776, 'learning_rate': 1.1428194272719729e-05, 'epoch': 0.47}
{'loss': 0.9962, 'grad_norm': 0.42611733078956604, 'learning_rate': 1.1422027023572052e-05, 'epoch': 0.47}
{'loss': 1.0235, 'grad_norm': 0.41831424832344055, 'learning_rate': 1.1415859222344525e-05, 'epoch': 0.47}
{'loss': 1.0209, 'grad_norm': 0.44834285974502563, 'learning_rate': 1.14096908714317e-05, 'epoch': 0.47}
{'loss': 1.0922, 'grad_norm': 0.44009190797805786, 'learning_rate': 1.1403521973228342e-05, 'epoch': 0.47}
{'loss': 1.0282, 'grad_norm': 0.44147565960884094, 'learning_rate': 1.1397352530129428e-05, 'epoch': 0.47}
{'loss': 1.0382, 'grad_norm': 0.4406171143054962, 'learning_rate': 1.139118254453015e-05, 'epoch': 0.47}
{'loss': 1.0214, 'grad_norm': 0.2889799177646637, 'learning_rate': 1.1385012018825907e-05, 'epoch': 0.47}
{'loss': 1.1393, 'grad_norm': 0.46534669399261475, 'learning_rate': 1.1378840955412313e-05, 'epoch': 0.47}
{'loss': 0.997, 'grad_norm': 0.44360631704330444, 'learning_rate': 1.1372669356685185e-05, 'epoch': 0.47}
{'loss': 0.9537, 'grad_norm': 0.47614771127700806, 'learning_rate': 1.1366497225040549e-05, 'epoch': 0.47}
{'loss': 1.1157, 'grad_norm': 0.48205268383026123, 'learning_rate': 1.1360324562874643e-05, 'epoch': 0.47}
{'loss': 1.0152, 'grad_norm': 0.40283238887786865, 'learning_rate': 1.1354151372583901e-05, 'epoch': 0.47}
{'loss': 1.0065, 'grad_norm': 0.4297661781311035, 'learning_rate': 1.1347977656564974e-05, 'epoch': 0.47}
{'loss': 1.0924, 'grad_norm': 0.4910353720188141, 'learning_rate': 1.1341803417214705e-05, 'epoch': 0.47}
{'loss': 0.9821, 'grad_norm': 0.46472030878067017, 'learning_rate': 1.1335628656930153e-05, 'epoch': 0.47}
{'loss': 0.9597, 'grad_norm': 0.4030635952949524, 'learning_rate': 1.132945337810857e-05, 'epoch': 0.47}
{'loss': 1.0778, 'grad_norm': 0.423114538192749, 'learning_rate': 1.132327758314741e-05, 'epoch': 0.47}
{'loss': 1.0368, 'grad_norm': 0.4341416656970978, 'learning_rate': 1.131710127444433e-05, 'epoch': 0.47}
{'loss': 0.9782, 'grad_norm': 0.4017905294895172, 'learning_rate': 1.1310924454397187e-05, 'epoch': 0.47}
{'loss': 1.0973, 'grad_norm': 0.46672138571739197, 'learning_rate': 1.1304747125404031e-05, 'epoch': 0.47}
{'loss': 0.9226, 'grad_norm': 0.4021638035774231, 'learning_rate': 1.129856928986312e-05, 'epoch': 0.47}
{'loss': 0.9722, 'grad_norm': 0.4528680443763733, 'learning_rate': 1.12923909501729e-05, 'epoch': 0.48}
{'loss': 0.9426, 'grad_norm': 0.3912089467048645, 'learning_rate': 1.1286212108732015e-05, 'epoch': 0.48}
{'loss': 0.9564, 'grad_norm': 0.40915125608444214, 'learning_rate': 1.1280032767939302e-05, 'epoch': 0.48}
{'loss': 1.0206, 'grad_norm': 0.45811113715171814, 'learning_rate': 1.1273852930193798e-05, 'epoch': 0.48}
{'loss': 0.9047, 'grad_norm': 0.43051210045814514, 'learning_rate': 1.1267672597894725e-05, 'epoch': 0.48}
{'loss': 0.9989, 'grad_norm': 0.4059034585952759, 'learning_rate': 1.12614917734415e-05, 'epoch': 0.48}
{'loss': 1.1554, 'grad_norm': 0.47371622920036316, 'learning_rate': 1.1255310459233737e-05, 'epoch': 0.48}
{'loss': 1.0974, 'grad_norm': 0.45315060019493103, 'learning_rate': 1.1249128657671233e-05, 'epoch': 0.48}
{'loss': 1.055, 'grad_norm': 0.4631667733192444, 'learning_rate': 1.1242946371153974e-05, 'epoch': 0.48}
{'loss': 1.0892, 'grad_norm': 0.4954019784927368, 'learning_rate': 1.1236763602082136e-05, 'epoch': 0.48}
{'loss': 1.0871, 'grad_norm': 0.4144839644432068, 'learning_rate': 1.1230580352856088e-05, 'epoch': 0.48}
{'loss': 0.9694, 'grad_norm': 0.4181824028491974, 'learning_rate': 1.1224396625876375e-05, 'epoch': 0.48}
{'loss': 1.0935, 'grad_norm': 0.4503152072429657, 'learning_rate': 1.1218212423543734e-05, 'epoch': 0.48}
{'loss': 0.9639, 'grad_norm': 0.42450907826423645, 'learning_rate': 1.1212027748259086e-05, 'epoch': 0.48}
{'loss': 0.9643, 'grad_norm': 0.41663122177124023, 'learning_rate': 1.1205842602423537e-05, 'epoch': 0.48}
{'loss': 1.068, 'grad_norm': 0.44692370295524597, 'learning_rate': 1.1199656988438373e-05, 'epoch': 0.48}
{'loss': 1.0002, 'grad_norm': 0.45645689964294434, 'learning_rate': 1.1193470908705055e-05, 'epoch': 0.48}
{'loss': 1.0628, 'grad_norm': 0.4895368218421936, 'learning_rate': 1.1187284365625241e-05, 'epoch': 0.48}
{'loss': 0.8732, 'grad_norm': 0.39215368032455444, 'learning_rate': 1.1181097361600754e-05, 'epoch': 0.48}
{'loss': 1.0305, 'grad_norm': 0.473733127117157, 'learning_rate': 1.1174909899033608e-05, 'epoch': 0.48}
{'loss': 1.0102, 'grad_norm': 0.3991176187992096, 'learning_rate': 1.1168721980325987e-05, 'epoch': 0.48}
{'loss': 1.0776, 'grad_norm': 0.4244432747364044, 'learning_rate': 1.1162533607880251e-05, 'epoch': 0.48}
{'loss': 0.9718, 'grad_norm': 0.4348691403865814, 'learning_rate': 1.1156344784098942e-05, 'epoch': 0.48}
{'loss': 1.0702, 'grad_norm': 0.4252280294895172, 'learning_rate': 1.1150155511384772e-05, 'epoch': 0.48}
{'loss': 1.0533, 'grad_norm': 0.42985212802886963, 'learning_rate': 1.1143965792140631e-05, 'epoch': 0.48}
{'loss': 1.0606, 'grad_norm': 0.42617061734199524, 'learning_rate': 1.1137775628769584e-05, 'epoch': 0.48}
{'loss': 0.9608, 'grad_norm': 0.4430290162563324, 'learning_rate': 1.1131585023674863e-05, 'epoch': 0.48}
{'loss': 0.9793, 'grad_norm': 0.41812533140182495, 'learning_rate': 1.1125393979259874e-05, 'epoch': 0.48}
{'loss': 1.0825, 'grad_norm': 0.4532258212566376, 'learning_rate': 1.1119202497928192e-05, 'epoch': 0.48}
{'loss': 1.0027, 'grad_norm': 0.4332345426082611, 'learning_rate': 1.1113010582083568e-05, 'epoch': 0.48}
{'loss': 0.9943, 'grad_norm': 0.4041162133216858, 'learning_rate': 1.1106818234129913e-05, 'epoch': 0.48}
{'loss': 1.0693, 'grad_norm': 0.5198859572410583, 'learning_rate': 1.1100625456471307e-05, 'epoch': 0.48}
{'loss': 1.0881, 'grad_norm': 0.5189880132675171, 'learning_rate': 1.1094432251512006e-05, 'epoch': 0.48}
{'loss': 1.0633, 'grad_norm': 0.457197368144989, 'learning_rate': 1.1088238621656422e-05, 'epoch': 0.48}
{'loss': 1.1087, 'grad_norm': 0.46992671489715576, 'learning_rate': 1.1082044569309138e-05, 'epoch': 0.48}
{'loss': 0.9464, 'grad_norm': 0.2766914665699005, 'learning_rate': 1.1075850096874894e-05, 'epoch': 0.48}
{'loss': 1.0533, 'grad_norm': 0.41857340931892395, 'learning_rate': 1.1069655206758603e-05, 'epoch': 0.48}
{'loss': 0.9683, 'grad_norm': 0.46461978554725647, 'learning_rate': 1.1063459901365325e-05, 'epoch': 0.48}
{'loss': 1.0001, 'grad_norm': 0.4248753786087036, 'learning_rate': 1.1057264183100303e-05, 'epoch': 0.48}
{'loss': 1.0225, 'grad_norm': 0.39903712272644043, 'learning_rate': 1.1051068054368921e-05, 'epoch': 0.48}
{'loss': 0.9257, 'grad_norm': 0.4451623857021332, 'learning_rate': 1.104487151757673e-05, 'epoch': 0.48}
{'loss': 0.9816, 'grad_norm': 0.47089216113090515, 'learning_rate': 1.1038674575129442e-05, 'epoch': 0.48}
{'loss': 0.9354, 'grad_norm': 0.25168290734291077, 'learning_rate': 1.1032477229432921e-05, 'epoch': 0.48}
{'loss': 1.1018, 'grad_norm': 0.4444798529148102, 'learning_rate': 1.1026279482893187e-05, 'epoch': 0.48}
{'loss': 0.9498, 'grad_norm': 0.2489161193370819, 'learning_rate': 1.1020081337916425e-05, 'epoch': 0.48}
{'loss': 1.0096, 'grad_norm': 0.448907732963562, 'learning_rate': 1.1013882796908963e-05, 'epoch': 0.48}
{'loss': 1.0391, 'grad_norm': 0.4450240433216095, 'learning_rate': 1.1007683862277292e-05, 'epoch': 0.48}
{'loss': 1.0396, 'grad_norm': 0.4484003186225891, 'learning_rate': 1.1001484536428052e-05, 'epoch': 0.48}
{'loss': 1.1009, 'grad_norm': 0.4705737829208374, 'learning_rate': 1.0995284821768029e-05, 'epoch': 0.48}
{'loss': 1.011, 'grad_norm': 0.4322291314601898, 'learning_rate': 1.098908472070417e-05, 'epoch': 0.48}
{'loss': 1.0381, 'grad_norm': 0.4407036602497101, 'learning_rate': 1.0982884235643567e-05, 'epoch': 0.48}
{'loss': 1.1344, 'grad_norm': 0.4403350353240967, 'learning_rate': 1.0976683368993464e-05, 'epoch': 0.48}
{'loss': 1.0446, 'grad_norm': 0.4243896007537842, 'learning_rate': 1.0970482123161249e-05, 'epoch': 0.49}
{'loss': 1.0349, 'grad_norm': 0.44931209087371826, 'learning_rate': 1.0964280500554459e-05, 'epoch': 0.49}
{'loss': 0.9956, 'grad_norm': 0.4110129773616791, 'learning_rate': 1.0958078503580776e-05, 'epoch': 0.49}
{'loss': 1.085, 'grad_norm': 0.505849301815033, 'learning_rate': 1.0951876134648032e-05, 'epoch': 0.49}
{'loss': 1.0038, 'grad_norm': 0.42279359698295593, 'learning_rate': 1.0945673396164198e-05, 'epoch': 0.49}
{'loss': 0.9615, 'grad_norm': 0.4278816282749176, 'learning_rate': 1.0939470290537389e-05, 'epoch': 0.49}
{'loss': 0.9715, 'grad_norm': 0.2660507261753082, 'learning_rate': 1.0933266820175868e-05, 'epoch': 0.49}
{'loss': 1.1327, 'grad_norm': 0.43975865840911865, 'learning_rate': 1.0927062987488035e-05, 'epoch': 0.49}
{'loss': 1.0327, 'grad_norm': 0.41241636872291565, 'learning_rate': 1.0920858794882429e-05, 'epoch': 0.49}
{'loss': 0.8718, 'grad_norm': 0.26077473163604736, 'learning_rate': 1.0914654244767736e-05, 'epoch': 0.49}
{'loss': 1.0184, 'grad_norm': 0.4580825865268707, 'learning_rate': 1.0908449339552769e-05, 'epoch': 0.49}
{'loss': 0.9257, 'grad_norm': 0.41899922490119934, 'learning_rate': 1.0902244081646489e-05, 'epoch': 0.49}
{'loss': 1.0387, 'grad_norm': 0.424542099237442, 'learning_rate': 1.0896038473457993e-05, 'epoch': 0.49}
{'loss': 1.0661, 'grad_norm': 0.4335753321647644, 'learning_rate': 1.0889832517396511e-05, 'epoch': 0.49}
{'loss': 1.1166, 'grad_norm': 0.4954282343387604, 'learning_rate': 1.0883626215871408e-05, 'epoch': 0.49}
{'loss': 1.1816, 'grad_norm': 0.5463941097259521, 'learning_rate': 1.0877419571292183e-05, 'epoch': 0.49}
{'loss': 1.0776, 'grad_norm': 0.4874834716320038, 'learning_rate': 1.0871212586068469e-05, 'epoch': 0.49}
{'loss': 0.9956, 'grad_norm': 0.38387298583984375, 'learning_rate': 1.0865005262610033e-05, 'epoch': 0.49}
{'loss': 1.0438, 'grad_norm': 0.46238675713539124, 'learning_rate': 1.085879760332677e-05, 'epoch': 0.49}
{'loss': 1.071, 'grad_norm': 0.45256608724594116, 'learning_rate': 1.085258961062871e-05, 'epoch': 0.49}
{'loss': 1.0601, 'grad_norm': 0.45778173208236694, 'learning_rate': 1.0846381286926007e-05, 'epoch': 0.49}
{'loss': 0.973, 'grad_norm': 0.43205130100250244, 'learning_rate': 1.0840172634628948e-05, 'epoch': 0.49}
{'loss': 0.8916, 'grad_norm': 0.27922946214675903, 'learning_rate': 1.0833963656147944e-05, 'epoch': 0.49}
{'loss': 1.0118, 'grad_norm': 0.4407826066017151, 'learning_rate': 1.082775435389353e-05, 'epoch': 0.49}
{'loss': 0.9683, 'grad_norm': 0.37907418608665466, 'learning_rate': 1.0821544730276379e-05, 'epoch': 0.49}
{'loss': 0.9956, 'grad_norm': 0.4052412211894989, 'learning_rate': 1.0815334787707277e-05, 'epoch': 0.49}
{'loss': 1.1056, 'grad_norm': 0.45932772755622864, 'learning_rate': 1.0809124528597138e-05, 'epoch': 0.49}
{'loss': 1.0272, 'grad_norm': 0.4881536066532135, 'learning_rate': 1.0802913955356998e-05, 'epoch': 0.49}
{'loss': 1.0071, 'grad_norm': 0.4195629954338074, 'learning_rate': 1.0796703070398016e-05, 'epoch': 0.49}
{'loss': 0.9692, 'grad_norm': 0.44406020641326904, 'learning_rate': 1.079049187613147e-05, 'epoch': 0.49}
{'loss': 1.1494, 'grad_norm': 0.44623109698295593, 'learning_rate': 1.0784280374968761e-05, 'epoch': 0.49}
{'loss': 1.045, 'grad_norm': 0.47547802329063416, 'learning_rate': 1.0778068569321403e-05, 'epoch': 0.49}
{'loss': 0.9998, 'grad_norm': 0.4451473355293274, 'learning_rate': 1.077185646160104e-05, 'epoch': 0.49}
{'loss': 0.9679, 'grad_norm': 0.4414432644844055, 'learning_rate': 1.0765644054219422e-05, 'epoch': 0.49}
{'loss': 1.0448, 'grad_norm': 0.4642699658870697, 'learning_rate': 1.0759431349588421e-05, 'epoch': 0.49}
{'loss': 1.0648, 'grad_norm': 0.45267820358276367, 'learning_rate': 1.0753218350120023e-05, 'epoch': 0.49}
{'loss': 0.9645, 'grad_norm': 0.40109264850616455, 'learning_rate': 1.0747005058226325e-05, 'epoch': 0.49}
{'loss': 1.0732, 'grad_norm': 0.4706873297691345, 'learning_rate': 1.0740791476319543e-05, 'epoch': 0.49}
{'loss': 1.0468, 'grad_norm': 0.4827163517475128, 'learning_rate': 1.0734577606812007e-05, 'epoch': 0.49}
{'loss': 0.9822, 'grad_norm': 0.42096707224845886, 'learning_rate': 1.0728363452116149e-05, 'epoch': 0.49}
{'loss': 1.0329, 'grad_norm': 0.4561094641685486, 'learning_rate': 1.0722149014644523e-05, 'epoch': 0.49}
{'loss': 1.0042, 'grad_norm': 0.418221116065979, 'learning_rate': 1.0715934296809782e-05, 'epoch': 0.49}
{'loss': 1.0828, 'grad_norm': 0.5139691233634949, 'learning_rate': 1.0709719301024698e-05, 'epoch': 0.49}
{'loss': 0.9972, 'grad_norm': 0.43377795815467834, 'learning_rate': 1.0703504029702148e-05, 'epoch': 0.49}
{'loss': 0.9166, 'grad_norm': 0.2501266598701477, 'learning_rate': 1.0697288485255107e-05, 'epoch': 0.49}
{'loss': 1.076, 'grad_norm': 0.4694054424762726, 'learning_rate': 1.0691072670096669e-05, 'epoch': 0.49}
{'loss': 0.9792, 'grad_norm': 0.4292658567428589, 'learning_rate': 1.0684856586640026e-05, 'epoch': 0.49}
{'loss': 1.0448, 'grad_norm': 0.41815513372421265, 'learning_rate': 1.0678640237298476e-05, 'epoch': 0.49}
{'loss': 0.9819, 'grad_norm': 0.4254639744758606, 'learning_rate': 1.0672423624485423e-05, 'epoch': 0.49}
{'loss': 1.0283, 'grad_norm': 0.3997913897037506, 'learning_rate': 1.0666206750614363e-05, 'epoch': 0.49}
{'loss': 1.0896, 'grad_norm': 0.4512350857257843, 'learning_rate': 1.0659989618098904e-05, 'epoch': 0.49}
{'loss': 1.0473, 'grad_norm': 0.4230560064315796, 'learning_rate': 1.065377222935275e-05, 'epoch': 0.49}
{'loss': 0.976, 'grad_norm': 0.43383631110191345, 'learning_rate': 1.0647554586789708e-05, 'epoch': 0.5}
{'loss': 1.0333, 'grad_norm': 0.42876389622688293, 'learning_rate': 1.064133669282368e-05, 'epoch': 0.5}
{'loss': 1.0363, 'grad_norm': 0.49400463700294495, 'learning_rate': 1.0635118549868668e-05, 'epoch': 0.5}
{'loss': 0.9379, 'grad_norm': 0.4166482985019684, 'learning_rate': 1.0628900160338764e-05, 'epoch': 0.5}
{'loss': 1.0154, 'grad_norm': 0.44065552949905396, 'learning_rate': 1.0622681526648167e-05, 'epoch': 0.5}
{'loss': 0.9874, 'grad_norm': 0.43721315264701843, 'learning_rate': 1.0616462651211156e-05, 'epoch': 0.5}
{'loss': 0.9222, 'grad_norm': 0.25711849331855774, 'learning_rate': 1.0610243536442125e-05, 'epoch': 0.5}
{'loss': 1.1521, 'grad_norm': 0.4826869070529938, 'learning_rate': 1.0604024184755539e-05, 'epoch': 0.5}
{'loss': 0.9966, 'grad_norm': 0.40901631116867065, 'learning_rate': 1.0597804598565969e-05, 'epoch': 0.5}
{'loss': 1.042, 'grad_norm': 0.4427226483821869, 'learning_rate': 1.0591584780288069e-05, 'epoch': 0.5}
{'loss': 1.0398, 'grad_norm': 0.4331184923648834, 'learning_rate': 1.0585364732336587e-05, 'epoch': 0.5}
{'loss': 1.0363, 'grad_norm': 0.43053609132766724, 'learning_rate': 1.0579144457126365e-05, 'epoch': 0.5}
{'loss': 1.0528, 'grad_norm': 0.42950794100761414, 'learning_rate': 1.057292395707232e-05, 'epoch': 0.5}
{'loss': 0.9646, 'grad_norm': 0.2591318190097809, 'learning_rate': 1.0566703234589471e-05, 'epoch': 0.5}
{'loss': 1.1029, 'grad_norm': 0.45152759552001953, 'learning_rate': 1.0560482292092912e-05, 'epoch': 0.5}
{'loss': 1.02, 'grad_norm': 0.42232879996299744, 'learning_rate': 1.0554261131997833e-05, 'epoch': 0.5}
{'loss': 1.0206, 'grad_norm': 0.445732444524765, 'learning_rate': 1.0548039756719497e-05, 'epoch': 0.5}
{'loss': 1.0365, 'grad_norm': 0.43829381465911865, 'learning_rate': 1.054181816867326e-05, 'epoch': 0.5}
{'loss': 1.0886, 'grad_norm': 0.44856369495391846, 'learning_rate': 1.053559637027455e-05, 'epoch': 0.5}
Error with image file is truncated (21 bytes not processed)
{'loss': 0.8778, 'grad_norm': 0.4229559302330017, 'learning_rate': 1.0529374363938888e-05, 'epoch': 0.5}
{'loss': 0.9876, 'grad_norm': 0.46948784589767456, 'learning_rate': 1.0523152152081875e-05, 'epoch': 0.5}
{'loss': 1.0226, 'grad_norm': 0.40737906098365784, 'learning_rate': 1.051692973711918e-05, 'epoch': 0.5}
{'loss': 0.8934, 'grad_norm': 0.27024921774864197, 'learning_rate': 1.0510707121466568e-05, 'epoch': 0.5}
{'loss': 1.0366, 'grad_norm': 0.4389422833919525, 'learning_rate': 1.0504484307539864e-05, 'epoch': 0.5}
{'loss': 1.004, 'grad_norm': 0.4639165699481964, 'learning_rate': 1.0498261297754984e-05, 'epoch': 0.5}
{'loss': 1.022, 'grad_norm': 0.4577272832393646, 'learning_rate': 1.0492038094527907e-05, 'epoch': 0.5}
Step 2599: Transitioning to Stage 2 - Reparameterization
Step 2599: Transitioning to Stage 2 - Reparameterization
Step 2599: Transitioning to Stage 2 - Reparameterization
Step 2599: Transitioning to Stage 2 - ReparameterizationStep 2599: Transitioning to Stage 2 - Reparameterization
Step 2599: Transitioning to Stage 2 - Reparameterization
Step 2599: Transitioning to Stage 2 - Reparameterization
Step 2599: Transitioning to Stage 2 - ReparameterizationStep 2599: Transitioning to Stage 2 - Reparameterization

Step 2599: Transitioning to Stage 2 - Reparameterization
Step 2599: Transitioning to Stage 2 - Reparameterization

Step 2599: Transitioning to Stage 2 - Reparameterization
Step 2599: Transitioning to Stage 2 - Reparameterization
Step 2599: Transitioning to Stage 2 - Reparameterization
Step 2599: Transitioning to Stage 2 - Reparameterization
Step 2599: Transitioning to Stage 2 - Reparameterization
  Calling reparam_moe_layers()...
  Calling reparam_moe_layers()...
  Calling reparam_moe_layers()...
  Calling reparam_moe_layers()...
  Calling reparam_moe_layers()...
  Calling reparam_moe_layers()...
  Calling reparam_moe_layers()...
  Calling reparam_moe_layers()...
  Calling reparam_moe_layers()...
  Calling reparam_moe_layers()...
  Calling reparam_moe_layers()...
  Calling reparam_moe_layers()...
  Calling reparam_moe_layers()...
  Calling reparam_moe_layers()...
  Calling reparam_moe_layers()...
  Calling reparam_moe_layers()...
  Reparameterization completed
  Updating optimizer after reparameterization...
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
  Handling DeepSpeed ZeRO optimizer reparameterization...
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
    Successfully recreated DeepSpeed optimizer with 11 groups
    Initialized 168 parameter states with step count 2599.0
    DeepSpeed optimizer reparameterization completed successfully
✓ Optimizer parameter sync verified: 168 parameters
Step 2599: Successfully transitioned to Stage 2
  Reparameterization completed
  Reparameterization completed
  Updating optimizer after reparameterization...
  Reparameterization completed
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
  Handling DeepSpeed ZeRO optimizer reparameterization...
  Updating optimizer after reparameterization...
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
  Updating optimizer after reparameterization...
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
  Reparameterization completed
  Handling DeepSpeed ZeRO optimizer reparameterization...
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
  Handling DeepSpeed ZeRO optimizer reparameterization...
  Reparameterization completed
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
  Updating optimizer after reparameterization...
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
  Updating optimizer after reparameterization...
  Handling DeepSpeed ZeRO optimizer reparameterization...
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
  Handling DeepSpeed ZeRO optimizer reparameterization...
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
  Reparameterization completed
  Updating optimizer after reparameterization...
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
  Handling DeepSpeed ZeRO optimizer reparameterization...
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
    Successfully recreated DeepSpeed optimizer with 11 groups
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
    Successfully recreated DeepSpeed optimizer with 11 groups
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
    Successfully recreated DeepSpeed optimizer with 11 groups
    Successfully recreated DeepSpeed optimizer with 11 groups
Skipping empty parameter group: no_decay_parameters
  Reparameterization completed
  Reparameterization completed
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
    Successfully recreated DeepSpeed optimizer with 11 groups
  Updating optimizer after reparameterization...
  Updating optimizer after reparameterization...
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
  Handling DeepSpeed ZeRO optimizer reparameterization...
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
  Handling DeepSpeed ZeRO optimizer reparameterization...
    Initialized 168 parameter states with step count 2599.0
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
    DeepSpeed optimizer reparameterization completed successfully
    Initialized 168 parameter states with step count 2599.0
    DeepSpeed optimizer reparameterization completed successfully
  Reparameterization completed
    Initialized 168 parameter states with step count 2599.0
    DeepSpeed optimizer reparameterization completed successfully
    Initialized 168 parameter states with step count 2599.0
    DeepSpeed optimizer reparameterization completed successfully
  Updating optimizer after reparameterization...
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
✓ Optimizer parameter sync verified: 168 parameters
  Handling DeepSpeed ZeRO optimizer reparameterization...
    Initialized 168 parameter states with step count 2599.0
Step 2599: Successfully transitioned to Stage 2
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
    DeepSpeed optimizer reparameterization completed successfully
✓ Optimizer parameter sync verified: 168 parameters
Step 2599: Successfully transitioned to Stage 2
✓ Optimizer parameter sync verified: 168 parameters
Step 2599: Successfully transitioned to Stage 2
✓ Optimizer parameter sync verified: 168 parameters
Step 2599: Successfully transitioned to Stage 2
Skipping empty parameter group: no_decay_parameters
✓ Optimizer parameter sync verified: 168 parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Step 2599: Successfully transitioned to Stage 2
    Successfully recreated DeepSpeed optimizer with 11 groups
  Reparameterization completed
  Updating optimizer after reparameterization...
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
  Handling DeepSpeed ZeRO optimizer reparameterization...
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
    Initialized 168 parameter states with step count 2599.0
    DeepSpeed optimizer reparameterization completed successfully
  Reparameterization completed
  Updating optimizer after reparameterization...
✓ Optimizer parameter sync verified: 168 parameters
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
Step 2599: Successfully transitioned to Stage 2
  Handling DeepSpeed ZeRO optimizer reparameterization...
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
  Reparameterization completed
    Successfully recreated DeepSpeed optimizer with 11 groups
  Updating optimizer after reparameterization...
Skipping empty parameter group: no_decay_parameters
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
  Handling DeepSpeed ZeRO optimizer reparameterization...
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
    Successfully recreated DeepSpeed optimizer with 11 groups
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
  Reparameterization completed
    Successfully recreated DeepSpeed optimizer with 11 groups
    Initialized 168 parameter states with step count 2599.0
    DeepSpeed optimizer reparameterization completed successfully
  Reparameterization completed
  Updating optimizer after reparameterization...
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
  Handling DeepSpeed ZeRO optimizer reparameterization...
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
  Updating optimizer after reparameterization...
  Reparameterization completed
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
  Handling DeepSpeed ZeRO optimizer reparameterization...
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
✓ Optimizer parameter sync verified: 168 parameters
Step 2599: Successfully transitioned to Stage 2
    Initialized 168 parameter states with step count 2599.0
    DeepSpeed optimizer reparameterization completed successfully
  Updating optimizer after reparameterization...
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
  Handling DeepSpeed ZeRO optimizer reparameterization...
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
    Initialized 168 parameter states with step count 2599.0
    DeepSpeed optimizer reparameterization completed successfully
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
✓ Optimizer parameter sync verified: 168 parameters
    Successfully recreated DeepSpeed optimizer with 11 groups
Step 2599: Successfully transitioned to Stage 2
✓ Optimizer parameter sync verified: 168 parameters
Step 2599: Successfully transitioned to Stage 2
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
    Initialized 168 parameter states with step count 2599.0
    DeepSpeed optimizer reparameterization completed successfully
    Successfully recreated DeepSpeed optimizer with 11 groups
✓ Optimizer parameter sync verified: 168 parameters
Step 2599: Successfully transitioned to Stage 2
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
    Successfully recreated DeepSpeed optimizer with 11 groups
    Initialized 168 parameter states with step count 2599.0
    DeepSpeed optimizer reparameterization completed successfully
Skipping empty parameter group: no_decay_parameters
✓ Optimizer parameter sync verified: 168 parameters
Step 2599: Successfully transitioned to Stage 2
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
    Successfully recreated DeepSpeed optimizer with 11 groups
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
    Initialized 168 parameter states with step count 2599.0
    DeepSpeed optimizer reparameterization completed successfully
    Successfully recreated DeepSpeed optimizer with 11 groups
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
    Successfully recreated DeepSpeed optimizer with 11 groups
✓ Optimizer parameter sync verified: 168 parameters
Step 2599: Successfully transitioned to Stage 2
    Initialized 168 parameter states with step count 2599.0
    DeepSpeed optimizer reparameterization completed successfully
    Initialized 168 parameter states with step count 2599.0
    DeepSpeed optimizer reparameterization completed successfully
    Initialized 168 parameter states with step count 2599.0
    DeepSpeed optimizer reparameterization completed successfully
✓ Optimizer parameter sync verified: 168 parameters
Step 2599: Successfully transitioned to Stage 2
✓ Optimizer parameter sync verified: 168 parameters
Step 2599: Successfully transitioned to Stage 2
✓ Optimizer parameter sync verified: 168 parameters
Step 2599: Successfully transitioned to Stage 2
{'loss': 1.006, 'grad_norm': 0.025109505280852318, 'learning_rate': 1.0485814700274706e-05, 'epoch': 0.5}
{'loss': 1.1183, 'grad_norm': 0.024486130103468895, 'learning_rate': 1.047959111741151e-05, 'epoch': 0.5}
{'loss': 1.0392, 'grad_norm': 0.026352638378739357, 'learning_rate': 1.0473367348354529e-05, 'epoch': 0.5}
{'loss': 0.9568, 'grad_norm': 0.0250255074352026, 'learning_rate': 1.0467143395520044e-05, 'epoch': 0.5}
{'loss': 1.039, 'grad_norm': 0.026859905570745468, 'learning_rate': 1.046091926132441e-05, 'epoch': 0.5}
{'loss': 1.0233, 'grad_norm': 0.024627026170492172, 'learning_rate': 1.0454694948184045e-05, 'epoch': 0.5}
{'loss': 1.0932, 'grad_norm': 0.022406378760933876, 'learning_rate': 1.044847045851545e-05, 'epoch': 0.5}
{'loss': 0.9341, 'grad_norm': 0.02380283735692501, 'learning_rate': 1.044224579473518e-05, 'epoch': 0.5}
{'loss': 0.982, 'grad_norm': 0.025866415351629257, 'learning_rate': 1.0436020959259862e-05, 'epoch': 0.5}
{'loss': 1.0002, 'grad_norm': 0.022813070565462112, 'learning_rate': 1.0429795954506203e-05, 'epoch': 0.5}
{'loss': 1.0512, 'grad_norm': 0.02793164923787117, 'learning_rate': 1.0423570782890951e-05, 'epoch': 0.5}
{'loss': 0.9838, 'grad_norm': 0.024137569591403008, 'learning_rate': 1.0417345446830938e-05, 'epoch': 0.5}
{'loss': 0.9973, 'grad_norm': 0.024789802730083466, 'learning_rate': 1.0411119948743052e-05, 'epoch': 0.5}
{'loss': 1.0571, 'grad_norm': 0.02558908984065056, 'learning_rate': 1.0404894291044247e-05, 'epoch': 0.5}
{'loss': 1.1229, 'grad_norm': 0.023456932976841927, 'learning_rate': 1.0398668476151538e-05, 'epoch': 0.5}
{'loss': 1.012, 'grad_norm': 0.02450530230998993, 'learning_rate': 1.0392442506482e-05, 'epoch': 0.5}
{'loss': 1.093, 'grad_norm': 0.0240517258644104, 'learning_rate': 1.038621638445277e-05, 'epoch': 0.5}
{'loss': 1.0843, 'grad_norm': 0.024536633864045143, 'learning_rate': 1.037999011248104e-05, 'epoch': 0.5}
{'loss': 1.044, 'grad_norm': 0.024419592693448067, 'learning_rate': 1.0373763692984062e-05, 'epoch': 0.5}
{'loss': 0.981, 'grad_norm': 0.022944169119000435, 'learning_rate': 1.0367537128379154e-05, 'epoch': 0.5}
{'loss': 0.9905, 'grad_norm': 0.02565581537783146, 'learning_rate': 1.0361310421083677e-05, 'epoch': 0.5}
{'loss': 1.0211, 'grad_norm': 0.023953640833497047, 'learning_rate': 1.0355083573515052e-05, 'epoch': 0.5}
{'loss': 1.1116, 'grad_norm': 0.02422424964606762, 'learning_rate': 1.0348856588090764e-05, 'epoch': 0.5}
{'loss': 1.0074, 'grad_norm': 0.025183051824569702, 'learning_rate': 1.0342629467228331e-05, 'epoch': 0.5}
{'loss': 1.0102, 'grad_norm': 0.02389618009328842, 'learning_rate': 1.0336402213345345e-05, 'epoch': 0.5}
{'loss': 1.086, 'grad_norm': 0.024185318499803543, 'learning_rate': 1.0330174828859434e-05, 'epoch': 0.51}
{'loss': 0.9809, 'grad_norm': 0.02314978651702404, 'learning_rate': 1.0323947316188288e-05, 'epoch': 0.51}
{'loss': 1.0471, 'grad_norm': 0.022487027570605278, 'learning_rate': 1.031771967774964e-05, 'epoch': 0.51}
{'loss': 1.166, 'grad_norm': 0.025611432269215584, 'learning_rate': 1.0311491915961271e-05, 'epoch': 0.51}
{'loss': 1.0187, 'grad_norm': 0.021823566406965256, 'learning_rate': 1.030526403324102e-05, 'epoch': 0.51}
{'loss': 0.9738, 'grad_norm': 0.025039350613951683, 'learning_rate': 1.0299036032006759e-05, 'epoch': 0.51}
{'loss': 1.0015, 'grad_norm': 0.02438514493405819, 'learning_rate': 1.0292807914676412e-05, 'epoch': 0.51}
{'loss': 0.9864, 'grad_norm': 0.024530814960598946, 'learning_rate': 1.0286579683667952e-05, 'epoch': 0.51}
{'loss': 0.9955, 'grad_norm': 0.02304970845580101, 'learning_rate': 1.0280351341399392e-05, 'epoch': 0.51}
{'loss': 1.1225, 'grad_norm': 0.024789750576019287, 'learning_rate': 1.027412289028879e-05, 'epoch': 0.51}
{'loss': 1.0221, 'grad_norm': 0.009430496953427792, 'learning_rate': 1.0267894332754243e-05, 'epoch': 0.51}
{'loss': 0.9817, 'grad_norm': 0.022825980558991432, 'learning_rate': 1.0261665671213891e-05, 'epoch': 0.51}
{'loss': 1.0963, 'grad_norm': 0.023580634966492653, 'learning_rate': 1.0255436908085919e-05, 'epoch': 0.51}
{'loss': 0.9537, 'grad_norm': 0.012007905170321465, 'learning_rate': 1.024920804578854e-05, 'epoch': 0.51}
{'loss': 0.9894, 'grad_norm': 0.0224381722509861, 'learning_rate': 1.0242979086740019e-05, 'epoch': 0.51}
{'loss': 0.9767, 'grad_norm': 0.02437260001897812, 'learning_rate': 1.023675003335865e-05, 'epoch': 0.51}
{'loss': 0.9843, 'grad_norm': 0.009180746041238308, 'learning_rate': 1.0230520888062765e-05, 'epoch': 0.51}
{'loss': 1.1125, 'grad_norm': 0.027700787410140038, 'learning_rate': 1.0224291653270739e-05, 'epoch': 0.51}
{'loss': 0.9661, 'grad_norm': 0.02691679447889328, 'learning_rate': 1.0218062331400969e-05, 'epoch': 0.51}
{'loss': 1.0824, 'grad_norm': 0.024055741727352142, 'learning_rate': 1.0211832924871889e-05, 'epoch': 0.51}
{'loss': 1.1482, 'grad_norm': 0.02489771693944931, 'learning_rate': 1.0205603436101978e-05, 'epoch': 0.51}
{'loss': 1.0086, 'grad_norm': 0.02395159751176834, 'learning_rate': 1.0199373867509734e-05, 'epoch': 0.51}
{'loss': 1.0315, 'grad_norm': 0.025654315948486328, 'learning_rate': 1.019314422151369e-05, 'epoch': 0.51}
{'loss': 1.1098, 'grad_norm': 0.027260802686214447, 'learning_rate': 1.0186914500532408e-05, 'epoch': 0.51}
{'loss': 1.0617, 'grad_norm': 0.024418968707323074, 'learning_rate': 1.0180684706984483e-05, 'epoch': 0.51}
{'loss': 1.0562, 'grad_norm': 0.021494105458259583, 'learning_rate': 1.0174454843288533e-05, 'epoch': 0.51}
{'loss': 1.0578, 'grad_norm': 0.025343747809529305, 'learning_rate': 1.0168224911863205e-05, 'epoch': 0.51}
{'loss': 0.9747, 'grad_norm': 0.022906064987182617, 'learning_rate': 1.0161994915127173e-05, 'epoch': 0.51}
{'loss': 0.996, 'grad_norm': 0.026278389617800713, 'learning_rate': 1.015576485549914e-05, 'epoch': 0.51}
{'loss': 1.0324, 'grad_norm': 0.02671033889055252, 'learning_rate': 1.0149534735397823e-05, 'epoch': 0.51}
{'loss': 1.0243, 'grad_norm': 0.02560034580528736, 'learning_rate': 1.0143304557241979e-05, 'epoch': 0.51}
{'loss': 1.1096, 'grad_norm': 0.028224781155586243, 'learning_rate': 1.0137074323450372e-05, 'epoch': 0.51}
{'loss': 0.9894, 'grad_norm': 0.025851523503661156, 'learning_rate': 1.0130844036441787e-05, 'epoch': 0.51}
{'loss': 0.9912, 'grad_norm': 0.023257765918970108, 'learning_rate': 1.0124613698635043e-05, 'epoch': 0.51}
{'loss': 1.0553, 'grad_norm': 0.02198675461113453, 'learning_rate': 1.0118383312448973e-05, 'epoch': 0.51}
{'loss': 0.9081, 'grad_norm': 0.008081241510808468, 'learning_rate': 1.0112152880302426e-05, 'epoch': 0.51}
{'loss': 1.0461, 'grad_norm': 0.022736940532922745, 'learning_rate': 1.0105922404614265e-05, 'epoch': 0.51}
{'loss': 1.0375, 'grad_norm': 0.023921718820929527, 'learning_rate': 1.0099691887803385e-05, 'epoch': 0.51}
{'loss': 1.1351, 'grad_norm': 0.026614338159561157, 'learning_rate': 1.0093461332288678e-05, 'epoch': 0.51}
{'loss': 0.9576, 'grad_norm': 0.009132768027484417, 'learning_rate': 1.0087230740489065e-05, 'epoch': 0.51}
{'loss': 1.1204, 'grad_norm': 0.023554790765047073, 'learning_rate': 1.0081000114823473e-05, 'epoch': 0.51}
{'loss': 1.1002, 'grad_norm': 0.024615243077278137, 'learning_rate': 1.007476945771085e-05, 'epoch': 0.51}
{'loss': 1.1097, 'grad_norm': 0.026626434177160263, 'learning_rate': 1.006853877157015e-05, 'epoch': 0.51}
{'loss': 1.1053, 'grad_norm': 0.02546965703368187, 'learning_rate': 1.0062308058820337e-05, 'epoch': 0.51}
{'loss': 1.0165, 'grad_norm': 0.02343149669468403, 'learning_rate': 1.0056077321880393e-05, 'epoch': 0.51}
{'loss': 1.0002, 'grad_norm': 0.02302113175392151, 'learning_rate': 1.0049846563169297e-05, 'epoch': 0.51}
{'loss': 1.0207, 'grad_norm': 0.02377985417842865, 'learning_rate': 1.0043615785106051e-05, 'epoch': 0.51}
{'loss': 1.0036, 'grad_norm': 0.021939342841506004, 'learning_rate': 1.0037384990109658e-05, 'epoch': 0.51}
{'loss': 1.1061, 'grad_norm': 0.025624308735132217, 'learning_rate': 1.0031154180599123e-05, 'epoch': 0.51}
{'loss': 1.0473, 'grad_norm': 0.02240024134516716, 'learning_rate': 1.0024923358993458e-05, 'epoch': 0.51}
{'loss': 1.0878, 'grad_norm': 0.0243073757737875, 'learning_rate': 1.0018692527711695e-05, 'epoch': 0.51}
{'loss': 0.9597, 'grad_norm': 0.02325797639787197, 'learning_rate': 1.0012461689172846e-05, 'epoch': 0.51}
{'loss': 1.1172, 'grad_norm': 0.02445424348115921, 'learning_rate': 1.0006230845795937e-05, 'epoch': 0.52}
{'loss': 0.9053, 'grad_norm': 0.022780893370509148, 'learning_rate': 1e-05, 'epoch': 0.52}
{'loss': 1.0202, 'grad_norm': 0.025365425273776054, 'learning_rate': 9.993769154204063e-06, 'epoch': 0.52}
{'loss': 0.963, 'grad_norm': 0.022932209074497223, 'learning_rate': 9.987538310827159e-06, 'epoch': 0.52}
{'loss': 1.0987, 'grad_norm': 0.023921078070998192, 'learning_rate': 9.981307472288308e-06, 'epoch': 0.52}
{'loss': 1.0022, 'grad_norm': 0.023636875674128532, 'learning_rate': 9.975076641006542e-06, 'epoch': 0.52}
{'loss': 1.0525, 'grad_norm': 0.025371694937348366, 'learning_rate': 9.968845819400883e-06, 'epoch': 0.52}
{'loss': 1.0501, 'grad_norm': 0.02537858858704567, 'learning_rate': 9.962615009890346e-06, 'epoch': 0.52}
{'loss': 1.0075, 'grad_norm': 0.0240117609500885, 'learning_rate': 9.956384214893949e-06, 'epoch': 0.52}
{'loss': 1.1156, 'grad_norm': 0.024169044569134712, 'learning_rate': 9.950153436830707e-06, 'epoch': 0.52}
{'loss': 1.0017, 'grad_norm': 0.022336795926094055, 'learning_rate': 9.94392267811961e-06, 'epoch': 0.52}
{'loss': 1.0214, 'grad_norm': 0.025405211374163628, 'learning_rate': 9.937691941179665e-06, 'epoch': 0.52}
{'loss': 1.0434, 'grad_norm': 0.024189505726099014, 'learning_rate': 9.931461228429856e-06, 'epoch': 0.52}
{'loss': 0.9615, 'grad_norm': 0.025770600885152817, 'learning_rate': 9.925230542289151e-06, 'epoch': 0.52}
{'loss': 0.9115, 'grad_norm': 0.009227327071130276, 'learning_rate': 9.91899988517653e-06, 'epoch': 0.52}
{'loss': 1.1138, 'grad_norm': 0.02387934736907482, 'learning_rate': 9.912769259510938e-06, 'epoch': 0.52}
{'loss': 0.992, 'grad_norm': 0.02380833402276039, 'learning_rate': 9.906538667711324e-06, 'epoch': 0.52}
{'loss': 1.0732, 'grad_norm': 0.023922203108668327, 'learning_rate': 9.90030811219662e-06, 'epoch': 0.52}
{'loss': 1.0769, 'grad_norm': 0.025533847510814667, 'learning_rate': 9.894077595385736e-06, 'epoch': 0.52}
{'loss': 1.0967, 'grad_norm': 0.023351259529590607, 'learning_rate': 9.887847119697577e-06, 'epoch': 0.52}
{'loss': 1.0104, 'grad_norm': 0.025525562465190887, 'learning_rate': 9.881616687551032e-06, 'epoch': 0.52}
{'loss': 1.017, 'grad_norm': 0.023599527776241302, 'learning_rate': 9.875386301364958e-06, 'epoch': 0.52}
{'loss': 1.0632, 'grad_norm': 0.023098642006516457, 'learning_rate': 9.869155963558215e-06, 'epoch': 0.52}
{'loss': 0.9847, 'grad_norm': 0.026397209614515305, 'learning_rate': 9.862925676549635e-06, 'epoch': 0.52}
WARNING: tokenization mismatch: 0 vs. 517. (ignored)
{'loss': 1.05, 'grad_norm': 0.0252060629427433, 'learning_rate': 9.856695442758023e-06, 'epoch': 0.52}
{'loss': 0.987, 'grad_norm': 0.022805454209446907, 'learning_rate': 9.850465264602175e-06, 'epoch': 0.52}
{'loss': 0.9563, 'grad_norm': 0.02299371175467968, 'learning_rate': 9.844235144500865e-06, 'epoch': 0.52}
{'loss': 1.0314, 'grad_norm': 0.02501850575208664, 'learning_rate': 9.83800508487283e-06, 'epoch': 0.52}
{'loss': 0.9622, 'grad_norm': 0.02340242825448513, 'learning_rate': 9.831775088136797e-06, 'epoch': 0.52}
{'loss': 1.075, 'grad_norm': 0.025621159002184868, 'learning_rate': 9.82554515671147e-06, 'epoch': 0.52}
{'loss': 1.0032, 'grad_norm': 0.023855024948716164, 'learning_rate': 9.819315293015519e-06, 'epoch': 0.52}
{'loss': 1.0441, 'grad_norm': 0.022260671481490135, 'learning_rate': 9.813085499467594e-06, 'epoch': 0.52}
{'loss': 1.0158, 'grad_norm': 0.02431294322013855, 'learning_rate': 9.806855778486314e-06, 'epoch': 0.52}
{'loss': 1.0447, 'grad_norm': 0.023786766454577446, 'learning_rate': 9.800626132490268e-06, 'epoch': 0.52}
{'loss': 0.9268, 'grad_norm': 0.021585294976830482, 'learning_rate': 9.794396563898022e-06, 'epoch': 0.52}
{'loss': 0.993, 'grad_norm': 0.02179037407040596, 'learning_rate': 9.788167075128113e-06, 'epoch': 0.52}
{'loss': 1.0812, 'grad_norm': 0.02725149877369404, 'learning_rate': 9.781937668599035e-06, 'epoch': 0.52}
{'loss': 1.0187, 'grad_norm': 0.02244802564382553, 'learning_rate': 9.775708346729263e-06, 'epoch': 0.52}
{'loss': 0.9175, 'grad_norm': 0.008699441328644753, 'learning_rate': 9.769479111937238e-06, 'epoch': 0.52}
{'loss': 1.0801, 'grad_norm': 0.022873368114233017, 'learning_rate': 9.763249966641352e-06, 'epoch': 0.52}
{'loss': 1.1128, 'grad_norm': 0.0254555381834507, 'learning_rate': 9.757020913259986e-06, 'epoch': 0.52}
{'loss': 0.9681, 'grad_norm': 0.024680227041244507, 'learning_rate': 9.750791954211464e-06, 'epoch': 0.52}
{'loss': 1.0999, 'grad_norm': 0.022604716941714287, 'learning_rate': 9.744563091914085e-06, 'epoch': 0.52}
{'loss': 1.0644, 'grad_norm': 0.021212413907051086, 'learning_rate': 9.738334328786114e-06, 'epoch': 0.52}
{'loss': 1.1321, 'grad_norm': 0.023100804537534714, 'learning_rate': 9.732105667245759e-06, 'epoch': 0.52}
{'loss': 1.0271, 'grad_norm': 0.02464858815073967, 'learning_rate': 9.725877109711212e-06, 'epoch': 0.52}
{'loss': 0.9834, 'grad_norm': 0.026145657524466515, 'learning_rate': 9.719648658600611e-06, 'epoch': 0.52}
{'loss': 1.0851, 'grad_norm': 0.023711463436484337, 'learning_rate': 9.71342031633205e-06, 'epoch': 0.52}
{'loss': 0.9663, 'grad_norm': 0.020424801856279373, 'learning_rate': 9.70719208532359e-06, 'epoch': 0.52}
{'loss': 1.0703, 'grad_norm': 0.023305919021368027, 'learning_rate': 9.700963967993246e-06, 'epoch': 0.52}
{'loss': 1.0457, 'grad_norm': 0.02472037263214588, 'learning_rate': 9.694735966758982e-06, 'epoch': 0.52}
{'loss': 0.9385, 'grad_norm': 0.0249362513422966, 'learning_rate': 9.688508084038729e-06, 'epoch': 0.52}
{'loss': 1.0492, 'grad_norm': 0.02333017997443676, 'learning_rate': 9.682280322250365e-06, 'epoch': 0.53}
{'loss': 1.0732, 'grad_norm': 0.022140858694911003, 'learning_rate': 9.676052683811715e-06, 'epoch': 0.53}
{'loss': 1.0164, 'grad_norm': 0.022236227989196777, 'learning_rate': 9.669825171140568e-06, 'epoch': 0.53}
{'loss': 1.0038, 'grad_norm': 0.02449583262205124, 'learning_rate': 9.66359778665466e-06, 'epoch': 0.53}
{'loss': 1.0777, 'grad_norm': 0.024885904043912888, 'learning_rate': 9.657370532771672e-06, 'epoch': 0.53}
{'loss': 1.0626, 'grad_norm': 0.025000086054205894, 'learning_rate': 9.651143411909241e-06, 'epoch': 0.53}
{'loss': 1.1106, 'grad_norm': 0.02152341604232788, 'learning_rate': 9.64491642648495e-06, 'epoch': 0.53}
{'loss': 1.1671, 'grad_norm': 0.02706390991806984, 'learning_rate': 9.638689578916326e-06, 'epoch': 0.53}
{'loss': 0.992, 'grad_norm': 0.026687337085604668, 'learning_rate': 9.632462871620847e-06, 'epoch': 0.53}
{'loss': 0.947, 'grad_norm': 0.008816969580948353, 'learning_rate': 9.62623630701594e-06, 'epoch': 0.53}
{'loss': 1.053, 'grad_norm': 0.024578209966421127, 'learning_rate': 9.620009887518963e-06, 'epoch': 0.53}
{'loss': 1.0117, 'grad_norm': 0.024401385337114334, 'learning_rate': 9.613783615547233e-06, 'epoch': 0.53}
{'loss': 1.0411, 'grad_norm': 0.02473030798137188, 'learning_rate': 9.607557493518006e-06, 'epoch': 0.53}
{'loss': 0.9995, 'grad_norm': 0.02220555581152439, 'learning_rate': 9.601331523848464e-06, 'epoch': 0.53}
{'loss': 1.1001, 'grad_norm': 0.0254990141838789, 'learning_rate': 9.595105708955758e-06, 'epoch': 0.53}
{'loss': 1.0322, 'grad_norm': 0.023745404556393623, 'learning_rate': 9.588880051256951e-06, 'epoch': 0.53}
{'loss': 0.9876, 'grad_norm': 0.024255456402897835, 'learning_rate': 9.582654553169064e-06, 'epoch': 0.53}
{'loss': 0.9619, 'grad_norm': 0.00930966529995203, 'learning_rate': 9.576429217109054e-06, 'epoch': 0.53}
{'loss': 1.0472, 'grad_norm': 0.024760866537690163, 'learning_rate': 9.5702040454938e-06, 'epoch': 0.53}
{'loss': 0.9579, 'grad_norm': 0.008185183629393578, 'learning_rate': 9.563979040740138e-06, 'epoch': 0.53}
{'loss': 1.0635, 'grad_norm': 0.024627340957522392, 'learning_rate': 9.557754205264826e-06, 'epoch': 0.53}
{'loss': 1.0241, 'grad_norm': 0.02696225419640541, 'learning_rate': 9.551529541484554e-06, 'epoch': 0.53}
{'loss': 1.1538, 'grad_norm': 0.025764986872673035, 'learning_rate': 9.545305051815957e-06, 'epoch': 0.53}
{'loss': 1.1027, 'grad_norm': 0.025877447798848152, 'learning_rate': 9.539080738675597e-06, 'epoch': 0.53}
{'loss': 0.99, 'grad_norm': 0.02590434066951275, 'learning_rate': 9.53285660447996e-06, 'epoch': 0.53}
{'loss': 1.06, 'grad_norm': 0.024579070508480072, 'learning_rate': 9.526632651645476e-06, 'epoch': 0.53}
{'loss': 1.0106, 'grad_norm': 0.02760741487145424, 'learning_rate': 9.520408882588497e-06, 'epoch': 0.53}
{'loss': 1.0057, 'grad_norm': 0.022057758644223213, 'learning_rate': 9.514185299725299e-06, 'epoch': 0.53}
{'loss': 0.9883, 'grad_norm': 0.02369445003569126, 'learning_rate': 9.507961905472093e-06, 'epoch': 0.53}
{'loss': 0.9748, 'grad_norm': 0.023307256400585175, 'learning_rate': 9.501738702245023e-06, 'epoch': 0.53}
{'loss': 1.0192, 'grad_norm': 0.025248903781175613, 'learning_rate': 9.495515692460138e-06, 'epoch': 0.53}
{'loss': 1.146, 'grad_norm': 0.023379303514957428, 'learning_rate': 9.489292878533436e-06, 'epoch': 0.53}
{'loss': 1.0034, 'grad_norm': 0.026983465999364853, 'learning_rate': 9.483070262880823e-06, 'epoch': 0.53}
{'loss': 1.0601, 'grad_norm': 0.024183090776205063, 'learning_rate': 9.476847847918126e-06, 'epoch': 0.53}
{'loss': 0.9908, 'grad_norm': 0.02370768040418625, 'learning_rate': 9.47062563606111e-06, 'epoch': 0.53}
{'loss': 1.0003, 'grad_norm': 0.024095168337225914, 'learning_rate': 9.464403629725454e-06, 'epoch': 0.53}
{'loss': 1.063, 'grad_norm': 0.026541268453001976, 'learning_rate': 9.458181831326744e-06, 'epoch': 0.53}
{'loss': 1.0639, 'grad_norm': 0.02333849109709263, 'learning_rate': 9.451960243280506e-06, 'epoch': 0.53}
{'loss': 1.0237, 'grad_norm': 0.025015026330947876, 'learning_rate': 9.44573886800217e-06, 'epoch': 0.53}
{'loss': 1.0783, 'grad_norm': 0.02927655540406704, 'learning_rate': 9.43951770790709e-06, 'epoch': 0.53}
{'loss': 0.9996, 'grad_norm': 0.008349211886525154, 'learning_rate': 9.433296765410534e-06, 'epoch': 0.53}
{'loss': 1.1339, 'grad_norm': 0.03489634767174721, 'learning_rate': 9.427076042927683e-06, 'epoch': 0.53}
{'loss': 1.0149, 'grad_norm': 0.02453937940299511, 'learning_rate': 9.420855542873638e-06, 'epoch': 0.53}
{'loss': 1.0007, 'grad_norm': 0.025059392675757408, 'learning_rate': 9.414635267663416e-06, 'epoch': 0.53}
{'loss': 0.9073, 'grad_norm': 0.023027056828141212, 'learning_rate': 9.408415219711934e-06, 'epoch': 0.53}
{'loss': 1.0723, 'grad_norm': 0.02567426860332489, 'learning_rate': 9.402195401434036e-06, 'epoch': 0.53}
{'loss': 1.0894, 'grad_norm': 0.025252435356378555, 'learning_rate': 9.395975815244468e-06, 'epoch': 0.53}
{'loss': 1.1069, 'grad_norm': 0.023973487317562103, 'learning_rate': 9.389756463557878e-06, 'epoch': 0.53}
{'loss': 0.9716, 'grad_norm': 0.016070421785116196, 'learning_rate': 9.383537348788844e-06, 'epoch': 0.53}
{'loss': 0.9363, 'grad_norm': 0.008809900842607021, 'learning_rate': 9.377318473351838e-06, 'epoch': 0.53}
{'loss': 0.9669, 'grad_norm': 0.021764418110251427, 'learning_rate': 9.371099839661238e-06, 'epoch': 0.53}
{'loss': 0.9961, 'grad_norm': 0.023398714140057564, 'learning_rate': 9.364881450131335e-06, 'epoch': 0.53}
{'loss': 1.0154, 'grad_norm': 0.023007366806268692, 'learning_rate': 9.358663307176323e-06, 'epoch': 0.54}
{'loss': 1.0232, 'grad_norm': 0.025316722691059113, 'learning_rate': 9.352445413210294e-06, 'epoch': 0.54}
{'loss': 1.0161, 'grad_norm': 0.02456725388765335, 'learning_rate': 9.346227770647251e-06, 'epoch': 0.54}
{'loss': 1.0351, 'grad_norm': 0.023776812478899956, 'learning_rate': 9.3400103819011e-06, 'epoch': 0.54}
{'loss': 1.1058, 'grad_norm': 0.025656195357441902, 'learning_rate': 9.33379324938564e-06, 'epoch': 0.54}
{'loss': 1.006, 'grad_norm': 0.027638744562864304, 'learning_rate': 9.327576375514582e-06, 'epoch': 0.54}
{'loss': 0.9391, 'grad_norm': 0.008848877623677254, 'learning_rate': 9.321359762701527e-06, 'epoch': 0.54}
{'loss': 0.9271, 'grad_norm': 0.021995432674884796, 'learning_rate': 9.315143413359975e-06, 'epoch': 0.54}
{'loss': 1.1044, 'grad_norm': 0.024784110486507416, 'learning_rate': 9.308927329903333e-06, 'epoch': 0.54}
{'loss': 1.031, 'grad_norm': 0.0234629288315773, 'learning_rate': 9.302711514744897e-06, 'epoch': 0.54}
{'loss': 0.9458, 'grad_norm': 0.022533005103468895, 'learning_rate': 9.296495970297855e-06, 'epoch': 0.54}
{'loss': 1.0984, 'grad_norm': 0.024369727820158005, 'learning_rate': 9.290280698975307e-06, 'epoch': 0.54}
{'loss': 1.1283, 'grad_norm': 0.024744339287281036, 'learning_rate': 9.284065703190221e-06, 'epoch': 0.54}
{'loss': 0.9235, 'grad_norm': 0.024411266669631004, 'learning_rate': 9.27785098535548e-06, 'epoch': 0.54}
{'loss': 0.9807, 'grad_norm': 0.023789284750819206, 'learning_rate': 9.271636547883856e-06, 'epoch': 0.54}
{'loss': 1.0168, 'grad_norm': 0.025291748344898224, 'learning_rate': 9.265422393187998e-06, 'epoch': 0.54}
{'loss': 0.9519, 'grad_norm': 0.023192526772618294, 'learning_rate': 9.259208523680457e-06, 'epoch': 0.54}
{'loss': 0.971, 'grad_norm': 0.02306702919304371, 'learning_rate': 9.252994941773679e-06, 'epoch': 0.54}
{'loss': 0.9581, 'grad_norm': 0.02312779426574707, 'learning_rate': 9.24678164987998e-06, 'epoch': 0.54}
{'loss': 0.9845, 'grad_norm': 0.024046916514635086, 'learning_rate': 9.24056865041158e-06, 'epoch': 0.54}
{'loss': 1.1132, 'grad_norm': 0.02922210283577442, 'learning_rate': 9.234355945780581e-06, 'epoch': 0.54}
{'loss': 1.0221, 'grad_norm': 0.023902352899312973, 'learning_rate': 9.228143538398963e-06, 'epoch': 0.54}
{'loss': 1.0577, 'grad_norm': 0.024573292583227158, 'learning_rate': 9.221931430678598e-06, 'epoch': 0.54}
{'loss': 1.1607, 'grad_norm': 0.028059357777237892, 'learning_rate': 9.215719625031245e-06, 'epoch': 0.54}
{'loss': 0.9598, 'grad_norm': 0.02312481962144375, 'learning_rate': 9.209508123868534e-06, 'epoch': 0.54}
{'loss': 1.0445, 'grad_norm': 0.0248606838285923, 'learning_rate': 9.203296929601986e-06, 'epoch': 0.54}
{'loss': 0.9561, 'grad_norm': 0.02430643141269684, 'learning_rate': 9.197086044643004e-06, 'epoch': 0.54}
{'loss': 0.9646, 'grad_norm': 0.02359316498041153, 'learning_rate': 9.190875471402865e-06, 'epoch': 0.54}
{'loss': 1.019, 'grad_norm': 0.02301434986293316, 'learning_rate': 9.184665212292723e-06, 'epoch': 0.54}
{'loss': 0.9824, 'grad_norm': 0.008374441415071487, 'learning_rate': 9.178455269723623e-06, 'epoch': 0.54}
{'loss': 0.9725, 'grad_norm': 0.02679256722331047, 'learning_rate': 9.172245646106471e-06, 'epoch': 0.54}
{'loss': 0.9737, 'grad_norm': 0.009463160298764706, 'learning_rate': 9.166036343852061e-06, 'epoch': 0.54}
{'loss': 1.0434, 'grad_norm': 0.02516305446624756, 'learning_rate': 9.159827365371055e-06, 'epoch': 0.54}
{'loss': 0.9903, 'grad_norm': 0.02361908368766308, 'learning_rate': 9.153618713073995e-06, 'epoch': 0.54}
{'loss': 1.1097, 'grad_norm': 0.026073915883898735, 'learning_rate': 9.14741038937129e-06, 'epoch': 0.54}
{'loss': 0.9882, 'grad_norm': 0.027565818279981613, 'learning_rate': 9.141202396673232e-06, 'epoch': 0.54}
{'loss': 1.0393, 'grad_norm': 0.023138443008065224, 'learning_rate': 9.13499473738997e-06, 'epoch': 0.54}
{'loss': 1.0517, 'grad_norm': 0.022234316915273666, 'learning_rate': 9.128787413931536e-06, 'epoch': 0.54}
{'loss': 0.9633, 'grad_norm': 0.025090718641877174, 'learning_rate': 9.122580428707822e-06, 'epoch': 0.54}
{'loss': 1.0504, 'grad_norm': 0.025387553498148918, 'learning_rate': 9.116373784128597e-06, 'epoch': 0.54}
{'loss': 1.0309, 'grad_norm': 0.020930498838424683, 'learning_rate': 9.110167482603494e-06, 'epoch': 0.54}
{'loss': 1.0367, 'grad_norm': 0.02429029531776905, 'learning_rate': 9.10396152654201e-06, 'epoch': 0.54}
{'loss': 1.0432, 'grad_norm': 0.024773042649030685, 'learning_rate': 9.097755918353513e-06, 'epoch': 0.54}
{'loss': 1.0306, 'grad_norm': 0.02413504756987095, 'learning_rate': 9.091550660447236e-06, 'epoch': 0.54}
{'loss': 0.9741, 'grad_norm': 0.02334638684988022, 'learning_rate': 9.08534575523227e-06, 'epoch': 0.54}
{'loss': 1.0569, 'grad_norm': 0.02877136692404747, 'learning_rate': 9.079141205117573e-06, 'epoch': 0.54}
{'loss': 1.0674, 'grad_norm': 0.02450622245669365, 'learning_rate': 9.072937012511968e-06, 'epoch': 0.54}
{'loss': 0.945, 'grad_norm': 0.02488608844578266, 'learning_rate': 9.066733179824134e-06, 'epoch': 0.54}
{'loss': 1.0647, 'grad_norm': 0.02253860980272293, 'learning_rate': 9.060529709462613e-06, 'epoch': 0.54}
{'loss': 1.1095, 'grad_norm': 0.025749405845999718, 'learning_rate': 9.054326603835807e-06, 'epoch': 0.54}
{'loss': 1.1392, 'grad_norm': 0.028455955907702446, 'learning_rate': 9.048123865351971e-06, 'epoch': 0.54}
{'loss': 1.0762, 'grad_norm': 0.025313949212431908, 'learning_rate': 9.041921496419225e-06, 'epoch': 0.54}
{'loss': 1.0446, 'grad_norm': 0.027603168040513992, 'learning_rate': 9.035719499445545e-06, 'epoch': 0.55}
{'loss': 1.0054, 'grad_norm': 0.009235626086592674, 'learning_rate': 9.029517876838755e-06, 'epoch': 0.55}
{'loss': 1.1019, 'grad_norm': 0.02431601472198963, 'learning_rate': 9.023316631006536e-06, 'epoch': 0.55}
{'loss': 0.9642, 'grad_norm': 0.008093295618891716, 'learning_rate': 9.017115764356436e-06, 'epoch': 0.55}
{'loss': 0.9762, 'grad_norm': 0.022046299651265144, 'learning_rate': 9.010915279295833e-06, 'epoch': 0.55}
{'loss': 0.9175, 'grad_norm': 0.009629831649363041, 'learning_rate': 9.004715178231975e-06, 'epoch': 0.55}
{'loss': 1.0524, 'grad_norm': 0.02318783663213253, 'learning_rate': 8.998515463571953e-06, 'epoch': 0.55}
{'loss': 0.9623, 'grad_norm': 0.023230114951729774, 'learning_rate': 8.992316137722711e-06, 'epoch': 0.55}
{'loss': 1.0776, 'grad_norm': 0.02172764763236046, 'learning_rate': 8.986117203091042e-06, 'epoch': 0.55}
{'loss': 1.0342, 'grad_norm': 0.023111671209335327, 'learning_rate': 8.97991866208358e-06, 'epoch': 0.55}
{'loss': 1.0818, 'grad_norm': 0.024750353768467903, 'learning_rate': 8.973720517106814e-06, 'epoch': 0.55}
{'loss': 0.9586, 'grad_norm': 0.02454129233956337, 'learning_rate': 8.967522770567086e-06, 'epoch': 0.55}
{'loss': 1.0791, 'grad_norm': 0.024975966662168503, 'learning_rate': 8.961325424870561e-06, 'epoch': 0.55}
{'loss': 1.0452, 'grad_norm': 0.022801704704761505, 'learning_rate': 8.955128482423271e-06, 'epoch': 0.55}
{'loss': 1.0603, 'grad_norm': 0.022883128374814987, 'learning_rate': 8.948931945631082e-06, 'epoch': 0.55}
{'loss': 1.0331, 'grad_norm': 0.0246925987303257, 'learning_rate': 8.9427358168997e-06, 'epoch': 0.55}
{'loss': 1.0046, 'grad_norm': 0.022696981206536293, 'learning_rate': 8.936540098634675e-06, 'epoch': 0.55}
{'loss': 1.0629, 'grad_norm': 0.025756359100341797, 'learning_rate': 8.930344793241404e-06, 'epoch': 0.55}
{'loss': 1.0014, 'grad_norm': 0.025045987218618393, 'learning_rate': 8.924149903125108e-06, 'epoch': 0.55}
{'loss': 0.9886, 'grad_norm': 0.022835748270154, 'learning_rate': 8.917955430690865e-06, 'epoch': 0.55}
{'loss': 0.9771, 'grad_norm': 0.021838480606675148, 'learning_rate': 8.91176137834358e-06, 'epoch': 0.55}
{'loss': 1.0172, 'grad_norm': 0.025632940232753754, 'learning_rate': 8.905567748487997e-06, 'epoch': 0.55}
{'loss': 0.991, 'grad_norm': 0.00999679509550333, 'learning_rate': 8.899374543528695e-06, 'epoch': 0.55}
{'loss': 1.0007, 'grad_norm': 0.023362040519714355, 'learning_rate': 8.893181765870094e-06, 'epoch': 0.55}
{'loss': 1.0333, 'grad_norm': 0.025167733430862427, 'learning_rate': 8.886989417916435e-06, 'epoch': 0.55}
{'loss': 0.9944, 'grad_norm': 0.02423168160021305, 'learning_rate': 8.88079750207181e-06, 'epoch': 0.55}
{'loss': 1.0212, 'grad_norm': 0.022015031427145004, 'learning_rate': 8.87460602074013e-06, 'epoch': 0.55}
{'loss': 0.9227, 'grad_norm': 0.02350975200533867, 'learning_rate': 8.86841497632514e-06, 'epoch': 0.55}
{'loss': 0.9437, 'grad_norm': 0.022093361243605614, 'learning_rate': 8.862224371230418e-06, 'epoch': 0.55}
{'loss': 1.0843, 'grad_norm': 0.023643450811505318, 'learning_rate': 8.85603420785937e-06, 'epoch': 0.55}
{'loss': 0.9362, 'grad_norm': 0.022711465135216713, 'learning_rate': 8.84984448861523e-06, 'epoch': 0.55}
{'loss': 0.9167, 'grad_norm': 0.021155115216970444, 'learning_rate': 8.84365521590106e-06, 'epoch': 0.55}
{'loss': 0.9815, 'grad_norm': 0.023507801815867424, 'learning_rate': 8.837466392119752e-06, 'epoch': 0.55}
{'loss': 0.9863, 'grad_norm': 0.023411346599459648, 'learning_rate': 8.831278019674017e-06, 'epoch': 0.55}
{'loss': 0.9901, 'grad_norm': 0.024480368942022324, 'learning_rate': 8.825090100966396e-06, 'epoch': 0.55}
{'loss': 1.0624, 'grad_norm': 0.02509312331676483, 'learning_rate': 8.818902638399247e-06, 'epoch': 0.55}
{'loss': 0.9699, 'grad_norm': 0.025825390592217445, 'learning_rate': 8.81271563437476e-06, 'epoch': 0.55}
{'loss': 1.0367, 'grad_norm': 0.022666405886411667, 'learning_rate': 8.806529091294948e-06, 'epoch': 0.55}
{'loss': 0.9931, 'grad_norm': 0.022426003590226173, 'learning_rate': 8.800343011561633e-06, 'epoch': 0.55}
{'loss': 1.0205, 'grad_norm': 0.008951744996011257, 'learning_rate': 8.794157397576464e-06, 'epoch': 0.55}
{'loss': 0.9638, 'grad_norm': 0.025047099217772484, 'learning_rate': 8.787972251740916e-06, 'epoch': 0.55}
{'loss': 1.0507, 'grad_norm': 0.024312157183885574, 'learning_rate': 8.781787576456269e-06, 'epoch': 0.55}
{'loss': 1.0701, 'grad_norm': 0.0246810894459486, 'learning_rate': 8.775603374123627e-06, 'epoch': 0.55}
{'loss': 0.9998, 'grad_norm': 0.0223472211509943, 'learning_rate': 8.769419647143917e-06, 'epoch': 0.55}
{'loss': 1.0784, 'grad_norm': 0.02416880428791046, 'learning_rate': 8.763236397917865e-06, 'epoch': 0.55}
{'loss': 0.971, 'grad_norm': 0.023727893829345703, 'learning_rate': 8.757053628846028e-06, 'epoch': 0.55}
{'loss': 0.9862, 'grad_norm': 0.024326905608177185, 'learning_rate': 8.75087134232877e-06, 'epoch': 0.55}
{'loss': 1.0186, 'grad_norm': 0.02396443672478199, 'learning_rate': 8.744689540766265e-06, 'epoch': 0.55}
{'loss': 1.0085, 'grad_norm': 0.022810187190771103, 'learning_rate': 8.738508226558499e-06, 'epoch': 0.55}
{'loss': 1.1034, 'grad_norm': 0.025646645575761795, 'learning_rate': 8.73232740210528e-06, 'epoch': 0.55}
{'loss': 1.1096, 'grad_norm': 0.023194069042801857, 'learning_rate': 8.726147069806206e-06, 'epoch': 0.55}
{'loss': 1.0131, 'grad_norm': 0.02355324849486351, 'learning_rate': 8.719967232060698e-06, 'epoch': 0.55}
{'loss': 1.0832, 'grad_norm': 0.02548227459192276, 'learning_rate': 8.713787891267988e-06, 'epoch': 0.56}
{'loss': 0.9751, 'grad_norm': 0.0232391394674778, 'learning_rate': 8.707609049827102e-06, 'epoch': 0.56}
{'loss': 1.0305, 'grad_norm': 0.028156260028481483, 'learning_rate': 8.70143071013688e-06, 'epoch': 0.56}
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA/wandb/offline-run-20250909_072932-inzqjgzp[0m
[1;34mwandb[0m: Find logs at: [1;35m../../../../../datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA/wandb/offline-run-20250909_072932-inzqjgzp/logs[0m

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>

g012:37627:38134 [0] misc/socket.cc:50 NCCL WARN socketProgress: Connection closed by remote peer g011.ib.cluster<33592>
