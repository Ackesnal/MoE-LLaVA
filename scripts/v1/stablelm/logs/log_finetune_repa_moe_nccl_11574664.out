Loading cuda/12.4.0
  Unloading conflict: cuda/12.8.1
VERSION: 1.6 (NCCL-only)
MASTER_PORT: 41541
MASTER_ADDR: g001
[2025-09-09 22:50:11,698] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-09 22:50:11,698] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-09 22:50:11,697] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-09 22:50:11,698] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-09 22:50:11,698] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-09 22:50:11,698] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-09 22:50:11,698] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-09 22:50:11,698] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-09 22:50:11,698] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-09 22:50:11,697] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-09 22:50:11,698] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-09 22:50:11,698] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-09 22:50:11,699] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-09 22:50:11,699] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-09 22:50:11,698] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-09 22:50:11,698] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.

Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.

Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.

Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.

Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.


[2025-09-09 22:50:34,253] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-09 22:50:34,253] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-09 22:50:34,253] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-09 22:50:34,253] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-09 22:50:34,252] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-09 22:50:34,252] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-09 22:50:34,252] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-09 22:50:34,252] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-09 22:50:34,253] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-09 22:50:34,253] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-09 22:50:34,252] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-09 22:50:34,252] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-09 22:50:34,253] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-09 22:50:34,253] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-09 22:50:34,253] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-09 22:50:34,253] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode

⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode



⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode

⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode

[2025-09-09 22:50:39,300] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 22:50:39,298] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 22:50:39,298] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 22:50:39,299] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 22:50:39,300] [INFO] [comm.py:852:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-09-09 22:50:39,300] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 22:50:39,300] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 22:50:39,299] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 22:50:39,300] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 22:50:39,300] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 22:50:39,300] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 22:50:39,300] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 22:50:39,300] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 22:50:39,299] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 22:50:39,299] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 22:50:39,300] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 22:50:39,300] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-09 22:50:39,661] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-09 22:50:39,663] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-09 22:50:39,665] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-09 22:50:39,666] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-09 22:50:39,669] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-09 22:50:39,670] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-09 22:50:39,672] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-09 22:50:39,674] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-09 22:50:39,675] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-09 22:50:39,677] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-09 22:50:39,679] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-09 22:50:39,680] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
Vision encoder and proj init.

Vision encoder and proj init.

Vision encoder and proj init.

Vision encoder and proj init.

Vision encoder and proj init.

Vision encoder and proj init.

Vision encoder and proj init.

Vision encoder and proj init.

RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Formatting inputs...Skip in lazy mode
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Setting up RePaMoE fine-tuning mode...
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Formatting inputs...Skip in lazy mode
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
Setting up RePaMoE fine-tuning mode...
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable

  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Total training steps: 5197.0
  Set initial gated ratio to 1.0  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps

  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1  Total training steps: 5197.0

  Stage 1 (gated ratio reduction): 2598.0 steps  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]

  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps  Set initial gated ratio to 1.0
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1

  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]  Total training steps: 5197.0

  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1  Set initial gated ratio to 1.0
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]

  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
No existing process group found, creating a new group named: ep_size_1
NCCL version 2.21.5+cuda12.4
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
No existing process group found, creating a new group named: ep_size_1
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
{'loss': 0.9712, 'grad_norm': 0.4138867259025574, 'learning_rate': 0.0, 'epoch': 0.0}
{'loss': 0.9337, 'grad_norm': 0.3456588685512543, 'learning_rate': 1.282051282051282e-07, 'epoch': 0.0}
{'loss': 1.0066, 'grad_norm': 0.40560510754585266, 'learning_rate': 2.564102564102564e-07, 'epoch': 0.0}
{'loss': 1.0517, 'grad_norm': 0.4010677635669708, 'learning_rate': 3.846153846153847e-07, 'epoch': 0.0}
{'loss': 0.9402, 'grad_norm': 0.3649410903453827, 'learning_rate': 5.128205128205128e-07, 'epoch': 0.0}
{'loss': 0.9073, 'grad_norm': 0.3823659420013428, 'learning_rate': 6.41025641025641e-07, 'epoch': 0.0}
{'loss': 1.0395, 'grad_norm': 0.39922621846199036, 'learning_rate': 7.692307692307694e-07, 'epoch': 0.0}
{'loss': 0.9659, 'grad_norm': 0.36740508675575256, 'learning_rate': 8.974358974358975e-07, 'epoch': 0.0}
{'loss': 1.0481, 'grad_norm': 0.36311250925064087, 'learning_rate': 1.0256410256410257e-06, 'epoch': 0.0}
{'loss': 0.9694, 'grad_norm': 0.40027305483818054, 'learning_rate': 1.153846153846154e-06, 'epoch': 0.0}
{'loss': 0.9505, 'grad_norm': 0.37375494837760925, 'learning_rate': 1.282051282051282e-06, 'epoch': 0.0}
{'loss': 0.9844, 'grad_norm': 0.4055088758468628, 'learning_rate': 1.4102564102564104e-06, 'epoch': 0.0}
{'loss': 0.8952, 'grad_norm': 0.32085785269737244, 'learning_rate': 1.5384615384615387e-06, 'epoch': 0.0}
{'loss': 0.9559, 'grad_norm': 0.40677765011787415, 'learning_rate': 1.6666666666666667e-06, 'epoch': 0.0}
{'loss': 0.9847, 'grad_norm': 0.35468292236328125, 'learning_rate': 1.794871794871795e-06, 'epoch': 0.0}
{'loss': 0.9944, 'grad_norm': 0.35894298553466797, 'learning_rate': 1.9230769230769234e-06, 'epoch': 0.0}
{'loss': 0.9356, 'grad_norm': 0.3799281716346741, 'learning_rate': 2.0512820512820513e-06, 'epoch': 0.0}
{'loss': 1.0166, 'grad_norm': 0.3887177109718323, 'learning_rate': 2.1794871794871797e-06, 'epoch': 0.0}
{'loss': 1.0396, 'grad_norm': 0.4015144109725952, 'learning_rate': 2.307692307692308e-06, 'epoch': 0.0}
{'loss': 1.0073, 'grad_norm': 0.3666123151779175, 'learning_rate': 2.435897435897436e-06, 'epoch': 0.0}
{'loss': 1.0201, 'grad_norm': 0.371426522731781, 'learning_rate': 2.564102564102564e-06, 'epoch': 0.0}
{'loss': 1.0253, 'grad_norm': 0.3449142277240753, 'learning_rate': 2.6923076923076923e-06, 'epoch': 0.0}
{'loss': 0.9302, 'grad_norm': 0.40018466114997864, 'learning_rate': 2.8205128205128207e-06, 'epoch': 0.0}
{'loss': 0.9812, 'grad_norm': 0.33025985956192017, 'learning_rate': 2.948717948717949e-06, 'epoch': 0.0}
{'loss': 0.9914, 'grad_norm': 0.41647079586982727, 'learning_rate': 3.0769230769230774e-06, 'epoch': 0.0}
{'loss': 1.0842, 'grad_norm': 0.4030408561229706, 'learning_rate': 3.205128205128206e-06, 'epoch': 0.01}
{'loss': 1.0814, 'grad_norm': 0.3583931028842926, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.01}
{'loss': 0.8407, 'grad_norm': 0.3311455547809601, 'learning_rate': 3.4615384615384617e-06, 'epoch': 0.01}
{'loss': 1.0068, 'grad_norm': 0.38666048645973206, 'learning_rate': 3.58974358974359e-06, 'epoch': 0.01}
{'loss': 1.0593, 'grad_norm': 0.3928186595439911, 'learning_rate': 3.7179487179487184e-06, 'epoch': 0.01}
{'loss': 0.9398, 'grad_norm': 0.373381644487381, 'learning_rate': 3.846153846153847e-06, 'epoch': 0.01}
{'loss': 0.9708, 'grad_norm': 0.3900740146636963, 'learning_rate': 3.974358974358974e-06, 'epoch': 0.01}
{'loss': 0.9524, 'grad_norm': 0.30989059805870056, 'learning_rate': 4.102564102564103e-06, 'epoch': 0.01}
{'loss': 1.0285, 'grad_norm': 0.3903157114982605, 'learning_rate': 4.230769230769231e-06, 'epoch': 0.01}
{'loss': 0.8715, 'grad_norm': 0.2549889087677002, 'learning_rate': 4.358974358974359e-06, 'epoch': 0.01}
{'loss': 0.8764, 'grad_norm': 0.3720085024833679, 'learning_rate': 4.487179487179488e-06, 'epoch': 0.01}
{'loss': 0.9808, 'grad_norm': 0.3828924000263214, 'learning_rate': 4.615384615384616e-06, 'epoch': 0.01}
{'loss': 1.0087, 'grad_norm': 0.4155600070953369, 'learning_rate': 4.743589743589744e-06, 'epoch': 0.01}
{'loss': 0.968, 'grad_norm': 0.32647043466567993, 'learning_rate': 4.871794871794872e-06, 'epoch': 0.01}
{'loss': 1.018, 'grad_norm': 0.3986247777938843, 'learning_rate': 5e-06, 'epoch': 0.01}
{'loss': 1.0626, 'grad_norm': 0.379441499710083, 'learning_rate': 5.128205128205128e-06, 'epoch': 0.01}
{'loss': 0.8797, 'grad_norm': 0.3793867826461792, 'learning_rate': 5.256410256410257e-06, 'epoch': 0.01}
{'loss': 0.9311, 'grad_norm': 0.3932839334011078, 'learning_rate': 5.384615384615385e-06, 'epoch': 0.01}
{'loss': 1.0195, 'grad_norm': 0.3354824185371399, 'learning_rate': 5.512820512820514e-06, 'epoch': 0.01}
{'loss': 0.9906, 'grad_norm': 0.40864884853363037, 'learning_rate': 5.641025641025641e-06, 'epoch': 0.01}
{'loss': 0.9777, 'grad_norm': 0.3782693147659302, 'learning_rate': 5.769230769230769e-06, 'epoch': 0.01}
{'loss': 1.0472, 'grad_norm': 0.41980457305908203, 'learning_rate': 5.897435897435898e-06, 'epoch': 0.01}
{'loss': 0.9696, 'grad_norm': 0.37409481406211853, 'learning_rate': 6.025641025641026e-06, 'epoch': 0.01}
{'loss': 0.9967, 'grad_norm': 0.41489648818969727, 'learning_rate': 6.153846153846155e-06, 'epoch': 0.01}
{'loss': 0.9415, 'grad_norm': 0.431462824344635, 'learning_rate': 6.282051282051282e-06, 'epoch': 0.01}
{'loss': 1.0379, 'grad_norm': 0.42776116728782654, 'learning_rate': 6.410256410256412e-06, 'epoch': 0.01}
{'loss': 1.0375, 'grad_norm': 0.4683612883090973, 'learning_rate': 6.538461538461539e-06, 'epoch': 0.01}
{'loss': 0.9897, 'grad_norm': 0.4145956039428711, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.01}
{'loss': 0.9608, 'grad_norm': 0.47071510553359985, 'learning_rate': 6.794871794871796e-06, 'epoch': 0.01}
{'loss': 1.0052, 'grad_norm': 0.39313608407974243, 'learning_rate': 6.923076923076923e-06, 'epoch': 0.01}
{'loss': 0.9791, 'grad_norm': 0.4153454303741455, 'learning_rate': 7.051282051282053e-06, 'epoch': 0.01}
{'loss': 1.1539, 'grad_norm': 0.4555756747722626, 'learning_rate': 7.17948717948718e-06, 'epoch': 0.01}
{'loss': 0.9982, 'grad_norm': 0.40173837542533875, 'learning_rate': 7.307692307692308e-06, 'epoch': 0.01}
{'loss': 1.007, 'grad_norm': 0.4240051209926605, 'learning_rate': 7.435897435897437e-06, 'epoch': 0.01}
{'loss': 0.9874, 'grad_norm': 0.4717938005924225, 'learning_rate': 7.564102564102564e-06, 'epoch': 0.01}
{'loss': 0.9804, 'grad_norm': 0.27040138840675354, 'learning_rate': 7.692307692307694e-06, 'epoch': 0.01}
{'loss': 0.9928, 'grad_norm': 0.3807523250579834, 'learning_rate': 7.820512820512822e-06, 'epoch': 0.01}
{'loss': 1.0432, 'grad_norm': 0.4279411733150482, 'learning_rate': 7.948717948717949e-06, 'epoch': 0.01}
{'loss': 0.9663, 'grad_norm': 0.3883122503757477, 'learning_rate': 8.076923076923077e-06, 'epoch': 0.01}
{'loss': 1.128, 'grad_norm': 0.4489095211029053, 'learning_rate': 8.205128205128205e-06, 'epoch': 0.01}
{'loss': 0.9486, 'grad_norm': 0.4112251400947571, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.01}
{'loss': 1.0111, 'grad_norm': 0.3918168544769287, 'learning_rate': 8.461538461538462e-06, 'epoch': 0.01}
{'loss': 1.0019, 'grad_norm': 0.44790565967559814, 'learning_rate': 8.58974358974359e-06, 'epoch': 0.01}
{'loss': 1.0387, 'grad_norm': 0.423890620470047, 'learning_rate': 8.717948717948719e-06, 'epoch': 0.01}
{'loss': 0.9501, 'grad_norm': 0.45136502385139465, 'learning_rate': 8.846153846153847e-06, 'epoch': 0.01}
{'loss': 1.0248, 'grad_norm': 0.4192107915878296, 'learning_rate': 8.974358974358976e-06, 'epoch': 0.01}
{'loss': 1.0535, 'grad_norm': 0.4379969537258148, 'learning_rate': 9.102564102564104e-06, 'epoch': 0.01}
{'loss': 1.0125, 'grad_norm': 0.4034101665019989, 'learning_rate': 9.230769230769232e-06, 'epoch': 0.01}
{'loss': 1.0161, 'grad_norm': 0.4171901047229767, 'learning_rate': 9.358974358974359e-06, 'epoch': 0.01}
{'loss': 0.9706, 'grad_norm': 0.4234028458595276, 'learning_rate': 9.487179487179487e-06, 'epoch': 0.01}
{'loss': 0.9496, 'grad_norm': 0.40727007389068604, 'learning_rate': 9.615384615384616e-06, 'epoch': 0.01}
{'loss': 0.9882, 'grad_norm': 0.43656593561172485, 'learning_rate': 9.743589743589744e-06, 'epoch': 0.01}
{'loss': 0.9475, 'grad_norm': 0.34595903754234314, 'learning_rate': 9.871794871794872e-06, 'epoch': 0.02}
{'loss': 0.9955, 'grad_norm': 0.42990541458129883, 'learning_rate': 1e-05, 'epoch': 0.02}
{'loss': 1.0066, 'grad_norm': 0.414687842130661, 'learning_rate': 1.012820512820513e-05, 'epoch': 0.02}
{'loss': 1.027, 'grad_norm': 0.42926865816116333, 'learning_rate': 1.0256410256410256e-05, 'epoch': 0.02}
{'loss': 0.9185, 'grad_norm': 0.34468701481819153, 'learning_rate': 1.0384615384615386e-05, 'epoch': 0.02}
{'loss': 0.946, 'grad_norm': 0.4231927692890167, 'learning_rate': 1.0512820512820514e-05, 'epoch': 0.02}
{'loss': 1.0507, 'grad_norm': 0.41214221715927124, 'learning_rate': 1.0641025641025643e-05, 'epoch': 0.02}
{'loss': 0.9946, 'grad_norm': 0.4057004153728485, 'learning_rate': 1.076923076923077e-05, 'epoch': 0.02}
{'loss': 0.9402, 'grad_norm': 0.3996480107307434, 'learning_rate': 1.0897435897435898e-05, 'epoch': 0.02}
{'loss': 0.9164, 'grad_norm': 0.39581388235092163, 'learning_rate': 1.1025641025641028e-05, 'epoch': 0.02}
{'loss': 1.0872, 'grad_norm': 0.4082619249820709, 'learning_rate': 1.1153846153846154e-05, 'epoch': 0.02}
{'loss': 1.0001, 'grad_norm': 0.4489831328392029, 'learning_rate': 1.1282051282051283e-05, 'epoch': 0.02}
{'loss': 0.9711, 'grad_norm': 0.38584309816360474, 'learning_rate': 1.1410256410256411e-05, 'epoch': 0.02}
{'loss': 1.0445, 'grad_norm': 0.4516128897666931, 'learning_rate': 1.1538461538461538e-05, 'epoch': 0.02}
{'loss': 1.1032, 'grad_norm': 0.45564571022987366, 'learning_rate': 1.1666666666666668e-05, 'epoch': 0.02}
{'loss': 0.9629, 'grad_norm': 0.4123271107673645, 'learning_rate': 1.1794871794871796e-05, 'epoch': 0.02}
{'loss': 0.9901, 'grad_norm': 0.38369765877723694, 'learning_rate': 1.1923076923076925e-05, 'epoch': 0.02}
{'loss': 1.0084, 'grad_norm': 0.37967318296432495, 'learning_rate': 1.2051282051282051e-05, 'epoch': 0.02}
{'loss': 1.0093, 'grad_norm': 0.390813410282135, 'learning_rate': 1.217948717948718e-05, 'epoch': 0.02}
{'loss': 0.9956, 'grad_norm': 0.3597714900970459, 'learning_rate': 1.230769230769231e-05, 'epoch': 0.02}
{'loss': 0.9911, 'grad_norm': 0.424048513174057, 'learning_rate': 1.2435897435897436e-05, 'epoch': 0.02}
{'loss': 0.9087, 'grad_norm': 0.4307820796966553, 'learning_rate': 1.2564102564102565e-05, 'epoch': 0.02}
{'loss': 0.9045, 'grad_norm': 0.4093697667121887, 'learning_rate': 1.2692307692307693e-05, 'epoch': 0.02}
{'loss': 1.0255, 'grad_norm': 0.39674457907676697, 'learning_rate': 1.2820512820512823e-05, 'epoch': 0.02}
{'loss': 0.984, 'grad_norm': 0.3382745683193207, 'learning_rate': 1.294871794871795e-05, 'epoch': 0.02}
{'loss': 1.0391, 'grad_norm': 0.4340570867061615, 'learning_rate': 1.3076923076923078e-05, 'epoch': 0.02}
{'loss': 0.9357, 'grad_norm': 0.4622649550437927, 'learning_rate': 1.3205128205128207e-05, 'epoch': 0.02}
{'loss': 1.0499, 'grad_norm': 0.4098980724811554, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.02}
{'loss': 1.0044, 'grad_norm': 0.4159148037433624, 'learning_rate': 1.3461538461538463e-05, 'epoch': 0.02}
{'loss': 1.0279, 'grad_norm': 0.3819412589073181, 'learning_rate': 1.3589743589743592e-05, 'epoch': 0.02}
{'loss': 1.0618, 'grad_norm': 0.4324944019317627, 'learning_rate': 1.3717948717948718e-05, 'epoch': 0.02}
{'loss': 0.9822, 'grad_norm': 0.4167919456958771, 'learning_rate': 1.3846153846153847e-05, 'epoch': 0.02}
{'loss': 0.9711, 'grad_norm': 0.4547096788883209, 'learning_rate': 1.3974358974358975e-05, 'epoch': 0.02}
{'loss': 1.0224, 'grad_norm': 0.44184204936027527, 'learning_rate': 1.4102564102564105e-05, 'epoch': 0.02}
{'loss': 0.9918, 'grad_norm': 0.43735092878341675, 'learning_rate': 1.4230769230769232e-05, 'epoch': 0.02}
{'loss': 0.9756, 'grad_norm': 0.4430139362812042, 'learning_rate': 1.435897435897436e-05, 'epoch': 0.02}
{'loss': 0.9367, 'grad_norm': 0.41719675064086914, 'learning_rate': 1.4487179487179489e-05, 'epoch': 0.02}
{'loss': 0.9326, 'grad_norm': 0.4641667306423187, 'learning_rate': 1.4615384615384615e-05, 'epoch': 0.02}
{'loss': 0.9477, 'grad_norm': 0.3309265971183777, 'learning_rate': 1.4743589743589745e-05, 'epoch': 0.02}
{'loss': 0.9978, 'grad_norm': 0.42003488540649414, 'learning_rate': 1.4871794871794874e-05, 'epoch': 0.02}
{'loss': 0.9855, 'grad_norm': 0.4155133366584778, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.02}
{'loss': 0.9011, 'grad_norm': 0.40599480271339417, 'learning_rate': 1.5128205128205129e-05, 'epoch': 0.02}
{'loss': 1.0168, 'grad_norm': 0.39911094307899475, 'learning_rate': 1.5256410256410257e-05, 'epoch': 0.02}
{'loss': 0.9301, 'grad_norm': 0.3740064799785614, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.02}
{'loss': 0.986, 'grad_norm': 0.3989793360233307, 'learning_rate': 1.5512820512820516e-05, 'epoch': 0.02}
{'loss': 1.08, 'grad_norm': 0.40814831852912903, 'learning_rate': 1.5641025641025644e-05, 'epoch': 0.02}
{'loss': 0.9595, 'grad_norm': 0.40058279037475586, 'learning_rate': 1.576923076923077e-05, 'epoch': 0.02}
{'loss': 1.0052, 'grad_norm': 0.43372422456741333, 'learning_rate': 1.5897435897435897e-05, 'epoch': 0.02}
{'loss': 0.9825, 'grad_norm': 0.4594890773296356, 'learning_rate': 1.602564102564103e-05, 'epoch': 0.02}
{'loss': 1.0132, 'grad_norm': 0.4023539125919342, 'learning_rate': 1.6153846153846154e-05, 'epoch': 0.02}
{'loss': 1.0744, 'grad_norm': 0.45699232816696167, 'learning_rate': 1.6282051282051282e-05, 'epoch': 0.02}
{'loss': 1.047, 'grad_norm': 0.4436572194099426, 'learning_rate': 1.641025641025641e-05, 'epoch': 0.02}
{'loss': 1.0119, 'grad_norm': 0.396832138299942, 'learning_rate': 1.653846153846154e-05, 'epoch': 0.03}
{'loss': 0.9269, 'grad_norm': 0.4361824691295624, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.03}
Error with image file is truncated (73 bytes not processed)
{'loss': 1.0738, 'grad_norm': 0.45961064100265503, 'learning_rate': 1.6794871794871796e-05, 'epoch': 0.03}
{'loss': 0.9836, 'grad_norm': 0.41595950722694397, 'learning_rate': 1.6923076923076924e-05, 'epoch': 0.03}
{'loss': 0.9817, 'grad_norm': 0.3301039934158325, 'learning_rate': 1.7051282051282053e-05, 'epoch': 0.03}
{'loss': 1.0745, 'grad_norm': 0.49519670009613037, 'learning_rate': 1.717948717948718e-05, 'epoch': 0.03}
{'loss': 0.9359, 'grad_norm': 0.41592806577682495, 'learning_rate': 1.730769230769231e-05, 'epoch': 0.03}
{'loss': 1.0532, 'grad_norm': 0.4320661425590515, 'learning_rate': 1.7435897435897438e-05, 'epoch': 0.03}
{'loss': 1.0949, 'grad_norm': 0.4250956177711487, 'learning_rate': 1.7564102564102566e-05, 'epoch': 0.03}
{'loss': 0.9591, 'grad_norm': 0.4131200909614563, 'learning_rate': 1.7692307692307694e-05, 'epoch': 0.03}
{'loss': 0.9072, 'grad_norm': 0.3867308497428894, 'learning_rate': 1.7820512820512823e-05, 'epoch': 0.03}
{'loss': 0.9379, 'grad_norm': 0.4047662615776062, 'learning_rate': 1.794871794871795e-05, 'epoch': 0.03}
{'loss': 1.0089, 'grad_norm': 0.41471701860427856, 'learning_rate': 1.807692307692308e-05, 'epoch': 0.03}
{'loss': 0.9961, 'grad_norm': 0.4125381112098694, 'learning_rate': 1.8205128205128208e-05, 'epoch': 0.03}
{'loss': 0.9916, 'grad_norm': 0.3922182619571686, 'learning_rate': 1.8333333333333333e-05, 'epoch': 0.03}
{'loss': 0.9796, 'grad_norm': 0.4242713749408722, 'learning_rate': 1.8461538461538465e-05, 'epoch': 0.03}
{'loss': 0.9328, 'grad_norm': 0.4287799894809723, 'learning_rate': 1.8589743589743593e-05, 'epoch': 0.03}
{'loss': 0.9801, 'grad_norm': 0.48904991149902344, 'learning_rate': 1.8717948717948718e-05, 'epoch': 0.03}
{'loss': 1.0878, 'grad_norm': 0.44745180010795593, 'learning_rate': 1.8846153846153846e-05, 'epoch': 0.03}
{'loss': 0.8968, 'grad_norm': 0.44215938448905945, 'learning_rate': 1.8974358974358975e-05, 'epoch': 0.03}
{'loss': 0.955, 'grad_norm': 0.39681559801101685, 'learning_rate': 1.9102564102564106e-05, 'epoch': 0.03}
{'loss': 1.0247, 'grad_norm': 0.4239698350429535, 'learning_rate': 1.923076923076923e-05, 'epoch': 0.03}
{'loss': 0.9824, 'grad_norm': 0.34454643726348877, 'learning_rate': 1.935897435897436e-05, 'epoch': 0.03}
{'loss': 0.9854, 'grad_norm': 0.40781983733177185, 'learning_rate': 1.9487179487179488e-05, 'epoch': 0.03}
{'loss': 1.0684, 'grad_norm': 0.45115160942077637, 'learning_rate': 1.9615384615384617e-05, 'epoch': 0.03}
{'loss': 0.9745, 'grad_norm': 0.4158075153827667, 'learning_rate': 1.9743589743589745e-05, 'epoch': 0.03}
{'loss': 1.0552, 'grad_norm': 0.39681732654571533, 'learning_rate': 1.9871794871794873e-05, 'epoch': 0.03}
{'loss': 0.9893, 'grad_norm': 0.4112309217453003, 'learning_rate': 2e-05, 'epoch': 0.03}
{'loss': 0.9882, 'grad_norm': 0.4505821466445923, 'learning_rate': 1.9999998058827844e-05, 'epoch': 0.03}
{'loss': 0.9799, 'grad_norm': 0.3814024031162262, 'learning_rate': 1.9999992235312136e-05, 'epoch': 0.03}
{'loss': 0.9121, 'grad_norm': 0.41333889961242676, 'learning_rate': 1.9999982529455127e-05, 'epoch': 0.03}
{'loss': 0.9631, 'grad_norm': 0.4215144217014313, 'learning_rate': 1.9999968941260596e-05, 'epoch': 0.03}
{'loss': 0.9949, 'grad_norm': 0.3904784917831421, 'learning_rate': 1.9999951470733808e-05, 'epoch': 0.03}
{'loss': 1.0762, 'grad_norm': 0.44176700711250305, 'learning_rate': 1.9999930117881548e-05, 'epoch': 0.03}
{'loss': 1.0133, 'grad_norm': 0.4689161777496338, 'learning_rate': 1.9999904882712115e-05, 'epoch': 0.03}
{'loss': 1.0607, 'grad_norm': 0.4163384735584259, 'learning_rate': 1.99998757652353e-05, 'epoch': 0.03}
{'loss': 0.9912, 'grad_norm': 0.40913382172584534, 'learning_rate': 1.9999842765462403e-05, 'epoch': 0.03}
{'loss': 0.9857, 'grad_norm': 0.38446569442749023, 'learning_rate': 1.999980588340624e-05, 'epoch': 0.03}
{'loss': 1.0702, 'grad_norm': 0.48356613516807556, 'learning_rate': 1.9999765119081132e-05, 'epoch': 0.03}
{'loss': 1.1238, 'grad_norm': 0.43143483996391296, 'learning_rate': 1.9999720472502902e-05, 'epoch': 0.03}
{'loss': 0.9735, 'grad_norm': 0.33428969979286194, 'learning_rate': 1.9999671943688885e-05, 'epoch': 0.03}
{'loss': 1.0273, 'grad_norm': 0.42817890644073486, 'learning_rate': 1.9999619532657915e-05, 'epoch': 0.03}
{'loss': 0.9514, 'grad_norm': 0.3897239565849304, 'learning_rate': 1.9999563239430352e-05, 'epoch': 0.03}
{'loss': 1.0807, 'grad_norm': 0.4243973195552826, 'learning_rate': 1.9999503064028043e-05, 'epoch': 0.03}
{'loss': 0.9439, 'grad_norm': 0.40577632188796997, 'learning_rate': 1.999943900647435e-05, 'epoch': 0.03}
{'loss': 0.9838, 'grad_norm': 0.4357401728630066, 'learning_rate': 1.9999371066794146e-05, 'epoch': 0.03}
{'loss': 0.926, 'grad_norm': 0.3663526475429535, 'learning_rate': 1.9999299245013805e-05, 'epoch': 0.03}
{'loss': 1.0123, 'grad_norm': 0.4100399613380432, 'learning_rate': 1.999922354116121e-05, 'epoch': 0.03}
{'loss': 1.0732, 'grad_norm': 0.39212751388549805, 'learning_rate': 1.999914395526575e-05, 'epoch': 0.03}
{'loss': 0.9907, 'grad_norm': 0.4228969216346741, 'learning_rate': 1.9999060487358333e-05, 'epoch': 0.03}
{'loss': 1.015, 'grad_norm': 0.4266895651817322, 'learning_rate': 1.9998973137471352e-05, 'epoch': 0.03}
{'loss': 0.9928, 'grad_norm': 0.4390372037887573, 'learning_rate': 1.9998881905638727e-05, 'epoch': 0.03}
{'loss': 1.0071, 'grad_norm': 0.4096473753452301, 'learning_rate': 1.9998786791895874e-05, 'epoch': 0.04}
{'loss': 0.9364, 'grad_norm': 0.40405040979385376, 'learning_rate': 1.999868779627972e-05, 'epoch': 0.04}
{'loss': 0.861, 'grad_norm': 0.4336627423763275, 'learning_rate': 1.9998584918828695e-05, 'epoch': 0.04}
{'loss': 0.9458, 'grad_norm': 0.43435317277908325, 'learning_rate': 1.9998478159582747e-05, 'epoch': 0.04}
{'loss': 1.0292, 'grad_norm': 0.42996495962142944, 'learning_rate': 1.999836751858332e-05, 'epoch': 0.04}
{'loss': 0.9592, 'grad_norm': 0.41994860768318176, 'learning_rate': 1.9998252995873367e-05, 'epoch': 0.04}
{'loss': 1.075, 'grad_norm': 0.4255489408969879, 'learning_rate': 1.999813459149735e-05, 'epoch': 0.04}
{'loss': 1.0336, 'grad_norm': 0.4473262429237366, 'learning_rate': 1.9998012305501243e-05, 'epoch': 0.04}
{'loss': 1.001, 'grad_norm': 0.4448668360710144, 'learning_rate': 1.999788613793251e-05, 'epoch': 0.04}
{'loss': 1.0552, 'grad_norm': 0.46083158254623413, 'learning_rate': 1.999775608884015e-05, 'epoch': 0.04}
Error with image file is truncated (8 bytes not processed)
{'loss': 0.9279, 'grad_norm': 0.4643978476524353, 'learning_rate': 1.9997622158274635e-05, 'epoch': 0.04}
{'loss': 1.0244, 'grad_norm': 0.4344116747379303, 'learning_rate': 1.9997484346287973e-05, 'epoch': 0.04}
{'loss': 0.988, 'grad_norm': 0.43295878171920776, 'learning_rate': 1.9997342652933668e-05, 'epoch': 0.04}
{'loss': 1.0703, 'grad_norm': 0.44445720314979553, 'learning_rate': 1.9997197078266723e-05, 'epoch': 0.04}
{'loss': 1.0167, 'grad_norm': 0.4113955497741699, 'learning_rate': 1.999704762234366e-05, 'epoch': 0.04}
{'loss': 0.9721, 'grad_norm': 0.4456787705421448, 'learning_rate': 1.99968942852225e-05, 'epoch': 0.04}
{'loss': 1.0337, 'grad_norm': 0.3938581645488739, 'learning_rate': 1.9996737066962778e-05, 'epoch': 0.04}
{'loss': 1.0668, 'grad_norm': 0.4434516131877899, 'learning_rate': 1.9996575967625525e-05, 'epoch': 0.04}
{'loss': 0.9974, 'grad_norm': 0.40424707531929016, 'learning_rate': 1.999641098727329e-05, 'epoch': 0.04}
{'loss': 1.0303, 'grad_norm': 0.44701912999153137, 'learning_rate': 1.999624212597013e-05, 'epoch': 0.04}
{'loss': 1.0335, 'grad_norm': 0.4472526013851166, 'learning_rate': 1.9996069383781587e-05, 'epoch': 0.04}
{'loss': 0.8886, 'grad_norm': 0.32939159870147705, 'learning_rate': 1.9995892760774738e-05, 'epoch': 0.04}
{'loss': 0.8957, 'grad_norm': 0.43146759271621704, 'learning_rate': 1.9995712257018153e-05, 'epoch': 0.04}
{'loss': 0.9696, 'grad_norm': 0.4159817695617676, 'learning_rate': 1.9995527872581903e-05, 'epoch': 0.04}
{'loss': 1.0141, 'grad_norm': 0.42163515090942383, 'learning_rate': 1.9995339607537578e-05, 'epoch': 0.04}
{'loss': 0.968, 'grad_norm': 0.4278615713119507, 'learning_rate': 1.9995147461958267e-05, 'epoch': 0.04}
{'loss': 1.0394, 'grad_norm': 0.4546298384666443, 'learning_rate': 1.999495143591857e-05, 'epoch': 0.04}
{'loss': 1.0691, 'grad_norm': 0.4601869285106659, 'learning_rate': 1.999475152949459e-05, 'epoch': 0.04}
{'loss': 0.9917, 'grad_norm': 0.45483627915382385, 'learning_rate': 1.9994547742763935e-05, 'epoch': 0.04}
{'loss': 0.9819, 'grad_norm': 0.35928475856781006, 'learning_rate': 1.9994340075805724e-05, 'epoch': 0.04}
{'loss': 0.9896, 'grad_norm': 0.4524659812450409, 'learning_rate': 1.9994128528700583e-05, 'epoch': 0.04}
{'loss': 0.9717, 'grad_norm': 0.40897202491760254, 'learning_rate': 1.9993913101530635e-05, 'epoch': 0.04}
{'loss': 1.096, 'grad_norm': 0.4458538293838501, 'learning_rate': 1.9993693794379525e-05, 'epoch': 0.04}
{'loss': 0.9616, 'grad_norm': 0.44772017002105713, 'learning_rate': 1.9993470607332387e-05, 'epoch': 0.04}
{'loss': 0.9747, 'grad_norm': 0.33889177441596985, 'learning_rate': 1.999324354047588e-05, 'epoch': 0.04}
{'loss': 0.9953, 'grad_norm': 0.4368370473384857, 'learning_rate': 1.9993012593898146e-05, 'epoch': 0.04}
{'loss': 0.9913, 'grad_norm': 0.4156085252761841, 'learning_rate': 1.9992777767688857e-05, 'epoch': 0.04}
{'loss': 0.9818, 'grad_norm': 0.4149833023548126, 'learning_rate': 1.9992539061939175e-05, 'epoch': 0.04}
{'loss': 1.0327, 'grad_norm': 0.4560394883155823, 'learning_rate': 1.999229647674178e-05, 'epoch': 0.04}
{'loss': 1.0004, 'grad_norm': 0.43300604820251465, 'learning_rate': 1.9992050012190845e-05, 'epoch': 0.04}
{'loss': 0.9891, 'grad_norm': 0.3930588960647583, 'learning_rate': 1.9991799668382058e-05, 'epoch': 0.04}
{'loss': 1.0192, 'grad_norm': 0.34658023715019226, 'learning_rate': 1.9991545445412614e-05, 'epoch': 0.04}
{'loss': 0.9724, 'grad_norm': 0.40619564056396484, 'learning_rate': 1.9991287343381208e-05, 'epoch': 0.04}
{'loss': 1.0666, 'grad_norm': 0.43091142177581787, 'learning_rate': 1.9991025362388044e-05, 'epoch': 0.04}
{'loss': 0.9561, 'grad_norm': 0.3287302255630493, 'learning_rate': 1.9990759502534835e-05, 'epoch': 0.04}
{'loss': 0.9508, 'grad_norm': 0.31925857067108154, 'learning_rate': 1.9990489763924796e-05, 'epoch': 0.04}
{'loss': 0.971, 'grad_norm': 0.4271974563598633, 'learning_rate': 1.9990216146662648e-05, 'epoch': 0.04}
{'loss': 1.024, 'grad_norm': 0.4379115104675293, 'learning_rate': 1.9989938650854618e-05, 'epoch': 0.04}
{'loss': 1.0302, 'grad_norm': 0.4335858225822449, 'learning_rate': 1.998965727660844e-05, 'epoch': 0.04}
{'loss': 0.9177, 'grad_norm': 0.32326269149780273, 'learning_rate': 1.9989372024033352e-05, 'epoch': 0.04}
{'loss': 0.9482, 'grad_norm': 0.4166170656681061, 'learning_rate': 1.99890828932401e-05, 'epoch': 0.04}
{'loss': 0.9593, 'grad_norm': 0.32567358016967773, 'learning_rate': 1.9988789884340938e-05, 'epoch': 0.04}
{'loss': 1.0146, 'grad_norm': 0.44945454597473145, 'learning_rate': 1.9988492997449615e-05, 'epoch': 0.05}
{'loss': 0.9363, 'grad_norm': 0.35099270939826965, 'learning_rate': 1.9988192232681398e-05, 'epoch': 0.05}
{'loss': 1.0172, 'grad_norm': 0.4395070970058441, 'learning_rate': 1.9987887590153055e-05, 'epoch': 0.05}
{'loss': 1.0768, 'grad_norm': 0.4782659411430359, 'learning_rate': 1.9987579069982856e-05, 'epoch': 0.05}
{'loss': 0.9815, 'grad_norm': 0.3398461639881134, 'learning_rate': 1.9987266672290577e-05, 'epoch': 0.05}
{'loss': 0.9275, 'grad_norm': 0.44793611764907837, 'learning_rate': 1.9986950397197503e-05, 'epoch': 0.05}
{'loss': 1.0699, 'grad_norm': 0.48598816990852356, 'learning_rate': 1.9986630244826425e-05, 'epoch': 0.05}
{'loss': 1.03, 'grad_norm': 0.4221866726875305, 'learning_rate': 1.998630621530164e-05, 'epoch': 0.05}
{'loss': 0.973, 'grad_norm': 0.47608911991119385, 'learning_rate': 1.998597830874894e-05, 'epoch': 0.05}
{'loss': 0.9533, 'grad_norm': 0.3948451280593872, 'learning_rate': 1.9985646525295634e-05, 'epoch': 0.05}
{'loss': 0.9778, 'grad_norm': 0.3772403299808502, 'learning_rate': 1.998531086507053e-05, 'epoch': 0.05}
{'loss': 0.954, 'grad_norm': 0.4433673322200775, 'learning_rate': 1.9984971328203945e-05, 'epoch': 0.05}
{'loss': 0.982, 'grad_norm': 0.39661937952041626, 'learning_rate': 1.9984627914827698e-05, 'epoch': 0.05}
Error with image file is truncated (91 bytes not processed)
{'loss': 0.8959, 'grad_norm': 0.4283427298069, 'learning_rate': 1.9984280625075115e-05, 'epoch': 0.05}
{'loss': 1.0388, 'grad_norm': 0.40783292055130005, 'learning_rate': 1.9983929459081022e-05, 'epoch': 0.05}
{'loss': 1.0528, 'grad_norm': 0.4410024583339691, 'learning_rate': 1.998357441698176e-05, 'epoch': 0.05}
{'loss': 1.0391, 'grad_norm': 0.4486311972141266, 'learning_rate': 1.998321549891516e-05, 'epoch': 0.05}
{'loss': 0.9672, 'grad_norm': 0.39827868342399597, 'learning_rate': 1.9982852705020572e-05, 'epoch': 0.05}
{'loss': 0.9533, 'grad_norm': 0.436223566532135, 'learning_rate': 1.9982486035438848e-05, 'epoch': 0.05}
{'loss': 1.0437, 'grad_norm': 0.4283832311630249, 'learning_rate': 1.9982115490312334e-05, 'epoch': 0.05}
{'loss': 1.016, 'grad_norm': 0.4145027697086334, 'learning_rate': 1.9981741069784894e-05, 'epoch': 0.05}
{'loss': 0.8993, 'grad_norm': 0.41637882590293884, 'learning_rate': 1.9981362774001886e-05, 'epoch': 0.05}
{'loss': 1.1043, 'grad_norm': 0.48389187455177307, 'learning_rate': 1.9980980603110185e-05, 'epoch': 0.05}
{'loss': 0.9393, 'grad_norm': 0.418157696723938, 'learning_rate': 1.9980594557258158e-05, 'epoch': 0.05}
{'loss': 0.9268, 'grad_norm': 0.4093269407749176, 'learning_rate': 1.9980204636595682e-05, 'epoch': 0.05}
{'loss': 1.0128, 'grad_norm': 0.41037988662719727, 'learning_rate': 1.9979810841274135e-05, 'epoch': 0.05}
{'loss': 0.9273, 'grad_norm': 0.4042990803718567, 'learning_rate': 1.9979413171446403e-05, 'epoch': 0.05}
{'loss': 1.0005, 'grad_norm': 0.42518550157546997, 'learning_rate': 1.9979011627266884e-05, 'epoch': 0.05}
{'loss': 0.9575, 'grad_norm': 0.41975897550582886, 'learning_rate': 1.997860620889146e-05, 'epoch': 0.05}
{'loss': 0.9554, 'grad_norm': 0.40599432587623596, 'learning_rate': 1.997819691647753e-05, 'epoch': 0.05}
{'loss': 0.9507, 'grad_norm': 0.33215659856796265, 'learning_rate': 1.9977783750184e-05, 'epoch': 0.05}
{'loss': 0.9641, 'grad_norm': 0.41209426522254944, 'learning_rate': 1.9977366710171274e-05, 'epoch': 0.05}
{'loss': 0.9412, 'grad_norm': 0.4174997806549072, 'learning_rate': 1.9976945796601258e-05, 'epoch': 0.05}
{'loss': 0.9855, 'grad_norm': 0.4438456594944, 'learning_rate': 1.9976521009637366e-05, 'epoch': 0.05}
{'loss': 0.9942, 'grad_norm': 0.43763530254364014, 'learning_rate': 1.997609234944452e-05, 'epoch': 0.05}
{'loss': 1.0339, 'grad_norm': 0.4036882519721985, 'learning_rate': 1.9975659816189137e-05, 'epoch': 0.05}
{'loss': 1.0581, 'grad_norm': 0.4696939289569855, 'learning_rate': 1.997522341003914e-05, 'epoch': 0.05}
{'loss': 1.0088, 'grad_norm': 0.39908185601234436, 'learning_rate': 1.9974783131163957e-05, 'epoch': 0.05}
{'loss': 1.035, 'grad_norm': 0.42869120836257935, 'learning_rate': 1.9974338979734523e-05, 'epoch': 0.05}
{'loss': 1.0531, 'grad_norm': 0.4290163218975067, 'learning_rate': 1.997389095592327e-05, 'epoch': 0.05}
{'loss': 0.9586, 'grad_norm': 0.44356703758239746, 'learning_rate': 1.9973439059904133e-05, 'epoch': 0.05}
{'loss': 1.0419, 'grad_norm': 0.4368678331375122, 'learning_rate': 1.9972983291852565e-05, 'epoch': 0.05}
{'loss': 0.9956, 'grad_norm': 0.4176175892353058, 'learning_rate': 1.9972523651945496e-05, 'epoch': 0.05}
{'loss': 1.0428, 'grad_norm': 0.47594353556632996, 'learning_rate': 1.9972060140361384e-05, 'epoch': 0.05}
{'loss': 0.9246, 'grad_norm': 0.377602756023407, 'learning_rate': 1.997159275728018e-05, 'epoch': 0.05}
{'loss': 0.9268, 'grad_norm': 0.332958847284317, 'learning_rate': 1.9971121502883332e-05, 'epoch': 0.05}
{'loss': 0.9531, 'grad_norm': 0.4167294204235077, 'learning_rate': 1.9970646377353802e-05, 'epoch': 0.05}
{'loss': 0.985, 'grad_norm': 0.4083266258239746, 'learning_rate': 1.997016738087605e-05, 'epoch': 0.05}
{'loss': 0.9555, 'grad_norm': 0.3876943588256836, 'learning_rate': 1.9969684513636035e-05, 'epoch': 0.05}
{'loss': 0.9951, 'grad_norm': 0.4346882402896881, 'learning_rate': 1.9969197775821227e-05, 'epoch': 0.05}
{'loss': 0.8886, 'grad_norm': 0.40937888622283936, 'learning_rate': 1.9968707167620593e-05, 'epoch': 0.05}
{'loss': 1.0018, 'grad_norm': 0.4054921567440033, 'learning_rate': 1.9968212689224603e-05, 'epoch': 0.05}
{'loss': 1.0823, 'grad_norm': 0.4154779016971588, 'learning_rate': 1.996771434082523e-05, 'epoch': 0.06}
{'loss': 0.9242, 'grad_norm': 0.342824250459671, 'learning_rate': 1.9967212122615958e-05, 'epoch': 0.06}
{'loss': 1.0732, 'grad_norm': 0.40123066306114197, 'learning_rate': 1.9966706034791752e-05, 'epoch': 0.06}
{'loss': 1.044, 'grad_norm': 0.44219937920570374, 'learning_rate': 1.9966196077549106e-05, 'epoch': 0.06}
{'loss': 1.0164, 'grad_norm': 0.48752331733703613, 'learning_rate': 1.996568225108599e-05, 'epoch': 0.06}
{'loss': 1.027, 'grad_norm': 0.4338366985321045, 'learning_rate': 1.99651645556019e-05, 'epoch': 0.06}
{'loss': 0.8988, 'grad_norm': 0.40742483735084534, 'learning_rate': 1.9964642991297817e-05, 'epoch': 0.06}
{'loss': 0.9703, 'grad_norm': 0.409915953874588, 'learning_rate': 1.996411755837623e-05, 'epoch': 0.06}
{'loss': 0.8985, 'grad_norm': 0.42094460129737854, 'learning_rate': 1.9963588257041137e-05, 'epoch': 0.06}
{'loss': 0.9947, 'grad_norm': 0.46793821454048157, 'learning_rate': 1.996305508749802e-05, 'epoch': 0.06}
{'loss': 1.0123, 'grad_norm': 0.4564460813999176, 'learning_rate': 1.9962518049953887e-05, 'epoch': 0.06}
{'loss': 1.1131, 'grad_norm': 0.465914785861969, 'learning_rate': 1.9961977144617225e-05, 'epoch': 0.06}
{'loss': 0.9915, 'grad_norm': 0.43228036165237427, 'learning_rate': 1.996143237169803e-05, 'epoch': 0.06}
{'loss': 1.035, 'grad_norm': 0.4538554549217224, 'learning_rate': 1.996088373140781e-05, 'epoch': 0.06}
{'loss': 1.0206, 'grad_norm': 0.3529127538204193, 'learning_rate': 1.9960331223959564e-05, 'epoch': 0.06}
{'loss': 1.0057, 'grad_norm': 0.4287051260471344, 'learning_rate': 1.995977484956779e-05, 'epoch': 0.06}
{'loss': 1.0725, 'grad_norm': 0.5115604996681213, 'learning_rate': 1.9959214608448495e-05, 'epoch': 0.06}
{'loss': 0.9735, 'grad_norm': 0.4126458764076233, 'learning_rate': 1.9958650500819183e-05, 'epoch': 0.06}
{'loss': 0.9669, 'grad_norm': 0.41323739290237427, 'learning_rate': 1.995808252689886e-05, 'epoch': 0.06}
{'loss': 0.976, 'grad_norm': 0.4246198236942291, 'learning_rate': 1.9957510686908034e-05, 'epoch': 0.06}
{'loss': 1.0125, 'grad_norm': 0.42881524562835693, 'learning_rate': 1.9956934981068713e-05, 'epoch': 0.06}
{'loss': 1.0613, 'grad_norm': 0.42850300669670105, 'learning_rate': 1.9956355409604402e-05, 'epoch': 0.06}
{'loss': 1.0729, 'grad_norm': 0.41759294271469116, 'learning_rate': 1.9955771972740118e-05, 'epoch': 0.06}
{'loss': 1.1593, 'grad_norm': 0.4803999066352844, 'learning_rate': 1.9955184670702363e-05, 'epoch': 0.06}
{'loss': 0.944, 'grad_norm': 0.38406336307525635, 'learning_rate': 1.995459350371915e-05, 'epoch': 0.06}
{'loss': 1.0364, 'grad_norm': 0.4462870955467224, 'learning_rate': 1.9953998472019996e-05, 'epoch': 0.06}
{'loss': 1.0546, 'grad_norm': 0.45056813955307007, 'learning_rate': 1.995339957583591e-05, 'epoch': 0.06}
{'loss': 0.9669, 'grad_norm': 0.3575119078159332, 'learning_rate': 1.9952796815399403e-05, 'epoch': 0.06}
{'loss': 0.9955, 'grad_norm': 0.35214143991470337, 'learning_rate': 1.9952190190944484e-05, 'epoch': 0.06}
{'loss': 0.9468, 'grad_norm': 0.4319358468055725, 'learning_rate': 1.9951579702706668e-05, 'epoch': 0.06}
{'loss': 1.0041, 'grad_norm': 0.3905731439590454, 'learning_rate': 1.9950965350922975e-05, 'epoch': 0.06}
{'loss': 0.9812, 'grad_norm': 0.39710408449172974, 'learning_rate': 1.9950347135831907e-05, 'epoch': 0.06}
{'loss': 1.0134, 'grad_norm': 0.4255269765853882, 'learning_rate': 1.994972505767348e-05, 'epoch': 0.06}
{'loss': 0.9632, 'grad_norm': 0.4002867639064789, 'learning_rate': 1.994909911668921e-05, 'epoch': 0.06}
{'loss': 0.9526, 'grad_norm': 0.3939320743083954, 'learning_rate': 1.99484693131221e-05, 'epoch': 0.06}
{'loss': 1.0029, 'grad_norm': 0.4205511510372162, 'learning_rate': 1.994783564721667e-05, 'epoch': 0.06}
{'loss': 1.0106, 'grad_norm': 0.39890703558921814, 'learning_rate': 1.9947198119218924e-05, 'epoch': 0.06}
{'loss': 0.9765, 'grad_norm': 0.38996708393096924, 'learning_rate': 1.994655672937638e-05, 'epoch': 0.06}
{'loss': 0.984, 'grad_norm': 0.478188157081604, 'learning_rate': 1.9945911477938044e-05, 'epoch': 0.06}
{'loss': 0.9626, 'grad_norm': 0.32674455642700195, 'learning_rate': 1.994526236515442e-05, 'epoch': 0.06}
{'loss': 1.0094, 'grad_norm': 0.39311322569847107, 'learning_rate': 1.994460939127753e-05, 'epoch': 0.06}
{'loss': 1.02, 'grad_norm': 0.36823177337646484, 'learning_rate': 1.9943952556560863e-05, 'epoch': 0.06}
{'loss': 1.0537, 'grad_norm': 0.45057058334350586, 'learning_rate': 1.9943291861259433e-05, 'epoch': 0.06}
{'loss': 1.0569, 'grad_norm': 0.4093908965587616, 'learning_rate': 1.9942627305629747e-05, 'epoch': 0.06}
{'loss': 0.9928, 'grad_norm': 0.3962520360946655, 'learning_rate': 1.9941958889929808e-05, 'epoch': 0.06}
{'loss': 1.0544, 'grad_norm': 0.42454251646995544, 'learning_rate': 1.9941286614419113e-05, 'epoch': 0.06}
{'loss': 0.9256, 'grad_norm': 0.3948007822036743, 'learning_rate': 1.994061047935867e-05, 'epoch': 0.06}
{'loss': 0.945, 'grad_norm': 0.4423644244670868, 'learning_rate': 1.9939930485010968e-05, 'epoch': 0.06}
{'loss': 1.1056, 'grad_norm': 0.4322754740715027, 'learning_rate': 1.9939246631640014e-05, 'epoch': 0.06}
{'loss': 0.9452, 'grad_norm': 0.3937719464302063, 'learning_rate': 1.99385589195113e-05, 'epoch': 0.06}
{'loss': 1.0332, 'grad_norm': 0.4355728328227997, 'learning_rate': 1.9937867348891815e-05, 'epoch': 0.06}
{'loss': 0.9219, 'grad_norm': 0.3999403417110443, 'learning_rate': 1.9937171920050057e-05, 'epoch': 0.06}
{'loss': 1.0509, 'grad_norm': 0.45943978428840637, 'learning_rate': 1.9936472633256012e-05, 'epoch': 0.07}
{'loss': 1.0498, 'grad_norm': 0.4228825271129608, 'learning_rate': 1.9935769488781167e-05, 'epoch': 0.07}
{'loss': 1.0767, 'grad_norm': 0.4599224925041199, 'learning_rate': 1.993506248689851e-05, 'epoch': 0.07}
{'loss': 0.98, 'grad_norm': 0.4088402986526489, 'learning_rate': 1.993435162788252e-05, 'epoch': 0.07}
{'loss': 1.0358, 'grad_norm': 0.4645938277244568, 'learning_rate': 1.993363691200918e-05, 'epoch': 0.07}
{'loss': 0.9356, 'grad_norm': 0.42708152532577515, 'learning_rate': 1.9932918339555965e-05, 'epoch': 0.07}
{'loss': 1.0202, 'grad_norm': 0.4743850529193878, 'learning_rate': 1.9932195910801848e-05, 'epoch': 0.07}
{'loss': 0.9775, 'grad_norm': 0.4060567319393158, 'learning_rate': 1.9931469626027305e-05, 'epoch': 0.07}
Error with image file is truncated (70 bytes not processed)
{'loss': 0.9939, 'grad_norm': 0.39745357632637024, 'learning_rate': 1.9930739485514304e-05, 'epoch': 0.07}
{'loss': 0.9469, 'grad_norm': 0.37014731764793396, 'learning_rate': 1.9930005489546308e-05, 'epoch': 0.07}
{'loss': 0.9715, 'grad_norm': 0.4515446126461029, 'learning_rate': 1.9929267638408277e-05, 'epoch': 0.07}
{'loss': 1.0122, 'grad_norm': 0.3994362950325012, 'learning_rate': 1.9928525932386678e-05, 'epoch': 0.07}
{'loss': 0.9928, 'grad_norm': 0.3264266848564148, 'learning_rate': 1.9927780371769463e-05, 'epoch': 0.07}
{'loss': 1.0632, 'grad_norm': 0.41188716888427734, 'learning_rate': 1.9927030956846083e-05, 'epoch': 0.07}
{'loss': 0.9927, 'grad_norm': 0.38337042927742004, 'learning_rate': 1.992627768790749e-05, 'epoch': 0.07}
{'loss': 1.0225, 'grad_norm': 0.410377562046051, 'learning_rate': 1.9925520565246125e-05, 'epoch': 0.07}
{'loss': 0.9761, 'grad_norm': 0.34673500061035156, 'learning_rate': 1.9924759589155932e-05, 'epoch': 0.07}
{'loss': 1.0403, 'grad_norm': 0.38585618138313293, 'learning_rate': 1.9923994759932344e-05, 'epoch': 0.07}
{'loss': 0.9636, 'grad_norm': 0.35733696818351746, 'learning_rate': 1.9923226077872296e-05, 'epoch': 0.07}
{'loss': 1.1288, 'grad_norm': 0.4562470018863678, 'learning_rate': 1.9922453543274223e-05, 'epoch': 0.07}
{'loss': 1.0252, 'grad_norm': 0.43070170283317566, 'learning_rate': 1.9921677156438044e-05, 'epoch': 0.07}
{'loss': 1.0139, 'grad_norm': 0.4143390357494354, 'learning_rate': 1.9920896917665178e-05, 'epoch': 0.07}
{'loss': 0.9298, 'grad_norm': 0.36462637782096863, 'learning_rate': 1.992011282725854e-05, 'epoch': 0.07}
WARNING: tokenization mismatch: 0 vs. 517. (ignored)
{'loss': 1.0103, 'grad_norm': 0.42654842138290405, 'learning_rate': 1.9919324885522548e-05, 'epoch': 0.07}
{'loss': 1.0181, 'grad_norm': 0.4350443482398987, 'learning_rate': 1.99185330927631e-05, 'epoch': 0.07}
{'loss': 0.9931, 'grad_norm': 0.3884412348270416, 'learning_rate': 1.99177374492876e-05, 'epoch': 0.07}
{'loss': 1.0121, 'grad_norm': 0.43747857213020325, 'learning_rate': 1.991693795540494e-05, 'epoch': 0.07}
{'loss': 1.0972, 'grad_norm': 0.3973068296909332, 'learning_rate': 1.9916134611425522e-05, 'epoch': 0.07}
{'loss': 1.0146, 'grad_norm': 0.45340514183044434, 'learning_rate': 1.9915327417661226e-05, 'epoch': 0.07}
{'loss': 1.092, 'grad_norm': 0.48508256673812866, 'learning_rate': 1.991451637442543e-05, 'epoch': 0.07}
{'loss': 0.9518, 'grad_norm': 0.4262319803237915, 'learning_rate': 1.9913701482033008e-05, 'epoch': 0.07}
{'loss': 1.0416, 'grad_norm': 0.42157474160194397, 'learning_rate': 1.9912882740800336e-05, 'epoch': 0.07}
{'loss': 0.9475, 'grad_norm': 0.34324464201927185, 'learning_rate': 1.9912060151045273e-05, 'epoch': 0.07}
{'loss': 0.9995, 'grad_norm': 0.4240153431892395, 'learning_rate': 1.9911233713087172e-05, 'epoch': 0.07}
{'loss': 1.1055, 'grad_norm': 0.4459368884563446, 'learning_rate': 1.9910403427246895e-05, 'epoch': 0.07}
{'loss': 1.0366, 'grad_norm': 0.40221309661865234, 'learning_rate': 1.990956929384678e-05, 'epoch': 0.07}
{'loss': 0.9973, 'grad_norm': 0.37898385524749756, 'learning_rate': 1.990873131321067e-05, 'epoch': 0.07}
{'loss': 0.9499, 'grad_norm': 0.4164743423461914, 'learning_rate': 1.9907889485663897e-05, 'epoch': 0.07}
{'loss': 0.9863, 'grad_norm': 0.40366968512535095, 'learning_rate': 1.9907043811533283e-05, 'epoch': 0.07}
{'loss': 1.0335, 'grad_norm': 0.4035782217979431, 'learning_rate': 1.9906194291147155e-05, 'epoch': 0.07}
{'loss': 0.9824, 'grad_norm': 0.41678765416145325, 'learning_rate': 1.9905340924835322e-05, 'epoch': 0.07}
{'loss': 1.0073, 'grad_norm': 0.48359546065330505, 'learning_rate': 1.9904483712929094e-05, 'epoch': 0.07}
{'loss': 0.9262, 'grad_norm': 0.3432007431983948, 'learning_rate': 1.9903622655761267e-05, 'epoch': 0.07}
{'loss': 1.024, 'grad_norm': 0.3912665843963623, 'learning_rate': 1.990275775366613e-05, 'epoch': 0.07}
{'loss': 1.0061, 'grad_norm': 0.44271382689476013, 'learning_rate': 1.9901889006979473e-05, 'epoch': 0.07}
{'loss': 1.0034, 'grad_norm': 0.41432657837867737, 'learning_rate': 1.990101641603857e-05, 'epoch': 0.07}
{'loss': 1.0654, 'grad_norm': 0.4310539662837982, 'learning_rate': 1.9900139981182193e-05, 'epoch': 0.07}
{'loss': 0.9946, 'grad_norm': 0.3934272527694702, 'learning_rate': 1.9899259702750604e-05, 'epoch': 0.07}
{'loss': 0.9116, 'grad_norm': 0.4225198030471802, 'learning_rate': 1.9898375581085555e-05, 'epoch': 0.07}
{'loss': 0.9597, 'grad_norm': 0.4363647699356079, 'learning_rate': 1.9897487616530296e-05, 'epoch': 0.07}
{'loss': 1.0025, 'grad_norm': 0.4712255895137787, 'learning_rate': 1.9896595809429565e-05, 'epoch': 0.07}
{'loss': 0.9637, 'grad_norm': 0.42078697681427, 'learning_rate': 1.9895700160129593e-05, 'epoch': 0.07}
{'loss': 1.0037, 'grad_norm': 0.4033239781856537, 'learning_rate': 1.9894800668978095e-05, 'epoch': 0.08}
{'loss': 1.0771, 'grad_norm': 0.45382118225097656, 'learning_rate': 1.9893897336324292e-05, 'epoch': 0.08}
{'loss': 1.0739, 'grad_norm': 0.4425191879272461, 'learning_rate': 1.9892990162518884e-05, 'epoch': 0.08}
{'loss': 1.0132, 'grad_norm': 0.4171886146068573, 'learning_rate': 1.9892079147914072e-05, 'epoch': 0.08}
{'loss': 0.9722, 'grad_norm': 0.40086761116981506, 'learning_rate': 1.9891164292863537e-05, 'epoch': 0.08}
{'loss': 0.9333, 'grad_norm': 0.33438971638679504, 'learning_rate': 1.9890245597722465e-05, 'epoch': 0.08}
{'loss': 1.0008, 'grad_norm': 0.4061201214790344, 'learning_rate': 1.9889323062847516e-05, 'epoch': 0.08}
{'loss': 0.9587, 'grad_norm': 0.4304163455963135, 'learning_rate': 1.988839668859686e-05, 'epoch': 0.08}
{'loss': 0.9177, 'grad_norm': 0.4093363583087921, 'learning_rate': 1.988746647533014e-05, 'epoch': 0.08}
{'loss': 0.9999, 'grad_norm': 0.4017554819583893, 'learning_rate': 1.9886532423408495e-05, 'epoch': 0.08}
{'loss': 1.0568, 'grad_norm': 0.4107896685600281, 'learning_rate': 1.9885594533194564e-05, 'epoch': 0.08}
{'loss': 0.9947, 'grad_norm': 0.39808350801467896, 'learning_rate': 1.9884652805052465e-05, 'epoch': 0.08}
{'loss': 0.9723, 'grad_norm': 0.3934711217880249, 'learning_rate': 1.9883707239347804e-05, 'epoch': 0.08}
{'loss': 0.9817, 'grad_norm': 0.4334370493888855, 'learning_rate': 1.988275783644769e-05, 'epoch': 0.08}
{'loss': 1.0478, 'grad_norm': 0.425913006067276, 'learning_rate': 1.988180459672071e-05, 'epoch': 0.08}
{'loss': 0.9523, 'grad_norm': 0.4110577404499054, 'learning_rate': 1.988084752053695e-05, 'epoch': 0.08}
{'loss': 1.0555, 'grad_norm': 0.4637351334095001, 'learning_rate': 1.9879886608267967e-05, 'epoch': 0.08}
{'loss': 1.0013, 'grad_norm': 0.42723968625068665, 'learning_rate': 1.9878921860286832e-05, 'epoch': 0.08}
{'loss': 0.9832, 'grad_norm': 0.4105646312236786, 'learning_rate': 1.9877953276968088e-05, 'epoch': 0.08}
{'loss': 0.9973, 'grad_norm': 0.462025910615921, 'learning_rate': 1.9876980858687777e-05, 'epoch': 0.08}
{'loss': 0.9357, 'grad_norm': 0.47517770528793335, 'learning_rate': 1.9876004605823417e-05, 'epoch': 0.08}
{'loss': 1.0342, 'grad_norm': 0.4102346897125244, 'learning_rate': 1.987502451875403e-05, 'epoch': 0.08}
{'loss': 1.0575, 'grad_norm': 0.47238001227378845, 'learning_rate': 1.987404059786012e-05, 'epoch': 0.08}
{'loss': 0.9797, 'grad_norm': 0.38989681005477905, 'learning_rate': 1.9873052843523676e-05, 'epoch': 0.08}
{'loss': 1.0691, 'grad_norm': 0.4567149877548218, 'learning_rate': 1.987206125612818e-05, 'epoch': 0.08}
{'loss': 1.0078, 'grad_norm': 0.4186174273490906, 'learning_rate': 1.98710658360586e-05, 'epoch': 0.08}
{'loss': 0.9791, 'grad_norm': 0.41257402300834656, 'learning_rate': 1.987006658370139e-05, 'epoch': 0.08}
{'loss': 0.9891, 'grad_norm': 0.3666328191757202, 'learning_rate': 1.9869063499444495e-05, 'epoch': 0.08}
{'loss': 0.9872, 'grad_norm': 0.38382184505462646, 'learning_rate': 1.9868056583677346e-05, 'epoch': 0.08}
{'loss': 0.9409, 'grad_norm': 0.4053336977958679, 'learning_rate': 1.9867045836790867e-05, 'epoch': 0.08}
{'loss': 0.9843, 'grad_norm': 0.33993127942085266, 'learning_rate': 1.9866031259177463e-05, 'epoch': 0.08}
{'loss': 0.989, 'grad_norm': 0.4419591724872589, 'learning_rate': 1.9865012851231022e-05, 'epoch': 0.08}
{'loss': 1.0529, 'grad_norm': 0.4798136055469513, 'learning_rate': 1.9863990613346936e-05, 'epoch': 0.08}
{'loss': 1.0904, 'grad_norm': 0.40940576791763306, 'learning_rate': 1.986296454592206e-05, 'epoch': 0.08}
{'loss': 1.0003, 'grad_norm': 0.44869738817214966, 'learning_rate': 1.9861934649354763e-05, 'epoch': 0.08}
{'loss': 1.0562, 'grad_norm': 0.43239814043045044, 'learning_rate': 1.9860900924044873e-05, 'epoch': 0.08}
{'loss': 0.9058, 'grad_norm': 0.3610895574092865, 'learning_rate': 1.9859863370393726e-05, 'epoch': 0.08}
{'loss': 0.9529, 'grad_norm': 0.4274458587169647, 'learning_rate': 1.9858821988804132e-05, 'epoch': 0.08}
{'loss': 1.0215, 'grad_norm': 0.4386266767978668, 'learning_rate': 1.9857776779680393e-05, 'epoch': 0.08}
{'loss': 1.0284, 'grad_norm': 0.4142237603664398, 'learning_rate': 1.98567277434283e-05, 'epoch': 0.08}
{'loss': 1.1113, 'grad_norm': 0.466928631067276, 'learning_rate': 1.9855674880455115e-05, 'epoch': 0.08}
{'loss': 1.0042, 'grad_norm': 0.4203948378562927, 'learning_rate': 1.98546181911696e-05, 'epoch': 0.08}
{'loss': 1.0143, 'grad_norm': 0.40361759066581726, 'learning_rate': 1.9853557675982e-05, 'epoch': 0.08}
{'loss': 1.0586, 'grad_norm': 0.3941107392311096, 'learning_rate': 1.985249333530404e-05, 'epoch': 0.08}
{'loss': 1.0382, 'grad_norm': 0.44304612278938293, 'learning_rate': 1.9851425169548938e-05, 'epoch': 0.08}
{'loss': 1.0027, 'grad_norm': 0.39199310541152954, 'learning_rate': 1.9850353179131392e-05, 'epoch': 0.08}
{'loss': 1.0853, 'grad_norm': 0.37487557530403137, 'learning_rate': 1.9849277364467585e-05, 'epoch': 0.08}
{'loss': 0.9529, 'grad_norm': 0.4098741114139557, 'learning_rate': 1.984819772597518e-05, 'epoch': 0.08}
{'loss': 1.0118, 'grad_norm': 0.43340548872947693, 'learning_rate': 1.9847114264073336e-05, 'epoch': 0.08}
{'loss': 1.0683, 'grad_norm': 0.3920518755912781, 'learning_rate': 1.984602697918269e-05, 'epoch': 0.08}
{'loss': 0.9795, 'grad_norm': 0.4154609143733978, 'learning_rate': 1.9844935871725363e-05, 'epoch': 0.08}
{'loss': 1.0443, 'grad_norm': 0.41843366622924805, 'learning_rate': 1.9843840942124956e-05, 'epoch': 0.08}
{'loss': 1.0041, 'grad_norm': 0.41399410367012024, 'learning_rate': 1.9842742190806566e-05, 'epoch': 0.09}
{'loss': 1.0142, 'grad_norm': 0.4368399679660797, 'learning_rate': 1.984163961819676e-05, 'epoch': 0.09}
{'loss': 0.9944, 'grad_norm': 0.41930434107780457, 'learning_rate': 1.9840533224723595e-05, 'epoch': 0.09}
{'loss': 1.0719, 'grad_norm': 0.4505223035812378, 'learning_rate': 1.9839423010816616e-05, 'epoch': 0.09}
{'loss': 0.928, 'grad_norm': 0.4027857780456543, 'learning_rate': 1.983830897690684e-05, 'epoch': 0.09}
{'loss': 0.9672, 'grad_norm': 0.4121624231338501, 'learning_rate': 1.9837191123426777e-05, 'epoch': 0.09}
{'loss': 1.0001, 'grad_norm': 0.41784629225730896, 'learning_rate': 1.983606945081042e-05, 'epoch': 0.09}
{'loss': 1.0454, 'grad_norm': 0.4171145260334015, 'learning_rate': 1.983494395949323e-05, 'epoch': 0.09}
{'loss': 1.1269, 'grad_norm': 0.5116902589797974, 'learning_rate': 1.983381464991217e-05, 'epoch': 0.09}
{'loss': 1.0412, 'grad_norm': 0.4774544835090637, 'learning_rate': 1.9832681522505676e-05, 'epoch': 0.09}
{'loss': 1.0181, 'grad_norm': 0.48454177379608154, 'learning_rate': 1.9831544577713663e-05, 'epoch': 0.09}
{'loss': 0.9474, 'grad_norm': 0.3963868021965027, 'learning_rate': 1.983040381597754e-05, 'epoch': 0.09}
{'loss': 0.9628, 'grad_norm': 0.3758397400379181, 'learning_rate': 1.982925923774018e-05, 'epoch': 0.09}
{'loss': 1.0159, 'grad_norm': 0.3561513125896454, 'learning_rate': 1.9828110843445954e-05, 'epoch': 0.09}
{'loss': 0.9464, 'grad_norm': 0.3193291127681732, 'learning_rate': 1.982695863354071e-05, 'epoch': 0.09}
{'loss': 1.1179, 'grad_norm': 0.4884563982486725, 'learning_rate': 1.9825802608471767e-05, 'epoch': 0.09}
{'loss': 0.9596, 'grad_norm': 0.3955442011356354, 'learning_rate': 1.982464276868794e-05, 'epoch': 0.09}
{'loss': 0.9965, 'grad_norm': 0.40302279591560364, 'learning_rate': 1.982347911463952e-05, 'epoch': 0.09}
{'loss': 1.0406, 'grad_norm': 0.37924665212631226, 'learning_rate': 1.9822311646778277e-05, 'epoch': 0.09}
{'loss': 1.0249, 'grad_norm': 0.45197269320487976, 'learning_rate': 1.982114036555746e-05, 'epoch': 0.09}
{'loss': 0.9374, 'grad_norm': 0.44149601459503174, 'learning_rate': 1.9819965271431797e-05, 'epoch': 0.09}
{'loss': 1.0696, 'grad_norm': 0.4441997706890106, 'learning_rate': 1.9818786364857506e-05, 'epoch': 0.09}
{'loss': 0.9755, 'grad_norm': 0.44670048356056213, 'learning_rate': 1.9817603646292278e-05, 'epoch': 0.09}
{'loss': 0.9, 'grad_norm': 0.4064946174621582, 'learning_rate': 1.9816417116195287e-05, 'epoch': 0.09}
{'loss': 1.0684, 'grad_norm': 0.4843685030937195, 'learning_rate': 1.9815226775027182e-05, 'epoch': 0.09}
{'loss': 0.9695, 'grad_norm': 0.4426425099372864, 'learning_rate': 1.9814032623250093e-05, 'epoch': 0.09}
{'loss': 1.1072, 'grad_norm': 0.43476906418800354, 'learning_rate': 1.9812834661327632e-05, 'epoch': 0.09}
{'loss': 0.9743, 'grad_norm': 0.39971980452537537, 'learning_rate': 1.9811632889724888e-05, 'epoch': 0.09}
{'loss': 0.9688, 'grad_norm': 0.4106648862361908, 'learning_rate': 1.9810427308908437e-05, 'epoch': 0.09}
{'loss': 0.9813, 'grad_norm': 0.4384101927280426, 'learning_rate': 1.9809217919346318e-05, 'epoch': 0.09}
{'loss': 1.0895, 'grad_norm': 0.4550025761127472, 'learning_rate': 1.980800472150806e-05, 'epoch': 0.09}
{'loss': 0.9747, 'grad_norm': 0.4811937212944031, 'learning_rate': 1.9806787715864674e-05, 'epoch': 0.09}
{'loss': 0.9666, 'grad_norm': 0.44748446345329285, 'learning_rate': 1.9805566902888637e-05, 'epoch': 0.09}
{'loss': 1.1089, 'grad_norm': 0.49835774302482605, 'learning_rate': 1.9804342283053916e-05, 'epoch': 0.09}
{'loss': 1.0577, 'grad_norm': 0.45809119939804077, 'learning_rate': 1.980311385683594e-05, 'epoch': 0.09}
{'loss': 0.9708, 'grad_norm': 0.40343376994132996, 'learning_rate': 1.980188162471164e-05, 'epoch': 0.09}
{'loss': 1.0098, 'grad_norm': 0.3531216084957123, 'learning_rate': 1.98006455871594e-05, 'epoch': 0.09}
{'loss': 0.976, 'grad_norm': 0.3012714982032776, 'learning_rate': 1.97994057446591e-05, 'epoch': 0.09}
{'loss': 0.9821, 'grad_norm': 0.4249669015407562, 'learning_rate': 1.979816209769209e-05, 'epoch': 0.09}
{'loss': 1.0146, 'grad_norm': 0.3989097774028778, 'learning_rate': 1.9796914646741187e-05, 'epoch': 0.09}
{'loss': 0.9674, 'grad_norm': 0.4281197190284729, 'learning_rate': 1.9795663392290702e-05, 'epoch': 0.09}
{'loss': 1.045, 'grad_norm': 0.4643394649028778, 'learning_rate': 1.9794408334826415e-05, 'epoch': 0.09}
{'loss': 0.9961, 'grad_norm': 0.41819754242897034, 'learning_rate': 1.979314947483558e-05, 'epoch': 0.09}
{'loss': 0.9709, 'grad_norm': 0.3931640386581421, 'learning_rate': 1.9791886812806932e-05, 'epoch': 0.09}
{'loss': 1.0602, 'grad_norm': 0.43763452768325806, 'learning_rate': 1.9790620349230676e-05, 'epoch': 0.09}
{'loss': 0.9826, 'grad_norm': 0.3878350853919983, 'learning_rate': 1.9789350084598504e-05, 'epoch': 0.09}
{'loss': 1.0949, 'grad_norm': 0.42138728499412537, 'learning_rate': 1.9788076019403565e-05, 'epoch': 0.09}
{'loss': 1.0062, 'grad_norm': 0.41660693287849426, 'learning_rate': 1.9786798154140507e-05, 'epoch': 0.09}
{'loss': 1.0239, 'grad_norm': 0.41483569145202637, 'learning_rate': 1.9785516489305437e-05, 'epoch': 0.09}
{'loss': 1.0471, 'grad_norm': 0.40063750743865967, 'learning_rate': 1.9784231025395936e-05, 'epoch': 0.09}
{'loss': 0.976, 'grad_norm': 0.41644909977912903, 'learning_rate': 1.9782941762911075e-05, 'epoch': 0.09}
{'loss': 0.9723, 'grad_norm': 0.4110104739665985, 'learning_rate': 1.9781648702351383e-05, 'epoch': 0.09}
{'loss': 1.0, 'grad_norm': 0.44248124957084656, 'learning_rate': 1.9780351844218874e-05, 'epoch': 0.1}
{'loss': 1.0221, 'grad_norm': 0.4244689643383026, 'learning_rate': 1.977905118901703e-05, 'epoch': 0.1}
{'loss': 1.0732, 'grad_norm': 0.46849435567855835, 'learning_rate': 1.977774673725081e-05, 'epoch': 0.1}
{'loss': 0.9702, 'grad_norm': 0.3975485861301422, 'learning_rate': 1.977643848942665e-05, 'epoch': 0.1}
{'loss': 1.0158, 'grad_norm': 0.45816004276275635, 'learning_rate': 1.977512644605246e-05, 'epoch': 0.1}
{'loss': 0.9812, 'grad_norm': 0.3950195014476776, 'learning_rate': 1.9773810607637612e-05, 'epoch': 0.1}
{'loss': 1.1071, 'grad_norm': 0.46932998299598694, 'learning_rate': 1.9772490974692962e-05, 'epoch': 0.1}
{'loss': 1.0113, 'grad_norm': 0.423010915517807, 'learning_rate': 1.9771167547730844e-05, 'epoch': 0.1}
{'loss': 0.9972, 'grad_norm': 0.42722153663635254, 'learning_rate': 1.976984032726505e-05, 'epoch': 0.1}
{'loss': 1.0076, 'grad_norm': 0.4449261724948883, 'learning_rate': 1.976850931381086e-05, 'epoch': 0.1}
{'loss': 1.0471, 'grad_norm': 0.43656402826309204, 'learning_rate': 1.976717450788501e-05, 'epoch': 0.1}
{'loss': 0.9587, 'grad_norm': 0.3839638829231262, 'learning_rate': 1.9765835910005726e-05, 'epoch': 0.1}
{'loss': 1.0018, 'grad_norm': 0.4455808997154236, 'learning_rate': 1.9764493520692685e-05, 'epoch': 0.1}
{'loss': 1.0011, 'grad_norm': 0.46641483902931213, 'learning_rate': 1.9763147340467067e-05, 'epoch': 0.1}
{'loss': 1.0184, 'grad_norm': 0.39597877860069275, 'learning_rate': 1.9761797369851498e-05, 'epoch': 0.1}
{'loss': 0.943, 'grad_norm': 0.42528480291366577, 'learning_rate': 1.9760443609370074e-05, 'epoch': 0.1}
{'loss': 1.0389, 'grad_norm': 0.41276341676712036, 'learning_rate': 1.975908605954838e-05, 'epoch': 0.1}
{'loss': 0.9909, 'grad_norm': 0.442059189081192, 'learning_rate': 1.9757724720913466e-05, 'epoch': 0.1}
{'loss': 1.0474, 'grad_norm': 0.45887482166290283, 'learning_rate': 1.9756359593993845e-05, 'epoch': 0.1}
{'loss': 0.9541, 'grad_norm': 0.43737033009529114, 'learning_rate': 1.975499067931951e-05, 'epoch': 0.1}
{'loss': 0.9687, 'grad_norm': 0.41740602254867554, 'learning_rate': 1.975361797742192e-05, 'epoch': 0.1}
{'loss': 1.0482, 'grad_norm': 0.4661955237388611, 'learning_rate': 1.9752241488834002e-05, 'epoch': 0.1}
{'loss': 1.113, 'grad_norm': 0.42450374364852905, 'learning_rate': 1.975086121409016e-05, 'epoch': 0.1}
{'loss': 0.9695, 'grad_norm': 0.4325371980667114, 'learning_rate': 1.974947715372626e-05, 'epoch': 0.1}
{'loss': 0.8752, 'grad_norm': 0.43585506081581116, 'learning_rate': 1.974808930827965e-05, 'epoch': 0.1}
{'loss': 0.9301, 'grad_norm': 0.3366125524044037, 'learning_rate': 1.9746697678289128e-05, 'epoch': 0.1}
{'loss': 1.0345, 'grad_norm': 0.4079311788082123, 'learning_rate': 1.9745302264294982e-05, 'epoch': 0.1}
{'loss': 0.9872, 'grad_norm': 0.4054624140262604, 'learning_rate': 1.9743903066838954e-05, 'epoch': 0.1}
{'loss': 0.9948, 'grad_norm': 0.43799370527267456, 'learning_rate': 1.9742500086464266e-05, 'epoch': 0.1}
{'loss': 0.9781, 'grad_norm': 0.3695937395095825, 'learning_rate': 1.9741093323715597e-05, 'epoch': 0.1}
{'loss': 1.0049, 'grad_norm': 0.41171708703041077, 'learning_rate': 1.9739682779139107e-05, 'epoch': 0.1}
{'loss': 0.987, 'grad_norm': 0.4069371521472931, 'learning_rate': 1.9738268453282414e-05, 'epoch': 0.1}
{'loss': 0.9875, 'grad_norm': 0.39963510632514954, 'learning_rate': 1.9736850346694608e-05, 'epoch': 0.1}
{'loss': 1.0225, 'grad_norm': 0.45219600200653076, 'learning_rate': 1.973542845992625e-05, 'epoch': 0.1}
{'loss': 1.0044, 'grad_norm': 0.4845360815525055, 'learning_rate': 1.9734002793529362e-05, 'epoch': 0.1}
{'loss': 0.9984, 'grad_norm': 0.38209661841392517, 'learning_rate': 1.9732573348057437e-05, 'epoch': 0.1}
{'loss': 1.0725, 'grad_norm': 0.4647517204284668, 'learning_rate': 1.973114012406544e-05, 'epoch': 0.1}
{'loss': 1.0404, 'grad_norm': 0.4501422643661499, 'learning_rate': 1.9729703122109788e-05, 'epoch': 0.1}
{'loss': 0.9507, 'grad_norm': 0.4369167387485504, 'learning_rate': 1.9728262342748384e-05, 'epoch': 0.1}
{'loss': 1.078, 'grad_norm': 0.42875880002975464, 'learning_rate': 1.9726817786540584e-05, 'epoch': 0.1}
{'loss': 0.9464, 'grad_norm': 0.43729478120803833, 'learning_rate': 1.9725369454047215e-05, 'epoch': 0.1}
{'loss': 0.9802, 'grad_norm': 0.442921906709671, 'learning_rate': 1.9723917345830568e-05, 'epoch': 0.1}
{'loss': 0.9731, 'grad_norm': 0.4383380115032196, 'learning_rate': 1.9722461462454405e-05, 'epoch': 0.1}
{'loss': 0.9953, 'grad_norm': 0.4143659174442291, 'learning_rate': 1.9721001804483947e-05, 'epoch': 0.1}
{'loss': 0.991, 'grad_norm': 0.43823298811912537, 'learning_rate': 1.9719538372485887e-05, 'epoch': 0.1}
{'loss': 0.993, 'grad_norm': 0.40071362257003784, 'learning_rate': 1.9718071167028376e-05, 'epoch': 0.1}
WARNING: tokenization mismatch: 0 vs. 1354. (ignored)
{'loss': 1.0898, 'grad_norm': 0.39925816655158997, 'learning_rate': 1.9716600188681038e-05, 'epoch': 0.1}
{'loss': 0.8838, 'grad_norm': 0.41021668910980225, 'learning_rate': 1.971512543801495e-05, 'epoch': 0.1}
{'loss': 1.0069, 'grad_norm': 0.3973142206668854, 'learning_rate': 1.9713646915602663e-05, 'epoch': 0.1}
{'loss': 1.0266, 'grad_norm': 0.44362401962280273, 'learning_rate': 1.9712164622018197e-05, 'epoch': 0.1}
{'loss': 0.9813, 'grad_norm': 0.4309122562408447, 'learning_rate': 1.9710678557837024e-05, 'epoch': 0.1}
{'loss': 1.0021, 'grad_norm': 0.41822367906570435, 'learning_rate': 1.9709188723636088e-05, 'epoch': 0.1}
{'loss': 0.9554, 'grad_norm': 0.3910546898841858, 'learning_rate': 1.970769511999379e-05, 'epoch': 0.11}
{'loss': 0.9497, 'grad_norm': 0.4099788963794708, 'learning_rate': 1.9706197747490004e-05, 'epoch': 0.11}
{'loss': 1.0236, 'grad_norm': 0.4360838830471039, 'learning_rate': 1.9704696606706055e-05, 'epoch': 0.11}
{'loss': 0.9818, 'grad_norm': 0.45342642068862915, 'learning_rate': 1.9703191698224742e-05, 'epoch': 0.11}
{'loss': 1.0609, 'grad_norm': 0.41911229491233826, 'learning_rate': 1.9701683022630323e-05, 'epoch': 0.11}
{'loss': 1.0538, 'grad_norm': 0.4106975197792053, 'learning_rate': 1.9700170580508514e-05, 'epoch': 0.11}
{'loss': 1.0577, 'grad_norm': 0.42231419682502747, 'learning_rate': 1.9698654372446495e-05, 'epoch': 0.11}
{'loss': 0.9622, 'grad_norm': 0.39049071073532104, 'learning_rate': 1.969713439903292e-05, 'epoch': 0.11}
{'loss': 0.937, 'grad_norm': 0.40927618741989136, 'learning_rate': 1.9695610660857886e-05, 'epoch': 0.11}
{'loss': 0.9465, 'grad_norm': 0.3553648591041565, 'learning_rate': 1.9694083158512965e-05, 'epoch': 0.11}
{'loss': 0.9837, 'grad_norm': 0.4409066140651703, 'learning_rate': 1.9692551892591185e-05, 'epoch': 0.11}
{'loss': 1.0386, 'grad_norm': 0.4428504705429077, 'learning_rate': 1.9691016863687037e-05, 'epoch': 0.11}
{'loss': 0.968, 'grad_norm': 0.36803075671195984, 'learning_rate': 1.968947807239647e-05, 'epoch': 0.11}
{'loss': 0.9894, 'grad_norm': 0.4062333106994629, 'learning_rate': 1.9687935519316897e-05, 'epoch': 0.11}
{'loss': 0.9411, 'grad_norm': 0.4731523096561432, 'learning_rate': 1.9686389205047186e-05, 'epoch': 0.11}
{'loss': 0.948, 'grad_norm': 0.4028327465057373, 'learning_rate': 1.9684839130187678e-05, 'epoch': 0.11}
{'loss': 0.9791, 'grad_norm': 0.4218562841415405, 'learning_rate': 1.968328529534016e-05, 'epoch': 0.11}
{'loss': 1.0387, 'grad_norm': 0.41127777099609375, 'learning_rate': 1.9681727701107885e-05, 'epoch': 0.11}
{'loss': 1.0041, 'grad_norm': 0.44758978486061096, 'learning_rate': 1.9680166348095568e-05, 'epoch': 0.11}
{'loss': 1.0196, 'grad_norm': 0.42632558941841125, 'learning_rate': 1.967860123690937e-05, 'epoch': 0.11}
{'loss': 1.0088, 'grad_norm': 0.4334072470664978, 'learning_rate': 1.9677032368156934e-05, 'epoch': 0.11}
{'loss': 1.0935, 'grad_norm': 0.41129282116889954, 'learning_rate': 1.967545974244734e-05, 'epoch': 0.11}
{'loss': 1.01, 'grad_norm': 0.4006902873516083, 'learning_rate': 1.9673883360391138e-05, 'epoch': 0.11}
{'loss': 1.0475, 'grad_norm': 0.5061725974082947, 'learning_rate': 1.9672303222600333e-05, 'epoch': 0.11}
{'loss': 1.0751, 'grad_norm': 0.4449731707572937, 'learning_rate': 1.967071932968839e-05, 'epoch': 0.11}
{'loss': 0.9803, 'grad_norm': 0.3901321291923523, 'learning_rate': 1.9669131682270232e-05, 'epoch': 0.11}
{'loss': 1.0489, 'grad_norm': 0.4370077848434448, 'learning_rate': 1.9667540280962235e-05, 'epoch': 0.11}
{'loss': 0.9732, 'grad_norm': 0.3764893412590027, 'learning_rate': 1.966594512638224e-05, 'epoch': 0.11}
{'loss': 0.9837, 'grad_norm': 0.3999693989753723, 'learning_rate': 1.9664346219149538e-05, 'epoch': 0.11}
{'loss': 1.0281, 'grad_norm': 0.3947281837463379, 'learning_rate': 1.966274355988488e-05, 'epoch': 0.11}
{'loss': 1.0877, 'grad_norm': 0.4731113016605377, 'learning_rate': 1.9661137149210473e-05, 'epoch': 0.11}
{'loss': 1.0362, 'grad_norm': 0.399425208568573, 'learning_rate': 1.9659526987749987e-05, 'epoch': 0.11}
{'loss': 0.9926, 'grad_norm': 0.3942953944206238, 'learning_rate': 1.9657913076128532e-05, 'epoch': 0.11}
{'loss': 0.9925, 'grad_norm': 0.44479960203170776, 'learning_rate': 1.965629541497269e-05, 'epoch': 0.11}
{'loss': 1.0609, 'grad_norm': 0.5192034840583801, 'learning_rate': 1.9654674004910493e-05, 'epoch': 0.11}
{'loss': 1.0743, 'grad_norm': 0.42998751997947693, 'learning_rate': 1.9653048846571427e-05, 'epoch': 0.11}
{'loss': 0.9249, 'grad_norm': 0.41399702429771423, 'learning_rate': 1.9651419940586437e-05, 'epoch': 0.11}
{'loss': 0.9887, 'grad_norm': 0.49163955450057983, 'learning_rate': 1.964978728758791e-05, 'epoch': 0.11}
{'loss': 1.1176, 'grad_norm': 0.44455525279045105, 'learning_rate': 1.9648150888209715e-05, 'epoch': 0.11}
{'loss': 0.9793, 'grad_norm': 0.4314962327480316, 'learning_rate': 1.9646510743087144e-05, 'epoch': 0.11}
{'loss': 0.9928, 'grad_norm': 0.40961965918540955, 'learning_rate': 1.964486685285697e-05, 'epoch': 0.11}
{'loss': 0.9107, 'grad_norm': 0.44347572326660156, 'learning_rate': 1.9643219218157395e-05, 'epoch': 0.11}
{'loss': 1.0366, 'grad_norm': 0.42565685510635376, 'learning_rate': 1.9641567839628092e-05, 'epoch': 0.11}
{'loss': 0.9952, 'grad_norm': 0.4031398594379425, 'learning_rate': 1.963991271791019e-05, 'epoch': 0.11}
{'loss': 1.0189, 'grad_norm': 0.4213571548461914, 'learning_rate': 1.9638253853646255e-05, 'epoch': 0.11}
{'loss': 0.9981, 'grad_norm': 0.40857264399528503, 'learning_rate': 1.9636591247480323e-05, 'epoch': 0.11}
{'loss': 0.9146, 'grad_norm': 0.32255223393440247, 'learning_rate': 1.9634924900057867e-05, 'epoch': 0.11}
{'loss': 0.982, 'grad_norm': 0.43689602613449097, 'learning_rate': 1.963325481202583e-05, 'epoch': 0.11}
{'loss': 0.9689, 'grad_norm': 0.384654700756073, 'learning_rate': 1.963158098403259e-05, 'epoch': 0.11}
{'loss': 1.0358, 'grad_norm': 0.42672401666641235, 'learning_rate': 1.9629903416727987e-05, 'epoch': 0.11}
{'loss': 0.9903, 'grad_norm': 0.39213860034942627, 'learning_rate': 1.962822211076331e-05, 'epoch': 0.11}
{'loss': 1.0421, 'grad_norm': 0.48202061653137207, 'learning_rate': 1.96265370667913e-05, 'epoch': 0.11}
{'loss': 0.9835, 'grad_norm': 0.40405693650245667, 'learning_rate': 1.9624848285466146e-05, 'epoch': 0.12}
{'loss': 0.9989, 'grad_norm': 0.42614930868148804, 'learning_rate': 1.9623155767443498e-05, 'epoch': 0.12}
{'loss': 1.1303, 'grad_norm': 0.46609795093536377, 'learning_rate': 1.9621459513380445e-05, 'epoch': 0.12}
{'loss': 1.0898, 'grad_norm': 0.48901158571243286, 'learning_rate': 1.9619759523935532e-05, 'epoch': 0.12}
{'loss': 1.0037, 'grad_norm': 0.4739260971546173, 'learning_rate': 1.9618055799768757e-05, 'epoch': 0.12}
{'loss': 1.0477, 'grad_norm': 0.42788273096084595, 'learning_rate': 1.961634834154156e-05, 'epoch': 0.12}
{'loss': 1.0068, 'grad_norm': 0.3918343186378479, 'learning_rate': 1.9614637149916834e-05, 'epoch': 0.12}
{'loss': 0.897, 'grad_norm': 0.332661509513855, 'learning_rate': 1.9612922225558924e-05, 'epoch': 0.12}
{'loss': 0.9329, 'grad_norm': 0.43817654252052307, 'learning_rate': 1.961120356913363e-05, 'epoch': 0.12}
{'loss': 1.0354, 'grad_norm': 0.41604721546173096, 'learning_rate': 1.960948118130818e-05, 'epoch': 0.12}
{'loss': 1.0447, 'grad_norm': 0.4155471920967102, 'learning_rate': 1.9607755062751273e-05, 'epoch': 0.12}
{'loss': 0.9809, 'grad_norm': 0.40491682291030884, 'learning_rate': 1.9606025214133046e-05, 'epoch': 0.12}
{'loss': 0.9604, 'grad_norm': 0.33966314792633057, 'learning_rate': 1.9604291636125084e-05, 'epoch': 0.12}
{'loss': 0.9336, 'grad_norm': 0.41044551134109497, 'learning_rate': 1.960255432940043e-05, 'epoch': 0.12}
{'loss': 1.076, 'grad_norm': 0.44781461358070374, 'learning_rate': 1.9600813294633552e-05, 'epoch': 0.12}
{'loss': 1.0341, 'grad_norm': 0.4496474266052246, 'learning_rate': 1.9599068532500394e-05, 'epoch': 0.12}
{'loss': 1.0366, 'grad_norm': 0.44855234026908875, 'learning_rate': 1.9597320043678322e-05, 'epoch': 0.12}
{'loss': 1.1026, 'grad_norm': 0.4648321568965912, 'learning_rate': 1.9595567828846166e-05, 'epoch': 0.12}
{'loss': 1.0524, 'grad_norm': 0.4898087978363037, 'learning_rate': 1.9593811888684192e-05, 'epoch': 0.12}
{'loss': 0.9871, 'grad_norm': 0.41399505734443665, 'learning_rate': 1.9592052223874115e-05, 'epoch': 0.12}
{'loss': 0.9781, 'grad_norm': 0.4137266278266907, 'learning_rate': 1.959028883509911e-05, 'epoch': 0.12}
{'loss': 0.9638, 'grad_norm': 0.41031861305236816, 'learning_rate': 1.9588521723043764e-05, 'epoch': 0.12}
{'loss': 0.9837, 'grad_norm': 0.3470591902732849, 'learning_rate': 1.958675088839415e-05, 'epoch': 0.12}
{'loss': 0.9552, 'grad_norm': 0.41024020314216614, 'learning_rate': 1.9584976331837758e-05, 'epoch': 0.12}
{'loss': 0.951, 'grad_norm': 0.33488723635673523, 'learning_rate': 1.9583198054063535e-05, 'epoch': 0.12}
{'loss': 1.0064, 'grad_norm': 0.4133288264274597, 'learning_rate': 1.9581416055761865e-05, 'epoch': 0.12}
{'loss': 0.9183, 'grad_norm': 0.38433638215065, 'learning_rate': 1.9579630337624585e-05, 'epoch': 0.12}
{'loss': 1.0684, 'grad_norm': 0.4466748535633087, 'learning_rate': 1.9577840900344974e-05, 'epoch': 0.12}
{'loss': 0.9705, 'grad_norm': 0.3404785990715027, 'learning_rate': 1.9576047744617752e-05, 'epoch': 0.12}
{'loss': 1.063, 'grad_norm': 0.48192644119262695, 'learning_rate': 1.957425087113908e-05, 'epoch': 0.12}
{'loss': 0.9711, 'grad_norm': 0.4670948088169098, 'learning_rate': 1.9572450280606568e-05, 'epoch': 0.12}
{'loss': 1.005, 'grad_norm': 0.44026845693588257, 'learning_rate': 1.9570645973719273e-05, 'epoch': 0.12}
{'loss': 0.8979, 'grad_norm': 0.4445217251777649, 'learning_rate': 1.9568837951177677e-05, 'epoch': 0.12}
{'loss': 1.098, 'grad_norm': 0.42926570773124695, 'learning_rate': 1.9567026213683728e-05, 'epoch': 0.12}
{'loss': 0.9764, 'grad_norm': 0.4518769085407257, 'learning_rate': 1.9565210761940798e-05, 'epoch': 0.12}
{'loss': 1.0433, 'grad_norm': 0.42939692735671997, 'learning_rate': 1.956339159665371e-05, 'epoch': 0.12}
{'loss': 0.9909, 'grad_norm': 0.431482195854187, 'learning_rate': 1.956156871852873e-05, 'epoch': 0.12}
{'loss': 1.0335, 'grad_norm': 0.44623276591300964, 'learning_rate': 1.9559742128273558e-05, 'epoch': 0.12}
{'loss': 0.9231, 'grad_norm': 0.42800381779670715, 'learning_rate': 1.9557911826597337e-05, 'epoch': 0.12}
{'loss': 0.9986, 'grad_norm': 0.36078011989593506, 'learning_rate': 1.9556077814210662e-05, 'epoch': 0.12}
{'loss': 1.0543, 'grad_norm': 0.4197540879249573, 'learning_rate': 1.955424009182555e-05, 'epoch': 0.12}
{'loss': 0.9477, 'grad_norm': 0.35663872957229614, 'learning_rate': 1.955239866015547e-05, 'epoch': 0.12}
{'loss': 1.0636, 'grad_norm': 0.42523619532585144, 'learning_rate': 1.9550553519915335e-05, 'epoch': 0.12}
{'loss': 1.0034, 'grad_norm': 0.41022172570228577, 'learning_rate': 1.954870467182149e-05, 'epoch': 0.12}
{'loss': 0.9603, 'grad_norm': 0.33947494626045227, 'learning_rate': 1.954685211659172e-05, 'epoch': 0.12}
{'loss': 1.0523, 'grad_norm': 0.4199533462524414, 'learning_rate': 1.9544995854945248e-05, 'epoch': 0.12}
{'loss': 1.0311, 'grad_norm': 0.4442564845085144, 'learning_rate': 1.954313588760274e-05, 'epoch': 0.12}
{'loss': 1.0259, 'grad_norm': 0.42541268467903137, 'learning_rate': 1.9541272215286304e-05, 'epoch': 0.12}
{'loss': 1.0194, 'grad_norm': 0.4424715042114258, 'learning_rate': 1.9539404838719477e-05, 'epoch': 0.12}
{'loss': 0.9854, 'grad_norm': 0.4416334629058838, 'learning_rate': 1.9537533758627242e-05, 'epoch': 0.12}
{'loss': 1.0398, 'grad_norm': 0.5017488598823547, 'learning_rate': 1.953565897573601e-05, 'epoch': 0.12}
{'loss': 0.9686, 'grad_norm': 0.45150867104530334, 'learning_rate': 1.9533780490773645e-05, 'epoch': 0.12}
{'loss': 0.9573, 'grad_norm': 0.40873846411705017, 'learning_rate': 1.9531898304469435e-05, 'epoch': 0.13}
{'loss': 0.9789, 'grad_norm': 0.36196380853652954, 'learning_rate': 1.953001241755411e-05, 'epoch': 0.13}
{'loss': 1.1014, 'grad_norm': 0.4347732663154602, 'learning_rate': 1.952812283075984e-05, 'epoch': 0.13}
{'loss': 0.9934, 'grad_norm': 0.3946079909801483, 'learning_rate': 1.952622954482022e-05, 'epoch': 0.13}
{'loss': 0.9833, 'grad_norm': 0.4643361568450928, 'learning_rate': 1.9524332560470293e-05, 'epoch': 0.13}
{'loss': 1.0087, 'grad_norm': 0.39521101117134094, 'learning_rate': 1.9522431878446536e-05, 'epoch': 0.13}
{'loss': 1.0114, 'grad_norm': 0.40778085589408875, 'learning_rate': 1.9520527499486856e-05, 'epoch': 0.13}
{'loss': 0.96, 'grad_norm': 0.43259555101394653, 'learning_rate': 1.95186194243306e-05, 'epoch': 0.13}
{'loss': 1.0406, 'grad_norm': 0.4475615918636322, 'learning_rate': 1.9516707653718546e-05, 'epoch': 0.13}
{'loss': 1.0357, 'grad_norm': 0.4153098165988922, 'learning_rate': 1.9514792188392914e-05, 'epoch': 0.13}
{'loss': 1.0277, 'grad_norm': 0.4271630644798279, 'learning_rate': 1.9512873029097347e-05, 'epoch': 0.13}
{'loss': 0.9646, 'grad_norm': 0.4077959954738617, 'learning_rate': 1.9510950176576933e-05, 'epoch': 0.13}
{'loss': 0.9863, 'grad_norm': 0.443668395280838, 'learning_rate': 1.950902363157819e-05, 'epoch': 0.13}
{'loss': 1.062, 'grad_norm': 0.4126708507537842, 'learning_rate': 1.950709339484907e-05, 'epoch': 0.13}
{'loss': 1.0751, 'grad_norm': 0.47185632586479187, 'learning_rate': 1.9505159467138954e-05, 'epoch': 0.13}
{'loss': 0.9688, 'grad_norm': 0.3579852283000946, 'learning_rate': 1.9503221849198655e-05, 'epoch': 0.13}
{'loss': 0.9909, 'grad_norm': 0.4236931800842285, 'learning_rate': 1.9501280541780435e-05, 'epoch': 0.13}
{'loss': 1.115, 'grad_norm': 0.4446602761745453, 'learning_rate': 1.9499335545637968e-05, 'epoch': 0.13}
{'loss': 1.025, 'grad_norm': 0.41815513372421265, 'learning_rate': 1.949738686152637e-05, 'epoch': 0.13}
{'loss': 1.0193, 'grad_norm': 0.4194337725639343, 'learning_rate': 1.9495434490202188e-05, 'epoch': 0.13}
{'loss': 0.9478, 'grad_norm': 0.4256085753440857, 'learning_rate': 1.94934784324234e-05, 'epoch': 0.13}
{'loss': 0.9615, 'grad_norm': 0.42887863516807556, 'learning_rate': 1.9491518688949417e-05, 'epoch': 0.13}
{'loss': 0.9729, 'grad_norm': 0.43014711141586304, 'learning_rate': 1.9489555260541074e-05, 'epoch': 0.13}
{'loss': 0.9823, 'grad_norm': 0.5132149457931519, 'learning_rate': 1.948758814796064e-05, 'epoch': 0.13}
{'loss': 1.0411, 'grad_norm': 0.4257964491844177, 'learning_rate': 1.9485617351971827e-05, 'epoch': 0.13}
{'loss': 1.0268, 'grad_norm': 0.45433536171913147, 'learning_rate': 1.9483642873339753e-05, 'epoch': 0.13}
{'loss': 1.0503, 'grad_norm': 0.4345104992389679, 'learning_rate': 1.9481664712830987e-05, 'epoch': 0.13}
{'loss': 0.9391, 'grad_norm': 0.47966623306274414, 'learning_rate': 1.9479682871213515e-05, 'epoch': 0.13}
{'loss': 1.0639, 'grad_norm': 0.4398851692676544, 'learning_rate': 1.9477697349256756e-05, 'epoch': 0.13}
{'loss': 1.0232, 'grad_norm': 0.4599728286266327, 'learning_rate': 1.947570814773156e-05, 'epoch': 0.13}
{'loss': 1.0142, 'grad_norm': 0.41922131180763245, 'learning_rate': 1.9473715267410206e-05, 'epoch': 0.13}
{'loss': 1.0467, 'grad_norm': 0.4187641739845276, 'learning_rate': 1.9471718709066392e-05, 'epoch': 0.13}
{'loss': 1.0138, 'grad_norm': 0.3650727868080139, 'learning_rate': 1.9469718473475256e-05, 'epoch': 0.13}
{'loss': 0.9894, 'grad_norm': 0.43650728464126587, 'learning_rate': 1.9467714561413358e-05, 'epoch': 0.13}
{'loss': 1.0468, 'grad_norm': 0.4396013617515564, 'learning_rate': 1.9465706973658683e-05, 'epoch': 0.13}
{'loss': 1.0033, 'grad_norm': 0.4118651747703552, 'learning_rate': 1.9463695710990648e-05, 'epoch': 0.13}
{'loss': 0.9953, 'grad_norm': 0.39264553785324097, 'learning_rate': 1.946168077419009e-05, 'epoch': 0.13}
{'loss': 0.9276, 'grad_norm': 0.4057270586490631, 'learning_rate': 1.9459662164039283e-05, 'epoch': 0.13}
{'loss': 0.9554, 'grad_norm': 0.40606778860092163, 'learning_rate': 1.9457639881321917e-05, 'epoch': 0.13}
{'loss': 1.1014, 'grad_norm': 0.5102773308753967, 'learning_rate': 1.9455613926823115e-05, 'epoch': 0.13}
{'loss': 1.0255, 'grad_norm': 0.42648813128471375, 'learning_rate': 1.945358430132942e-05, 'epoch': 0.13}
{'loss': 0.9879, 'grad_norm': 0.4112108647823334, 'learning_rate': 1.9451551005628803e-05, 'epoch': 0.13}
{'loss': 0.9915, 'grad_norm': 0.5011237263679504, 'learning_rate': 1.9449514040510654e-05, 'epoch': 0.13}
{'loss': 1.0136, 'grad_norm': 0.35000067949295044, 'learning_rate': 1.9447473406765803e-05, 'epoch': 0.13}
{'loss': 1.0005, 'grad_norm': 0.39637765288352966, 'learning_rate': 1.9445429105186487e-05, 'epoch': 0.13}
{'loss': 1.0083, 'grad_norm': 0.4397892355918884, 'learning_rate': 1.9443381136566382e-05, 'epoch': 0.13}
{'loss': 0.9861, 'grad_norm': 0.39498981833457947, 'learning_rate': 1.9441329501700568e-05, 'epoch': 0.13}
{'loss': 0.9944, 'grad_norm': 0.45212069153785706, 'learning_rate': 1.943927420138557e-05, 'epoch': 0.13}
{'loss': 1.0422, 'grad_norm': 0.44813844561576843, 'learning_rate': 1.9437215236419322e-05, 'epoch': 0.13}
{'loss': 1.0487, 'grad_norm': 0.40828728675842285, 'learning_rate': 1.9435152607601187e-05, 'epoch': 0.13}
{'loss': 0.9626, 'grad_norm': 0.42362257838249207, 'learning_rate': 1.943308631573195e-05, 'epoch': 0.13}
{'loss': 1.0355, 'grad_norm': 0.4012993276119232, 'learning_rate': 1.9431016361613816e-05, 'epoch': 0.13}
{'loss': 0.9826, 'grad_norm': 0.41374096274375916, 'learning_rate': 1.9428942746050406e-05, 'epoch': 0.14}
{'loss': 0.9667, 'grad_norm': 0.42270565032958984, 'learning_rate': 1.9426865469846773e-05, 'epoch': 0.14}
{'loss': 1.0611, 'grad_norm': 0.4029664099216461, 'learning_rate': 1.9424784533809393e-05, 'epoch': 0.14}
{'loss': 1.0324, 'grad_norm': 0.4513038694858551, 'learning_rate': 1.942269993874615e-05, 'epoch': 0.14}
{'loss': 1.0105, 'grad_norm': 0.40894386172294617, 'learning_rate': 1.9420611685466358e-05, 'epoch': 0.14}
{'loss': 0.924, 'grad_norm': 0.33156198263168335, 'learning_rate': 1.9418519774780748e-05, 'epoch': 0.14}
{'loss': 1.0694, 'grad_norm': 0.44070708751678467, 'learning_rate': 1.9416424207501474e-05, 'epoch': 0.14}
{'loss': 0.9582, 'grad_norm': 0.43210339546203613, 'learning_rate': 1.9414324984442102e-05, 'epoch': 0.14}
{'loss': 1.0065, 'grad_norm': 0.44068604707717896, 'learning_rate': 1.9412222106417632e-05, 'epoch': 0.14}
{'loss': 0.9768, 'grad_norm': 0.4262389540672302, 'learning_rate': 1.9410115574244462e-05, 'epoch': 0.14}
{'loss': 0.9119, 'grad_norm': 0.4708080589771271, 'learning_rate': 1.9408005388740433e-05, 'epoch': 0.14}
Error with image file is truncated (46 bytes not processed)
{'loss': 0.9124, 'grad_norm': 0.4114006459712982, 'learning_rate': 1.9405891550724778e-05, 'epoch': 0.14}
{'loss': 1.0012, 'grad_norm': 0.4159806966781616, 'learning_rate': 1.940377406101817e-05, 'epoch': 0.14}
{'loss': 0.9567, 'grad_norm': 0.45845136046409607, 'learning_rate': 1.9401652920442694e-05, 'epoch': 0.14}
{'loss': 0.945, 'grad_norm': 0.3788171112537384, 'learning_rate': 1.9399528129821842e-05, 'epoch': 0.14}
{'loss': 1.065, 'grad_norm': 0.4452694356441498, 'learning_rate': 1.939739968998054e-05, 'epoch': 0.14}
{'loss': 0.9983, 'grad_norm': 0.45378750562667847, 'learning_rate': 1.939526760174511e-05, 'epoch': 0.14}
{'loss': 0.9811, 'grad_norm': 0.42770132422447205, 'learning_rate': 1.939313186594331e-05, 'epoch': 0.14}
{'loss': 0.958, 'grad_norm': 0.34169989824295044, 'learning_rate': 1.9390992483404308e-05, 'epoch': 0.14}
{'loss': 1.0564, 'grad_norm': 0.4066769778728485, 'learning_rate': 1.938884945495868e-05, 'epoch': 0.14}
{'loss': 0.951, 'grad_norm': 0.40620139241218567, 'learning_rate': 1.9386702781438425e-05, 'epoch': 0.14}
{'loss': 0.989, 'grad_norm': 0.414673775434494, 'learning_rate': 1.938455246367696e-05, 'epoch': 0.14}
{'loss': 1.0053, 'grad_norm': 0.40629085898399353, 'learning_rate': 1.9382398502509107e-05, 'epoch': 0.14}
{'loss': 0.9682, 'grad_norm': 0.4373791813850403, 'learning_rate': 1.938024089877111e-05, 'epoch': 0.14}
{'loss': 1.0832, 'grad_norm': 0.4633252024650574, 'learning_rate': 1.9378079653300624e-05, 'epoch': 0.14}
{'loss': 1.0373, 'grad_norm': 0.4741121232509613, 'learning_rate': 1.9375914766936723e-05, 'epoch': 0.14}
{'loss': 1.0027, 'grad_norm': 0.4450746178627014, 'learning_rate': 1.9373746240519884e-05, 'epoch': 0.14}
{'loss': 1.0529, 'grad_norm': 0.40797826647758484, 'learning_rate': 1.937157407489201e-05, 'epoch': 0.14}
{'loss': 1.0593, 'grad_norm': 0.4287305176258087, 'learning_rate': 1.9369398270896403e-05, 'epoch': 0.14}
{'loss': 1.012, 'grad_norm': 0.43062254786491394, 'learning_rate': 1.936721882937779e-05, 'epoch': 0.14}
{'loss': 0.9591, 'grad_norm': 0.440017968416214, 'learning_rate': 1.9365035751182307e-05, 'epoch': 0.14}
{'loss': 1.0007, 'grad_norm': 0.3458596169948578, 'learning_rate': 1.93628490371575e-05, 'epoch': 0.14}
{'loss': 1.0113, 'grad_norm': 0.43608972430229187, 'learning_rate': 1.9360658688152322e-05, 'epoch': 0.14}
{'loss': 1.0115, 'grad_norm': 0.4288026988506317, 'learning_rate': 1.9358464705017143e-05, 'epoch': 0.14}
{'loss': 0.9277, 'grad_norm': 0.31725969910621643, 'learning_rate': 1.9356267088603745e-05, 'epoch': 0.14}
{'loss': 0.9762, 'grad_norm': 0.42659997940063477, 'learning_rate': 1.9354065839765316e-05, 'epoch': 0.14}
{'loss': 0.9472, 'grad_norm': 0.4120202660560608, 'learning_rate': 1.9351860959356462e-05, 'epoch': 0.14}
{'loss': 0.9895, 'grad_norm': 0.3472670912742615, 'learning_rate': 1.9349652448233187e-05, 'epoch': 0.14}
{'loss': 1.0096, 'grad_norm': 0.4065953493118286, 'learning_rate': 1.934744030725291e-05, 'epoch': 0.14}
{'loss': 1.0196, 'grad_norm': 0.4248685836791992, 'learning_rate': 1.934522453727447e-05, 'epoch': 0.14}
{'loss': 0.9433, 'grad_norm': 0.3985781669616699, 'learning_rate': 1.93430051391581e-05, 'epoch': 0.14}
{'loss': 0.9909, 'grad_norm': 0.39776065945625305, 'learning_rate': 1.934078211376544e-05, 'epoch': 0.14}
{'loss': 0.9955, 'grad_norm': 0.3815450370311737, 'learning_rate': 1.9338555461959554e-05, 'epoch': 0.14}
{'loss': 0.9738, 'grad_norm': 0.412069708108902, 'learning_rate': 1.93363251846049e-05, 'epoch': 0.14}
{'loss': 1.0665, 'grad_norm': 0.3745127320289612, 'learning_rate': 1.9334091282567352e-05, 'epoch': 0.14}
{'loss': 0.9184, 'grad_norm': 0.4557199478149414, 'learning_rate': 1.9331853756714185e-05, 'epoch': 0.14}
{'loss': 1.0398, 'grad_norm': 0.4566417336463928, 'learning_rate': 1.9329612607914088e-05, 'epoch': 0.14}
{'loss': 1.0214, 'grad_norm': 0.42347055673599243, 'learning_rate': 1.9327367837037142e-05, 'epoch': 0.14}
{'loss': 0.982, 'grad_norm': 0.4289897680282593, 'learning_rate': 1.9325119444954855e-05, 'epoch': 0.14}
{'loss': 1.0213, 'grad_norm': 0.4031934142112732, 'learning_rate': 1.9322867432540126e-05, 'epoch': 0.14}
{'loss': 0.9448, 'grad_norm': 0.43257540464401245, 'learning_rate': 1.9320611800667268e-05, 'epoch': 0.14}
{'loss': 1.037, 'grad_norm': 0.4634353220462799, 'learning_rate': 1.9318352550211986e-05, 'epoch': 0.14}
{'loss': 0.9832, 'grad_norm': 0.3561452031135559, 'learning_rate': 1.9316089682051403e-05, 'epoch': 0.15}
{'loss': 1.0286, 'grad_norm': 0.4100801348686218, 'learning_rate': 1.9313823197064042e-05, 'epoch': 0.15}
{'loss': 1.0031, 'grad_norm': 0.39900922775268555, 'learning_rate': 1.9311553096129835e-05, 'epoch': 0.15}
{'loss': 0.9636, 'grad_norm': 0.3829232156276703, 'learning_rate': 1.9309279380130112e-05, 'epoch': 0.15}
{'loss': 0.9939, 'grad_norm': 0.41612759232521057, 'learning_rate': 1.93070020499476e-05, 'epoch': 0.15}
{'loss': 1.0378, 'grad_norm': 0.4931945204734802, 'learning_rate': 1.930472110646645e-05, 'epoch': 0.15}
{'loss': 1.0786, 'grad_norm': 0.43441203236579895, 'learning_rate': 1.9302436550572187e-05, 'epoch': 0.15}
{'loss': 0.9119, 'grad_norm': 0.4359288215637207, 'learning_rate': 1.930014838315177e-05, 'epoch': 0.15}
{'loss': 0.9904, 'grad_norm': 0.4364364445209503, 'learning_rate': 1.9297856605093534e-05, 'epoch': 0.15}
{'loss': 1.0154, 'grad_norm': 0.45708656311035156, 'learning_rate': 1.9295561217287226e-05, 'epoch': 0.15}
{'loss': 1.0603, 'grad_norm': 0.4197121262550354, 'learning_rate': 1.9293262220624002e-05, 'epoch': 0.15}
{'loss': 1.0088, 'grad_norm': 0.4056633710861206, 'learning_rate': 1.9290959615996407e-05, 'epoch': 0.15}
{'loss': 0.9424, 'grad_norm': 0.42652037739753723, 'learning_rate': 1.9288653404298392e-05, 'epoch': 0.15}
{'loss': 0.9604, 'grad_norm': 0.42627131938934326, 'learning_rate': 1.9286343586425307e-05, 'epoch': 0.15}
{'loss': 1.0255, 'grad_norm': 0.40750840306282043, 'learning_rate': 1.9284030163273907e-05, 'epoch': 0.15}
{'loss': 0.9976, 'grad_norm': 0.333361953496933, 'learning_rate': 1.9281713135742333e-05, 'epoch': 0.15}
{'loss': 0.9888, 'grad_norm': 0.4556448459625244, 'learning_rate': 1.9279392504730147e-05, 'epoch': 0.15}
{'loss': 0.9349, 'grad_norm': 0.39350607991218567, 'learning_rate': 1.9277068271138287e-05, 'epoch': 0.15}
{'loss': 1.0374, 'grad_norm': 0.38746219873428345, 'learning_rate': 1.9274740435869107e-05, 'epoch': 0.15}
{'loss': 0.9882, 'grad_norm': 0.41343504190444946, 'learning_rate': 1.927240899982635e-05, 'epoch': 0.15}
Error with image file is truncated (16 bytes not processed)
{'loss': 1.0358, 'grad_norm': 0.4558784067630768, 'learning_rate': 1.9270073963915162e-05, 'epoch': 0.15}
{'loss': 0.9944, 'grad_norm': 0.3337308466434479, 'learning_rate': 1.9267735329042086e-05, 'epoch': 0.15}
{'loss': 1.0102, 'grad_norm': 0.42535650730133057, 'learning_rate': 1.9265393096115056e-05, 'epoch': 0.15}
{'loss': 1.0816, 'grad_norm': 0.41064217686653137, 'learning_rate': 1.926304726604341e-05, 'epoch': 0.15}
{'loss': 1.0478, 'grad_norm': 0.4276311993598938, 'learning_rate': 1.9260697839737875e-05, 'epoch': 0.15}
{'loss': 1.0337, 'grad_norm': 0.4004541039466858, 'learning_rate': 1.925834481811059e-05, 'epoch': 0.15}
{'loss': 0.9547, 'grad_norm': 0.3187924921512604, 'learning_rate': 1.9255988202075065e-05, 'epoch': 0.15}
{'loss': 1.0237, 'grad_norm': 0.4299384355545044, 'learning_rate': 1.925362799254623e-05, 'epoch': 0.15}
{'loss': 1.0457, 'grad_norm': 0.38548150658607483, 'learning_rate': 1.9251264190440398e-05, 'epoch': 0.15}
{'loss': 1.0068, 'grad_norm': 0.42088016867637634, 'learning_rate': 1.9248896796675277e-05, 'epoch': 0.15}
{'loss': 1.0347, 'grad_norm': 0.4091658294200897, 'learning_rate': 1.924652581216997e-05, 'epoch': 0.15}
{'loss': 0.9897, 'grad_norm': 0.45544105768203735, 'learning_rate': 1.9244151237844975e-05, 'epoch': 0.15}
{'loss': 0.9825, 'grad_norm': 0.47188931703567505, 'learning_rate': 1.9241773074622182e-05, 'epoch': 0.15}
{'loss': 0.9638, 'grad_norm': 0.38986214995384216, 'learning_rate': 1.923939132342488e-05, 'epoch': 0.15}
{'loss': 0.9935, 'grad_norm': 0.42578673362731934, 'learning_rate': 1.923700598517775e-05, 'epoch': 0.15}
{'loss': 1.037, 'grad_norm': 0.40923139452934265, 'learning_rate': 1.923461706080685e-05, 'epoch': 0.15}
{'loss': 1.0075, 'grad_norm': 0.4023685157299042, 'learning_rate': 1.923222455123965e-05, 'epoch': 0.15}
{'loss': 1.0151, 'grad_norm': 0.5311588048934937, 'learning_rate': 1.9229828457405005e-05, 'epoch': 0.15}
{'loss': 0.9465, 'grad_norm': 0.394288569688797, 'learning_rate': 1.9227428780233162e-05, 'epoch': 0.15}
{'loss': 0.8962, 'grad_norm': 0.3842698931694031, 'learning_rate': 1.922502552065576e-05, 'epoch': 0.15}
{'loss': 1.0313, 'grad_norm': 0.462031751871109, 'learning_rate': 1.922261867960582e-05, 'epoch': 0.15}
{'loss': 0.9944, 'grad_norm': 0.4644881784915924, 'learning_rate': 1.9220208258017763e-05, 'epoch': 0.15}
{'loss': 0.9493, 'grad_norm': 0.4841453731060028, 'learning_rate': 1.92177942568274e-05, 'epoch': 0.15}
{'loss': 0.982, 'grad_norm': 0.4279901385307312, 'learning_rate': 1.921537667697193e-05, 'epoch': 0.15}
{'loss': 1.0581, 'grad_norm': 0.40241092443466187, 'learning_rate': 1.9212955519389938e-05, 'epoch': 0.15}
{'loss': 1.069, 'grad_norm': 0.4835880398750305, 'learning_rate': 1.9210530785021405e-05, 'epoch': 0.15}
{'loss': 1.0851, 'grad_norm': 0.4315789043903351, 'learning_rate': 1.9208102474807692e-05, 'epoch': 0.15}
{'loss': 0.9387, 'grad_norm': 0.40239793062210083, 'learning_rate': 1.920567058969155e-05, 'epoch': 0.15}
{'loss': 1.0024, 'grad_norm': 0.46298936009407043, 'learning_rate': 1.920323513061713e-05, 'epoch': 0.15}
{'loss': 0.9747, 'grad_norm': 0.3926546275615692, 'learning_rate': 1.9200796098529956e-05, 'epoch': 0.15}
{'loss': 1.0001, 'grad_norm': 0.39600151777267456, 'learning_rate': 1.919835349437694e-05, 'epoch': 0.15}
{'loss': 0.9282, 'grad_norm': 0.308146595954895, 'learning_rate': 1.9195907319106394e-05, 'epoch': 0.15}
{'loss': 1.0684, 'grad_norm': 0.4785511791706085, 'learning_rate': 1.9193457573667996e-05, 'epoch': 0.16}
{'loss': 0.9678, 'grad_norm': 0.3640787601470947, 'learning_rate': 1.919100425901283e-05, 'epoch': 0.16}
{'loss': 1.0067, 'grad_norm': 0.38514444231987, 'learning_rate': 1.9188547376093355e-05, 'epoch': 0.16}
{'loss': 1.0395, 'grad_norm': 0.4159444272518158, 'learning_rate': 1.918608692586342e-05, 'epoch': 0.16}
{'loss': 0.9873, 'grad_norm': 0.3970343768596649, 'learning_rate': 1.918362290927825e-05, 'epoch': 0.16}
{'loss': 1.062, 'grad_norm': 0.4889393150806427, 'learning_rate': 1.9181155327294468e-05, 'epoch': 0.16}
{'loss': 0.9509, 'grad_norm': 0.4301738739013672, 'learning_rate': 1.9178684180870072e-05, 'epoch': 0.16}
{'loss': 0.9481, 'grad_norm': 0.44346359372138977, 'learning_rate': 1.9176209470964446e-05, 'epoch': 0.16}
{'loss': 1.0242, 'grad_norm': 0.41556596755981445, 'learning_rate': 1.9173731198538354e-05, 'epoch': 0.16}
{'loss': 1.0532, 'grad_norm': 0.4458788335323334, 'learning_rate': 1.9171249364553956e-05, 'epoch': 0.16}
{'loss': 1.0421, 'grad_norm': 0.4736228585243225, 'learning_rate': 1.9168763969974773e-05, 'epoch': 0.16}
{'loss': 0.9863, 'grad_norm': 0.4217676520347595, 'learning_rate': 1.916627501576573e-05, 'epoch': 0.16}
{'loss': 1.0547, 'grad_norm': 0.42166656255722046, 'learning_rate': 1.916378250289312e-05, 'epoch': 0.16}
{'loss': 0.9506, 'grad_norm': 0.44989773631095886, 'learning_rate': 1.9161286432324628e-05, 'epoch': 0.16}
{'loss': 1.0275, 'grad_norm': 0.4171198904514313, 'learning_rate': 1.9158786805029307e-05, 'epoch': 0.16}
{'loss': 1.0847, 'grad_norm': 0.43053507804870605, 'learning_rate': 1.9156283621977603e-05, 'epoch': 0.16}
{'loss': 0.9926, 'grad_norm': 0.42315173149108887, 'learning_rate': 1.9153776884141336e-05, 'epoch': 0.16}
{'loss': 0.908, 'grad_norm': 0.39271724224090576, 'learning_rate': 1.915126659249371e-05, 'epoch': 0.16}
{'loss': 0.9622, 'grad_norm': 0.40651485323905945, 'learning_rate': 1.9148752748009304e-05, 'epoch': 0.16}
{'loss': 0.9671, 'grad_norm': 0.4586798846721649, 'learning_rate': 1.914623535166408e-05, 'epoch': 0.16}
{'loss': 0.9715, 'grad_norm': 0.399575799703598, 'learning_rate': 1.9143714404435382e-05, 'epoch': 0.16}
{'loss': 0.9701, 'grad_norm': 0.3887043595314026, 'learning_rate': 1.9141189907301922e-05, 'epoch': 0.16}
{'loss': 0.9401, 'grad_norm': 0.3991042971611023, 'learning_rate': 1.9138661861243802e-05, 'epoch': 0.16}
{'loss': 1.1044, 'grad_norm': 0.437813937664032, 'learning_rate': 1.913613026724249e-05, 'epoch': 0.16}
{'loss': 0.9449, 'grad_norm': 0.3957996070384979, 'learning_rate': 1.9133595126280848e-05, 'epoch': 0.16}
{'loss': 0.9823, 'grad_norm': 0.4363408386707306, 'learning_rate': 1.9131056439343095e-05, 'epoch': 0.16}
{'loss': 1.0264, 'grad_norm': 0.45726293325424194, 'learning_rate': 1.9128514207414838e-05, 'epoch': 0.16}
{'loss': 1.0706, 'grad_norm': 0.49763262271881104, 'learning_rate': 1.9125968431483068e-05, 'epoch': 0.16}
{'loss': 1.0838, 'grad_norm': 0.38905513286590576, 'learning_rate': 1.9123419112536132e-05, 'epoch': 0.16}
{'loss': 0.9987, 'grad_norm': 0.42279183864593506, 'learning_rate': 1.912086625156377e-05, 'epoch': 0.16}
{'loss': 1.096, 'grad_norm': 0.42935043573379517, 'learning_rate': 1.911830984955709e-05, 'epoch': 0.16}
{'loss': 0.9731, 'grad_norm': 0.36697614192962646, 'learning_rate': 1.911574990750857e-05, 'epoch': 0.16}
{'loss': 1.0742, 'grad_norm': 0.44383081793785095, 'learning_rate': 1.9113186426412073e-05, 'epoch': 0.16}
{'loss': 0.9527, 'grad_norm': 0.40795931220054626, 'learning_rate': 1.9110619407262828e-05, 'epoch': 0.16}
{'loss': 1.0299, 'grad_norm': 0.423173725605011, 'learning_rate': 1.9108048851057447e-05, 'epoch': 0.16}
{'loss': 0.9948, 'grad_norm': 0.41622546315193176, 'learning_rate': 1.9105474758793897e-05, 'epoch': 0.16}
{'loss': 1.0074, 'grad_norm': 0.45197737216949463, 'learning_rate': 1.9102897131471536e-05, 'epoch': 0.16}
{'loss': 1.0379, 'grad_norm': 0.39921754598617554, 'learning_rate': 1.9100315970091088e-05, 'epoch': 0.16}
{'loss': 0.9665, 'grad_norm': 0.4041367471218109, 'learning_rate': 1.9097731275654645e-05, 'epoch': 0.16}
{'loss': 1.1381, 'grad_norm': 0.4950582981109619, 'learning_rate': 1.909514304916568e-05, 'epoch': 0.16}
{'loss': 1.0285, 'grad_norm': 0.394981324672699, 'learning_rate': 1.9092551291629026e-05, 'epoch': 0.16}
{'loss': 0.9753, 'grad_norm': 0.4034373164176941, 'learning_rate': 1.9089956004050893e-05, 'epoch': 0.16}
{'loss': 0.971, 'grad_norm': 0.41354408860206604, 'learning_rate': 1.908735718743887e-05, 'epoch': 0.16}
{'loss': 0.98, 'grad_norm': 0.4569517970085144, 'learning_rate': 1.908475484280189e-05, 'epoch': 0.16}
{'loss': 0.9615, 'grad_norm': 0.41077637672424316, 'learning_rate': 1.908214897115029e-05, 'epoch': 0.16}
{'loss': 0.941, 'grad_norm': 0.45046496391296387, 'learning_rate': 1.907953957349575e-05, 'epoch': 0.16}
{'loss': 1.0427, 'grad_norm': 0.4148745834827423, 'learning_rate': 1.907692665085133e-05, 'epoch': 0.16}
{'loss': 0.9572, 'grad_norm': 0.42406532168388367, 'learning_rate': 1.9074310204231457e-05, 'epoch': 0.16}
{'loss': 0.9521, 'grad_norm': 0.42963576316833496, 'learning_rate': 1.9071690234651923e-05, 'epoch': 0.16}
{'loss': 1.0413, 'grad_norm': 0.4728461503982544, 'learning_rate': 1.9069066743129893e-05, 'epoch': 0.16}
{'loss': 0.9974, 'grad_norm': 0.45773428678512573, 'learning_rate': 1.90664397306839e-05, 'epoch': 0.16}
{'loss': 1.0053, 'grad_norm': 0.4347352385520935, 'learning_rate': 1.9063809198333832e-05, 'epoch': 0.16}
{'loss': 1.0708, 'grad_norm': 0.453116238117218, 'learning_rate': 1.9061175147100957e-05, 'epoch': 0.17}
{'loss': 1.0111, 'grad_norm': 0.4290972948074341, 'learning_rate': 1.905853757800791e-05, 'epoch': 0.17}
{'loss': 1.0733, 'grad_norm': 0.4491947889328003, 'learning_rate': 1.9055896492078675e-05, 'epoch': 0.17}
{'loss': 0.9168, 'grad_norm': 0.410753071308136, 'learning_rate': 1.905325189033862e-05, 'epoch': 0.17}
{'loss': 1.0744, 'grad_norm': 0.4212597906589508, 'learning_rate': 1.905060377381447e-05, 'epoch': 0.17}
{'loss': 1.0199, 'grad_norm': 0.4253508448600769, 'learning_rate': 1.904795214353431e-05, 'epoch': 0.17}
{'loss': 1.069, 'grad_norm': 0.438239723443985, 'learning_rate': 1.90452970005276e-05, 'epoch': 0.17}
{'loss': 0.9862, 'grad_norm': 0.4164283275604248, 'learning_rate': 1.9042638345825155e-05, 'epoch': 0.17}
{'loss': 0.9868, 'grad_norm': 0.4006142020225525, 'learning_rate': 1.9039976180459158e-05, 'epoch': 0.17}
{'loss': 1.0287, 'grad_norm': 0.40877413749694824, 'learning_rate': 1.9037310505463153e-05, 'epoch': 0.17}
{'loss': 1.0219, 'grad_norm': 0.4035244584083557, 'learning_rate': 1.9034641321872043e-05, 'epoch': 0.17}
{'loss': 0.9418, 'grad_norm': 0.31559839844703674, 'learning_rate': 1.9031968630722104e-05, 'epoch': 0.17}
{'loss': 1.0257, 'grad_norm': 0.4152871072292328, 'learning_rate': 1.902929243305096e-05, 'epoch': 0.17}
{'loss': 1.0208, 'grad_norm': 0.43671146035194397, 'learning_rate': 1.902661272989761e-05, 'epoch': 0.17}
{'loss': 1.0433, 'grad_norm': 0.481228768825531, 'learning_rate': 1.9023929522302394e-05, 'epoch': 0.17}
{'loss': 0.9923, 'grad_norm': 0.4129261374473572, 'learning_rate': 1.9021242811307044e-05, 'epoch': 0.17}
Error with image file is truncated (32 bytes not processed)
{'loss': 0.9871, 'grad_norm': 0.42384618520736694, 'learning_rate': 1.901855259795462e-05, 'epoch': 0.17}
{'loss': 0.967, 'grad_norm': 0.4122689366340637, 'learning_rate': 1.9015858883289556e-05, 'epoch': 0.17}
{'loss': 1.0292, 'grad_norm': 0.4155072271823883, 'learning_rate': 1.9013161668357655e-05, 'epoch': 0.17}
{'loss': 1.0789, 'grad_norm': 0.45294538140296936, 'learning_rate': 1.901046095420606e-05, 'epoch': 0.17}
{'loss': 0.9966, 'grad_norm': 0.40117859840393066, 'learning_rate': 1.9007756741883284e-05, 'epoch': 0.17}
{'loss': 0.9734, 'grad_norm': 0.37334227561950684, 'learning_rate': 1.9005049032439193e-05, 'epoch': 0.17}
{'loss': 1.058, 'grad_norm': 0.4351469576358795, 'learning_rate': 1.9002337826925012e-05, 'epoch': 0.17}
{'loss': 0.9084, 'grad_norm': 0.38724684715270996, 'learning_rate': 1.899962312639333e-05, 'epoch': 0.17}
{'loss': 0.9622, 'grad_norm': 0.32633349299430847, 'learning_rate': 1.8996904931898085e-05, 'epoch': 0.17}
{'loss': 1.0733, 'grad_norm': 0.43399402499198914, 'learning_rate': 1.899418324449457e-05, 'epoch': 0.17}
{'loss': 0.9916, 'grad_norm': 0.4111140966415405, 'learning_rate': 1.8991458065239444e-05, 'epoch': 0.17}
{'loss': 0.9177, 'grad_norm': 0.28096187114715576, 'learning_rate': 1.8988729395190712e-05, 'epoch': 0.17}
{'loss': 1.0298, 'grad_norm': 0.38102149963378906, 'learning_rate': 1.8985997235407735e-05, 'epoch': 0.17}
{'loss': 0.9486, 'grad_norm': 0.39753904938697815, 'learning_rate': 1.898326158695124e-05, 'epoch': 0.17}
{'loss': 1.0358, 'grad_norm': 0.43101853132247925, 'learning_rate': 1.8980522450883287e-05, 'epoch': 0.17}
{'loss': 0.8948, 'grad_norm': 0.3959154486656189, 'learning_rate': 1.8977779828267314e-05, 'epoch': 0.17}
{'loss': 1.067, 'grad_norm': 0.4499163031578064, 'learning_rate': 1.8975033720168094e-05, 'epoch': 0.17}
{'loss': 1.0054, 'grad_norm': 0.39897236227989197, 'learning_rate': 1.897228412765177e-05, 'epoch': 0.17}
{'loss': 0.9462, 'grad_norm': 0.4387403130531311, 'learning_rate': 1.896953105178582e-05, 'epoch': 0.17}
{'loss': 0.9133, 'grad_norm': 0.41663092374801636, 'learning_rate': 1.8966774493639084e-05, 'epoch': 0.17}
{'loss': 1.0236, 'grad_norm': 0.40280774235725403, 'learning_rate': 1.896401445428176e-05, 'epoch': 0.17}
{'loss': 1.0243, 'grad_norm': 0.41792523860931396, 'learning_rate': 1.896125093478538e-05, 'epoch': 0.17}
{'loss': 1.0843, 'grad_norm': 0.47462359070777893, 'learning_rate': 1.895848393622284e-05, 'epoch': 0.17}
{'loss': 0.8878, 'grad_norm': 0.4168910086154938, 'learning_rate': 1.895571345966839e-05, 'epoch': 0.17}
{'loss': 1.0166, 'grad_norm': 0.40605252981185913, 'learning_rate': 1.8952939506197622e-05, 'epoch': 0.17}
{'loss': 0.9318, 'grad_norm': 0.4805721342563629, 'learning_rate': 1.8950162076887477e-05, 'epoch': 0.17}
{'loss': 1.042, 'grad_norm': 0.4415493607521057, 'learning_rate': 1.894738117281625e-05, 'epoch': 0.17}
{'loss': 0.9487, 'grad_norm': 0.41716820001602173, 'learning_rate': 1.8944596795063584e-05, 'epoch': 0.17}
{'loss': 0.9573, 'grad_norm': 0.41607919335365295, 'learning_rate': 1.894180894471047e-05, 'epoch': 0.17}
{'loss': 1.0685, 'grad_norm': 0.4584035277366638, 'learning_rate': 1.8939017622839253e-05, 'epoch': 0.17}
{'loss': 1.0085, 'grad_norm': 0.38904592394828796, 'learning_rate': 1.8936222830533613e-05, 'epoch': 0.17}
{'loss': 1.0083, 'grad_norm': 0.4458966553211212, 'learning_rate': 1.8933424568878586e-05, 'epoch': 0.17}
{'loss': 1.0825, 'grad_norm': 0.48868271708488464, 'learning_rate': 1.8930622838960555e-05, 'epoch': 0.17}
{'loss': 1.0563, 'grad_norm': 0.44070151448249817, 'learning_rate': 1.8927817641867244e-05, 'epoch': 0.17}
{'loss': 1.0122, 'grad_norm': 0.41351327300071716, 'learning_rate': 1.8925008978687737e-05, 'epoch': 0.17}
{'loss': 1.0709, 'grad_norm': 0.4424286186695099, 'learning_rate': 1.8922196850512446e-05, 'epoch': 0.17}
{'loss': 1.0157, 'grad_norm': 0.40613725781440735, 'learning_rate': 1.8919381258433135e-05, 'epoch': 0.18}
{'loss': 1.0486, 'grad_norm': 0.47659239172935486, 'learning_rate': 1.8916562203542916e-05, 'epoch': 0.18}
{'loss': 0.9998, 'grad_norm': 0.4203328490257263, 'learning_rate': 1.8913739686936244e-05, 'epoch': 0.18}
{'loss': 0.8952, 'grad_norm': 0.41505011916160583, 'learning_rate': 1.8910913709708918e-05, 'epoch': 0.18}
{'loss': 0.9952, 'grad_norm': 0.4215352237224579, 'learning_rate': 1.8908084272958077e-05, 'epoch': 0.18}
{'loss': 1.0986, 'grad_norm': 0.49972090125083923, 'learning_rate': 1.8905251377782206e-05, 'epoch': 0.18}
{'loss': 0.9714, 'grad_norm': 0.41056716442108154, 'learning_rate': 1.8902415025281136e-05, 'epoch': 0.18}
{'loss': 1.0302, 'grad_norm': 0.469731867313385, 'learning_rate': 1.889957521655603e-05, 'epoch': 0.18}
{'loss': 1.0172, 'grad_norm': 0.4222813844680786, 'learning_rate': 1.8896731952709408e-05, 'epoch': 0.18}
{'loss': 0.9973, 'grad_norm': 0.40367749333381653, 'learning_rate': 1.8893885234845117e-05, 'epoch': 0.18}
{'loss': 1.1028, 'grad_norm': 0.4497271180152893, 'learning_rate': 1.8891035064068354e-05, 'epoch': 0.18}
{'loss': 0.9445, 'grad_norm': 0.4552984833717346, 'learning_rate': 1.888818144148565e-05, 'epoch': 0.18}
{'loss': 0.9747, 'grad_norm': 0.469995379447937, 'learning_rate': 1.888532436820488e-05, 'epoch': 0.18}
{'loss': 0.9981, 'grad_norm': 0.46041303873062134, 'learning_rate': 1.8882463845335263e-05, 'epoch': 0.18}
{'loss': 0.9877, 'grad_norm': 0.4050656259059906, 'learning_rate': 1.8879599873987343e-05, 'epoch': 0.18}
{'loss': 1.0316, 'grad_norm': 0.398153692483902, 'learning_rate': 1.8876732455273022e-05, 'epoch': 0.18}
{'loss': 0.999, 'grad_norm': 0.40414148569107056, 'learning_rate': 1.8873861590305527e-05, 'epoch': 0.18}
{'loss': 1.0477, 'grad_norm': 0.459637314081192, 'learning_rate': 1.8870987280199428e-05, 'epoch': 0.18}
{'loss': 0.9491, 'grad_norm': 0.41712477803230286, 'learning_rate': 1.886810952607063e-05, 'epoch': 0.18}
{'loss': 0.9719, 'grad_norm': 0.3927720785140991, 'learning_rate': 1.8865228329036372e-05, 'epoch': 0.18}
{'loss': 1.013, 'grad_norm': 0.4388538897037506, 'learning_rate': 1.886234369021524e-05, 'epoch': 0.18}
{'loss': 1.0443, 'grad_norm': 0.43728676438331604, 'learning_rate': 1.885945561072715e-05, 'epoch': 0.18}
{'loss': 1.0186, 'grad_norm': 0.42876821756362915, 'learning_rate': 1.885656409169335e-05, 'epoch': 0.18}
{'loss': 0.8466, 'grad_norm': 0.3156774640083313, 'learning_rate': 1.885366913423643e-05, 'epoch': 0.18}
{'loss': 1.0067, 'grad_norm': 0.4163433909416199, 'learning_rate': 1.8850770739480312e-05, 'epoch': 0.18}
{'loss': 1.0822, 'grad_norm': 0.44142627716064453, 'learning_rate': 1.8847868908550252e-05, 'epoch': 0.18}
{'loss': 1.0148, 'grad_norm': 0.4305114150047302, 'learning_rate': 1.8844963642572837e-05, 'epoch': 0.18}
{'loss': 0.9221, 'grad_norm': 0.3235132396221161, 'learning_rate': 1.8842054942676e-05, 'epoch': 0.18}
{'loss': 0.9589, 'grad_norm': 0.4153977930545807, 'learning_rate': 1.8839142809988987e-05, 'epoch': 0.18}
{'loss': 0.988, 'grad_norm': 0.405793696641922, 'learning_rate': 1.88362272456424e-05, 'epoch': 0.18}
{'loss': 1.0089, 'grad_norm': 0.35202282667160034, 'learning_rate': 1.8833308250768153e-05, 'epoch': 0.18}
{'loss': 0.9537, 'grad_norm': 0.3593457341194153, 'learning_rate': 1.8830385826499507e-05, 'epoch': 0.18}
{'loss': 1.099, 'grad_norm': 0.45512765645980835, 'learning_rate': 1.882745997397104e-05, 'epoch': 0.18}
{'loss': 0.9868, 'grad_norm': 0.4441176652908325, 'learning_rate': 1.8824530694318675e-05, 'epoch': 0.18}
{'loss': 0.9869, 'grad_norm': 0.42522698640823364, 'learning_rate': 1.882159798867966e-05, 'epoch': 0.18}
{'loss': 1.0883, 'grad_norm': 0.4246460497379303, 'learning_rate': 1.8818661858192562e-05, 'epoch': 0.18}
{'loss': 0.9971, 'grad_norm': 0.44319894909858704, 'learning_rate': 1.88157223039973e-05, 'epoch': 0.18}
{'loss': 1.0241, 'grad_norm': 0.3459124267101288, 'learning_rate': 1.8812779327235106e-05, 'epoch': 0.18}
{'loss': 0.9927, 'grad_norm': 0.4216682016849518, 'learning_rate': 1.880983292904854e-05, 'epoch': 0.18}
{'loss': 1.0724, 'grad_norm': 0.4445556700229645, 'learning_rate': 1.88068831105815e-05, 'epoch': 0.18}
{'loss': 1.1007, 'grad_norm': 0.4860994815826416, 'learning_rate': 1.8803929872979214e-05, 'epoch': 0.18}
{'loss': 1.0376, 'grad_norm': 0.3923422396183014, 'learning_rate': 1.8800973217388215e-05, 'epoch': 0.18}
{'loss': 1.0309, 'grad_norm': 0.43001341819763184, 'learning_rate': 1.879801314495639e-05, 'epoch': 0.18}
{'loss': 1.0884, 'grad_norm': 0.4115165174007416, 'learning_rate': 1.879504965683294e-05, 'epoch': 0.18}
{'loss': 0.9718, 'grad_norm': 0.4398670792579651, 'learning_rate': 1.8792082754168385e-05, 'epoch': 0.18}
{'loss': 0.9269, 'grad_norm': 0.43964046239852905, 'learning_rate': 1.878911243811459e-05, 'epoch': 0.18}
{'loss': 1.0689, 'grad_norm': 0.47533702850341797, 'learning_rate': 1.8786138709824726e-05, 'epoch': 0.18}
{'loss': 0.8871, 'grad_norm': 0.39625436067581177, 'learning_rate': 1.8783161570453295e-05, 'epoch': 0.18}
{'loss': 0.9987, 'grad_norm': 0.41138485074043274, 'learning_rate': 1.878018102115614e-05, 'epoch': 0.18}
{'loss': 1.0178, 'grad_norm': 0.4032477140426636, 'learning_rate': 1.8777197063090394e-05, 'epoch': 0.18}
{'loss': 1.1148, 'grad_norm': 0.47606807947158813, 'learning_rate': 1.877420969741454e-05, 'epoch': 0.18}
{'loss': 0.9832, 'grad_norm': 0.3958694636821747, 'learning_rate': 1.877121892528838e-05, 'epoch': 0.18}
{'loss': 0.9759, 'grad_norm': 0.41242119669914246, 'learning_rate': 1.876822474787303e-05, 'epoch': 0.19}
{'loss': 1.0901, 'grad_norm': 0.43322327733039856, 'learning_rate': 1.8765227166330933e-05, 'epoch': 0.19}
{'loss': 1.0389, 'grad_norm': 0.4456825852394104, 'learning_rate': 1.8762226181825857e-05, 'epoch': 0.19}
{'loss': 0.9954, 'grad_norm': 0.35010838508605957, 'learning_rate': 1.875922179552288e-05, 'epoch': 0.19}
{'loss': 1.0598, 'grad_norm': 0.44170430302619934, 'learning_rate': 1.875621400858842e-05, 'epoch': 0.19}
{'loss': 0.9539, 'grad_norm': 0.35020962357521057, 'learning_rate': 1.875320282219019e-05, 'epoch': 0.19}
{'loss': 1.0013, 'grad_norm': 0.42337262630462646, 'learning_rate': 1.8750188237497247e-05, 'epoch': 0.19}
{'loss': 1.0954, 'grad_norm': 0.45415210723876953, 'learning_rate': 1.874717025567995e-05, 'epoch': 0.19}
{'loss': 0.9431, 'grad_norm': 0.34773868322372437, 'learning_rate': 1.874414887790999e-05, 'epoch': 0.19}
{'loss': 0.9758, 'grad_norm': 0.4390297830104828, 'learning_rate': 1.8741124105360363e-05, 'epoch': 0.19}
{'loss': 0.9813, 'grad_norm': 0.3965737521648407, 'learning_rate': 1.873809593920539e-05, 'epoch': 0.19}
{'loss': 1.036, 'grad_norm': 0.39936211705207825, 'learning_rate': 1.8735064380620717e-05, 'epoch': 0.19}
{'loss': 1.0158, 'grad_norm': 0.4107351005077362, 'learning_rate': 1.873202943078329e-05, 'epoch': 0.19}
{'loss': 0.9442, 'grad_norm': 0.35423538088798523, 'learning_rate': 1.8728991090871387e-05, 'epoch': 0.19}
{'loss': 0.926, 'grad_norm': 0.4159512221813202, 'learning_rate': 1.8725949362064596e-05, 'epoch': 0.19}
{'loss': 1.0658, 'grad_norm': 0.510258674621582, 'learning_rate': 1.8722904245543817e-05, 'epoch': 0.19}
{'loss': 1.0543, 'grad_norm': 0.41469764709472656, 'learning_rate': 1.871985574249127e-05, 'epoch': 0.19}
{'loss': 1.0416, 'grad_norm': 0.44052010774612427, 'learning_rate': 1.8716803854090495e-05, 'epoch': 0.19}
{'loss': 1.053, 'grad_norm': 0.45349451899528503, 'learning_rate': 1.8713748581526334e-05, 'epoch': 0.19}
{'loss': 1.0043, 'grad_norm': 0.44677606225013733, 'learning_rate': 1.871068992598495e-05, 'epoch': 0.19}
{'loss': 1.0408, 'grad_norm': 0.3646797835826874, 'learning_rate': 1.8707627888653816e-05, 'epoch': 0.19}
{'loss': 1.0534, 'grad_norm': 0.4473390281200409, 'learning_rate': 1.8704562470721728e-05, 'epoch': 0.19}
{'loss': 0.97, 'grad_norm': 0.4170054793357849, 'learning_rate': 1.870149367337878e-05, 'epoch': 0.19}
{'loss': 1.134, 'grad_norm': 0.4015049636363983, 'learning_rate': 1.8698421497816386e-05, 'epoch': 0.19}
{'loss': 1.0547, 'grad_norm': 0.3467113673686981, 'learning_rate': 1.869534594522727e-05, 'epoch': 0.19}
{'loss': 1.0294, 'grad_norm': 0.4233798384666443, 'learning_rate': 1.8692267016805473e-05, 'epoch': 0.19}
{'loss': 0.9796, 'grad_norm': 0.37807390093803406, 'learning_rate': 1.8689184713746333e-05, 'epoch': 0.19}
{'loss': 0.9615, 'grad_norm': 0.4124746322631836, 'learning_rate': 1.868609903724651e-05, 'epoch': 0.19}
{'loss': 0.9909, 'grad_norm': 0.4277338683605194, 'learning_rate': 1.8683009988503972e-05, 'epoch': 0.19}
{'loss': 1.1078, 'grad_norm': 0.501694917678833, 'learning_rate': 1.867991756871799e-05, 'epoch': 0.19}
{'loss': 0.9789, 'grad_norm': 0.3505493104457855, 'learning_rate': 1.867682177908915e-05, 'epoch': 0.19}
{'loss': 1.019, 'grad_norm': 0.4131557047367096, 'learning_rate': 1.867372262081934e-05, 'epoch': 0.19}
{'loss': 1.052, 'grad_norm': 0.41138750314712524, 'learning_rate': 1.8670620095111766e-05, 'epoch': 0.19}
{'loss': 0.9612, 'grad_norm': 0.4256047010421753, 'learning_rate': 1.8667514203170934e-05, 'epoch': 0.19}
{'loss': 0.9097, 'grad_norm': 0.37756258249282837, 'learning_rate': 1.8664404946202658e-05, 'epoch': 0.19}
{'loss': 1.0169, 'grad_norm': 0.4094494879245758, 'learning_rate': 1.8661292325414058e-05, 'epoch': 0.19}
{'loss': 0.9961, 'grad_norm': 0.41071608662605286, 'learning_rate': 1.865817634201356e-05, 'epoch': 0.19}
{'loss': 1.0185, 'grad_norm': 0.48453259468078613, 'learning_rate': 1.8655056997210893e-05, 'epoch': 0.19}
{'loss': 0.9862, 'grad_norm': 0.4202328324317932, 'learning_rate': 1.8651934292217097e-05, 'epoch': 0.19}
{'loss': 1.0025, 'grad_norm': 0.448547899723053, 'learning_rate': 1.864880822824452e-05, 'epoch': 0.19}
{'loss': 1.01, 'grad_norm': 0.4422158896923065, 'learning_rate': 1.8645678806506795e-05, 'epoch': 0.19}
{'loss': 1.0053, 'grad_norm': 0.41360199451446533, 'learning_rate': 1.864254602821888e-05, 'epoch': 0.19}
{'loss': 1.0129, 'grad_norm': 0.419733464717865, 'learning_rate': 1.8639409894597026e-05, 'epoch': 0.19}
{'loss': 1.0189, 'grad_norm': 0.4199140667915344, 'learning_rate': 1.8636270406858786e-05, 'epoch': 0.19}
{'loss': 1.1054, 'grad_norm': 0.4462531805038452, 'learning_rate': 1.8633127566223023e-05, 'epoch': 0.19}
{'loss': 1.038, 'grad_norm': 0.4148557782173157, 'learning_rate': 1.862998137390989e-05, 'epoch': 0.19}
{'loss': 0.9643, 'grad_norm': 0.4046401381492615, 'learning_rate': 1.8626831831140845e-05, 'epoch': 0.19}
{'loss': 0.9183, 'grad_norm': 0.4438769221305847, 'learning_rate': 1.8623678939138652e-05, 'epoch': 0.19}
{'loss': 0.9199, 'grad_norm': 0.3683610260486603, 'learning_rate': 1.8620522699127374e-05, 'epoch': 0.19}
{'loss': 0.8911, 'grad_norm': 0.4041861295700073, 'learning_rate': 1.8617363112332376e-05, 'epoch': 0.19}
{'loss': 1.0623, 'grad_norm': 0.46348610520362854, 'learning_rate': 1.8614200179980307e-05, 'epoch': 0.19}
{'loss': 1.0837, 'grad_norm': 0.49585965275764465, 'learning_rate': 1.8611033903299136e-05, 'epoch': 0.19}
{'loss': 0.9632, 'grad_norm': 0.44406479597091675, 'learning_rate': 1.8607864283518116e-05, 'epoch': 0.2}
{'loss': 0.9661, 'grad_norm': 0.35797280073165894, 'learning_rate': 1.8604691321867804e-05, 'epoch': 0.2}
{'loss': 0.9808, 'grad_norm': 0.4207358658313751, 'learning_rate': 1.8601515019580053e-05, 'epoch': 0.2}
{'loss': 0.98, 'grad_norm': 0.41405969858169556, 'learning_rate': 1.8598335377888012e-05, 'epoch': 0.2}
{'loss': 0.9804, 'grad_norm': 0.4306982159614563, 'learning_rate': 1.8595152398026128e-05, 'epoch': 0.2}
{'loss': 0.9793, 'grad_norm': 0.4368756413459778, 'learning_rate': 1.8591966081230142e-05, 'epoch': 0.2}
{'loss': 0.9619, 'grad_norm': 0.4266969561576843, 'learning_rate': 1.8588776428737095e-05, 'epoch': 0.2}
{'loss': 0.9915, 'grad_norm': 0.43919605016708374, 'learning_rate': 1.858558344178532e-05, 'epoch': 0.2}
{'loss': 0.901, 'grad_norm': 0.42536139488220215, 'learning_rate': 1.8582387121614437e-05, 'epoch': 0.2}
{'loss': 0.9462, 'grad_norm': 0.34192606806755066, 'learning_rate': 1.857918746946538e-05, 'epoch': 0.2}
{'loss': 0.9699, 'grad_norm': 0.43786677718162537, 'learning_rate': 1.8575984486580353e-05, 'epoch': 0.2}
{'loss': 1.1097, 'grad_norm': 0.4685971140861511, 'learning_rate': 1.857277817420287e-05, 'epoch': 0.2}
{'loss': 0.9293, 'grad_norm': 0.3889278471469879, 'learning_rate': 1.8569568533577727e-05, 'epoch': 0.2}
{'loss': 1.0297, 'grad_norm': 0.4279000163078308, 'learning_rate': 1.8566355565951023e-05, 'epoch': 0.2}
{'loss': 0.985, 'grad_norm': 0.39218655228614807, 'learning_rate': 1.8563139272570142e-05, 'epoch': 0.2}
{'loss': 1.0186, 'grad_norm': 0.46770793199539185, 'learning_rate': 1.8559919654683756e-05, 'epoch': 0.2}
{'loss': 1.0951, 'grad_norm': 0.44407883286476135, 'learning_rate': 1.8556696713541833e-05, 'epoch': 0.2}
{'loss': 1.0777, 'grad_norm': 0.44278132915496826, 'learning_rate': 1.855347045039563e-05, 'epoch': 0.2}
{'loss': 1.0315, 'grad_norm': 0.45935243368148804, 'learning_rate': 1.8550240866497697e-05, 'epoch': 0.2}
{'loss': 0.9561, 'grad_norm': 0.4352576434612274, 'learning_rate': 1.854700796310186e-05, 'epoch': 0.2}
{'loss': 0.9169, 'grad_norm': 0.408216267824173, 'learning_rate': 1.8543771741463254e-05, 'epoch': 0.2}
{'loss': 1.037, 'grad_norm': 0.4175971448421478, 'learning_rate': 1.8540532202838286e-05, 'epoch': 0.2}
{'loss': 0.9983, 'grad_norm': 0.4202600121498108, 'learning_rate': 1.8537289348484658e-05, 'epoch': 0.2}
{'loss': 1.0868, 'grad_norm': 0.46884021162986755, 'learning_rate': 1.8534043179661357e-05, 'epoch': 0.2}
{'loss': 1.0692, 'grad_norm': 0.43560245633125305, 'learning_rate': 1.8530793697628658e-05, 'epoch': 0.2}
{'loss': 0.9228, 'grad_norm': 0.451270192861557, 'learning_rate': 1.8527540903648122e-05, 'epoch': 0.2}
{'loss': 0.9453, 'grad_norm': 0.41611775755882263, 'learning_rate': 1.8524284798982595e-05, 'epoch': 0.2}
{'loss': 1.0273, 'grad_norm': 0.43022415041923523, 'learning_rate': 1.852102538489621e-05, 'epoch': 0.2}
{'loss': 0.9852, 'grad_norm': 0.35102951526641846, 'learning_rate': 1.8517762662654383e-05, 'epoch': 0.2}
{'loss': 1.0512, 'grad_norm': 0.43351754546165466, 'learning_rate': 1.851449663352381e-05, 'epoch': 0.2}
{'loss': 0.9895, 'grad_norm': 0.40536797046661377, 'learning_rate': 1.851122729877249e-05, 'epoch': 0.2}
{'loss': 0.9892, 'grad_norm': 0.3682297170162201, 'learning_rate': 1.8507954659669677e-05, 'epoch': 0.2}
{'loss': 0.9938, 'grad_norm': 0.40706345438957214, 'learning_rate': 1.850467871748593e-05, 'epoch': 0.2}
{'loss': 1.0271, 'grad_norm': 0.36077550053596497, 'learning_rate': 1.850139947349308e-05, 'epoch': 0.2}
{'loss': 1.056, 'grad_norm': 0.44806310534477234, 'learning_rate': 1.8498116928964244e-05, 'epoch': 0.2}
{'loss': 0.9914, 'grad_norm': 0.40471386909484863, 'learning_rate': 1.849483108517381e-05, 'epoch': 0.2}
{'loss': 1.0105, 'grad_norm': 0.4616903066635132, 'learning_rate': 1.849154194339747e-05, 'epoch': 0.2}
{'loss': 0.9998, 'grad_norm': 0.42308279871940613, 'learning_rate': 1.8488249504912173e-05, 'epoch': 0.2}
{'loss': 1.0008, 'grad_norm': 0.42125335335731506, 'learning_rate': 1.8484953770996163e-05, 'epoch': 0.2}
{'loss': 1.0198, 'grad_norm': 0.43644583225250244, 'learning_rate': 1.848165474292895e-05, 'epoch': 0.2}
{'loss': 0.9305, 'grad_norm': 0.42128539085388184, 'learning_rate': 1.8478352421991334e-05, 'epoch': 0.2}
{'loss': 1.0049, 'grad_norm': 0.40998974442481995, 'learning_rate': 1.847504680946539e-05, 'epoch': 0.2}
{'loss': 1.043, 'grad_norm': 0.47056692838668823, 'learning_rate': 1.847173790663447e-05, 'epoch': 0.2}
{'loss': 1.0307, 'grad_norm': 0.421235054731369, 'learning_rate': 1.8468425714783206e-05, 'epoch': 0.2}
{'loss': 1.0544, 'grad_norm': 0.4309694170951843, 'learning_rate': 1.84651102351975e-05, 'epoch': 0.2}
{'loss': 0.954, 'grad_norm': 0.3270047605037689, 'learning_rate': 1.846179146916454e-05, 'epoch': 0.2}
{'loss': 0.9102, 'grad_norm': 0.33615487813949585, 'learning_rate': 1.8458469417972783e-05, 'epoch': 0.2}
{'loss': 1.0112, 'grad_norm': 0.41762590408325195, 'learning_rate': 1.8455144082911965e-05, 'epoch': 0.2}
{'loss': 1.0071, 'grad_norm': 0.4264666736125946, 'learning_rate': 1.8451815465273097e-05, 'epoch': 0.2}
{'loss': 1.0147, 'grad_norm': 0.3933914601802826, 'learning_rate': 1.8448483566348456e-05, 'epoch': 0.2}
{'loss': 0.9979, 'grad_norm': 0.45601293444633484, 'learning_rate': 1.8445148387431605e-05, 'epoch': 0.2}
{'loss': 1.0714, 'grad_norm': 0.4192291498184204, 'learning_rate': 1.8441809929817382e-05, 'epoch': 0.2}
{'loss': 0.9442, 'grad_norm': 0.4462430775165558, 'learning_rate': 1.8438468194801876e-05, 'epoch': 0.21}
{'loss': 1.1007, 'grad_norm': 0.4410993158817291, 'learning_rate': 1.8435123183682475e-05, 'epoch': 0.21}
{'loss': 1.0229, 'grad_norm': 0.43468061089515686, 'learning_rate': 1.8431774897757824e-05, 'epoch': 0.21}
{'loss': 1.0529, 'grad_norm': 0.3863648772239685, 'learning_rate': 1.8428423338327847e-05, 'epoch': 0.21}
{'loss': 1.1359, 'grad_norm': 0.43394482135772705, 'learning_rate': 1.8425068506693727e-05, 'epoch': 0.21}
{'loss': 1.0365, 'grad_norm': 0.4622071087360382, 'learning_rate': 1.842171040415793e-05, 'epoch': 0.21}
{'loss': 1.0399, 'grad_norm': 0.42005613446235657, 'learning_rate': 1.8418349032024185e-05, 'epoch': 0.21}
{'loss': 1.0342, 'grad_norm': 0.41918644309043884, 'learning_rate': 1.8414984391597492e-05, 'epoch': 0.21}
{'loss': 1.032, 'grad_norm': 0.419293075799942, 'learning_rate': 1.8411616484184126e-05, 'epoch': 0.21}
{'loss': 1.0111, 'grad_norm': 0.3857780694961548, 'learning_rate': 1.8408245311091618e-05, 'epoch': 0.21}
{'loss': 0.9916, 'grad_norm': 0.40242040157318115, 'learning_rate': 1.8404870873628774e-05, 'epoch': 0.21}
{'loss': 1.0176, 'grad_norm': 0.36714643239974976, 'learning_rate': 1.8401493173105675e-05, 'epoch': 0.21}
{'loss': 0.9889, 'grad_norm': 0.4369126856327057, 'learning_rate': 1.8398112210833648e-05, 'epoch': 0.21}
{'loss': 1.016, 'grad_norm': 0.39713647961616516, 'learning_rate': 1.8394727988125308e-05, 'epoch': 0.21}
{'loss': 1.0252, 'grad_norm': 0.4448290765285492, 'learning_rate': 1.8391340506294524e-05, 'epoch': 0.21}
{'loss': 1.074, 'grad_norm': 0.4250127077102661, 'learning_rate': 1.8387949766656434e-05, 'epoch': 0.21}
{'loss': 0.9729, 'grad_norm': 0.36746707558631897, 'learning_rate': 1.8384555770527438e-05, 'epoch': 0.21}
{'loss': 0.9726, 'grad_norm': 0.4322353005409241, 'learning_rate': 1.8381158519225204e-05, 'epoch': 0.21}
{'loss': 1.0228, 'grad_norm': 0.4030570089817047, 'learning_rate': 1.8377758014068662e-05, 'epoch': 0.21}
{'loss': 1.0382, 'grad_norm': 0.5127428770065308, 'learning_rate': 1.8374354256378e-05, 'epoch': 0.21}
{'loss': 0.916, 'grad_norm': 0.4328828752040863, 'learning_rate': 1.837094724747468e-05, 'epoch': 0.21}
{'loss': 1.0247, 'grad_norm': 0.4512344300746918, 'learning_rate': 1.8367536988681422e-05, 'epoch': 0.21}
{'loss': 0.9668, 'grad_norm': 0.33957651257514954, 'learning_rate': 1.83641234813222e-05, 'epoch': 0.21}
{'loss': 1.0635, 'grad_norm': 0.4266812205314636, 'learning_rate': 1.8360706726722253e-05, 'epoch': 0.21}
{'loss': 0.9925, 'grad_norm': 0.4239480197429657, 'learning_rate': 1.835728672620809e-05, 'epoch': 0.21}
{'loss': 1.0574, 'grad_norm': 0.367708295583725, 'learning_rate': 1.8353863481107473e-05, 'epoch': 0.21}
{'loss': 0.9397, 'grad_norm': 0.40650859475135803, 'learning_rate': 1.835043699274942e-05, 'epoch': 0.21}
{'loss': 1.0098, 'grad_norm': 0.42686551809310913, 'learning_rate': 1.8347007262464206e-05, 'epoch': 0.21}
{'loss': 1.0589, 'grad_norm': 0.3909609615802765, 'learning_rate': 1.8343574291583385e-05, 'epoch': 0.21}
{'loss': 1.0362, 'grad_norm': 0.4084170162677765, 'learning_rate': 1.8340138081439743e-05, 'epoch': 0.21}
{'loss': 0.9899, 'grad_norm': 0.3585118353366852, 'learning_rate': 1.833669863336734e-05, 'epoch': 0.21}
{'loss': 1.0657, 'grad_norm': 0.44572746753692627, 'learning_rate': 1.833325594870148e-05, 'epoch': 0.21}
{'loss': 1.0467, 'grad_norm': 0.41174134612083435, 'learning_rate': 1.8329810028778747e-05, 'epoch': 0.21}
{'loss': 0.9779, 'grad_norm': 0.3789272606372833, 'learning_rate': 1.8326360874936952e-05, 'epoch': 0.21}
{'loss': 1.0783, 'grad_norm': 0.45609644055366516, 'learning_rate': 1.8322908488515182e-05, 'epoch': 0.21}
{'loss': 0.9698, 'grad_norm': 0.39241719245910645, 'learning_rate': 1.8319452870853772e-05, 'epoch': 0.21}
{'loss': 1.0883, 'grad_norm': 0.47648754715919495, 'learning_rate': 1.8315994023294306e-05, 'epoch': 0.21}
{'loss': 0.9246, 'grad_norm': 0.424689382314682, 'learning_rate': 1.8312531947179634e-05, 'epoch': 0.21}
