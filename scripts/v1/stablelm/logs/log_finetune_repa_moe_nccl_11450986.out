VERSION: 1.3 (NCCL-only)
MASTER_PORT: 49609
MASTER_ADDR: g003
WORLD_SIZE: 4
RANK: 0
LOCAL_RANK: 0
NODE_RANK: 0
SLURM_NNODES: 4
SLURM_NTASKS: 4
SLURM_PROCID: 0
SLURM_LOCALID: 0
[2025-09-05 17:54:55,015] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-05 17:54:55,015] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-05 17:54:55,015] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-05 17:54:55,015] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-09-05 17:55:03,509] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-05 17:55:03,509] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-05 17:55:03,509] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-05 17:55:03,509] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
[2025-09-05 17:55:08,677] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-05 17:55:08,677] [INFO] [comm.py:852:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-09-05 17:55:08,677] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-05 17:55:08,677] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-05 17:55:08,677] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-05 17:55:09,743] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 17:55:09,745] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 17:55:09,747] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 17:55:09,749] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 17:55:09,750] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 17:55:09,753] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 17:55:09,754] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 17:55:09,756] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 17:55:09,758] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 17:55:09,759] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 17:55:09,761] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 17:55:09,762] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
Vision encoder and proj init.

RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
Vision encoder and proj init.

RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
Vision encoder and proj init.

RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
Vision encoder and proj init.

RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Set initial gated ratio to 1.0
  Total training steps: 20790.0
  Stage 1 (gated ratio reduction): 10395.0 steps
  Stage 2 (post-reparam training): 10395.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 20790.0
  Set initial gated ratio to 1.0
  Stage 1 (gated ratio reduction): 10395.0 steps
  Stage 2 (post-reparam training): 10395.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Total training steps: 20790.0
  Stage 1 (gated ratio reduction): 10395.0 steps
  Stage 2 (post-reparam training): 10395.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Set initial gated ratio to 1.0
  Total training steps: 20790.0
  Stage 1 (gated ratio reduction): 10395.0 steps
  Stage 2 (post-reparam training): 10395.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
NCCL version 2.21.5+cuda12.4
WARNING: tokenization mismatch: 0 vs. 1275. (ignored)
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
{'loss': 1.1232, 'grad_norm': 0.7346453070640564, 'learning_rate': 0.0, 'epoch': 0.0}
{'loss': 0.9433, 'grad_norm': 0.8658612966537476, 'learning_rate': 3.205128205128205e-08, 'epoch': 0.0}
{'loss': 0.968, 'grad_norm': 0.7745547294616699, 'learning_rate': 6.41025641025641e-08, 'epoch': 0.0}
{'loss': 1.1997, 'grad_norm': 0.8405510783195496, 'learning_rate': 9.615384615384617e-08, 'epoch': 0.0}
{'loss': 1.0243, 'grad_norm': 0.7384202480316162, 'learning_rate': 1.282051282051282e-07, 'epoch': 0.0}
{'loss': 0.9746, 'grad_norm': 0.7129664421081543, 'learning_rate': 1.6025641025641025e-07, 'epoch': 0.0}
{'loss': 1.0285, 'grad_norm': 0.7874494791030884, 'learning_rate': 1.9230769230769234e-07, 'epoch': 0.0}
{'loss': 0.8708, 'grad_norm': 0.6497871279716492, 'learning_rate': 2.2435897435897438e-07, 'epoch': 0.0}
{'loss': 0.8756, 'grad_norm': 0.851702868938446, 'learning_rate': 2.564102564102564e-07, 'epoch': 0.0}
{'loss': 0.8216, 'grad_norm': 0.615994393825531, 'learning_rate': 2.884615384615385e-07, 'epoch': 0.0}
{'loss': 1.0776, 'grad_norm': 0.88820880651474, 'learning_rate': 3.205128205128205e-07, 'epoch': 0.0}
{'loss': 0.8811, 'grad_norm': 0.40982866287231445, 'learning_rate': 3.525641025641026e-07, 'epoch': 0.0}
{'loss': 1.0917, 'grad_norm': 0.798264741897583, 'learning_rate': 3.846153846153847e-07, 'epoch': 0.0}
{'loss': 0.9139, 'grad_norm': 0.7052625417709351, 'learning_rate': 4.1666666666666667e-07, 'epoch': 0.0}
{'loss': 0.921, 'grad_norm': 0.7767878770828247, 'learning_rate': 4.4871794871794876e-07, 'epoch': 0.0}
{'loss': 1.071, 'grad_norm': 0.6507918834686279, 'learning_rate': 4.807692307692308e-07, 'epoch': 0.0}
{'loss': 1.0818, 'grad_norm': 0.698488175868988, 'learning_rate': 5.128205128205128e-07, 'epoch': 0.0}
{'loss': 1.2059, 'grad_norm': 0.7766163945198059, 'learning_rate': 5.448717948717949e-07, 'epoch': 0.0}
{'loss': 1.0705, 'grad_norm': 0.8002466559410095, 'learning_rate': 5.76923076923077e-07, 'epoch': 0.0}
{'loss': 0.8728, 'grad_norm': 0.7351226210594177, 'learning_rate': 6.08974358974359e-07, 'epoch': 0.0}
{'loss': 1.0417, 'grad_norm': 0.8805649876594543, 'learning_rate': 6.41025641025641e-07, 'epoch': 0.0}
{'loss': 0.9461, 'grad_norm': 0.44140326976776123, 'learning_rate': 6.730769230769231e-07, 'epoch': 0.0}
{'loss': 0.922, 'grad_norm': 0.745796263217926, 'learning_rate': 7.051282051282052e-07, 'epoch': 0.0}
{'loss': 0.9771, 'grad_norm': 0.7609661817550659, 'learning_rate': 7.371794871794873e-07, 'epoch': 0.0}
{'loss': 1.0417, 'grad_norm': 0.8720885515213013, 'learning_rate': 7.692307692307694e-07, 'epoch': 0.0}
{'loss': 0.9467, 'grad_norm': 0.6266030073165894, 'learning_rate': 8.012820512820515e-07, 'epoch': 0.0}
{'loss': 1.1289, 'grad_norm': 0.888419508934021, 'learning_rate': 8.333333333333333e-07, 'epoch': 0.0}
{'loss': 1.0457, 'grad_norm': 0.7333884835243225, 'learning_rate': 8.653846153846154e-07, 'epoch': 0.0}
{'loss': 1.1499, 'grad_norm': 0.7689816355705261, 'learning_rate': 8.974358974358975e-07, 'epoch': 0.0}
{'loss': 0.9996, 'grad_norm': 0.6405341625213623, 'learning_rate': 9.294871794871796e-07, 'epoch': 0.0}
{'loss': 1.0512, 'grad_norm': 0.716891348361969, 'learning_rate': 9.615384615384617e-07, 'epoch': 0.0}
{'loss': 1.0098, 'grad_norm': 0.8753493428230286, 'learning_rate': 9.935897435897436e-07, 'epoch': 0.0}
{'loss': 0.9238, 'grad_norm': 0.8167503476142883, 'learning_rate': 1.0256410256410257e-06, 'epoch': 0.0}
{'loss': 1.0305, 'grad_norm': 0.7647764086723328, 'learning_rate': 1.0576923076923078e-06, 'epoch': 0.0}
{'loss': 0.9954, 'grad_norm': 0.7764376401901245, 'learning_rate': 1.0897435897435899e-06, 'epoch': 0.0}
{'loss': 0.9569, 'grad_norm': 0.7835245728492737, 'learning_rate': 1.121794871794872e-06, 'epoch': 0.0}
{'loss': 1.0546, 'grad_norm': 0.7557371258735657, 'learning_rate': 1.153846153846154e-06, 'epoch': 0.0}
{'loss': 0.9687, 'grad_norm': 0.8974735736846924, 'learning_rate': 1.185897435897436e-06, 'epoch': 0.0}
{'loss': 1.0566, 'grad_norm': 0.7083285450935364, 'learning_rate': 1.217948717948718e-06, 'epoch': 0.0}
{'loss': 0.8383, 'grad_norm': 0.7361480593681335, 'learning_rate': 1.25e-06, 'epoch': 0.0}
{'loss': 0.961, 'grad_norm': 0.7312727570533752, 'learning_rate': 1.282051282051282e-06, 'epoch': 0.0}
{'loss': 0.9303, 'grad_norm': 0.7442266345024109, 'learning_rate': 1.3141025641025643e-06, 'epoch': 0.0}
{'loss': 1.0113, 'grad_norm': 0.6962597370147705, 'learning_rate': 1.3461538461538462e-06, 'epoch': 0.0}
{'loss': 1.0981, 'grad_norm': 0.7654569745063782, 'learning_rate': 1.3782051282051285e-06, 'epoch': 0.0}
{'loss': 0.9983, 'grad_norm': 0.7963366508483887, 'learning_rate': 1.4102564102564104e-06, 'epoch': 0.0}
{'loss': 0.931, 'grad_norm': 0.6781545877456665, 'learning_rate': 1.4423076923076922e-06, 'epoch': 0.0}
{'loss': 0.9392, 'grad_norm': 0.8807646632194519, 'learning_rate': 1.4743589743589745e-06, 'epoch': 0.0}
{'loss': 0.9953, 'grad_norm': 0.4821305274963379, 'learning_rate': 1.5064102564102564e-06, 'epoch': 0.0}
{'loss': 0.9941, 'grad_norm': 0.694184422492981, 'learning_rate': 1.5384615384615387e-06, 'epoch': 0.0}
{'loss': 0.9607, 'grad_norm': 0.6813448667526245, 'learning_rate': 1.5705128205128206e-06, 'epoch': 0.0}
{'loss': 1.1309, 'grad_norm': 0.7209964990615845, 'learning_rate': 1.602564102564103e-06, 'epoch': 0.0}
{'loss': 0.9954, 'grad_norm': 0.6617453694343567, 'learning_rate': 1.6346153846153848e-06, 'epoch': 0.0}
{'loss': 1.1365, 'grad_norm': 0.7281421422958374, 'learning_rate': 1.6666666666666667e-06, 'epoch': 0.0}
{'loss': 1.0296, 'grad_norm': 0.7587380409240723, 'learning_rate': 1.698717948717949e-06, 'epoch': 0.0}
{'loss': 0.9003, 'grad_norm': 0.8673439621925354, 'learning_rate': 1.7307692307692308e-06, 'epoch': 0.0}
{'loss': 0.9577, 'grad_norm': 0.7661265134811401, 'learning_rate': 1.7628205128205131e-06, 'epoch': 0.0}
{'loss': 0.9192, 'grad_norm': 0.473714143037796, 'learning_rate': 1.794871794871795e-06, 'epoch': 0.0}
{'loss': 1.0058, 'grad_norm': 0.8532356023788452, 'learning_rate': 1.826923076923077e-06, 'epoch': 0.0}
{'loss': 1.1424, 'grad_norm': 0.8935666084289551, 'learning_rate': 1.8589743589743592e-06, 'epoch': 0.0}
{'loss': 1.1551, 'grad_norm': 0.9828083515167236, 'learning_rate': 1.891025641025641e-06, 'epoch': 0.0}
{'loss': 0.9937, 'grad_norm': 0.7202500700950623, 'learning_rate': 1.9230769230769234e-06, 'epoch': 0.0}
{'loss': 0.965, 'grad_norm': 0.6496585607528687, 'learning_rate': 1.9551282051282055e-06, 'epoch': 0.0}
{'loss': 1.1114, 'grad_norm': 0.9430243968963623, 'learning_rate': 1.987179487179487e-06, 'epoch': 0.0}
{'loss': 1.0463, 'grad_norm': 0.7217549085617065, 'learning_rate': 2.0192307692307692e-06, 'epoch': 0.0}
{'loss': 1.0426, 'grad_norm': 0.7796053290367126, 'learning_rate': 2.0512820512820513e-06, 'epoch': 0.0}
{'loss': 0.9943, 'grad_norm': 0.7545168995857239, 'learning_rate': 2.0833333333333334e-06, 'epoch': 0.0}
{'loss': 1.1123, 'grad_norm': 0.7785674929618835, 'learning_rate': 2.1153846153846155e-06, 'epoch': 0.0}
{'loss': 0.7782, 'grad_norm': 0.40166521072387695, 'learning_rate': 2.1474358974358976e-06, 'epoch': 0.0}
{'loss': 0.9291, 'grad_norm': 0.48096200823783875, 'learning_rate': 2.1794871794871797e-06, 'epoch': 0.0}
{'loss': 0.9868, 'grad_norm': 0.7534613013267517, 'learning_rate': 2.211538461538462e-06, 'epoch': 0.0}
{'loss': 1.0268, 'grad_norm': 0.8035364151000977, 'learning_rate': 2.243589743589744e-06, 'epoch': 0.0}
{'loss': 1.0519, 'grad_norm': 0.9338764548301697, 'learning_rate': 2.275641025641026e-06, 'epoch': 0.0}
{'loss': 0.977, 'grad_norm': 0.6699870228767395, 'learning_rate': 2.307692307692308e-06, 'epoch': 0.0}
{'loss': 0.9846, 'grad_norm': 0.7955189347267151, 'learning_rate': 2.3397435897435897e-06, 'epoch': 0.0}
{'loss': 0.8496, 'grad_norm': 0.8569226264953613, 'learning_rate': 2.371794871794872e-06, 'epoch': 0.0}
{'loss': 1.0979, 'grad_norm': 0.869810938835144, 'learning_rate': 2.403846153846154e-06, 'epoch': 0.0}
{'loss': 0.97, 'grad_norm': 0.6150739789009094, 'learning_rate': 2.435897435897436e-06, 'epoch': 0.0}
{'loss': 0.9727, 'grad_norm': 0.7090671062469482, 'learning_rate': 2.467948717948718e-06, 'epoch': 0.0}
{'loss': 1.1236, 'grad_norm': 0.7664570212364197, 'learning_rate': 2.5e-06, 'epoch': 0.0}
{'loss': 0.8992, 'grad_norm': 0.722884476184845, 'learning_rate': 2.5320512820512823e-06, 'epoch': 0.0}
{'loss': 1.0514, 'grad_norm': 0.7612175345420837, 'learning_rate': 2.564102564102564e-06, 'epoch': 0.0}
{'loss': 0.8645, 'grad_norm': 0.7066642045974731, 'learning_rate': 2.5961538461538465e-06, 'epoch': 0.0}
{'loss': 1.1045, 'grad_norm': 0.8040696382522583, 'learning_rate': 2.6282051282051286e-06, 'epoch': 0.0}
{'loss': 0.9356, 'grad_norm': 0.7384250164031982, 'learning_rate': 2.6602564102564107e-06, 'epoch': 0.0}
{'loss': 0.8088, 'grad_norm': 0.7148752808570862, 'learning_rate': 2.6923076923076923e-06, 'epoch': 0.0}
{'loss': 1.0361, 'grad_norm': 0.7278321981430054, 'learning_rate': 2.7243589743589744e-06, 'epoch': 0.0}
{'loss': 1.0447, 'grad_norm': 0.4598863422870636, 'learning_rate': 2.756410256410257e-06, 'epoch': 0.0}
{'loss': 1.0855, 'grad_norm': 0.8985128402709961, 'learning_rate': 2.7884615384615386e-06, 'epoch': 0.0}
{'loss': 0.9563, 'grad_norm': 0.8218650221824646, 'learning_rate': 2.8205128205128207e-06, 'epoch': 0.0}
{'loss': 1.1473, 'grad_norm': 0.7012301087379456, 'learning_rate': 2.852564102564103e-06, 'epoch': 0.0}
{'loss': 0.9616, 'grad_norm': 0.6615275740623474, 'learning_rate': 2.8846153846153845e-06, 'epoch': 0.0}
{'loss': 1.1172, 'grad_norm': 0.7813872694969177, 'learning_rate': 2.916666666666667e-06, 'epoch': 0.0}
{'loss': 1.2915, 'grad_norm': 0.8110423684120178, 'learning_rate': 2.948717948717949e-06, 'epoch': 0.0}
{'loss': 1.1223, 'grad_norm': 0.8036441206932068, 'learning_rate': 2.980769230769231e-06, 'epoch': 0.0}
{'loss': 1.0468, 'grad_norm': 0.7697298526763916, 'learning_rate': 3.012820512820513e-06, 'epoch': 0.0}
{'loss': 0.9131, 'grad_norm': 0.7349860072135925, 'learning_rate': 3.044871794871795e-06, 'epoch': 0.0}
{'loss': 1.0046, 'grad_norm': 0.679222047328949, 'learning_rate': 3.0769230769230774e-06, 'epoch': 0.0}
{'loss': 0.9707, 'grad_norm': 0.7885054349899292, 'learning_rate': 3.108974358974359e-06, 'epoch': 0.0}
{'loss': 1.0252, 'grad_norm': 0.9305999279022217, 'learning_rate': 3.141025641025641e-06, 'epoch': 0.0}
{'loss': 0.8965, 'grad_norm': 0.7150251865386963, 'learning_rate': 3.1730769230769233e-06, 'epoch': 0.0}
{'loss': 1.0496, 'grad_norm': 0.698447585105896, 'learning_rate': 3.205128205128206e-06, 'epoch': 0.0}
{'loss': 0.9705, 'grad_norm': 0.834738552570343, 'learning_rate': 3.2371794871794875e-06, 'epoch': 0.0}
{'loss': 0.9746, 'grad_norm': 0.8178761005401611, 'learning_rate': 3.2692307692307696e-06, 'epoch': 0.0}
{'loss': 1.0356, 'grad_norm': 0.8254198431968689, 'learning_rate': 3.3012820512820517e-06, 'epoch': 0.01}
{'loss': 1.1255, 'grad_norm': 0.6885182857513428, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.01}
{'loss': 0.9752, 'grad_norm': 0.7827274203300476, 'learning_rate': 3.365384615384616e-06, 'epoch': 0.01}
{'loss': 0.9801, 'grad_norm': 0.6227502822875977, 'learning_rate': 3.397435897435898e-06, 'epoch': 0.01}
{'loss': 1.1009, 'grad_norm': 0.7387815117835999, 'learning_rate': 3.4294871794871796e-06, 'epoch': 0.01}
{'loss': 1.1123, 'grad_norm': 0.7853328585624695, 'learning_rate': 3.4615384615384617e-06, 'epoch': 0.01}
{'loss': 1.0594, 'grad_norm': 0.7758388519287109, 'learning_rate': 3.4935897435897438e-06, 'epoch': 0.01}
{'loss': 1.0317, 'grad_norm': 0.7472776174545288, 'learning_rate': 3.5256410256410263e-06, 'epoch': 0.01}
{'loss': 1.0921, 'grad_norm': 0.7704225778579712, 'learning_rate': 3.557692307692308e-06, 'epoch': 0.01}
{'loss': 1.1051, 'grad_norm': 0.8308534622192383, 'learning_rate': 3.58974358974359e-06, 'epoch': 0.01}
{'loss': 0.9081, 'grad_norm': 0.8427358865737915, 'learning_rate': 3.621794871794872e-06, 'epoch': 0.01}
{'loss': 0.9253, 'grad_norm': 0.79060298204422, 'learning_rate': 3.653846153846154e-06, 'epoch': 0.01}
{'loss': 1.0865, 'grad_norm': 0.8497333526611328, 'learning_rate': 3.6858974358974363e-06, 'epoch': 0.01}
{'loss': 0.9266, 'grad_norm': 0.8818671703338623, 'learning_rate': 3.7179487179487184e-06, 'epoch': 0.01}
{'loss': 0.94, 'grad_norm': 0.8031657338142395, 'learning_rate': 3.7500000000000005e-06, 'epoch': 0.01}
{'loss': 0.9969, 'grad_norm': 0.9718274474143982, 'learning_rate': 3.782051282051282e-06, 'epoch': 0.01}
{'loss': 1.0525, 'grad_norm': 0.7952992916107178, 'learning_rate': 3.8141025641025643e-06, 'epoch': 0.01}
{'loss': 1.0666, 'grad_norm': 0.9602630734443665, 'learning_rate': 3.846153846153847e-06, 'epoch': 0.01}
{'loss': 1.1069, 'grad_norm': 0.8703478574752808, 'learning_rate': 3.878205128205129e-06, 'epoch': 0.01}
{'loss': 1.1811, 'grad_norm': 1.02843177318573, 'learning_rate': 3.910256410256411e-06, 'epoch': 0.01}
{'loss': 0.9936, 'grad_norm': 0.7514126300811768, 'learning_rate': 3.942307692307692e-06, 'epoch': 0.01}
{'loss': 1.0485, 'grad_norm': 0.7449830770492554, 'learning_rate': 3.974358974358974e-06, 'epoch': 0.01}
{'loss': 0.9649, 'grad_norm': 0.73967045545578, 'learning_rate': 4.006410256410257e-06, 'epoch': 0.01}
{'loss': 1.1391, 'grad_norm': 0.8219652771949768, 'learning_rate': 4.0384615384615385e-06, 'epoch': 0.01}
{'loss': 1.0407, 'grad_norm': 0.6603196263313293, 'learning_rate': 4.070512820512821e-06, 'epoch': 0.01}
{'loss': 0.8467, 'grad_norm': 0.69997638463974, 'learning_rate': 4.102564102564103e-06, 'epoch': 0.01}
{'loss': 0.899, 'grad_norm': 0.8386327028274536, 'learning_rate': 4.134615384615385e-06, 'epoch': 0.01}
{'loss': 1.1421, 'grad_norm': 0.8165775537490845, 'learning_rate': 4.166666666666667e-06, 'epoch': 0.01}
{'loss': 1.0263, 'grad_norm': 0.7490881085395813, 'learning_rate': 4.198717948717949e-06, 'epoch': 0.01}
{'loss': 0.8638, 'grad_norm': 0.7245509028434753, 'learning_rate': 4.230769230769231e-06, 'epoch': 0.01}
{'loss': 1.0217, 'grad_norm': 0.7292881608009338, 'learning_rate': 4.262820512820513e-06, 'epoch': 0.01}
{'loss': 0.8719, 'grad_norm': 0.8488866090774536, 'learning_rate': 4.294871794871795e-06, 'epoch': 0.01}
{'loss': 1.0779, 'grad_norm': 0.7408648729324341, 'learning_rate': 4.326923076923077e-06, 'epoch': 0.01}
{'loss': 1.0431, 'grad_norm': 0.8134539723396301, 'learning_rate': 4.358974358974359e-06, 'epoch': 0.01}
{'loss': 1.0985, 'grad_norm': 0.8740552663803101, 'learning_rate': 4.3910256410256415e-06, 'epoch': 0.01}
{'loss': 1.0854, 'grad_norm': 0.9986398816108704, 'learning_rate': 4.423076923076924e-06, 'epoch': 0.01}
{'loss': 0.9124, 'grad_norm': 0.697490930557251, 'learning_rate': 4.455128205128206e-06, 'epoch': 0.01}
{'loss': 0.9996, 'grad_norm': 0.7370495200157166, 'learning_rate': 4.487179487179488e-06, 'epoch': 0.01}
{'loss': 0.9573, 'grad_norm': 0.7946805953979492, 'learning_rate': 4.51923076923077e-06, 'epoch': 0.01}
{'loss': 0.7026, 'grad_norm': 0.8031627535820007, 'learning_rate': 4.551282051282052e-06, 'epoch': 0.01}
{'loss': 0.8847, 'grad_norm': 0.7140896916389465, 'learning_rate': 4.583333333333333e-06, 'epoch': 0.01}
{'loss': 0.825, 'grad_norm': 0.6742275357246399, 'learning_rate': 4.615384615384616e-06, 'epoch': 0.01}
{'loss': 1.0018, 'grad_norm': 0.8231996893882751, 'learning_rate': 4.647435897435898e-06, 'epoch': 0.01}
{'loss': 1.0447, 'grad_norm': 0.920106053352356, 'learning_rate': 4.6794871794871795e-06, 'epoch': 0.01}
{'loss': 0.9467, 'grad_norm': 0.4462890028953552, 'learning_rate': 4.711538461538462e-06, 'epoch': 0.01}
{'loss': 0.9751, 'grad_norm': 0.8895015716552734, 'learning_rate': 4.743589743589744e-06, 'epoch': 0.01}
{'loss': 1.0205, 'grad_norm': 0.7905024886131287, 'learning_rate': 4.775641025641027e-06, 'epoch': 0.01}
{'loss': 1.0236, 'grad_norm': 0.714477002620697, 'learning_rate': 4.807692307692308e-06, 'epoch': 0.01}
{'loss': 0.9416, 'grad_norm': 0.6797372698783875, 'learning_rate': 4.83974358974359e-06, 'epoch': 0.01}
{'loss': 1.0467, 'grad_norm': 0.9706989526748657, 'learning_rate': 4.871794871794872e-06, 'epoch': 0.01}
{'loss': 0.9122, 'grad_norm': 0.8423725366592407, 'learning_rate': 4.903846153846154e-06, 'epoch': 0.01}
{'loss': 0.9872, 'grad_norm': 0.9082308411598206, 'learning_rate': 4.935897435897436e-06, 'epoch': 0.01}
{'loss': 1.0161, 'grad_norm': 0.7391225695610046, 'learning_rate': 4.967948717948718e-06, 'epoch': 0.01}
{'loss': 1.0769, 'grad_norm': 0.8187804222106934, 'learning_rate': 5e-06, 'epoch': 0.01}
{'loss': 1.1089, 'grad_norm': 0.7073215842247009, 'learning_rate': 5.0320512820512825e-06, 'epoch': 0.01}
{'loss': 0.8913, 'grad_norm': 0.7090733647346497, 'learning_rate': 5.064102564102565e-06, 'epoch': 0.01}
{'loss': 1.0966, 'grad_norm': 0.7597970962524414, 'learning_rate': 5.096153846153846e-06, 'epoch': 0.01}
{'loss': 1.0164, 'grad_norm': 0.7461898922920227, 'learning_rate': 5.128205128205128e-06, 'epoch': 0.01}
{'loss': 0.9746, 'grad_norm': 0.8809359669685364, 'learning_rate': 5.160256410256411e-06, 'epoch': 0.01}
{'loss': 0.849, 'grad_norm': 0.764234185218811, 'learning_rate': 5.192307692307693e-06, 'epoch': 0.01}
{'loss': 0.9039, 'grad_norm': 0.705437183380127, 'learning_rate': 5.224358974358975e-06, 'epoch': 0.01}
{'loss': 0.9119, 'grad_norm': 0.7729788422584534, 'learning_rate': 5.256410256410257e-06, 'epoch': 0.01}
{'loss': 0.928, 'grad_norm': 0.6808947324752808, 'learning_rate': 5.288461538461539e-06, 'epoch': 0.01}
{'loss': 1.1245, 'grad_norm': 0.9163240790367126, 'learning_rate': 5.320512820512821e-06, 'epoch': 0.01}
{'loss': 0.9665, 'grad_norm': 0.9122369289398193, 'learning_rate': 5.3525641025641026e-06, 'epoch': 0.01}
{'loss': 0.9393, 'grad_norm': 0.6239672899246216, 'learning_rate': 5.384615384615385e-06, 'epoch': 0.01}
{'loss': 1.1349, 'grad_norm': 0.9807758331298828, 'learning_rate': 5.416666666666667e-06, 'epoch': 0.01}
{'loss': 1.0532, 'grad_norm': 0.7912998795509338, 'learning_rate': 5.448717948717949e-06, 'epoch': 0.01}
{'loss': 1.1619, 'grad_norm': 0.991852343082428, 'learning_rate': 5.480769230769232e-06, 'epoch': 0.01}
{'loss': 1.0865, 'grad_norm': 0.8625998497009277, 'learning_rate': 5.512820512820514e-06, 'epoch': 0.01}
{'loss': 0.9586, 'grad_norm': 0.714884340763092, 'learning_rate': 5.544871794871796e-06, 'epoch': 0.01}
{'loss': 0.932, 'grad_norm': 0.8652503490447998, 'learning_rate': 5.576923076923077e-06, 'epoch': 0.01}
{'loss': 0.8802, 'grad_norm': 0.786421000957489, 'learning_rate': 5.608974358974359e-06, 'epoch': 0.01}
{'loss': 0.9485, 'grad_norm': 0.49889010190963745, 'learning_rate': 5.641025641025641e-06, 'epoch': 0.01}
{'loss': 0.7956, 'grad_norm': 0.8088740110397339, 'learning_rate': 5.6730769230769235e-06, 'epoch': 0.01}
{'loss': 0.9923, 'grad_norm': 0.6907709240913391, 'learning_rate': 5.705128205128206e-06, 'epoch': 0.01}
{'loss': 0.8945, 'grad_norm': 0.7330012321472168, 'learning_rate': 5.737179487179487e-06, 'epoch': 0.01}
{'loss': 1.0447, 'grad_norm': 0.7536466717720032, 'learning_rate': 5.769230769230769e-06, 'epoch': 0.01}
{'loss': 0.958, 'grad_norm': 0.8129327297210693, 'learning_rate': 5.801282051282052e-06, 'epoch': 0.01}
{'loss': 1.0548, 'grad_norm': 0.867510974407196, 'learning_rate': 5.833333333333334e-06, 'epoch': 0.01}
{'loss': 1.0197, 'grad_norm': 0.7877752184867859, 'learning_rate': 5.865384615384616e-06, 'epoch': 0.01}
{'loss': 1.123, 'grad_norm': 0.9035240411758423, 'learning_rate': 5.897435897435898e-06, 'epoch': 0.01}
{'loss': 1.014, 'grad_norm': 0.7491537928581238, 'learning_rate': 5.92948717948718e-06, 'epoch': 0.01}
{'loss': 0.8882, 'grad_norm': 0.4917007088661194, 'learning_rate': 5.961538461538462e-06, 'epoch': 0.01}
{'loss': 1.1034, 'grad_norm': 0.7958183288574219, 'learning_rate': 5.9935897435897436e-06, 'epoch': 0.01}
{'loss': 1.2471, 'grad_norm': 0.9495847821235657, 'learning_rate': 6.025641025641026e-06, 'epoch': 0.01}
{'loss': 1.0065, 'grad_norm': 0.7751139402389526, 'learning_rate': 6.057692307692308e-06, 'epoch': 0.01}
{'loss': 0.9845, 'grad_norm': 0.766950786113739, 'learning_rate': 6.08974358974359e-06, 'epoch': 0.01}
{'loss': 0.9429, 'grad_norm': 0.836237907409668, 'learning_rate': 6.121794871794873e-06, 'epoch': 0.01}
{'loss': 1.0547, 'grad_norm': 0.7966693043708801, 'learning_rate': 6.153846153846155e-06, 'epoch': 0.01}
{'loss': 0.9009, 'grad_norm': 0.8977542519569397, 'learning_rate': 6.185897435897437e-06, 'epoch': 0.01}
{'loss': 0.9895, 'grad_norm': 0.8226582407951355, 'learning_rate': 6.217948717948718e-06, 'epoch': 0.01}
{'loss': 1.0154, 'grad_norm': 0.8954423666000366, 'learning_rate': 6.25e-06, 'epoch': 0.01}
{'loss': 0.9071, 'grad_norm': 0.4759182631969452, 'learning_rate': 6.282051282051282e-06, 'epoch': 0.01}
{'loss': 1.0187, 'grad_norm': 0.8792895674705505, 'learning_rate': 6.3141025641025645e-06, 'epoch': 0.01}
{'loss': 0.8708, 'grad_norm': 0.748599112033844, 'learning_rate': 6.3461538461538466e-06, 'epoch': 0.01}
{'loss': 1.0707, 'grad_norm': 0.7634356617927551, 'learning_rate': 6.378205128205129e-06, 'epoch': 0.01}
{'loss': 0.9463, 'grad_norm': 0.6955954432487488, 'learning_rate': 6.410256410256412e-06, 'epoch': 0.01}
{'loss': 0.9751, 'grad_norm': 0.7873680591583252, 'learning_rate': 6.442307692307693e-06, 'epoch': 0.01}
{'loss': 1.1035, 'grad_norm': 0.7626460194587708, 'learning_rate': 6.474358974358975e-06, 'epoch': 0.01}
{'loss': 1.0978, 'grad_norm': 0.8128613829612732, 'learning_rate': 6.506410256410257e-06, 'epoch': 0.01}
{'loss': 0.9085, 'grad_norm': 0.8113188147544861, 'learning_rate': 6.538461538461539e-06, 'epoch': 0.01}
{'loss': 1.0929, 'grad_norm': 0.9802234768867493, 'learning_rate': 6.570512820512821e-06, 'epoch': 0.01}
{'loss': 1.0561, 'grad_norm': 0.7876992225646973, 'learning_rate': 6.602564102564103e-06, 'epoch': 0.01}
{'loss': 1.2428, 'grad_norm': 0.9771642088890076, 'learning_rate': 6.6346153846153846e-06, 'epoch': 0.01}
{'loss': 1.1153, 'grad_norm': 0.6620322465896606, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.01}
{'loss': 1.0013, 'grad_norm': 0.7356395721435547, 'learning_rate': 6.698717948717949e-06, 'epoch': 0.01}
{'loss': 1.0158, 'grad_norm': 0.7505675554275513, 'learning_rate': 6.730769230769232e-06, 'epoch': 0.01}
{'loss': 1.0942, 'grad_norm': 0.7600553035736084, 'learning_rate': 6.762820512820514e-06, 'epoch': 0.01}
{'loss': 1.1096, 'grad_norm': 0.9353742599487305, 'learning_rate': 6.794871794871796e-06, 'epoch': 0.01}
{'loss': 1.0479, 'grad_norm': 0.8042851686477661, 'learning_rate': 6.826923076923078e-06, 'epoch': 0.01}
{'loss': 1.0214, 'grad_norm': 0.9486929774284363, 'learning_rate': 6.858974358974359e-06, 'epoch': 0.01}
{'loss': 0.9516, 'grad_norm': 0.7658194899559021, 'learning_rate': 6.891025641025641e-06, 'epoch': 0.01}
{'loss': 1.0105, 'grad_norm': 0.7697544693946838, 'learning_rate': 6.923076923076923e-06, 'epoch': 0.01}
{'loss': 1.1454, 'grad_norm': 0.9215983152389526, 'learning_rate': 6.9551282051282055e-06, 'epoch': 0.01}
{'loss': 0.9591, 'grad_norm': 0.7853250503540039, 'learning_rate': 6.9871794871794876e-06, 'epoch': 0.01}
{'loss': 0.8277, 'grad_norm': 0.5069642066955566, 'learning_rate': 7.01923076923077e-06, 'epoch': 0.01}
{'loss': 1.0618, 'grad_norm': 0.7908379435539246, 'learning_rate': 7.051282051282053e-06, 'epoch': 0.01}
{'loss': 1.0696, 'grad_norm': 0.8245830535888672, 'learning_rate': 7.083333333333335e-06, 'epoch': 0.01}
{'loss': 1.0895, 'grad_norm': 0.8216590881347656, 'learning_rate': 7.115384615384616e-06, 'epoch': 0.01}
{'loss': 1.1714, 'grad_norm': 1.0756977796554565, 'learning_rate': 7.147435897435898e-06, 'epoch': 0.01}
{'loss': 0.9218, 'grad_norm': 0.7754010558128357, 'learning_rate': 7.17948717948718e-06, 'epoch': 0.01}
{'loss': 1.0441, 'grad_norm': 0.9314786791801453, 'learning_rate': 7.211538461538462e-06, 'epoch': 0.01}
{'loss': 0.8876, 'grad_norm': 0.869553804397583, 'learning_rate': 7.243589743589744e-06, 'epoch': 0.01}
{'loss': 1.0239, 'grad_norm': 0.881282389163971, 'learning_rate': 7.2756410256410255e-06, 'epoch': 0.01}
{'loss': 1.085, 'grad_norm': 0.7742785811424255, 'learning_rate': 7.307692307692308e-06, 'epoch': 0.01}
{'loss': 0.9621, 'grad_norm': 0.8200156092643738, 'learning_rate': 7.33974358974359e-06, 'epoch': 0.01}
{'loss': 1.0418, 'grad_norm': 0.8985370993614197, 'learning_rate': 7.371794871794873e-06, 'epoch': 0.01}
{'loss': 0.9903, 'grad_norm': 0.7585825324058533, 'learning_rate': 7.403846153846155e-06, 'epoch': 0.01}
{'loss': 1.1447, 'grad_norm': 0.8435874581336975, 'learning_rate': 7.435897435897437e-06, 'epoch': 0.01}
{'loss': 0.9627, 'grad_norm': 0.7717691659927368, 'learning_rate': 7.467948717948719e-06, 'epoch': 0.01}
{'loss': 0.8835, 'grad_norm': 0.4850675165653229, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.01}
{'loss': 0.9494, 'grad_norm': 0.7622286081314087, 'learning_rate': 7.532051282051282e-06, 'epoch': 0.01}
{'loss': 1.24, 'grad_norm': 0.9228008389472961, 'learning_rate': 7.564102564102564e-06, 'epoch': 0.01}
{'loss': 0.9457, 'grad_norm': 0.6707049608230591, 'learning_rate': 7.5961538461538465e-06, 'epoch': 0.01}
{'loss': 0.9563, 'grad_norm': 0.7041646838188171, 'learning_rate': 7.6282051282051286e-06, 'epoch': 0.01}
{'loss': 0.9177, 'grad_norm': 0.6865636110305786, 'learning_rate': 7.660256410256411e-06, 'epoch': 0.01}
{'loss': 1.0308, 'grad_norm': 0.8535304069519043, 'learning_rate': 7.692307692307694e-06, 'epoch': 0.01}
{'loss': 0.9173, 'grad_norm': 0.7612820863723755, 'learning_rate': 7.724358974358976e-06, 'epoch': 0.01}
{'loss': 1.041, 'grad_norm': 0.7453786134719849, 'learning_rate': 7.756410256410258e-06, 'epoch': 0.01}
{'loss': 1.0171, 'grad_norm': 0.9006149172782898, 'learning_rate': 7.78846153846154e-06, 'epoch': 0.01}
{'loss': 1.0168, 'grad_norm': 0.7056439518928528, 'learning_rate': 7.820512820512822e-06, 'epoch': 0.01}
{'loss': 1.0535, 'grad_norm': 0.7863231897354126, 'learning_rate': 7.852564102564102e-06, 'epoch': 0.01}
{'loss': 1.1821, 'grad_norm': 0.9472348093986511, 'learning_rate': 7.884615384615384e-06, 'epoch': 0.01}
{'loss': 0.9715, 'grad_norm': 0.5254724025726318, 'learning_rate': 7.916666666666667e-06, 'epoch': 0.01}
{'loss': 1.1143, 'grad_norm': 0.8354716300964355, 'learning_rate': 7.948717948717949e-06, 'epoch': 0.01}
{'loss': 1.1399, 'grad_norm': 0.9560990333557129, 'learning_rate': 7.980769230769232e-06, 'epoch': 0.01}
{'loss': 1.0645, 'grad_norm': 0.8137416243553162, 'learning_rate': 8.012820512820515e-06, 'epoch': 0.01}
{'loss': 1.1369, 'grad_norm': 0.8571953177452087, 'learning_rate': 8.044871794871797e-06, 'epoch': 0.01}
{'loss': 1.1016, 'grad_norm': 0.8315253853797913, 'learning_rate': 8.076923076923077e-06, 'epoch': 0.01}
{'loss': 1.1104, 'grad_norm': 0.7689362168312073, 'learning_rate': 8.108974358974359e-06, 'epoch': 0.01}
{'loss': 0.9301, 'grad_norm': 0.4726784825325012, 'learning_rate': 8.141025641025641e-06, 'epoch': 0.01}
Error with image file is truncated (16 bytes not processed)
{'loss': 1.0701, 'grad_norm': 0.8396438956260681, 'learning_rate': 8.173076923076923e-06, 'epoch': 0.01}
{'loss': 1.0986, 'grad_norm': 1.0049985647201538, 'learning_rate': 8.205128205128205e-06, 'epoch': 0.01}
{'loss': 1.1364, 'grad_norm': 0.8268184661865234, 'learning_rate': 8.237179487179487e-06, 'epoch': 0.01}
{'loss': 0.9517, 'grad_norm': 0.8291117548942566, 'learning_rate': 8.26923076923077e-06, 'epoch': 0.01}
{'loss': 0.9311, 'grad_norm': 0.7753311395645142, 'learning_rate': 8.301282051282052e-06, 'epoch': 0.01}
{'loss': 1.0225, 'grad_norm': 0.8192550539970398, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.01}
{'loss': 0.9414, 'grad_norm': 0.9109320044517517, 'learning_rate': 8.365384615384616e-06, 'epoch': 0.01}
{'loss': 0.8487, 'grad_norm': 0.4126605689525604, 'learning_rate': 8.397435897435898e-06, 'epoch': 0.01}
{'loss': 1.061, 'grad_norm': 0.8138024210929871, 'learning_rate': 8.42948717948718e-06, 'epoch': 0.01}
{'loss': 1.1042, 'grad_norm': 0.9361063241958618, 'learning_rate': 8.461538461538462e-06, 'epoch': 0.01}
{'loss': 1.031, 'grad_norm': 0.8878121376037598, 'learning_rate': 8.493589743589744e-06, 'epoch': 0.01}
{'loss': 1.0163, 'grad_norm': 0.7624115943908691, 'learning_rate': 8.525641025641026e-06, 'epoch': 0.01}
{'loss': 1.0739, 'grad_norm': 0.7484866380691528, 'learning_rate': 8.557692307692308e-06, 'epoch': 0.01}
{'loss': 0.8972, 'grad_norm': 0.7875263690948486, 'learning_rate': 8.58974358974359e-06, 'epoch': 0.01}
{'loss': 1.0099, 'grad_norm': 0.4650314152240753, 'learning_rate': 8.621794871794873e-06, 'epoch': 0.01}
{'loss': 1.2025, 'grad_norm': 0.9859452247619629, 'learning_rate': 8.653846153846155e-06, 'epoch': 0.01}
{'loss': 0.9353, 'grad_norm': 0.7530560493469238, 'learning_rate': 8.685897435897437e-06, 'epoch': 0.01}
{'loss': 0.9895, 'grad_norm': 0.8714633584022522, 'learning_rate': 8.717948717948719e-06, 'epoch': 0.01}
{'loss': 1.0792, 'grad_norm': 0.852337121963501, 'learning_rate': 8.750000000000001e-06, 'epoch': 0.01}
{'loss': 1.0453, 'grad_norm': 0.7719342708587646, 'learning_rate': 8.782051282051283e-06, 'epoch': 0.01}
{'loss': 1.0715, 'grad_norm': 0.8957366347312927, 'learning_rate': 8.814102564102565e-06, 'epoch': 0.01}
{'loss': 1.1353, 'grad_norm': 0.7608689665794373, 'learning_rate': 8.846153846153847e-06, 'epoch': 0.01}
{'loss': 0.9094, 'grad_norm': 0.9131405353546143, 'learning_rate': 8.87820512820513e-06, 'epoch': 0.01}
{'loss': 1.2489, 'grad_norm': 0.949594259262085, 'learning_rate': 8.910256410256411e-06, 'epoch': 0.01}
{'loss': 1.1314, 'grad_norm': 0.7576457858085632, 'learning_rate': 8.942307692307693e-06, 'epoch': 0.01}
{'loss': 1.0967, 'grad_norm': 0.787002444267273, 'learning_rate': 8.974358974358976e-06, 'epoch': 0.01}
{'loss': 1.0522, 'grad_norm': 0.8251609206199646, 'learning_rate': 9.006410256410258e-06, 'epoch': 0.01}
{'loss': 0.8733, 'grad_norm': 0.5264119505882263, 'learning_rate': 9.03846153846154e-06, 'epoch': 0.01}
{'loss': 0.8695, 'grad_norm': 0.8692463040351868, 'learning_rate': 9.070512820512822e-06, 'epoch': 0.01}
{'loss': 1.2063, 'grad_norm': 0.9674739837646484, 'learning_rate': 9.102564102564104e-06, 'epoch': 0.01}
{'loss': 0.8285, 'grad_norm': 0.6874659061431885, 'learning_rate': 9.134615384615384e-06, 'epoch': 0.01}
{'loss': 1.0214, 'grad_norm': 0.7979446649551392, 'learning_rate': 9.166666666666666e-06, 'epoch': 0.01}
{'loss': 1.0265, 'grad_norm': 0.8433622121810913, 'learning_rate': 9.198717948717949e-06, 'epoch': 0.01}
{'loss': 0.9341, 'grad_norm': 0.7569125890731812, 'learning_rate': 9.230769230769232e-06, 'epoch': 0.01}
{'loss': 0.9913, 'grad_norm': 0.8837589025497437, 'learning_rate': 9.262820512820514e-06, 'epoch': 0.01}
{'loss': 0.9334, 'grad_norm': 0.7943586111068726, 'learning_rate': 9.294871794871796e-06, 'epoch': 0.01}
{'loss': 0.9832, 'grad_norm': 0.8041925430297852, 'learning_rate': 9.326923076923079e-06, 'epoch': 0.01}
{'loss': 0.9357, 'grad_norm': 0.9006113409996033, 'learning_rate': 9.358974358974359e-06, 'epoch': 0.01}
{'loss': 1.099, 'grad_norm': 0.7178593873977661, 'learning_rate': 9.391025641025641e-06, 'epoch': 0.01}
{'loss': 1.0122, 'grad_norm': 0.7438172101974487, 'learning_rate': 9.423076923076923e-06, 'epoch': 0.01}
{'loss': 0.9254, 'grad_norm': 0.6925869584083557, 'learning_rate': 9.455128205128205e-06, 'epoch': 0.01}
{'loss': 0.9618, 'grad_norm': 0.7714251279830933, 'learning_rate': 9.487179487179487e-06, 'epoch': 0.01}
{'loss': 0.9694, 'grad_norm': 0.9464249610900879, 'learning_rate': 9.51923076923077e-06, 'epoch': 0.01}
{'loss': 0.9356, 'grad_norm': 0.8312007188796997, 'learning_rate': 9.551282051282053e-06, 'epoch': 0.01}
{'loss': 0.9331, 'grad_norm': 0.7601397037506104, 'learning_rate': 9.583333333333335e-06, 'epoch': 0.01}
{'loss': 1.0521, 'grad_norm': 0.8021330237388611, 'learning_rate': 9.615384615384616e-06, 'epoch': 0.01}
{'loss': 1.1491, 'grad_norm': 0.8953589797019958, 'learning_rate': 9.647435897435898e-06, 'epoch': 0.01}
{'loss': 1.0114, 'grad_norm': 0.8946727514266968, 'learning_rate': 9.67948717948718e-06, 'epoch': 0.01}
{'loss': 0.9617, 'grad_norm': 0.7330889105796814, 'learning_rate': 9.711538461538462e-06, 'epoch': 0.01}
{'loss': 0.945, 'grad_norm': 0.8259220719337463, 'learning_rate': 9.743589743589744e-06, 'epoch': 0.01}
{'loss': 1.0813, 'grad_norm': 0.7887072563171387, 'learning_rate': 9.775641025641026e-06, 'epoch': 0.01}
{'loss': 0.8945, 'grad_norm': 0.7594128847122192, 'learning_rate': 9.807692307692308e-06, 'epoch': 0.01}
{'loss': 0.9414, 'grad_norm': 0.8838337063789368, 'learning_rate': 9.83974358974359e-06, 'epoch': 0.01}
{'loss': 1.0958, 'grad_norm': 0.8122049570083618, 'learning_rate': 9.871794871794872e-06, 'epoch': 0.01}
{'loss': 0.9612, 'grad_norm': 0.880275547504425, 'learning_rate': 9.903846153846155e-06, 'epoch': 0.01}
{'loss': 0.9692, 'grad_norm': 0.8659293055534363, 'learning_rate': 9.935897435897437e-06, 'epoch': 0.01}
{'loss': 1.0061, 'grad_norm': 0.8181499242782593, 'learning_rate': 9.967948717948719e-06, 'epoch': 0.02}
{'loss': 0.9989, 'grad_norm': 0.8373975157737732, 'learning_rate': 1e-05, 'epoch': 0.02}
{'loss': 0.9502, 'grad_norm': 0.9609209299087524, 'learning_rate': 1.0032051282051283e-05, 'epoch': 0.02}
{'loss': 1.0458, 'grad_norm': 1.168709635734558, 'learning_rate': 1.0064102564102565e-05, 'epoch': 0.02}
{'loss': 0.9804, 'grad_norm': 0.7867110967636108, 'learning_rate': 1.0096153846153847e-05, 'epoch': 0.02}
{'loss': 0.9875, 'grad_norm': 0.8241321444511414, 'learning_rate': 1.012820512820513e-05, 'epoch': 0.02}
{'loss': 1.0355, 'grad_norm': 0.8013815879821777, 'learning_rate': 1.0160256410256411e-05, 'epoch': 0.02}
{'loss': 1.0874, 'grad_norm': 1.0526238679885864, 'learning_rate': 1.0192307692307692e-05, 'epoch': 0.02}
{'loss': 1.1426, 'grad_norm': 0.7636016011238098, 'learning_rate': 1.0224358974358974e-05, 'epoch': 0.02}
{'loss': 1.0706, 'grad_norm': 0.7977803945541382, 'learning_rate': 1.0256410256410256e-05, 'epoch': 0.02}
{'loss': 0.9589, 'grad_norm': 0.48699426651000977, 'learning_rate': 1.0288461538461538e-05, 'epoch': 0.02}
{'loss': 0.9486, 'grad_norm': 0.8492399454116821, 'learning_rate': 1.0320512820512822e-05, 'epoch': 0.02}
{'loss': 1.0565, 'grad_norm': 0.8877197504043579, 'learning_rate': 1.0352564102564104e-05, 'epoch': 0.02}
{'loss': 0.916, 'grad_norm': 0.48763033747673035, 'learning_rate': 1.0384615384615386e-05, 'epoch': 0.02}
{'loss': 1.0177, 'grad_norm': 0.7288446426391602, 'learning_rate': 1.0416666666666668e-05, 'epoch': 0.02}
{'loss': 1.1093, 'grad_norm': 0.7866412401199341, 'learning_rate': 1.044871794871795e-05, 'epoch': 0.02}
{'loss': 0.9872, 'grad_norm': 0.7336021661758423, 'learning_rate': 1.0480769230769232e-05, 'epoch': 0.02}
{'loss': 0.8433, 'grad_norm': 0.7243996262550354, 'learning_rate': 1.0512820512820514e-05, 'epoch': 0.02}
{'loss': 0.893, 'grad_norm': 0.7853103876113892, 'learning_rate': 1.0544871794871796e-05, 'epoch': 0.02}
{'loss': 0.9891, 'grad_norm': 0.8790009617805481, 'learning_rate': 1.0576923076923078e-05, 'epoch': 0.02}
{'loss': 1.1408, 'grad_norm': 0.93251633644104, 'learning_rate': 1.060897435897436e-05, 'epoch': 0.02}
{'loss': 1.1111, 'grad_norm': 0.8151869773864746, 'learning_rate': 1.0641025641025643e-05, 'epoch': 0.02}
{'loss': 0.9664, 'grad_norm': 0.4907141923904419, 'learning_rate': 1.0673076923076923e-05, 'epoch': 0.02}
{'loss': 0.976, 'grad_norm': 0.8305631875991821, 'learning_rate': 1.0705128205128205e-05, 'epoch': 0.02}
{'loss': 1.0005, 'grad_norm': 0.8487404584884644, 'learning_rate': 1.0737179487179487e-05, 'epoch': 0.02}
{'loss': 1.0187, 'grad_norm': 1.1144624948501587, 'learning_rate': 1.076923076923077e-05, 'epoch': 0.02}
{'loss': 0.9506, 'grad_norm': 0.8864677548408508, 'learning_rate': 1.0801282051282051e-05, 'epoch': 0.02}
{'loss': 1.199, 'grad_norm': 1.0523189306259155, 'learning_rate': 1.0833333333333334e-05, 'epoch': 0.02}
{'loss': 1.0305, 'grad_norm': 0.7712177634239197, 'learning_rate': 1.0865384615384616e-05, 'epoch': 0.02}
{'loss': 1.0122, 'grad_norm': 0.6501137018203735, 'learning_rate': 1.0897435897435898e-05, 'epoch': 0.02}
{'loss': 1.0847, 'grad_norm': 0.7558361887931824, 'learning_rate': 1.092948717948718e-05, 'epoch': 0.02}
{'loss': 0.9959, 'grad_norm': 0.8542006611824036, 'learning_rate': 1.0961538461538464e-05, 'epoch': 0.02}
{'loss': 1.0769, 'grad_norm': 0.9553850889205933, 'learning_rate': 1.0993589743589746e-05, 'epoch': 0.02}
{'loss': 0.9462, 'grad_norm': 0.86053067445755, 'learning_rate': 1.1025641025641028e-05, 'epoch': 0.02}
{'loss': 1.0423, 'grad_norm': 0.8385511040687561, 'learning_rate': 1.105769230769231e-05, 'epoch': 0.02}
{'loss': 1.0965, 'grad_norm': 0.485492080450058, 'learning_rate': 1.1089743589743592e-05, 'epoch': 0.02}
{'loss': 1.0164, 'grad_norm': 0.8292092680931091, 'learning_rate': 1.1121794871794872e-05, 'epoch': 0.02}
{'loss': 1.1134, 'grad_norm': 0.8412925601005554, 'learning_rate': 1.1153846153846154e-05, 'epoch': 0.02}
{'loss': 1.085, 'grad_norm': 0.8623247742652893, 'learning_rate': 1.1185897435897437e-05, 'epoch': 0.02}
{'loss': 1.0214, 'grad_norm': 0.864758312702179, 'learning_rate': 1.1217948717948719e-05, 'epoch': 0.02}
{'loss': 1.0851, 'grad_norm': 0.9292628169059753, 'learning_rate': 1.125e-05, 'epoch': 0.02}
{'loss': 0.9452, 'grad_norm': 0.7392873764038086, 'learning_rate': 1.1282051282051283e-05, 'epoch': 0.02}
{'loss': 0.9551, 'grad_norm': 0.8283436894416809, 'learning_rate': 1.1314102564102565e-05, 'epoch': 0.02}
{'loss': 0.9352, 'grad_norm': 0.8054791688919067, 'learning_rate': 1.1346153846153847e-05, 'epoch': 0.02}
{'loss': 0.8658, 'grad_norm': 0.8599320650100708, 'learning_rate': 1.1378205128205129e-05, 'epoch': 0.02}
{'loss': 1.0052, 'grad_norm': 0.8626261353492737, 'learning_rate': 1.1410256410256411e-05, 'epoch': 0.02}
{'loss': 1.0867, 'grad_norm': 0.9039695262908936, 'learning_rate': 1.1442307692307693e-05, 'epoch': 0.02}
{'loss': 0.8889, 'grad_norm': 0.8910717964172363, 'learning_rate': 1.1474358974358974e-05, 'epoch': 0.02}
{'loss': 1.0181, 'grad_norm': 0.8409796357154846, 'learning_rate': 1.1506410256410256e-05, 'epoch': 0.02}
{'loss': 0.9958, 'grad_norm': 0.8650527596473694, 'learning_rate': 1.1538461538461538e-05, 'epoch': 0.02}
{'loss': 0.991, 'grad_norm': 0.8368943333625793, 'learning_rate': 1.1570512820512823e-05, 'epoch': 0.02}
{'loss': 0.916, 'grad_norm': 0.8053172826766968, 'learning_rate': 1.1602564102564104e-05, 'epoch': 0.02}
{'loss': 1.0845, 'grad_norm': 0.9140489101409912, 'learning_rate': 1.1634615384615386e-05, 'epoch': 0.02}
{'loss': 0.9306, 'grad_norm': 0.7601543068885803, 'learning_rate': 1.1666666666666668e-05, 'epoch': 0.02}
{'loss': 1.0598, 'grad_norm': 0.7799732685089111, 'learning_rate': 1.169871794871795e-05, 'epoch': 0.02}
{'loss': 0.9875, 'grad_norm': 0.7541820406913757, 'learning_rate': 1.1730769230769232e-05, 'epoch': 0.02}
{'loss': 0.9779, 'grad_norm': 0.49251729249954224, 'learning_rate': 1.1762820512820514e-05, 'epoch': 0.02}
{'loss': 1.0278, 'grad_norm': 0.9506229162216187, 'learning_rate': 1.1794871794871796e-05, 'epoch': 0.02}
{'loss': 1.1229, 'grad_norm': 0.8689315319061279, 'learning_rate': 1.1826923076923078e-05, 'epoch': 0.02}
{'loss': 0.9434, 'grad_norm': 0.7556555271148682, 'learning_rate': 1.185897435897436e-05, 'epoch': 0.02}
{'loss': 1.11, 'grad_norm': 0.8855016231536865, 'learning_rate': 1.1891025641025643e-05, 'epoch': 0.02}
{'loss': 1.0005, 'grad_norm': 0.7893589735031128, 'learning_rate': 1.1923076923076925e-05, 'epoch': 0.02}
{'loss': 1.0496, 'grad_norm': 0.8270902633666992, 'learning_rate': 1.1955128205128205e-05, 'epoch': 0.02}
{'loss': 1.0264, 'grad_norm': 0.9075480699539185, 'learning_rate': 1.1987179487179487e-05, 'epoch': 0.02}
{'loss': 1.0088, 'grad_norm': 0.8415918350219727, 'learning_rate': 1.201923076923077e-05, 'epoch': 0.02}
{'loss': 1.0739, 'grad_norm': 0.7432675361633301, 'learning_rate': 1.2051282051282051e-05, 'epoch': 0.02}
{'loss': 0.9151, 'grad_norm': 0.8287276029586792, 'learning_rate': 1.2083333333333333e-05, 'epoch': 0.02}
{'loss': 1.0524, 'grad_norm': 0.5366940498352051, 'learning_rate': 1.2115384615384615e-05, 'epoch': 0.02}
{'loss': 1.0182, 'grad_norm': 0.9499604105949402, 'learning_rate': 1.2147435897435898e-05, 'epoch': 0.02}
{'loss': 1.1593, 'grad_norm': 1.0318331718444824, 'learning_rate': 1.217948717948718e-05, 'epoch': 0.02}
{'loss': 1.0827, 'grad_norm': 0.8368136882781982, 'learning_rate': 1.2211538461538463e-05, 'epoch': 0.02}
{'loss': 1.0768, 'grad_norm': 0.9492408633232117, 'learning_rate': 1.2243589743589746e-05, 'epoch': 0.02}
{'loss': 1.1142, 'grad_norm': 0.9446510672569275, 'learning_rate': 1.2275641025641028e-05, 'epoch': 0.02}
{'loss': 1.0561, 'grad_norm': 0.9230983257293701, 'learning_rate': 1.230769230769231e-05, 'epoch': 0.02}
{'loss': 0.9784, 'grad_norm': 1.030484914779663, 'learning_rate': 1.2339743589743592e-05, 'epoch': 0.02}
{'loss': 1.1195, 'grad_norm': 0.8043011426925659, 'learning_rate': 1.2371794871794874e-05, 'epoch': 0.02}
{'loss': 0.7984, 'grad_norm': 1.1977514028549194, 'learning_rate': 1.2403846153846156e-05, 'epoch': 0.02}
{'loss': 1.1472, 'grad_norm': 1.0698308944702148, 'learning_rate': 1.2435897435897436e-05, 'epoch': 0.02}
{'loss': 0.9529, 'grad_norm': 0.8378473520278931, 'learning_rate': 1.2467948717948719e-05, 'epoch': 0.02}
{'loss': 1.0688, 'grad_norm': 0.891542911529541, 'learning_rate': 1.25e-05, 'epoch': 0.02}
{'loss': 0.8685, 'grad_norm': 0.7502710223197937, 'learning_rate': 1.2532051282051283e-05, 'epoch': 0.02}
{'loss': 1.0378, 'grad_norm': 0.980305552482605, 'learning_rate': 1.2564102564102565e-05, 'epoch': 0.02}
{'loss': 1.0212, 'grad_norm': 0.8197718262672424, 'learning_rate': 1.2596153846153847e-05, 'epoch': 0.02}
{'loss': 1.0204, 'grad_norm': 0.8808158040046692, 'learning_rate': 1.2628205128205129e-05, 'epoch': 0.02}
{'loss': 0.9869, 'grad_norm': 0.7125662565231323, 'learning_rate': 1.2660256410256411e-05, 'epoch': 0.02}
{'loss': 0.9954, 'grad_norm': 0.7741401195526123, 'learning_rate': 1.2692307692307693e-05, 'epoch': 0.02}
{'loss': 1.0946, 'grad_norm': 0.8366885781288147, 'learning_rate': 1.2724358974358975e-05, 'epoch': 0.02}
{'loss': 1.0949, 'grad_norm': 0.7486923336982727, 'learning_rate': 1.2756410256410257e-05, 'epoch': 0.02}
{'loss': 1.0822, 'grad_norm': 0.9579061269760132, 'learning_rate': 1.2788461538461538e-05, 'epoch': 0.02}
{'loss': 0.9227, 'grad_norm': 0.7659354209899902, 'learning_rate': 1.2820512820512823e-05, 'epoch': 0.02}
{'loss': 1.0185, 'grad_norm': 0.8931983709335327, 'learning_rate': 1.2852564102564105e-05, 'epoch': 0.02}
{'loss': 0.9277, 'grad_norm': 0.751075029373169, 'learning_rate': 1.2884615384615386e-05, 'epoch': 0.02}
{'loss': 0.9321, 'grad_norm': 0.9120343327522278, 'learning_rate': 1.2916666666666668e-05, 'epoch': 0.02}
{'loss': 0.8821, 'grad_norm': 0.7943890690803528, 'learning_rate': 1.294871794871795e-05, 'epoch': 0.02}
{'loss': 1.1277, 'grad_norm': 0.811974823474884, 'learning_rate': 1.2980769230769232e-05, 'epoch': 0.02}
{'loss': 0.9089, 'grad_norm': 0.8658586740493774, 'learning_rate': 1.3012820512820514e-05, 'epoch': 0.02}
{'loss': 0.8876, 'grad_norm': 0.8665717840194702, 'learning_rate': 1.3044871794871796e-05, 'epoch': 0.02}
{'loss': 1.0416, 'grad_norm': 0.843483030796051, 'learning_rate': 1.3076923076923078e-05, 'epoch': 0.02}
{'loss': 1.1594, 'grad_norm': 0.8893237709999084, 'learning_rate': 1.310897435897436e-05, 'epoch': 0.02}
{'loss': 1.0107, 'grad_norm': 0.6929143667221069, 'learning_rate': 1.3141025641025642e-05, 'epoch': 0.02}
{'loss': 1.1148, 'grad_norm': 0.7887783646583557, 'learning_rate': 1.3173076923076925e-05, 'epoch': 0.02}
{'loss': 1.0895, 'grad_norm': 1.0664738416671753, 'learning_rate': 1.3205128205128207e-05, 'epoch': 0.02}
{'loss': 1.1067, 'grad_norm': 0.8104729652404785, 'learning_rate': 1.3237179487179487e-05, 'epoch': 0.02}
{'loss': 1.1219, 'grad_norm': 0.8746957182884216, 'learning_rate': 1.3269230769230769e-05, 'epoch': 0.02}
{'loss': 1.0719, 'grad_norm': 0.8616982698440552, 'learning_rate': 1.3301282051282051e-05, 'epoch': 0.02}
{'loss': 1.1378, 'grad_norm': 0.8739815354347229, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.02}
{'loss': 0.9788, 'grad_norm': 0.9019032716751099, 'learning_rate': 1.3365384615384615e-05, 'epoch': 0.02}
{'loss': 0.9758, 'grad_norm': 0.7221888303756714, 'learning_rate': 1.3397435897435897e-05, 'epoch': 0.02}
{'loss': 1.0441, 'grad_norm': 0.9289770722389221, 'learning_rate': 1.342948717948718e-05, 'epoch': 0.02}
{'loss': 1.0751, 'grad_norm': 0.9848849177360535, 'learning_rate': 1.3461538461538463e-05, 'epoch': 0.02}
{'loss': 1.0844, 'grad_norm': 0.8759377002716064, 'learning_rate': 1.3493589743589745e-05, 'epoch': 0.02}
{'loss': 0.9586, 'grad_norm': 0.7017856240272522, 'learning_rate': 1.3525641025641028e-05, 'epoch': 0.02}
{'loss': 1.0478, 'grad_norm': 0.9793335795402527, 'learning_rate': 1.355769230769231e-05, 'epoch': 0.02}
{'loss': 1.1803, 'grad_norm': 0.9773218631744385, 'learning_rate': 1.3589743589743592e-05, 'epoch': 0.02}
{'loss': 0.9164, 'grad_norm': 0.9285746812820435, 'learning_rate': 1.3621794871794874e-05, 'epoch': 0.02}
{'loss': 0.8338, 'grad_norm': 0.7990737557411194, 'learning_rate': 1.3653846153846156e-05, 'epoch': 0.02}
{'loss': 1.0622, 'grad_norm': 0.7605093121528625, 'learning_rate': 1.3685897435897438e-05, 'epoch': 0.02}
{'loss': 1.0337, 'grad_norm': 0.821921169757843, 'learning_rate': 1.3717948717948718e-05, 'epoch': 0.02}
{'loss': 1.0221, 'grad_norm': 0.7428910136222839, 'learning_rate': 1.375e-05, 'epoch': 0.02}
{'loss': 0.9439, 'grad_norm': 0.7657183408737183, 'learning_rate': 1.3782051282051283e-05, 'epoch': 0.02}
{'loss': 0.9938, 'grad_norm': 0.6762785911560059, 'learning_rate': 1.3814102564102565e-05, 'epoch': 0.02}
{'loss': 1.1712, 'grad_norm': 0.8050851821899414, 'learning_rate': 1.3846153846153847e-05, 'epoch': 0.02}
{'loss': 0.9698, 'grad_norm': 0.8822957873344421, 'learning_rate': 1.3878205128205129e-05, 'epoch': 0.02}
{'loss': 1.1052, 'grad_norm': 0.8173323273658752, 'learning_rate': 1.3910256410256411e-05, 'epoch': 0.02}
{'loss': 1.1537, 'grad_norm': 0.8944966197013855, 'learning_rate': 1.3942307692307693e-05, 'epoch': 0.02}
{'loss': 1.0676, 'grad_norm': 0.9797828197479248, 'learning_rate': 1.3974358974358975e-05, 'epoch': 0.02}
{'loss': 1.1654, 'grad_norm': 0.7872297167778015, 'learning_rate': 1.4006410256410257e-05, 'epoch': 0.02}
{'loss': 1.2088, 'grad_norm': 0.9217735528945923, 'learning_rate': 1.403846153846154e-05, 'epoch': 0.02}
{'loss': 1.0087, 'grad_norm': 0.8513192534446716, 'learning_rate': 1.4070512820512823e-05, 'epoch': 0.02}
{'loss': 1.1247, 'grad_norm': 0.8572331666946411, 'learning_rate': 1.4102564102564105e-05, 'epoch': 0.02}
{'loss': 1.0863, 'grad_norm': 0.9303196668624878, 'learning_rate': 1.4134615384615387e-05, 'epoch': 0.02}
{'loss': 0.9351, 'grad_norm': 0.7330754995346069, 'learning_rate': 1.416666666666667e-05, 'epoch': 0.02}
{'loss': 0.9561, 'grad_norm': 0.953164279460907, 'learning_rate': 1.419871794871795e-05, 'epoch': 0.02}
{'loss': 1.0634, 'grad_norm': 0.9147376418113708, 'learning_rate': 1.4230769230769232e-05, 'epoch': 0.02}
{'loss': 0.937, 'grad_norm': 0.7156210541725159, 'learning_rate': 1.4262820512820514e-05, 'epoch': 0.02}
{'loss': 1.1467, 'grad_norm': 0.9031754732131958, 'learning_rate': 1.4294871794871796e-05, 'epoch': 0.02}
{'loss': 0.9283, 'grad_norm': 0.45619848370552063, 'learning_rate': 1.4326923076923078e-05, 'epoch': 0.02}
{'loss': 1.0041, 'grad_norm': 0.7701382637023926, 'learning_rate': 1.435897435897436e-05, 'epoch': 0.02}
{'loss': 0.8835, 'grad_norm': 0.7164175510406494, 'learning_rate': 1.4391025641025642e-05, 'epoch': 0.02}
{'loss': 1.0041, 'grad_norm': 0.7835347056388855, 'learning_rate': 1.4423076923076924e-05, 'epoch': 0.02}
{'loss': 1.0707, 'grad_norm': 0.5901227593421936, 'learning_rate': 1.4455128205128207e-05, 'epoch': 0.02}
{'loss': 1.0978, 'grad_norm': 0.8161952495574951, 'learning_rate': 1.4487179487179489e-05, 'epoch': 0.02}
{'loss': 1.0691, 'grad_norm': 1.0288360118865967, 'learning_rate': 1.451923076923077e-05, 'epoch': 0.02}
{'loss': 0.9399, 'grad_norm': 0.6631008386611938, 'learning_rate': 1.4551282051282051e-05, 'epoch': 0.02}
{'loss': 0.9662, 'grad_norm': 0.9878602623939514, 'learning_rate': 1.4583333333333333e-05, 'epoch': 0.02}
{'loss': 0.9676, 'grad_norm': 0.816513180732727, 'learning_rate': 1.4615384615384615e-05, 'epoch': 0.02}
{'loss': 1.0471, 'grad_norm': 0.8745043873786926, 'learning_rate': 1.4647435897435897e-05, 'epoch': 0.02}
{'loss': 1.121, 'grad_norm': 0.7726949453353882, 'learning_rate': 1.467948717948718e-05, 'epoch': 0.02}
{'loss': 1.1302, 'grad_norm': 0.8875095844268799, 'learning_rate': 1.4711538461538463e-05, 'epoch': 0.02}
{'loss': 0.9942, 'grad_norm': 1.1173715591430664, 'learning_rate': 1.4743589743589745e-05, 'epoch': 0.02}
{'loss': 1.0468, 'grad_norm': 0.9335753917694092, 'learning_rate': 1.4775641025641027e-05, 'epoch': 0.02}
{'loss': 0.8716, 'grad_norm': 0.7776471376419067, 'learning_rate': 1.480769230769231e-05, 'epoch': 0.02}
{'loss': 0.8001, 'grad_norm': 0.7120575308799744, 'learning_rate': 1.4839743589743592e-05, 'epoch': 0.02}
{'loss': 0.8806, 'grad_norm': 0.7087351083755493, 'learning_rate': 1.4871794871794874e-05, 'epoch': 0.02}
{'loss': 1.0105, 'grad_norm': 0.7751616835594177, 'learning_rate': 1.4903846153846156e-05, 'epoch': 0.02}
{'loss': 0.8958, 'grad_norm': 0.5769370198249817, 'learning_rate': 1.4935897435897438e-05, 'epoch': 0.02}
{'loss': 0.9722, 'grad_norm': 0.7683243751525879, 'learning_rate': 1.496794871794872e-05, 'epoch': 0.02}
{'loss': 0.9539, 'grad_norm': 0.7776650190353394, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.02}
{'loss': 1.1796, 'grad_norm': 0.9425191879272461, 'learning_rate': 1.5032051282051282e-05, 'epoch': 0.02}
{'loss': 0.9621, 'grad_norm': 0.8172399997711182, 'learning_rate': 1.5064102564102565e-05, 'epoch': 0.02}
{'loss': 1.1855, 'grad_norm': 0.8585169315338135, 'learning_rate': 1.5096153846153847e-05, 'epoch': 0.02}
{'loss': 1.2155, 'grad_norm': 0.8819155693054199, 'learning_rate': 1.5128205128205129e-05, 'epoch': 0.02}
{'loss': 0.9331, 'grad_norm': 1.0481159687042236, 'learning_rate': 1.516025641025641e-05, 'epoch': 0.02}
{'loss': 1.0734, 'grad_norm': 0.8143359422683716, 'learning_rate': 1.5192307692307693e-05, 'epoch': 0.02}
{'loss': 1.1773, 'grad_norm': 0.9667927622795105, 'learning_rate': 1.5224358974358975e-05, 'epoch': 0.02}
{'loss': 1.0079, 'grad_norm': 0.7538761496543884, 'learning_rate': 1.5256410256410257e-05, 'epoch': 0.02}
{'loss': 1.0773, 'grad_norm': 0.8719086050987244, 'learning_rate': 1.528846153846154e-05, 'epoch': 0.02}
{'loss': 0.9684, 'grad_norm': 0.9289817214012146, 'learning_rate': 1.5320512820512823e-05, 'epoch': 0.02}
{'loss': 1.1247, 'grad_norm': 0.9280340671539307, 'learning_rate': 1.5352564102564103e-05, 'epoch': 0.02}
{'loss': 1.1276, 'grad_norm': 0.9147471189498901, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.02}
{'loss': 1.0106, 'grad_norm': 0.7998715043067932, 'learning_rate': 1.5416666666666668e-05, 'epoch': 0.02}
{'loss': 1.1583, 'grad_norm': 1.0356007814407349, 'learning_rate': 1.544871794871795e-05, 'epoch': 0.02}
{'loss': 0.959, 'grad_norm': 0.5210798978805542, 'learning_rate': 1.5480769230769232e-05, 'epoch': 0.02}
{'loss': 0.8717, 'grad_norm': 0.9352130889892578, 'learning_rate': 1.5512820512820516e-05, 'epoch': 0.02}
{'loss': 0.962, 'grad_norm': 0.7873100638389587, 'learning_rate': 1.5544871794871796e-05, 'epoch': 0.02}
{'loss': 0.9198, 'grad_norm': 0.505851149559021, 'learning_rate': 1.557692307692308e-05, 'epoch': 0.02}
{'loss': 0.9705, 'grad_norm': 0.7245312333106995, 'learning_rate': 1.560897435897436e-05, 'epoch': 0.02}
{'loss': 1.0225, 'grad_norm': 0.9194921255111694, 'learning_rate': 1.5641025641025644e-05, 'epoch': 0.02}
{'loss': 0.9855, 'grad_norm': 0.876746654510498, 'learning_rate': 1.5673076923076924e-05, 'epoch': 0.02}
{'loss': 0.9325, 'grad_norm': 0.7689483165740967, 'learning_rate': 1.5705128205128205e-05, 'epoch': 0.02}
{'loss': 1.0938, 'grad_norm': 0.8991034030914307, 'learning_rate': 1.573717948717949e-05, 'epoch': 0.02}
{'loss': 1.0069, 'grad_norm': 0.8125128149986267, 'learning_rate': 1.576923076923077e-05, 'epoch': 0.02}
{'loss': 0.9484, 'grad_norm': 0.8421835899353027, 'learning_rate': 1.5801282051282053e-05, 'epoch': 0.02}
{'loss': 1.0889, 'grad_norm': 0.979990541934967, 'learning_rate': 1.5833333333333333e-05, 'epoch': 0.02}
{'loss': 1.0819, 'grad_norm': 0.7914952039718628, 'learning_rate': 1.5865384615384617e-05, 'epoch': 0.02}
{'loss': 1.1266, 'grad_norm': 0.7555192112922668, 'learning_rate': 1.5897435897435897e-05, 'epoch': 0.02}
{'loss': 1.0538, 'grad_norm': 0.8527073860168457, 'learning_rate': 1.592948717948718e-05, 'epoch': 0.02}
{'loss': 1.0299, 'grad_norm': 0.5252654552459717, 'learning_rate': 1.5961538461538465e-05, 'epoch': 0.02}
{'loss': 1.0932, 'grad_norm': 0.8349073529243469, 'learning_rate': 1.5993589743589745e-05, 'epoch': 0.02}
{'loss': 1.0887, 'grad_norm': 0.9752267599105835, 'learning_rate': 1.602564102564103e-05, 'epoch': 0.02}
{'loss': 0.859, 'grad_norm': 0.876521646976471, 'learning_rate': 1.605769230769231e-05, 'epoch': 0.02}
{'loss': 1.1165, 'grad_norm': 0.8230202198028564, 'learning_rate': 1.6089743589743593e-05, 'epoch': 0.02}
{'loss': 1.1117, 'grad_norm': 0.776627779006958, 'learning_rate': 1.6121794871794874e-05, 'epoch': 0.02}
{'loss': 0.9818, 'grad_norm': 0.8607798218727112, 'learning_rate': 1.6153846153846154e-05, 'epoch': 0.02}
{'loss': 0.9686, 'grad_norm': 1.0405066013336182, 'learning_rate': 1.6185897435897438e-05, 'epoch': 0.02}
{'loss': 1.1403, 'grad_norm': 0.9718841910362244, 'learning_rate': 1.6217948717948718e-05, 'epoch': 0.02}
{'loss': 0.8649, 'grad_norm': 0.9906110763549805, 'learning_rate': 1.6250000000000002e-05, 'epoch': 0.02}
{'loss': 0.8969, 'grad_norm': 0.7363932728767395, 'learning_rate': 1.6282051282051282e-05, 'epoch': 0.02}
{'loss': 1.1201, 'grad_norm': 0.9276478290557861, 'learning_rate': 1.6314102564102566e-05, 'epoch': 0.02}
{'loss': 1.0249, 'grad_norm': 0.7989928722381592, 'learning_rate': 1.6346153846153847e-05, 'epoch': 0.02}
{'loss': 1.0237, 'grad_norm': 0.845063328742981, 'learning_rate': 1.637820512820513e-05, 'epoch': 0.02}
{'loss': 0.9618, 'grad_norm': 0.7910226583480835, 'learning_rate': 1.641025641025641e-05, 'epoch': 0.02}
{'loss': 0.9649, 'grad_norm': 0.8171007037162781, 'learning_rate': 1.6442307692307695e-05, 'epoch': 0.02}
{'loss': 0.8506, 'grad_norm': 0.43649524450302124, 'learning_rate': 1.6474358974358975e-05, 'epoch': 0.02}
{'loss': 0.9855, 'grad_norm': 0.7860857844352722, 'learning_rate': 1.6506410256410255e-05, 'epoch': 0.02}
{'loss': 0.9472, 'grad_norm': 0.7255142331123352, 'learning_rate': 1.653846153846154e-05, 'epoch': 0.02}
{'loss': 1.1071, 'grad_norm': 0.8994663953781128, 'learning_rate': 1.6570512820512823e-05, 'epoch': 0.02}
{'loss': 1.0259, 'grad_norm': 0.8597750067710876, 'learning_rate': 1.6602564102564103e-05, 'epoch': 0.02}
{'loss': 1.0091, 'grad_norm': 0.8028473258018494, 'learning_rate': 1.6634615384615387e-05, 'epoch': 0.03}
{'loss': 1.0035, 'grad_norm': 0.9689092636108398, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.03}
{'loss': 0.9331, 'grad_norm': 0.8507857918739319, 'learning_rate': 1.669871794871795e-05, 'epoch': 0.03}
{'loss': 1.0211, 'grad_norm': 0.8171395659446716, 'learning_rate': 1.673076923076923e-05, 'epoch': 0.03}
{'loss': 1.2516, 'grad_norm': 0.9201432466506958, 'learning_rate': 1.6762820512820515e-05, 'epoch': 0.03}
{'loss': 0.9429, 'grad_norm': 0.8406478762626648, 'learning_rate': 1.6794871794871796e-05, 'epoch': 0.03}
{'loss': 0.8635, 'grad_norm': 0.7572209239006042, 'learning_rate': 1.682692307692308e-05, 'epoch': 0.03}
{'loss': 0.9142, 'grad_norm': 0.5041148066520691, 'learning_rate': 1.685897435897436e-05, 'epoch': 0.03}
{'loss': 1.1304, 'grad_norm': 0.7914142608642578, 'learning_rate': 1.6891025641025644e-05, 'epoch': 0.03}
{'loss': 0.8612, 'grad_norm': 0.489139199256897, 'learning_rate': 1.6923076923076924e-05, 'epoch': 0.03}
{'loss': 0.9263, 'grad_norm': 0.806972861289978, 'learning_rate': 1.6955128205128205e-05, 'epoch': 0.03}
{'loss': 1.0184, 'grad_norm': 0.8222934007644653, 'learning_rate': 1.698717948717949e-05, 'epoch': 0.03}
{'loss': 1.0821, 'grad_norm': 1.0109585523605347, 'learning_rate': 1.701923076923077e-05, 'epoch': 0.03}
{'loss': 1.0783, 'grad_norm': 0.9022929668426514, 'learning_rate': 1.7051282051282053e-05, 'epoch': 0.03}
{'loss': 0.8608, 'grad_norm': 0.4421054422855377, 'learning_rate': 1.7083333333333333e-05, 'epoch': 0.03}
{'loss': 1.0811, 'grad_norm': 0.9006183743476868, 'learning_rate': 1.7115384615384617e-05, 'epoch': 0.03}
{'loss': 1.029, 'grad_norm': 0.8913034200668335, 'learning_rate': 1.7147435897435897e-05, 'epoch': 0.03}
{'loss': 0.8323, 'grad_norm': 0.9283513426780701, 'learning_rate': 1.717948717948718e-05, 'epoch': 0.03}
{'loss': 0.9451, 'grad_norm': 0.9288182258605957, 'learning_rate': 1.7211538461538465e-05, 'epoch': 0.03}
{'loss': 1.0024, 'grad_norm': 0.9504963159561157, 'learning_rate': 1.7243589743589745e-05, 'epoch': 0.03}
{'loss': 1.1492, 'grad_norm': 0.8850305676460266, 'learning_rate': 1.727564102564103e-05, 'epoch': 0.03}
{'loss': 1.1679, 'grad_norm': 0.8456391096115112, 'learning_rate': 1.730769230769231e-05, 'epoch': 0.03}
{'loss': 0.9564, 'grad_norm': 0.781772792339325, 'learning_rate': 1.7339743589743593e-05, 'epoch': 0.03}
{'loss': 0.9293, 'grad_norm': 0.5397504568099976, 'learning_rate': 1.7371794871794873e-05, 'epoch': 0.03}
{'loss': 1.0553, 'grad_norm': 0.8853873610496521, 'learning_rate': 1.7403846153846157e-05, 'epoch': 0.03}
{'loss': 1.0647, 'grad_norm': 0.6781436204910278, 'learning_rate': 1.7435897435897438e-05, 'epoch': 0.03}
{'loss': 1.1571, 'grad_norm': 0.816059410572052, 'learning_rate': 1.7467948717948718e-05, 'epoch': 0.03}
{'loss': 0.9724, 'grad_norm': 0.8206017017364502, 'learning_rate': 1.7500000000000002e-05, 'epoch': 0.03}
{'loss': 1.1462, 'grad_norm': 0.9627972841262817, 'learning_rate': 1.7532051282051282e-05, 'epoch': 0.03}
{'loss': 0.9361, 'grad_norm': 0.43413588404655457, 'learning_rate': 1.7564102564102566e-05, 'epoch': 0.03}
{'loss': 0.9649, 'grad_norm': 0.7762959599494934, 'learning_rate': 1.7596153846153846e-05, 'epoch': 0.03}
{'loss': 0.9753, 'grad_norm': 0.7297112345695496, 'learning_rate': 1.762820512820513e-05, 'epoch': 0.03}
{'loss': 1.0899, 'grad_norm': 0.7440792918205261, 'learning_rate': 1.766025641025641e-05, 'epoch': 0.03}
{'loss': 1.0402, 'grad_norm': 0.798061728477478, 'learning_rate': 1.7692307692307694e-05, 'epoch': 0.03}
{'loss': 1.0466, 'grad_norm': 0.7347354292869568, 'learning_rate': 1.7724358974358975e-05, 'epoch': 0.03}
{'loss': 1.1075, 'grad_norm': 0.7356337904930115, 'learning_rate': 1.775641025641026e-05, 'epoch': 0.03}
{'loss': 1.0602, 'grad_norm': 0.7138788104057312, 'learning_rate': 1.778846153846154e-05, 'epoch': 0.03}
{'loss': 0.9122, 'grad_norm': 0.9080460071563721, 'learning_rate': 1.7820512820512823e-05, 'epoch': 0.03}
{'loss': 1.1935, 'grad_norm': 0.8865441083908081, 'learning_rate': 1.7852564102564107e-05, 'epoch': 0.03}
{'loss': 0.9822, 'grad_norm': 0.7201153039932251, 'learning_rate': 1.7884615384615387e-05, 'epoch': 0.03}
{'loss': 0.9733, 'grad_norm': 0.719513475894928, 'learning_rate': 1.7916666666666667e-05, 'epoch': 0.03}
{'loss': 0.8943, 'grad_norm': 0.4748765528202057, 'learning_rate': 1.794871794871795e-05, 'epoch': 0.03}
{'loss': 1.1452, 'grad_norm': 0.9044029116630554, 'learning_rate': 1.798076923076923e-05, 'epoch': 0.03}
{'loss': 1.2045, 'grad_norm': 0.8939404487609863, 'learning_rate': 1.8012820512820515e-05, 'epoch': 0.03}
{'loss': 1.1487, 'grad_norm': 0.860660970211029, 'learning_rate': 1.8044871794871796e-05, 'epoch': 0.03}
{'loss': 1.0809, 'grad_norm': 0.8819774389266968, 'learning_rate': 1.807692307692308e-05, 'epoch': 0.03}
{'loss': 0.7902, 'grad_norm': 1.019918441772461, 'learning_rate': 1.810897435897436e-05, 'epoch': 0.03}
{'loss': 0.952, 'grad_norm': 1.0371226072311401, 'learning_rate': 1.8141025641025644e-05, 'epoch': 0.03}
{'loss': 0.8871, 'grad_norm': 0.4819643795490265, 'learning_rate': 1.8173076923076924e-05, 'epoch': 0.03}
{'loss': 0.8172, 'grad_norm': 0.8441538214683533, 'learning_rate': 1.8205128205128208e-05, 'epoch': 0.03}
{'loss': 1.0972, 'grad_norm': 0.8649625182151794, 'learning_rate': 1.8237179487179488e-05, 'epoch': 0.03}
{'loss': 1.114, 'grad_norm': 0.8443472385406494, 'learning_rate': 1.826923076923077e-05, 'epoch': 0.03}
{'loss': 1.0109, 'grad_norm': 0.7499252557754517, 'learning_rate': 1.8301282051282052e-05, 'epoch': 0.03}
{'loss': 1.0372, 'grad_norm': 0.8547368049621582, 'learning_rate': 1.8333333333333333e-05, 'epoch': 0.03}
{'loss': 0.8309, 'grad_norm': 0.7832189798355103, 'learning_rate': 1.8365384615384617e-05, 'epoch': 0.03}
{'loss': 0.8735, 'grad_norm': 0.8158490657806396, 'learning_rate': 1.8397435897435897e-05, 'epoch': 0.03}
{'loss': 1.0689, 'grad_norm': 0.8716453909873962, 'learning_rate': 1.842948717948718e-05, 'epoch': 0.03}
{'loss': 0.9922, 'grad_norm': 0.8008766770362854, 'learning_rate': 1.8461538461538465e-05, 'epoch': 0.03}
{'loss': 1.0466, 'grad_norm': 0.894132673740387, 'learning_rate': 1.8493589743589745e-05, 'epoch': 0.03}
{'loss': 1.0474, 'grad_norm': 0.834247350692749, 'learning_rate': 1.852564102564103e-05, 'epoch': 0.03}
{'loss': 0.9821, 'grad_norm': 0.8444258570671082, 'learning_rate': 1.855769230769231e-05, 'epoch': 0.03}
{'loss': 0.881, 'grad_norm': 0.8866081237792969, 'learning_rate': 1.8589743589743593e-05, 'epoch': 0.03}
{'loss': 1.0802, 'grad_norm': 1.0215085744857788, 'learning_rate': 1.8621794871794873e-05, 'epoch': 0.03}
{'loss': 1.0622, 'grad_norm': 0.8301052451133728, 'learning_rate': 1.8653846153846157e-05, 'epoch': 0.03}
{'loss': 1.0443, 'grad_norm': 1.0220427513122559, 'learning_rate': 1.8685897435897438e-05, 'epoch': 0.03}
{'loss': 0.9673, 'grad_norm': 1.0681463479995728, 'learning_rate': 1.8717948717948718e-05, 'epoch': 0.03}
{'loss': 1.1111, 'grad_norm': 0.9176316857337952, 'learning_rate': 1.8750000000000002e-05, 'epoch': 0.03}
{'loss': 1.0037, 'grad_norm': 0.7627912163734436, 'learning_rate': 1.8782051282051282e-05, 'epoch': 0.03}
{'loss': 0.9953, 'grad_norm': 0.7811419367790222, 'learning_rate': 1.8814102564102566e-05, 'epoch': 0.03}
{'loss': 1.0252, 'grad_norm': 0.7343289256095886, 'learning_rate': 1.8846153846153846e-05, 'epoch': 0.03}
{'loss': 1.1235, 'grad_norm': 0.921813428401947, 'learning_rate': 1.887820512820513e-05, 'epoch': 0.03}
{'loss': 1.0732, 'grad_norm': 0.7727545499801636, 'learning_rate': 1.891025641025641e-05, 'epoch': 0.03}
{'loss': 1.0245, 'grad_norm': 0.7740411162376404, 'learning_rate': 1.8942307692307694e-05, 'epoch': 0.03}
{'loss': 0.9371, 'grad_norm': 0.7624741792678833, 'learning_rate': 1.8974358974358975e-05, 'epoch': 0.03}
{'loss': 0.9888, 'grad_norm': 0.7791551351547241, 'learning_rate': 1.900641025641026e-05, 'epoch': 0.03}
{'loss': 1.1608, 'grad_norm': 1.0896766185760498, 'learning_rate': 1.903846153846154e-05, 'epoch': 0.03}
{'loss': 1.1399, 'grad_norm': 0.9492138028144836, 'learning_rate': 1.9070512820512823e-05, 'epoch': 0.03}
{'loss': 1.1743, 'grad_norm': 0.7768984436988831, 'learning_rate': 1.9102564102564106e-05, 'epoch': 0.03}
{'loss': 1.1252, 'grad_norm': 0.9573771953582764, 'learning_rate': 1.9134615384615387e-05, 'epoch': 0.03}
{'loss': 1.0494, 'grad_norm': 0.8622171878814697, 'learning_rate': 1.916666666666667e-05, 'epoch': 0.03}
{'loss': 0.9263, 'grad_norm': 0.5601080060005188, 'learning_rate': 1.919871794871795e-05, 'epoch': 0.03}
{'loss': 1.2152, 'grad_norm': 1.0053064823150635, 'learning_rate': 1.923076923076923e-05, 'epoch': 0.03}
{'loss': 1.1492, 'grad_norm': 0.9926511645317078, 'learning_rate': 1.9262820512820515e-05, 'epoch': 0.03}
{'loss': 1.0477, 'grad_norm': 0.8026167750358582, 'learning_rate': 1.9294871794871796e-05, 'epoch': 0.03}
{'loss': 0.9325, 'grad_norm': 1.0011976957321167, 'learning_rate': 1.932692307692308e-05, 'epoch': 0.03}
{'loss': 0.8913, 'grad_norm': 0.8613121509552002, 'learning_rate': 1.935897435897436e-05, 'epoch': 0.03}
{'loss': 1.0005, 'grad_norm': 0.867433488368988, 'learning_rate': 1.9391025641025644e-05, 'epoch': 0.03}
{'loss': 1.1212, 'grad_norm': 0.7681829333305359, 'learning_rate': 1.9423076923076924e-05, 'epoch': 0.03}
{'loss': 1.1085, 'grad_norm': 0.7649393677711487, 'learning_rate': 1.9455128205128208e-05, 'epoch': 0.03}
{'loss': 0.9263, 'grad_norm': 0.8906941413879395, 'learning_rate': 1.9487179487179488e-05, 'epoch': 0.03}
{'loss': 0.9539, 'grad_norm': 0.8797250986099243, 'learning_rate': 1.9519230769230772e-05, 'epoch': 0.03}
{'loss': 1.1359, 'grad_norm': 0.9029298424720764, 'learning_rate': 1.9551282051282052e-05, 'epoch': 0.03}
{'loss': 0.974, 'grad_norm': 0.8391796350479126, 'learning_rate': 1.9583333333333333e-05, 'epoch': 0.03}
{'loss': 1.0144, 'grad_norm': 0.8303431272506714, 'learning_rate': 1.9615384615384617e-05, 'epoch': 0.03}
{'loss': 0.991, 'grad_norm': 0.8061428070068359, 'learning_rate': 1.9647435897435897e-05, 'epoch': 0.03}
{'loss': 0.951, 'grad_norm': 0.8306142091751099, 'learning_rate': 1.967948717948718e-05, 'epoch': 0.03}
{'loss': 0.9228, 'grad_norm': 0.7656044960021973, 'learning_rate': 1.9711538461538465e-05, 'epoch': 0.03}
{'loss': 0.8526, 'grad_norm': 0.7281187176704407, 'learning_rate': 1.9743589743589745e-05, 'epoch': 0.03}
{'loss': 0.9609, 'grad_norm': 0.7101075053215027, 'learning_rate': 1.977564102564103e-05, 'epoch': 0.03}
{'loss': 0.996, 'grad_norm': 1.0152934789657593, 'learning_rate': 1.980769230769231e-05, 'epoch': 0.03}
{'loss': 0.9195, 'grad_norm': 0.8108090758323669, 'learning_rate': 1.9839743589743593e-05, 'epoch': 0.03}
{'loss': 1.0339, 'grad_norm': 0.8401195406913757, 'learning_rate': 1.9871794871794873e-05, 'epoch': 0.03}
{'loss': 1.0791, 'grad_norm': 0.9618529677391052, 'learning_rate': 1.9903846153846157e-05, 'epoch': 0.03}
{'loss': 1.0452, 'grad_norm': 0.9070309400558472, 'learning_rate': 1.9935897435897437e-05, 'epoch': 0.03}
{'loss': 1.0101, 'grad_norm': 0.732002317905426, 'learning_rate': 1.996794871794872e-05, 'epoch': 0.03}
{'loss': 0.9591, 'grad_norm': 0.824485719203949, 'learning_rate': 2e-05, 'epoch': 0.03}
{'loss': 1.1965, 'grad_norm': 0.8943451642990112, 'learning_rate': 1.9999999878664707e-05, 'epoch': 0.03}
{'loss': 1.058, 'grad_norm': 0.8792058825492859, 'learning_rate': 1.999999951465882e-05, 'epoch': 0.03}
{'loss': 0.9965, 'grad_norm': 0.8299339413642883, 'learning_rate': 1.999999890798236e-05, 'epoch': 0.03}
{'loss': 1.018, 'grad_norm': 0.766758382320404, 'learning_rate': 1.9999998058635333e-05, 'epoch': 0.03}
{'loss': 1.1595, 'grad_norm': 0.8804039359092712, 'learning_rate': 1.999999696661776e-05, 'epoch': 0.03}
{'loss': 0.9911, 'grad_norm': 0.8228870630264282, 'learning_rate': 1.999999563192967e-05, 'epoch': 0.03}
{'loss': 0.9557, 'grad_norm': 0.7611209750175476, 'learning_rate': 1.9999994054571096e-05, 'epoch': 0.03}
{'loss': 1.2527, 'grad_norm': 0.9995893836021423, 'learning_rate': 1.9999992234542078e-05, 'epoch': 0.03}
{'loss': 1.1879, 'grad_norm': 0.8959020972251892, 'learning_rate': 1.9999990171842654e-05, 'epoch': 0.03}
{'loss': 0.9085, 'grad_norm': 0.8982900381088257, 'learning_rate': 1.9999987866472878e-05, 'epoch': 0.03}
{'loss': 1.0637, 'grad_norm': 0.8901443481445312, 'learning_rate': 1.9999985318432804e-05, 'epoch': 0.03}
{'loss': 1.0468, 'grad_norm': 0.7981840372085571, 'learning_rate': 1.9999982527722498e-05, 'epoch': 0.03}
{'loss': 0.8838, 'grad_norm': 0.5084526538848877, 'learning_rate': 1.9999979494342022e-05, 'epoch': 0.03}
{'loss': 1.0853, 'grad_norm': 0.8749194741249084, 'learning_rate': 1.9999976218291455e-05, 'epoch': 0.03}
{'loss': 1.0171, 'grad_norm': 1.1300978660583496, 'learning_rate': 1.9999972699570876e-05, 'epoch': 0.03}
{'loss': 1.0539, 'grad_norm': 0.7758652567863464, 'learning_rate': 1.9999968938180364e-05, 'epoch': 0.03}
{'loss': 1.062, 'grad_norm': 0.7996286153793335, 'learning_rate': 1.9999964934120016e-05, 'epoch': 0.03}
{'loss': 1.0528, 'grad_norm': 0.8658843636512756, 'learning_rate': 1.999996068738993e-05, 'epoch': 0.03}
{'loss': 1.1255, 'grad_norm': 0.8524185419082642, 'learning_rate': 1.9999956197990205e-05, 'epoch': 0.03}
{'loss': 1.0002, 'grad_norm': 0.7807876467704773, 'learning_rate': 1.9999951465920953e-05, 'epoch': 0.03}
{'loss': 1.0126, 'grad_norm': 0.8397097587585449, 'learning_rate': 1.9999946491182284e-05, 'epoch': 0.03}
{'loss': 1.1678, 'grad_norm': 0.8205142617225647, 'learning_rate': 1.9999941273774327e-05, 'epoch': 0.03}
{'loss': 1.068, 'grad_norm': 0.8775274157524109, 'learning_rate': 1.9999935813697204e-05, 'epoch': 0.03}
{'loss': 0.9614, 'grad_norm': 0.8445140719413757, 'learning_rate': 1.9999930110951043e-05, 'epoch': 0.03}
{'loss': 0.9922, 'grad_norm': 0.8312506079673767, 'learning_rate': 1.999992416553599e-05, 'epoch': 0.03}
{'loss': 1.1396, 'grad_norm': 1.0140995979309082, 'learning_rate': 1.9999917977452187e-05, 'epoch': 0.03}
{'loss': 1.0606, 'grad_norm': 0.555687665939331, 'learning_rate': 1.9999911546699785e-05, 'epoch': 0.03}
{'loss': 0.9873, 'grad_norm': 0.7423077821731567, 'learning_rate': 1.9999904873278933e-05, 'epoch': 0.03}
{'loss': 1.0707, 'grad_norm': 0.729114294052124, 'learning_rate': 1.9999897957189802e-05, 'epoch': 0.03}
{'loss': 0.8322, 'grad_norm': 0.77780681848526, 'learning_rate': 1.9999890798432556e-05, 'epoch': 0.03}
{'loss': 1.0747, 'grad_norm': 0.7786168456077576, 'learning_rate': 1.9999883397007366e-05, 'epoch': 0.03}
{'loss': 1.1591, 'grad_norm': 0.959309458732605, 'learning_rate': 1.999987575291442e-05, 'epoch': 0.03}
{'loss': 1.1311, 'grad_norm': 0.8740542531013489, 'learning_rate': 1.9999867866153894e-05, 'epoch': 0.03}
{'loss': 0.8142, 'grad_norm': 0.8147522211074829, 'learning_rate': 1.9999859736725984e-05, 'epoch': 0.03}
{'loss': 0.96, 'grad_norm': 0.705296516418457, 'learning_rate': 1.9999851364630886e-05, 'epoch': 0.03}
{'loss': 0.8808, 'grad_norm': 1.195220947265625, 'learning_rate': 1.9999842749868808e-05, 'epoch': 0.03}
{'loss': 0.8609, 'grad_norm': 0.7754303812980652, 'learning_rate': 1.9999833892439952e-05, 'epoch': 0.03}
{'loss': 1.1375, 'grad_norm': 0.8945397734642029, 'learning_rate': 1.9999824792344536e-05, 'epoch': 0.03}
{'loss': 1.0067, 'grad_norm': 0.8930292725563049, 'learning_rate': 1.999981544958278e-05, 'epoch': 0.03}
{'loss': 0.9937, 'grad_norm': 0.5574138760566711, 'learning_rate': 1.9999805864154913e-05, 'epoch': 0.03}
{'loss': 1.1689, 'grad_norm': 0.8945481777191162, 'learning_rate': 1.9999796036061164e-05, 'epoch': 0.03}
{'loss': 1.0338, 'grad_norm': 0.6813008189201355, 'learning_rate': 1.9999785965301776e-05, 'epoch': 0.03}
{'loss': 1.0954, 'grad_norm': 0.8144582509994507, 'learning_rate': 1.999977565187699e-05, 'epoch': 0.03}
{'loss': 0.9174, 'grad_norm': 0.48041215538978577, 'learning_rate': 1.9999765095787055e-05, 'epoch': 0.03}
{'loss': 1.093, 'grad_norm': 0.8725295662879944, 'learning_rate': 1.999975429703223e-05, 'epoch': 0.03}
{'loss': 0.983, 'grad_norm': 0.8409543633460999, 'learning_rate': 1.999974325561278e-05, 'epoch': 0.03}
{'loss': 1.0983, 'grad_norm': 0.8723016381263733, 'learning_rate': 1.9999731971528965e-05, 'epoch': 0.03}
{'loss': 0.9923, 'grad_norm': 0.8563247919082642, 'learning_rate': 1.999972044478107e-05, 'epoch': 0.03}
{'loss': 0.9446, 'grad_norm': 0.7938975095748901, 'learning_rate': 1.999970867536936e-05, 'epoch': 0.03}
{'loss': 1.0418, 'grad_norm': 0.717082142829895, 'learning_rate': 1.9999696663294133e-05, 'epoch': 0.03}
{'loss': 0.9779, 'grad_norm': 0.7758868336677551, 'learning_rate': 1.9999684408555673e-05, 'epoch': 0.03}
{'loss': 0.9723, 'grad_norm': 0.741668701171875, 'learning_rate': 1.9999671911154285e-05, 'epoch': 0.03}
{'loss': 0.9492, 'grad_norm': 0.7958852648735046, 'learning_rate': 1.9999659171090263e-05, 'epoch': 0.03}
{'loss': 1.0359, 'grad_norm': 0.8220642805099487, 'learning_rate': 1.9999646188363925e-05, 'epoch': 0.03}
{'loss': 1.0589, 'grad_norm': 0.8423656225204468, 'learning_rate': 1.9999632962975578e-05, 'epoch': 0.03}
{'loss': 1.005, 'grad_norm': 0.8628412485122681, 'learning_rate': 1.999961949492555e-05, 'epoch': 0.03}
{'loss': 1.014, 'grad_norm': 0.7503682971000671, 'learning_rate': 1.999960578421416e-05, 'epoch': 0.03}
{'loss': 0.8986, 'grad_norm': 0.8813170790672302, 'learning_rate': 1.999959183084175e-05, 'epoch': 0.03}
{'loss': 1.0168, 'grad_norm': 0.8500957489013672, 'learning_rate': 1.999957763480865e-05, 'epoch': 0.03}
{'loss': 1.0746, 'grad_norm': 0.9111309051513672, 'learning_rate': 1.999956319611521e-05, 'epoch': 0.03}
{'loss': 0.9603, 'grad_norm': 0.7704781293869019, 'learning_rate': 1.9999548514761785e-05, 'epoch': 0.03}
{'loss': 1.0695, 'grad_norm': 0.7772977948188782, 'learning_rate': 1.999953359074872e-05, 'epoch': 0.03}
{'loss': 0.9851, 'grad_norm': 0.5565125942230225, 'learning_rate': 1.999951842407638e-05, 'epoch': 0.03}
{'loss': 0.9505, 'grad_norm': 0.7883571982383728, 'learning_rate': 1.9999503014745138e-05, 'epoch': 0.03}
{'loss': 0.9484, 'grad_norm': 0.8663218021392822, 'learning_rate': 1.9999487362755366e-05, 'epoch': 0.03}
{'loss': 1.1364, 'grad_norm': 0.9546367526054382, 'learning_rate': 1.9999471468107444e-05, 'epoch': 0.03}
{'loss': 1.1793, 'grad_norm': 0.8044337630271912, 'learning_rate': 1.9999455330801752e-05, 'epoch': 0.03}
{'loss': 1.0602, 'grad_norm': 0.9777510166168213, 'learning_rate': 1.999943895083869e-05, 'epoch': 0.03}
{'loss': 1.0001, 'grad_norm': 0.8003023266792297, 'learning_rate': 1.999942232821865e-05, 'epoch': 0.03}
{'loss': 0.9568, 'grad_norm': 0.7726084589958191, 'learning_rate': 1.999940546294204e-05, 'epoch': 0.03}
{'loss': 1.1008, 'grad_norm': 0.8622085452079773, 'learning_rate': 1.9999388355009266e-05, 'epoch': 0.03}
{'loss': 1.0364, 'grad_norm': 0.8952621817588806, 'learning_rate': 1.9999371004420744e-05, 'epoch': 0.03}
{'loss': 0.8373, 'grad_norm': 0.707632303237915, 'learning_rate': 1.9999353411176893e-05, 'epoch': 0.03}
{'loss': 1.1555, 'grad_norm': 0.969174325466156, 'learning_rate': 1.9999335575278143e-05, 'epoch': 0.03}
{'loss': 1.0358, 'grad_norm': 0.8397956490516663, 'learning_rate': 1.9999317496724925e-05, 'epoch': 0.03}
{'loss': 0.9493, 'grad_norm': 0.7012098431587219, 'learning_rate': 1.999929917551768e-05, 'epoch': 0.03}
{'loss': 1.0172, 'grad_norm': 0.9666372537612915, 'learning_rate': 1.999928061165685e-05, 'epoch': 0.03}
{'loss': 0.901, 'grad_norm': 0.9178546667098999, 'learning_rate': 1.9999261805142885e-05, 'epoch': 0.03}
{'loss': 0.9293, 'grad_norm': 0.5096225142478943, 'learning_rate': 1.9999242755976246e-05, 'epoch': 0.03}
{'loss': 1.0431, 'grad_norm': 0.8703485727310181, 'learning_rate': 1.999922346415739e-05, 'epoch': 0.03}
{'loss': 1.1411, 'grad_norm': 1.0122673511505127, 'learning_rate': 1.9999203929686786e-05, 'epoch': 0.03}
{'loss': 1.0092, 'grad_norm': 0.7231246829032898, 'learning_rate': 1.9999184152564907e-05, 'epoch': 0.03}
{'loss': 0.9555, 'grad_norm': 0.7111489176750183, 'learning_rate': 1.999916413279224e-05, 'epoch': 0.03}
{'loss': 1.1732, 'grad_norm': 0.8974031209945679, 'learning_rate': 1.9999143870369265e-05, 'epoch': 0.03}
{'loss': 1.177, 'grad_norm': 0.9024126529693604, 'learning_rate': 1.9999123365296473e-05, 'epoch': 0.03}
{'loss': 1.0626, 'grad_norm': 0.7614410519599915, 'learning_rate': 1.9999102617574366e-05, 'epoch': 0.03}
{'loss': 1.0349, 'grad_norm': 0.811133086681366, 'learning_rate': 1.999908162720344e-05, 'epoch': 0.03}
{'loss': 1.1779, 'grad_norm': 0.9135233759880066, 'learning_rate': 1.9999060394184214e-05, 'epoch': 0.03}
{'loss': 0.9958, 'grad_norm': 0.8559321761131287, 'learning_rate': 1.99990389185172e-05, 'epoch': 0.03}
{'loss': 0.9253, 'grad_norm': 0.8558052778244019, 'learning_rate': 1.999901720020291e-05, 'epoch': 0.03}
{'loss': 0.9356, 'grad_norm': 0.862046480178833, 'learning_rate': 1.9998995239241883e-05, 'epoch': 0.03}
{'loss': 1.0428, 'grad_norm': 0.7719448804855347, 'learning_rate': 1.9998973035634648e-05, 'epoch': 0.03}
{'loss': 1.0109, 'grad_norm': 0.7963664531707764, 'learning_rate': 1.9998950589381743e-05, 'epoch': 0.03}
{'loss': 0.9218, 'grad_norm': 0.9356434345245361, 'learning_rate': 1.9998927900483714e-05, 'epoch': 0.03}
{'loss': 1.0641, 'grad_norm': 0.8593753576278687, 'learning_rate': 1.9998904968941107e-05, 'epoch': 0.03}
{'loss': 1.1466, 'grad_norm': 0.9269425868988037, 'learning_rate': 1.9998881794754484e-05, 'epoch': 0.03}
{'loss': 0.8661, 'grad_norm': 0.9485945105552673, 'learning_rate': 1.9998858377924408e-05, 'epoch': 0.03}
{'loss': 1.0248, 'grad_norm': 0.7664346098899841, 'learning_rate': 1.9998834718451444e-05, 'epoch': 0.03}
{'loss': 1.0203, 'grad_norm': 1.0792516469955444, 'learning_rate': 1.999881081633616e-05, 'epoch': 0.03}
{'loss': 1.0079, 'grad_norm': 0.9836418628692627, 'learning_rate': 1.999878667157915e-05, 'epoch': 0.03}
{'loss': 0.9586, 'grad_norm': 0.8273385763168335, 'learning_rate': 1.9998762284180996e-05, 'epoch': 0.03}
{'loss': 0.9788, 'grad_norm': 0.7844257354736328, 'learning_rate': 1.999873765414228e-05, 'epoch': 0.03}
{'loss': 1.1152, 'grad_norm': 0.9082342386245728, 'learning_rate': 1.999871278146361e-05, 'epoch': 0.04}
{'loss': 1.0293, 'grad_norm': 0.814767062664032, 'learning_rate': 1.9998687666145585e-05, 'epoch': 0.04}
{'loss': 1.0257, 'grad_norm': 0.758752703666687, 'learning_rate': 1.9998662308188813e-05, 'epoch': 0.04}
{'loss': 1.0232, 'grad_norm': 0.7608567476272583, 'learning_rate': 1.9998636707593913e-05, 'epoch': 0.04}
{'loss': 1.0752, 'grad_norm': 0.9188902378082275, 'learning_rate': 1.9998610864361506e-05, 'epoch': 0.04}
{'loss': 1.201, 'grad_norm': 0.8942911028862, 'learning_rate': 1.999858477849222e-05, 'epoch': 0.04}
{'loss': 0.8983, 'grad_norm': 0.44649234414100647, 'learning_rate': 1.9998558449986684e-05, 'epoch': 0.04}
{'loss': 1.02, 'grad_norm': 0.7655828595161438, 'learning_rate': 1.9998531878845536e-05, 'epoch': 0.04}
{'loss': 1.2434, 'grad_norm': 1.007041335105896, 'learning_rate': 1.999850506506943e-05, 'epoch': 0.04}
{'loss': 0.9825, 'grad_norm': 0.7736786603927612, 'learning_rate': 1.999847800865901e-05, 'epoch': 0.04}
{'loss': 0.9375, 'grad_norm': 0.7957237958908081, 'learning_rate': 1.9998450709614928e-05, 'epoch': 0.04}
{'loss': 1.067, 'grad_norm': 0.723564088344574, 'learning_rate': 1.9998423167937852e-05, 'epoch': 0.04}
{'loss': 1.1229, 'grad_norm': 0.8337560892105103, 'learning_rate': 1.9998395383628457e-05, 'epoch': 0.04}
{'loss': 0.9919, 'grad_norm': 0.7687285542488098, 'learning_rate': 1.9998367356687405e-05, 'epoch': 0.04}
{'loss': 0.9067, 'grad_norm': 0.8048933148384094, 'learning_rate': 1.9998339087115378e-05, 'epoch': 0.04}
{'loss': 1.1465, 'grad_norm': 0.8885508179664612, 'learning_rate': 1.9998310574913074e-05, 'epoch': 0.04}
{'loss': 0.9148, 'grad_norm': 0.655806303024292, 'learning_rate': 1.999828182008117e-05, 'epoch': 0.04}
{'loss': 1.0555, 'grad_norm': 0.9123980402946472, 'learning_rate': 1.9998252822620373e-05, 'epoch': 0.04}
{'loss': 1.0021, 'grad_norm': 0.9584470987319946, 'learning_rate': 1.9998223582531386e-05, 'epoch': 0.04}
{'loss': 0.9778, 'grad_norm': 0.5120870471000671, 'learning_rate': 1.9998194099814913e-05, 'epoch': 0.04}
{'loss': 1.0189, 'grad_norm': 0.8599615097045898, 'learning_rate': 1.9998164374471673e-05, 'epoch': 0.04}
{'loss': 1.0395, 'grad_norm': 0.9558629393577576, 'learning_rate': 1.9998134406502384e-05, 'epoch': 0.04}
{'loss': 0.8667, 'grad_norm': 0.7748287916183472, 'learning_rate': 1.999810419590778e-05, 'epoch': 0.04}
{'loss': 0.9211, 'grad_norm': 1.108821153640747, 'learning_rate': 1.9998073742688592e-05, 'epoch': 0.04}
{'loss': 1.0112, 'grad_norm': 0.785932719707489, 'learning_rate': 1.9998043046845555e-05, 'epoch': 0.04}
{'loss': 1.1042, 'grad_norm': 0.7336040139198303, 'learning_rate': 1.9998012108379417e-05, 'epoch': 0.04}
{'loss': 1.0459, 'grad_norm': 0.7296402454376221, 'learning_rate': 1.9997980927290928e-05, 'epoch': 0.04}
{'loss': 0.9969, 'grad_norm': 0.8552024364471436, 'learning_rate': 1.9997949503580844e-05, 'epoch': 0.04}
{'loss': 0.9896, 'grad_norm': 0.8021774291992188, 'learning_rate': 1.999791783724993e-05, 'epoch': 0.04}
{'loss': 0.9249, 'grad_norm': 0.8763478994369507, 'learning_rate': 1.999788592829895e-05, 'epoch': 0.04}
{'loss': 1.0581, 'grad_norm': 0.7976691126823425, 'learning_rate': 1.999785377672869e-05, 'epoch': 0.04}
{'loss': 0.9346, 'grad_norm': 0.7739353179931641, 'learning_rate': 1.9997821382539914e-05, 'epoch': 0.04}
{'loss': 1.1068, 'grad_norm': 0.7747472524642944, 'learning_rate': 1.9997788745733415e-05, 'epoch': 0.04}
{'loss': 1.0939, 'grad_norm': 0.8065820932388306, 'learning_rate': 1.9997755866309988e-05, 'epoch': 0.04}
{'loss': 0.9971, 'grad_norm': 0.7314136624336243, 'learning_rate': 1.999772274427043e-05, 'epoch': 0.04}
{'loss': 1.0162, 'grad_norm': 0.8080140948295593, 'learning_rate': 1.999768937961554e-05, 'epoch': 0.04}
{'loss': 0.8822, 'grad_norm': 0.7593541145324707, 'learning_rate': 1.9997655772346132e-05, 'epoch': 0.04}
{'loss': 0.9119, 'grad_norm': 0.699510931968689, 'learning_rate': 1.999762192246302e-05, 'epoch': 0.04}
{'loss': 1.0931, 'grad_norm': 0.8867325186729431, 'learning_rate': 1.9997587829967027e-05, 'epoch': 0.04}
{'loss': 1.0619, 'grad_norm': 0.9062314629554749, 'learning_rate': 1.999755349485898e-05, 'epoch': 0.04}
{'loss': 1.0621, 'grad_norm': 0.8427708745002747, 'learning_rate': 1.999751891713971e-05, 'epoch': 0.04}
{'loss': 1.189, 'grad_norm': 1.0028254985809326, 'learning_rate': 1.9997484096810054e-05, 'epoch': 0.04}
{'loss': 0.9503, 'grad_norm': 1.019435167312622, 'learning_rate': 1.999744903387087e-05, 'epoch': 0.04}
{'loss': 1.1914, 'grad_norm': 0.8391841650009155, 'learning_rate': 1.9997413728322992e-05, 'epoch': 0.04}
{'loss': 0.9626, 'grad_norm': 0.7452781200408936, 'learning_rate': 1.9997378180167285e-05, 'epoch': 0.04}
{'loss': 0.9605, 'grad_norm': 0.802740752696991, 'learning_rate': 1.999734238940461e-05, 'epoch': 0.04}
{'loss': 1.0677, 'grad_norm': 0.8834456205368042, 'learning_rate': 1.9997306356035838e-05, 'epoch': 0.04}
{'loss': 1.1022, 'grad_norm': 0.8571411967277527, 'learning_rate': 1.9997270080061843e-05, 'epoch': 0.04}
{'loss': 1.043, 'grad_norm': 0.7549683451652527, 'learning_rate': 1.9997233561483503e-05, 'epoch': 0.04}
{'loss': 1.0637, 'grad_norm': 0.837230384349823, 'learning_rate': 1.9997196800301704e-05, 'epoch': 0.04}
{'loss': 1.2136, 'grad_norm': 1.0480402708053589, 'learning_rate': 1.9997159796517342e-05, 'epoch': 0.04}
{'loss': 1.0794, 'grad_norm': 0.9670071005821228, 'learning_rate': 1.999712255013131e-05, 'epoch': 0.04}
{'loss': 1.0386, 'grad_norm': 0.7531297206878662, 'learning_rate': 1.9997085061144514e-05, 'epoch': 0.04}
{'loss': 1.0952, 'grad_norm': 0.7790042757987976, 'learning_rate': 1.9997047329557867e-05, 'epoch': 0.04}
{'loss': 0.9327, 'grad_norm': 0.5661066174507141, 'learning_rate': 1.9997009355372276e-05, 'epoch': 0.04}
{'loss': 1.2278, 'grad_norm': 0.8266929388046265, 'learning_rate': 1.9996971138588674e-05, 'epoch': 0.04}
{'loss': 0.8732, 'grad_norm': 0.9192249774932861, 'learning_rate': 1.9996932679207977e-05, 'epoch': 0.04}
{'loss': 1.1066, 'grad_norm': 0.6885008215904236, 'learning_rate': 1.999689397723113e-05, 'epoch': 0.04}
{'loss': 1.14, 'grad_norm': 0.8151462078094482, 'learning_rate': 1.9996855032659065e-05, 'epoch': 0.04}
{'loss': 1.1294, 'grad_norm': 0.9162700176239014, 'learning_rate': 1.9996815845492722e-05, 'epoch': 0.04}
{'loss': 1.0952, 'grad_norm': 0.7691985964775085, 'learning_rate': 1.9996776415733063e-05, 'epoch': 0.04}
{'loss': 0.8725, 'grad_norm': 0.7318562269210815, 'learning_rate': 1.9996736743381037e-05, 'epoch': 0.04}
{'loss': 0.8952, 'grad_norm': 0.9237099289894104, 'learning_rate': 1.9996696828437613e-05, 'epoch': 0.04}
{'loss': 1.0588, 'grad_norm': 0.7458556890487671, 'learning_rate': 1.9996656670903753e-05, 'epoch': 0.04}
{'loss': 1.0813, 'grad_norm': 0.8520091772079468, 'learning_rate': 1.999661627078044e-05, 'epoch': 0.04}
{'loss': 1.0247, 'grad_norm': 0.7406187653541565, 'learning_rate': 1.9996575628068645e-05, 'epoch': 0.04}
{'loss': 0.9055, 'grad_norm': 0.48157602548599243, 'learning_rate': 1.9996534742769355e-05, 'epoch': 0.04}
{'loss': 1.1282, 'grad_norm': 0.9449561238288879, 'learning_rate': 1.999649361488357e-05, 'epoch': 0.04}
{'loss': 1.1106, 'grad_norm': 0.8651115298271179, 'learning_rate': 1.9996452244412285e-05, 'epoch': 0.04}
{'loss': 1.254, 'grad_norm': 0.9121912717819214, 'learning_rate': 1.9996410631356496e-05, 'epoch': 0.04}
{'loss': 1.0326, 'grad_norm': 0.8469183444976807, 'learning_rate': 1.9996368775717228e-05, 'epoch': 0.04}
{'loss': 1.2312, 'grad_norm': 0.8977660536766052, 'learning_rate': 1.9996326677495482e-05, 'epoch': 0.04}
{'loss': 0.9805, 'grad_norm': 0.7823070883750916, 'learning_rate': 1.9996284336692286e-05, 'epoch': 0.04}
{'loss': 1.1614, 'grad_norm': 0.8901556134223938, 'learning_rate': 1.9996241753308673e-05, 'epoch': 0.04}
{'loss': 1.0246, 'grad_norm': 0.9160606265068054, 'learning_rate': 1.9996198927345665e-05, 'epoch': 0.04}
{'loss': 0.9374, 'grad_norm': 0.6739393472671509, 'learning_rate': 1.9996155858804307e-05, 'epoch': 0.04}
{'loss': 1.148, 'grad_norm': 1.0022462606430054, 'learning_rate': 1.9996112547685645e-05, 'epoch': 0.04}
{'loss': 1.0806, 'grad_norm': 0.8263349533081055, 'learning_rate': 1.9996068993990727e-05, 'epoch': 0.04}
{'loss': 1.0676, 'grad_norm': 0.9510481953620911, 'learning_rate': 1.9996025197720615e-05, 'epoch': 0.04}
{'loss': 1.2371, 'grad_norm': 0.8842639923095703, 'learning_rate': 1.999598115887637e-05, 'epoch': 0.04}
{'loss': 0.9317, 'grad_norm': 0.7209568619728088, 'learning_rate': 1.9995936877459053e-05, 'epoch': 0.04}
{'loss': 0.9282, 'grad_norm': 0.8651599884033203, 'learning_rate': 1.999589235346975e-05, 'epoch': 0.04}
{'loss': 1.0753, 'grad_norm': 0.7460667490959167, 'learning_rate': 1.9995847586909537e-05, 'epoch': 0.04}
{'loss': 0.981, 'grad_norm': 0.7722512483596802, 'learning_rate': 1.9995802577779498e-05, 'epoch': 0.04}
{'loss': 1.0435, 'grad_norm': 0.7498313188552856, 'learning_rate': 1.9995757326080727e-05, 'epoch': 0.04}
{'loss': 1.0414, 'grad_norm': 0.5391947031021118, 'learning_rate': 1.999571183181432e-05, 'epoch': 0.04}
{'loss': 0.9795, 'grad_norm': 0.7846492528915405, 'learning_rate': 1.9995666094981385e-05, 'epoch': 0.04}
{'loss': 0.9021, 'grad_norm': 0.7258723974227905, 'learning_rate': 1.9995620115583033e-05, 'epoch': 0.04}
{'loss': 0.8565, 'grad_norm': 0.703369677066803, 'learning_rate': 1.999557389362037e-05, 'epoch': 0.04}
{'loss': 1.11, 'grad_norm': 0.8252620100975037, 'learning_rate': 1.9995527429094534e-05, 'epoch': 0.04}
{'loss': 0.8927, 'grad_norm': 0.9810166954994202, 'learning_rate': 1.9995480722006636e-05, 'epoch': 0.04}
{'loss': 1.0043, 'grad_norm': 0.7404947280883789, 'learning_rate': 1.999543377235782e-05, 'epoch': 0.04}
{'loss': 0.9431, 'grad_norm': 0.7224720120429993, 'learning_rate': 1.999538658014922e-05, 'epoch': 0.04}
{'loss': 1.0024, 'grad_norm': 0.7027321457862854, 'learning_rate': 1.9995339145381982e-05, 'epoch': 0.04}
{'loss': 1.0367, 'grad_norm': 0.937564492225647, 'learning_rate': 1.999529146805726e-05, 'epoch': 0.04}
{'loss': 1.0715, 'grad_norm': 0.7910696268081665, 'learning_rate': 1.999524354817621e-05, 'epoch': 0.04}
{'loss': 1.0318, 'grad_norm': 0.8361924886703491, 'learning_rate': 1.999519538573999e-05, 'epoch': 0.04}
