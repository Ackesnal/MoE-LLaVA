g002 slots=1
g038 slots=1
g045 slots=1
g098 slots=1
VERSION: 1.1
MASTER_PORT: 33789
MASTER_ADDR: g002
Computed NODE_RANK: 0
Launching DeepSpeed on NODE_RANK 0 (host: g002)
[2025-09-05 16:03:05,871] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-09-05 16:04:21,475] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-05 16:04:59,713] [INFO] [runner.py:610:main] cmd = srun -n 4 --nodes 4 --gpus 1 --export=ALL,NCCL_ROOT=/apps/nccl/2.20.5-cu124,NCCL_ROOT_modshare=/apps/nccl/2.20.5-cu124:1,PYTHONSTARTUP=/etc/pythonstart,NCCL_HOME=/apps/nccl/2.20.5-cu124,PYTHONPATH=/datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA /home/li309/pct_code/venv/moellava-test2/bin/python -u moellava/train/train_mem.py --moe_enable True --num_experts 4 --top_k_experts 2 --capacity_factor 1.5 --moe_mode sparse --use_residual False --router_aux_loss_coef 0.01 --train_modules gate_proj up_proj down_proj wg --deepspeed ./scripts/zero2.json --model_name_or_path ./checkpoints/MoE-LLaVA-StableLM-1.6B-4e --version stablelm --data_path /scratch3/li309/data/llava_data/train_json/llava_image_tune_.json /scratch3/li309/data/llava_data/train_json/nlp_tune.json --image_folder /scratch3/li309/data/llava_data/train_data --image_tower openai/clip-vit-large-patch14-336 --image_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./finetuned_checkpoints/MoE-LLaVA-StableLM-1.6B-4e-RePa-2 --num_train_epochs 1 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --gradient_accumulation_steps 1 --eval_strategy no --save_strategy steps --save_steps 4000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 18 --lazy_preprocess True --report_to tensorboard --cache_dir ./cache_dir --report_to wandb --finetune_repa_mode true --repa_gated_ratio 1.0
