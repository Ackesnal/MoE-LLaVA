Loading cuda/12.4.0
  Unloading conflict: cuda/12.8.1
VERSION: 1.6 (NCCL-only)
MASTER_PORT: 57095
MASTER_ADDR: g015
[2025-09-17 04:46:25,757] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-17 04:46:25,757] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-17 04:46:25,757] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-17 04:46:25,757] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-17 04:46:25,758] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-17 04:46:25,758] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-17 04:46:25,758] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-17 04:46:25,758] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-17 04:46:25,758] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-17 04:46:25,758] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-17 04:46:25,758] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-17 04:46:25,758] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-17 04:46:25,758] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-17 04:46:25,758] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-17 04:46:25,759] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-17 04:46:25,759] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.

[2025-09-17 04:46:41,647] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-17 04:46:41,647] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-17 04:46:41,648] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-17 04:46:41,648] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-17 04:46:41,648] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-17 04:46:41,648] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-17 04:46:41,648] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-17 04:46:41,648] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-17 04:46:41,648] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-17 04:46:41,648] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-17 04:46:41,648] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-17 04:46:41,648] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-17 04:46:41,648] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-17 04:46:41,648] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-17 04:46:41,647] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-17 04:46:41,647] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode

⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode

⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode



⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
[2025-09-17 04:46:47,221] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-17 04:46:47,221] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-17 04:46:47,222] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-17 04:46:47,221] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-17 04:46:47,221] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-17 04:46:47,222] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-17 04:46:47,221] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-17 04:46:47,221] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-17 04:46:47,221] [INFO] [comm.py:852:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-09-17 04:46:47,222] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-17 04:46:47,222] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-17 04:46:47,222] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-17 04:46:47,222] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-17 04:46:47,222] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-17 04:46:47,222] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-17 04:46:47,229] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-17 04:46:47,229] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-17 04:46:47,713] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-17 04:46:47,744] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-17 04:46:47,745] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-17 04:46:47,747] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-17 04:46:47,749] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-17 04:46:47,751] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-17 04:46:47,753] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-17 04:46:47,754] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-17 04:46:47,756] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-17 04:46:47,758] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-17 04:46:47,759] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-17 04:46:47,761] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
Vision encoder and proj init.

Vision encoder and proj init.

Vision encoder and proj init.

Vision encoder and proj init.

Vision encoder and proj init.

Vision encoder and proj init.

Vision encoder and proj init.

Vision encoder and proj init.

RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Setting up RePaMoE fine-tuning mode...Setting up RePaMoE fine-tuning mode...

  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM

Setting up RePaMoE fine-tuning mode...
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Unfreeze all layers, except image_tower and mm_projector if present
  Unfreeze all layers, except image_tower and mm_projector if present
  Unfreeze all layers, except image_tower and mm_projector if present
  Unfreeze all layers, except image_tower and mm_projector if present
  Unfreeze all layers, except image_tower and mm_projector if present
  Unfreeze all layers, except image_tower and mm_projector if present
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Unfreeze all layers, except image_tower and mm_projector if present
  Unfreeze all layers, except image_tower and mm_projector if present
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Unfreeze all layers, except image_tower and mm_projector if present
Setting up RePaMoE fine-tuning mode...Setting up RePaMoE fine-tuning mode...

  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM

  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Unfreeze all layers, except image_tower and mm_projector if present
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Unfreeze all layers, except image_tower and mm_projector if present
  Unfreeze all layers, except image_tower and mm_projector if present
Formatting inputs...Skip in lazy mode
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Unfreeze all layers, except image_tower and mm_projector if present
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0  Set initial gated ratio to 1.0

  Total training steps: 5197.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps  Stage 1 (gated ratio reduction): 2598.0 steps

  Stage 2 (post-reparam training): 2599.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1
  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
Formatting inputs...Skip in lazy mode
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Unfreeze all layers, except image_tower and mm_projector if present
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Unfreeze all layers, except image_tower and mm_projector if present
  Unfreeze all layers, except image_tower and mm_projector if present
  Set initial gated ratio to 1.0  Set initial gated ratio to 1.0

  Total training steps: 5197.0  Total training steps: 5197.0

  Stage 1 (gated ratio reduction): 2598.0 steps  Stage 1 (gated ratio reduction): 2598.0 steps

  Stage 2 (post-reparam training): 2599.0 steps  Stage 2 (post-reparam training): 2599.0 steps

  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1

  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
NCCL version 2.21.5+cuda12.4
No existing process group found, creating a new group named: ep_size_1
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
No existing process group found, creating a new group named: ep_size_1
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
No existing process group found, creating a new group named: ep_size_1
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
No existing process group found, creating a new group named: ep_size_1
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
No existing process group found, creating a new group named: ep_size_1
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
No existing process group found, creating a new group named: ep_size_1
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
{'loss': 0.9712, 'grad_norm': 2.1067395210266113, 'learning_rate': 0.0, 'epoch': 0.0}
{'loss': 0.9337, 'grad_norm': 1.6501312255859375, 'learning_rate': 1.282051282051282e-07, 'epoch': 0.0}
{'loss': 1.0065, 'grad_norm': 1.9932762384414673, 'learning_rate': 2.564102564102564e-07, 'epoch': 0.0}
{'loss': 1.052, 'grad_norm': 1.9865272045135498, 'learning_rate': 3.846153846153847e-07, 'epoch': 0.0}
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
{'loss': 0.9423, 'grad_norm': 1.8718581199645996, 'learning_rate': 5.128205128205128e-07, 'epoch': 0.0}
{'loss': 0.9081, 'grad_norm': 1.947080135345459, 'learning_rate': 6.41025641025641e-07, 'epoch': 0.0}
{'loss': 1.0415, 'grad_norm': 2.0280871391296387, 'learning_rate': 7.692307692307694e-07, 'epoch': 0.0}
{'loss': 0.9689, 'grad_norm': 1.772114634513855, 'learning_rate': 8.974358974358975e-07, 'epoch': 0.0}
{'loss': 1.0493, 'grad_norm': 1.86968994140625, 'learning_rate': 1.0256410256410257e-06, 'epoch': 0.0}
{'loss': 0.972, 'grad_norm': 1.9444338083267212, 'learning_rate': 1.153846153846154e-06, 'epoch': 0.0}
Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)
Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)
Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)
Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)
Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)
Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)
Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)
Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)
Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)
Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)
Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)
Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)
Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)
Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)
Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)
Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)
{'loss': 0.9521, 'grad_norm': 1.8450895547866821, 'learning_rate': 1.282051282051282e-06, 'epoch': 0.0}
{'loss': 0.9867, 'grad_norm': 2.096595048904419, 'learning_rate': 1.4102564102564104e-06, 'epoch': 0.0}
{'loss': 0.8992, 'grad_norm': 1.5547245740890503, 'learning_rate': 1.5384615384615387e-06, 'epoch': 0.0}
{'loss': 0.9568, 'grad_norm': 2.0165467262268066, 'learning_rate': 1.6666666666666667e-06, 'epoch': 0.0}
{'loss': 0.9876, 'grad_norm': 1.8249282836914062, 'learning_rate': 1.794871794871795e-06, 'epoch': 0.0}
Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)
Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)
Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)
Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)
Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)
Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)
Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)
Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)
Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)
Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)
Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)
Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)
Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)
Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)

Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)
{'loss': 0.9959, 'grad_norm': 1.8054940700531006, 'learning_rate': 1.9230769230769234e-06, 'epoch': 0.0}
{'loss': 0.9378, 'grad_norm': 1.9226030111312866, 'learning_rate': 2.0512820512820513e-06, 'epoch': 0.0}
{'loss': 1.0189, 'grad_norm': 1.8644425868988037, 'learning_rate': 2.1794871794871797e-06, 'epoch': 0.0}
{'loss': 1.0437, 'grad_norm': 2.0103037357330322, 'learning_rate': 2.307692307692308e-06, 'epoch': 0.0}
{'loss': 1.0112, 'grad_norm': 1.8590580224990845, 'learning_rate': 2.435897435897436e-06, 'epoch': 0.0}
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
{'loss': 1.0225, 'grad_norm': 1.8255934715270996, 'learning_rate': 2.564102564102564e-06, 'epoch': 0.0}
{'loss': 1.0303, 'grad_norm': 1.7922981977462769, 'learning_rate': 2.6923076923076923e-06, 'epoch': 0.0}
{'loss': 0.933, 'grad_norm': 2.0338845252990723, 'learning_rate': 2.8205128205128207e-06, 'epoch': 0.0}
{'loss': 0.9877, 'grad_norm': 1.608060598373413, 'learning_rate': 2.948717948717949e-06, 'epoch': 0.0}
{'loss': 0.9944, 'grad_norm': 2.145236015319824, 'learning_rate': 3.0769230769230774e-06, 'epoch': 0.0}
Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)

Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)
Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)
Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)
Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)
Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)
Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)
Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)
Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)
Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)
Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)
Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)

Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)
Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)
{'loss': 1.0904, 'grad_norm': 2.074601888656616, 'learning_rate': 3.205128205128206e-06, 'epoch': 0.01}
{'loss': 1.0871, 'grad_norm': 1.8594281673431396, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.01}
{'loss': 0.8466, 'grad_norm': 1.5886766910552979, 'learning_rate': 3.4615384615384617e-06, 'epoch': 0.01}
{'loss': 1.0117, 'grad_norm': 1.915635108947754, 'learning_rate': 3.58974358974359e-06, 'epoch': 0.01}
{'loss': 1.0636, 'grad_norm': 2.0409433841705322, 'learning_rate': 3.7179487179487184e-06, 'epoch': 0.01}
Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)
Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)
Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)
Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)
Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)
Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)

Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)
Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)
Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)
Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)
Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)
Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)
Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)
Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)
Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)
{'loss': 0.9451, 'grad_norm': 1.9499057531356812, 'learning_rate': 3.846153846153847e-06, 'epoch': 0.01}
{'loss': 0.9759, 'grad_norm': 1.9987874031066895, 'learning_rate': 3.974358974358974e-06, 'epoch': 0.01}
{'loss': 0.9617, 'grad_norm': 1.5631229877471924, 'learning_rate': 4.102564102564103e-06, 'epoch': 0.01}
{'loss': 1.0361, 'grad_norm': 2.084942579269409, 'learning_rate': 4.230769230769231e-06, 'epoch': 0.01}
{'loss': 0.8807, 'grad_norm': 1.0320818424224854, 'learning_rate': 4.358974358974359e-06, 'epoch': 0.01}
{'loss': 0.8803, 'grad_norm': 1.8911787271499634, 'learning_rate': 4.487179487179488e-06, 'epoch': 0.01}
Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)
Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)
Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)
Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)
Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)
Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)
Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)
Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)
Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)
Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)
Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)
Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)
Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)
Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)
Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)
Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)
{'loss': 0.9867, 'grad_norm': 1.94944429397583, 'learning_rate': 4.615384615384616e-06, 'epoch': 0.01}
{'loss': 1.017, 'grad_norm': 2.2687950134277344, 'learning_rate': 4.743589743589744e-06, 'epoch': 0.01}
{'loss': 0.9748, 'grad_norm': 1.6799674034118652, 'learning_rate': 4.871794871794872e-06, 'epoch': 0.01}
{'loss': 1.0243, 'grad_norm': 2.0671067237854004, 'learning_rate': 5e-06, 'epoch': 0.01}
{'loss': 1.0706, 'grad_norm': 1.9479042291641235, 'learning_rate': 5.128205128205128e-06, 'epoch': 0.01}
Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)
Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)
Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)
Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)
Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)
Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)
Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)
Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)

Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)
Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)
Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)

Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)
Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)
Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)
{'loss': 0.8889, 'grad_norm': 2.0101840496063232, 'learning_rate': 5.256410256410257e-06, 'epoch': 0.01}
{'loss': 0.9389, 'grad_norm': 2.092902898788452, 'learning_rate': 5.384615384615385e-06, 'epoch': 0.01}
{'loss': 1.0341, 'grad_norm': 1.7985800504684448, 'learning_rate': 5.512820512820514e-06, 'epoch': 0.01}
{'loss': 0.9991, 'grad_norm': 2.1329832077026367, 'learning_rate': 5.641025641025641e-06, 'epoch': 0.01}
{'loss': 0.9822, 'grad_norm': 2.0709097385406494, 'learning_rate': 5.769230769230769e-06, 'epoch': 0.01}
Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)
Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)
Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)
Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)
Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)
Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)
Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)

Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)
Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)
Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)
Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)
Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)
Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)
Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)
Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)
{'loss': 1.0593, 'grad_norm': 2.1991984844207764, 'learning_rate': 5.897435897435898e-06, 'epoch': 0.01}
{'loss': 0.979, 'grad_norm': 1.8930310010910034, 'learning_rate': 6.025641025641026e-06, 'epoch': 0.01}
{'loss': 1.007, 'grad_norm': 2.3038580417633057, 'learning_rate': 6.153846153846155e-06, 'epoch': 0.01}
{'loss': 0.95, 'grad_norm': 2.173776149749756, 'learning_rate': 6.282051282051282e-06, 'epoch': 0.01}
{'loss': 1.0494, 'grad_norm': 2.161076545715332, 'learning_rate': 6.410256410256412e-06, 'epoch': 0.01}
Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)
Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)
Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)
Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)

Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)
Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)
Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)
Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)
Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)
Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)
Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)
Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)
Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)
Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)
Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)
{'loss': 1.0524, 'grad_norm': 2.380882740020752, 'learning_rate': 6.538461538461539e-06, 'epoch': 0.01}
{'loss': 1.0039, 'grad_norm': 2.100721836090088, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.01}
{'loss': 0.9727, 'grad_norm': 2.3452792167663574, 'learning_rate': 6.794871794871796e-06, 'epoch': 0.01}
{'loss': 1.0216, 'grad_norm': 2.0755605697631836, 'learning_rate': 6.923076923076923e-06, 'epoch': 0.01}
{'loss': 0.9919, 'grad_norm': 2.099017381668091, 'learning_rate': 7.051282051282053e-06, 'epoch': 0.01}
Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)
Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)
Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)
Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)
Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)
Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)
Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)
Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)
Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)
Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)
Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)
Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)
Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)
Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)
Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)
Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)
{'loss': 1.1696, 'grad_norm': 2.2103493213653564, 'learning_rate': 7.17948717948718e-06, 'epoch': 0.01}
{'loss': 1.0127, 'grad_norm': 2.113892078399658, 'learning_rate': 7.307692307692308e-06, 'epoch': 0.01}
{'loss': 1.0195, 'grad_norm': 2.1248397827148438, 'learning_rate': 7.435897435897437e-06, 'epoch': 0.01}
{'loss': 1.0053, 'grad_norm': 2.279320240020752, 'learning_rate': 7.564102564102564e-06, 'epoch': 0.01}
{'loss': 1.004, 'grad_norm': 1.231959342956543, 'learning_rate': 7.692307692307694e-06, 'epoch': 0.01}
{'loss': 1.0075, 'grad_norm': 1.825676441192627, 'learning_rate': 7.820512820512822e-06, 'epoch': 0.01}
Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)
Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)
Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)
Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)
Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)
Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)
Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)
Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)
Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)
Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)
Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)
Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)
Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)
Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)
Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)
Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)
{'loss': 1.0632, 'grad_norm': 2.2991931438446045, 'learning_rate': 7.948717948717949e-06, 'epoch': 0.01}
{'loss': 0.9802, 'grad_norm': 2.03702449798584, 'learning_rate': 8.076923076923077e-06, 'epoch': 0.01}
{'loss': 1.1493, 'grad_norm': 2.2512121200561523, 'learning_rate': 8.205128205128205e-06, 'epoch': 0.01}
{'loss': 0.9659, 'grad_norm': 2.086735725402832, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.01}
{'loss': 1.0278, 'grad_norm': 1.9638153314590454, 'learning_rate': 8.461538461538462e-06, 'epoch': 0.01}
Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)
Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)
Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)
Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)

Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)
Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)
Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)
Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)

Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)
Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)
Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)
Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)
Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)
Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)
{'loss': 1.015, 'grad_norm': 2.2578468322753906, 'learning_rate': 8.58974358974359e-06, 'epoch': 0.01}
{'loss': 1.0586, 'grad_norm': 2.1224117279052734, 'learning_rate': 8.717948717948719e-06, 'epoch': 0.01}
{'loss': 0.9678, 'grad_norm': 2.1996755599975586, 'learning_rate': 8.846153846153847e-06, 'epoch': 0.01}
{'loss': 1.048, 'grad_norm': 2.1235499382019043, 'learning_rate': 8.974358974358976e-06, 'epoch': 0.01}
{'loss': 1.0729, 'grad_norm': 2.149801731109619, 'learning_rate': 9.102564102564104e-06, 'epoch': 0.01}
Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)
Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)
Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)
Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)
Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)
Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)
Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)
Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)
Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)
Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)

Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)
Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)
Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)
Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)
Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)
{'loss': 1.0313, 'grad_norm': 1.9695842266082764, 'learning_rate': 9.230769230769232e-06, 'epoch': 0.01}
{'loss': 1.0342, 'grad_norm': 2.0529608726501465, 'learning_rate': 9.358974358974359e-06, 'epoch': 0.01}
{'loss': 0.9887, 'grad_norm': 2.1658294200897217, 'learning_rate': 9.487179487179487e-06, 'epoch': 0.01}
{'loss': 0.9674, 'grad_norm': 2.0660746097564697, 'learning_rate': 9.615384615384616e-06, 'epoch': 0.01}
{'loss': 1.0088, 'grad_norm': 2.152891159057617, 'learning_rate': 9.743589743589744e-06, 'epoch': 0.01}
Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)
Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)
Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)
Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)
Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)
Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)
Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)

Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)
Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)
Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)
Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)
Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)
Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)
Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)
Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)
{'loss': 0.9741, 'grad_norm': 1.7147961854934692, 'learning_rate': 9.871794871794872e-06, 'epoch': 0.02}
{'loss': 1.0153, 'grad_norm': 2.120802879333496, 'learning_rate': 1e-05, 'epoch': 0.02}
{'loss': 1.0354, 'grad_norm': 2.116314172744751, 'learning_rate': 1.012820512820513e-05, 'epoch': 0.02}
{'loss': 1.0553, 'grad_norm': 2.174734354019165, 'learning_rate': 1.0256410256410256e-05, 'epoch': 0.02}
{'loss': 0.948, 'grad_norm': 1.6535601615905762, 'learning_rate': 1.0384615384615386e-05, 'epoch': 0.02}
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
{'loss': 0.9747, 'grad_norm': 2.2307419776916504, 'learning_rate': 1.0512820512820514e-05, 'epoch': 0.02}
{'loss': 1.0787, 'grad_norm': 2.1838202476501465, 'learning_rate': 1.0641025641025643e-05, 'epoch': 0.02}
{'loss': 1.0166, 'grad_norm': 2.0318315029144287, 'learning_rate': 1.076923076923077e-05, 'epoch': 0.02}
{'loss': 0.9666, 'grad_norm': 1.9704267978668213, 'learning_rate': 1.0897435897435898e-05, 'epoch': 0.02}
{'loss': 0.9416, 'grad_norm': 2.1164464950561523, 'learning_rate': 1.1025641025641028e-05, 'epoch': 0.02}
{'loss': 1.1184, 'grad_norm': 2.1126904487609863, 'learning_rate': 1.1153846153846154e-05, 'epoch': 0.02}
Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)
Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)
Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)
Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)
Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)
Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)
Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)
Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)
Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)

Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)
Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)
Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)
Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)
Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)
Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)
{'loss': 1.0224, 'grad_norm': 2.306532621383667, 'learning_rate': 1.1282051282051283e-05, 'epoch': 0.02}
{'loss': 0.9993, 'grad_norm': 1.8937013149261475, 'learning_rate': 1.1410256410256411e-05, 'epoch': 0.02}
{'loss': 1.0668, 'grad_norm': 2.119593381881714, 'learning_rate': 1.1538461538461538e-05, 'epoch': 0.02}
{'loss': 1.1294, 'grad_norm': 2.208087682723999, 'learning_rate': 1.1666666666666668e-05, 'epoch': 0.02}
{'loss': 0.9877, 'grad_norm': 2.0619356632232666, 'learning_rate': 1.1794871794871796e-05, 'epoch': 0.02}
Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)
Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)
Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)
Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)
Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)
Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)
Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)
Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)
Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)
Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)
Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)
Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)
Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)
Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)

Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)
{'loss': 1.0158, 'grad_norm': 1.9073189496994019, 'learning_rate': 1.1923076923076925e-05, 'epoch': 0.02}
{'loss': 1.0442, 'grad_norm': 1.9988511800765991, 'learning_rate': 1.2051282051282051e-05, 'epoch': 0.02}
{'loss': 1.0387, 'grad_norm': 1.9285635948181152, 'learning_rate': 1.217948717948718e-05, 'epoch': 0.02}
{'loss': 1.0352, 'grad_norm': 1.673008918762207, 'learning_rate': 1.230769230769231e-05, 'epoch': 0.02}
{'loss': 1.0189, 'grad_norm': 2.002919912338257, 'learning_rate': 1.2435897435897436e-05, 'epoch': 0.02}
Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)
Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)
Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)
Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)

Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)

Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)
Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)
Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)
Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)
Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)
Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)
Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)
Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)
Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)
{'loss': 0.9434, 'grad_norm': 2.066514730453491, 'learning_rate': 1.2564102564102565e-05, 'epoch': 0.02}
{'loss': 0.936, 'grad_norm': 2.0393850803375244, 'learning_rate': 1.2692307692307693e-05, 'epoch': 0.02}
{'loss': 1.0609, 'grad_norm': 2.0573127269744873, 'learning_rate': 1.2820512820512823e-05, 'epoch': 0.02}
{'loss': 1.0192, 'grad_norm': 1.6923434734344482, 'learning_rate': 1.294871794871795e-05, 'epoch': 0.02}
{'loss': 1.0691, 'grad_norm': 2.113877058029175, 'learning_rate': 1.3076923076923078e-05, 'epoch': 0.02}
Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)
Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)
Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)
Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)
Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)
Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)
Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)
Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)
Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)
Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)
Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)
Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)
Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)
Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)
Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)
Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)
{'loss': 0.9677, 'grad_norm': 2.176558017730713, 'learning_rate': 1.3205128205128207e-05, 'epoch': 0.02}
{'loss': 1.0847, 'grad_norm': 2.051579236984253, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.02}
{'loss': 1.0416, 'grad_norm': 2.070068120956421, 'learning_rate': 1.3461538461538463e-05, 'epoch': 0.02}
{'loss': 1.0681, 'grad_norm': 2.038982391357422, 'learning_rate': 1.3589743589743592e-05, 'epoch': 0.02}
{'loss': 1.0963, 'grad_norm': 2.1716299057006836, 'learning_rate': 1.3717948717948718e-05, 'epoch': 0.02}
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
{'loss': 1.0207, 'grad_norm': 2.178654670715332, 'learning_rate': 1.3846153846153847e-05, 'epoch': 0.02}
{'loss': 1.0118, 'grad_norm': 2.356813907623291, 'learning_rate': 1.3974358974358975e-05, 'epoch': 0.02}
{'loss': 1.0548, 'grad_norm': 2.019775152206421, 'learning_rate': 1.4102564102564105e-05, 'epoch': 0.02}
{'loss': 1.0272, 'grad_norm': 2.065387725830078, 'learning_rate': 1.4230769230769232e-05, 'epoch': 0.02}
{'loss': 1.019, 'grad_norm': 2.386430025100708, 'learning_rate': 1.435897435897436e-05, 'epoch': 0.02}
{'loss': 0.974, 'grad_norm': 2.1211202144622803, 'learning_rate': 1.4487179487179489e-05, 'epoch': 0.02}
Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)
Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)
Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)
Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)
Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)
Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)
Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)
Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)
Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)
Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)
Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)
Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)
Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)
Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)
Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)
Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)
{'loss': 0.9749, 'grad_norm': 2.4379677772521973, 'learning_rate': 1.4615384615384615e-05, 'epoch': 0.02}
{'loss': 0.9971, 'grad_norm': 2.1345865726470947, 'learning_rate': 1.4743589743589745e-05, 'epoch': 0.02}
{'loss': 1.0374, 'grad_norm': 2.313218355178833, 'learning_rate': 1.4871794871794874e-05, 'epoch': 0.02}
{'loss': 1.0219, 'grad_norm': 2.186553716659546, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.02}
{'loss': 0.9407, 'grad_norm': 2.132465124130249, 'learning_rate': 1.5128205128205129e-05, 'epoch': 0.02}
Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)
Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)
Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)
Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)

Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)
Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)

Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)
Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)

Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)
Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)

Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)
Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)
{'loss': 1.0673, 'grad_norm': 2.120687484741211, 'learning_rate': 1.5256410256410257e-05, 'epoch': 0.02}
{'loss': 0.9712, 'grad_norm': 1.815264105796814, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.02}
{'loss': 1.027, 'grad_norm': 2.1406710147857666, 'learning_rate': 1.5512820512820516e-05, 'epoch': 0.02}
{'loss': 1.1219, 'grad_norm': 2.2495474815368652, 'learning_rate': 1.5641025641025644e-05, 'epoch': 0.02}
{'loss': 1.0023, 'grad_norm': 2.136378049850464, 'learning_rate': 1.576923076923077e-05, 'epoch': 0.02}
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
{'loss': 1.0573, 'grad_norm': 2.232698678970337, 'learning_rate': 1.5897435897435897e-05, 'epoch': 0.02}
{'loss': 1.0209, 'grad_norm': 2.1819190979003906, 'learning_rate': 1.602564102564103e-05, 'epoch': 0.02}
{'loss': 1.0589, 'grad_norm': 2.002498149871826, 'learning_rate': 1.6153846153846154e-05, 'epoch': 0.02}
{'loss': 1.1138, 'grad_norm': 2.24326753616333, 'learning_rate': 1.6282051282051282e-05, 'epoch': 0.02}
{'loss': 1.0919, 'grad_norm': 2.228090524673462, 'learning_rate': 1.641025641025641e-05, 'epoch': 0.02}
Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)
Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)
Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)
Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)

Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)
Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)
Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)
Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)
Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)
Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)
Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)
Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)
Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)
Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)
Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)
{'loss': 1.0563, 'grad_norm': 2.0286054611206055, 'learning_rate': 1.653846153846154e-05, 'epoch': 0.03}
{'loss': 0.9608, 'grad_norm': 2.1250874996185303, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.03}
Error with image file is truncated (73 bytes not processed)
{'loss': 1.1243, 'grad_norm': 2.3941755294799805, 'learning_rate': 1.6794871794871796e-05, 'epoch': 0.03}
{'loss': 1.0306, 'grad_norm': 2.1719229221343994, 'learning_rate': 1.6923076923076924e-05, 'epoch': 0.03}
{'loss': 1.0407, 'grad_norm': 1.6675630807876587, 'learning_rate': 1.7051282051282053e-05, 'epoch': 0.03}
Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)
Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)
Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)
Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)
Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)
Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)
Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)
Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)
Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)
Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)
Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)
Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)
Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)
Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)
Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)
Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)
{'loss': 1.1062, 'grad_norm': 2.2619006633758545, 'learning_rate': 1.717948717948718e-05, 'epoch': 0.03}
{'loss': 0.9802, 'grad_norm': 2.0674335956573486, 'learning_rate': 1.730769230769231e-05, 'epoch': 0.03}
{'loss': 1.1033, 'grad_norm': 2.267812490463257, 'learning_rate': 1.7435897435897438e-05, 'epoch': 0.03}
{'loss': 1.1485, 'grad_norm': 2.1373727321624756, 'learning_rate': 1.7564102564102566e-05, 'epoch': 0.03}
{'loss': 1.003, 'grad_norm': 2.112478494644165, 'learning_rate': 1.7692307692307694e-05, 'epoch': 0.03}
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
{'loss': 0.9551, 'grad_norm': 1.969823956489563, 'learning_rate': 1.7820512820512823e-05, 'epoch': 0.03}
{'loss': 0.9877, 'grad_norm': 2.057075023651123, 'learning_rate': 1.794871794871795e-05, 'epoch': 0.03}
{'loss': 1.0614, 'grad_norm': 2.261923313140869, 'learning_rate': 1.807692307692308e-05, 'epoch': 0.03}
{'loss': 1.0493, 'grad_norm': 2.091593027114868, 'learning_rate': 1.8205128205128208e-05, 'epoch': 0.03}
{'loss': 1.0505, 'grad_norm': 2.004371166229248, 'learning_rate': 1.8333333333333333e-05, 'epoch': 0.03}
{'loss': 1.0263, 'grad_norm': 2.203083038330078, 'learning_rate': 1.8461538461538465e-05, 'epoch': 0.03}
Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)
Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)
Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)
Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)

Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)
Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)
Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)
Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)
Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)
Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)
Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)
Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)
Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)
Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)
Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)
{'loss': 0.985, 'grad_norm': 2.10931658744812, 'learning_rate': 1.8589743589743593e-05, 'epoch': 0.03}
{'loss': 1.0333, 'grad_norm': 2.5971920490264893, 'learning_rate': 1.8717948717948718e-05, 'epoch': 0.03}
{'loss': 1.1577, 'grad_norm': 2.5950140953063965, 'learning_rate': 1.8846153846153846e-05, 'epoch': 0.03}
{'loss': 0.942, 'grad_norm': 2.0563480854034424, 'learning_rate': 1.8974358974358975e-05, 'epoch': 0.03}
{'loss': 1.0048, 'grad_norm': 1.8488478660583496, 'learning_rate': 1.9102564102564106e-05, 'epoch': 0.03}
Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)
Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)
Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)
Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)
Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)
Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)
Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)
Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)
Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)
Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)
Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)
Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)
Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)
Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)
Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)
Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)
{'loss': 1.0835, 'grad_norm': 2.203705072402954, 'learning_rate': 1.923076923076923e-05, 'epoch': 0.03}
{'loss': 1.0468, 'grad_norm': 1.5957435369491577, 'learning_rate': 1.935897435897436e-05, 'epoch': 0.03}
{'loss': 1.0358, 'grad_norm': 2.191175699234009, 'learning_rate': 1.9487179487179488e-05, 'epoch': 0.03}
{'loss': 1.1243, 'grad_norm': 2.0204575061798096, 'learning_rate': 1.9615384615384617e-05, 'epoch': 0.03}
{'loss': 1.0325, 'grad_norm': 2.1229186058044434, 'learning_rate': 1.9743589743589745e-05, 'epoch': 0.03}
Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)
Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)
Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)
Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)
Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)
Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)
Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)
Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)
Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)
Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)
Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)
Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)
Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)
Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)
Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)
Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)
{'loss': 1.1113, 'grad_norm': 1.9671717882156372, 'learning_rate': 1.9871794871794873e-05, 'epoch': 0.03}
{'loss': 1.0446, 'grad_norm': 1.9350924491882324, 'learning_rate': 2e-05, 'epoch': 0.03}
{'loss': 1.0413, 'grad_norm': 2.138885974884033, 'learning_rate': 1.9999998058827844e-05, 'epoch': 0.03}
{'loss': 1.0414, 'grad_norm': 2.0499043464660645, 'learning_rate': 1.9999992235312136e-05, 'epoch': 0.03}
{'loss': 0.9682, 'grad_norm': 1.984792709350586, 'learning_rate': 1.9999982529455127e-05, 'epoch': 0.03}
Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)
Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)
Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)

Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)
Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)
Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)
Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)
Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)
Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)
Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)
Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)
Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)
Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)
Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)
Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)
{'loss': 1.0033, 'grad_norm': 1.916777491569519, 'learning_rate': 1.9999968941260596e-05, 'epoch': 0.03}
{'loss': 1.0474, 'grad_norm': 1.8977103233337402, 'learning_rate': 1.9999951470733808e-05, 'epoch': 0.03}
{'loss': 1.1411, 'grad_norm': 2.2477636337280273, 'learning_rate': 1.9999930117881548e-05, 'epoch': 0.03}
{'loss': 1.0656, 'grad_norm': 2.1103172302246094, 'learning_rate': 1.9999904882712115e-05, 'epoch': 0.03}
{'loss': 1.1106, 'grad_norm': 2.077603816986084, 'learning_rate': 1.99998757652353e-05, 'epoch': 0.03}
Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)
Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)
Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)
Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)

Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)
Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)
Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)
Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)
Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)
Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)
Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)
Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)
Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)
Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)
Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)
{'loss': 1.0489, 'grad_norm': 1.9979641437530518, 'learning_rate': 1.9999842765462403e-05, 'epoch': 0.03}
{'loss': 1.0394, 'grad_norm': 1.8244857788085938, 'learning_rate': 1.999980588340624e-05, 'epoch': 0.03}
{'loss': 1.1247, 'grad_norm': 2.296830654144287, 'learning_rate': 1.9999765119081132e-05, 'epoch': 0.03}
{'loss': 1.1818, 'grad_norm': 2.1315364837646484, 'learning_rate': 1.9999720472502902e-05, 'epoch': 0.03}
{'loss': 1.036, 'grad_norm': 1.554783582687378, 'learning_rate': 1.9999671943688885e-05, 'epoch': 0.03}
{'loss': 1.0894, 'grad_norm': 2.008894681930542, 'learning_rate': 1.9999619532657915e-05, 'epoch': 0.03}
Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)
Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)
Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)
Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)
Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)
Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)
Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)
Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)
Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)
Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)
Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)
Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)
Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)
Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)
Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)
Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)
{'loss': 1.0088, 'grad_norm': 1.8634803295135498, 'learning_rate': 1.9999563239430352e-05, 'epoch': 0.03}
{'loss': 1.1326, 'grad_norm': 1.9225883483886719, 'learning_rate': 1.9999503064028043e-05, 'epoch': 0.03}
{'loss': 0.9978, 'grad_norm': 1.8797730207443237, 'learning_rate': 1.999943900647435e-05, 'epoch': 0.03}
{'loss': 1.0345, 'grad_norm': 2.0368764400482178, 'learning_rate': 1.9999371066794146e-05, 'epoch': 0.03}
{'loss': 0.989, 'grad_norm': 1.9746570587158203, 'learning_rate': 1.9999299245013805e-05, 'epoch': 0.03}
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
{'loss': 1.0729, 'grad_norm': 1.914988398551941, 'learning_rate': 1.999922354116121e-05, 'epoch': 0.03}
{'loss': 1.1448, 'grad_norm': 1.9990342855453491, 'learning_rate': 1.999914395526575e-05, 'epoch': 0.03}
{'loss': 1.053, 'grad_norm': 2.1870462894439697, 'learning_rate': 1.9999060487358333e-05, 'epoch': 0.03}
{'loss': 1.0713, 'grad_norm': 2.1650633811950684, 'learning_rate': 1.9998973137471352e-05, 'epoch': 0.03}
{'loss': 1.0639, 'grad_norm': 2.072848081588745, 'learning_rate': 1.9998881905638727e-05, 'epoch': 0.03}
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
{'loss': 1.0676, 'grad_norm': 2.061399459838867, 'learning_rate': 1.9998786791895874e-05, 'epoch': 0.04}
{'loss': 0.9984, 'grad_norm': 1.9574124813079834, 'learning_rate': 1.999868779627972e-05, 'epoch': 0.04}
{'loss': 0.9203, 'grad_norm': 2.050428628921509, 'learning_rate': 1.9998584918828695e-05, 'epoch': 0.04}
{'loss': 1.0069, 'grad_norm': 2.275550365447998, 'learning_rate': 1.9998478159582747e-05, 'epoch': 0.04}
{'loss': 1.0975, 'grad_norm': 2.113217830657959, 'learning_rate': 1.999836751858332e-05, 'epoch': 0.04}
Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)
Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)
Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)
Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)
Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)

Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)
Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)
Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)
Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)
Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)
Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)
Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)
Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)
Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)
Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)
{'loss': 1.0218, 'grad_norm': 2.092437505722046, 'learning_rate': 1.9998252995873367e-05, 'epoch': 0.04}
{'loss': 1.1412, 'grad_norm': 2.064178705215454, 'learning_rate': 1.999813459149735e-05, 'epoch': 0.04}
{'loss': 1.1072, 'grad_norm': 2.287230968475342, 'learning_rate': 1.9998012305501243e-05, 'epoch': 0.04}
{'loss': 1.0669, 'grad_norm': 2.138475179672241, 'learning_rate': 1.999788613793251e-05, 'epoch': 0.04}
{'loss': 1.1114, 'grad_norm': 2.155648708343506, 'learning_rate': 1.999775608884015e-05, 'epoch': 0.04}
Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)
Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)
Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)
Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)
Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)
Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)
Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)
Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)
Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)
Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)
Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)
Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)
Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)
Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)

Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)
Error with image file is truncated (8 bytes not processed)
{'loss': 0.9957, 'grad_norm': 2.2929790019989014, 'learning_rate': 1.9997622158274635e-05, 'epoch': 0.04}
{'loss': 1.0899, 'grad_norm': 2.041909694671631, 'learning_rate': 1.9997484346287973e-05, 'epoch': 0.04}
{'loss': 1.0555, 'grad_norm': 2.057776927947998, 'learning_rate': 1.9997342652933668e-05, 'epoch': 0.04}
{'loss': 1.1509, 'grad_norm': 2.1145851612091064, 'learning_rate': 1.9997197078266723e-05, 'epoch': 0.04}
{'loss': 1.0812, 'grad_norm': 1.9838875532150269, 'learning_rate': 1.999704762234366e-05, 'epoch': 0.04}
{'loss': 1.0422, 'grad_norm': 2.055176258087158, 'learning_rate': 1.99968942852225e-05, 'epoch': 0.04}
Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)
Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)
Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)
Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)
Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)
Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)

Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)
Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)

Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)
Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)
Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)
Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)
Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)
Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)
{'loss': 1.0971, 'grad_norm': 1.7539644241333008, 'learning_rate': 1.9996737066962778e-05, 'epoch': 0.04}
{'loss': 1.1367, 'grad_norm': 2.0726702213287354, 'learning_rate': 1.9996575967625525e-05, 'epoch': 0.04}
{'loss': 1.0638, 'grad_norm': 1.8196443319320679, 'learning_rate': 1.999641098727329e-05, 'epoch': 0.04}
{'loss': 1.0925, 'grad_norm': 2.019803047180176, 'learning_rate': 1.999624212597013e-05, 'epoch': 0.04}
{'loss': 1.0965, 'grad_norm': 1.9591131210327148, 'learning_rate': 1.9996069383781587e-05, 'epoch': 0.04}
Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)
Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)

Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)

Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)
Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)
Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)
Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)
Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)
Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)
Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)
Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)
Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)
Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)
Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)
{'loss': 0.9733, 'grad_norm': 1.527554988861084, 'learning_rate': 1.9995892760774738e-05, 'epoch': 0.04}
{'loss': 0.9619, 'grad_norm': 2.0584774017333984, 'learning_rate': 1.9995712257018153e-05, 'epoch': 0.04}
{'loss': 1.0309, 'grad_norm': 2.019218921661377, 'learning_rate': 1.9995527872581903e-05, 'epoch': 0.04}
{'loss': 1.0777, 'grad_norm': 1.9504411220550537, 'learning_rate': 1.9995339607537578e-05, 'epoch': 0.04}
{'loss': 1.0329, 'grad_norm': 2.0789778232574463, 'learning_rate': 1.9995147461958267e-05, 'epoch': 0.04}
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
{'loss': 1.1153, 'grad_norm': 2.224687337875366, 'learning_rate': 1.999495143591857e-05, 'epoch': 0.04}
{'loss': 1.13, 'grad_norm': 2.092410087585449, 'learning_rate': 1.999475152949459e-05, 'epoch': 0.04}
{'loss': 1.0559, 'grad_norm': 2.192159414291382, 'learning_rate': 1.9994547742763935e-05, 'epoch': 0.04}
{'loss': 1.061, 'grad_norm': 1.5695462226867676, 'learning_rate': 1.9994340075805724e-05, 'epoch': 0.04}
{'loss': 1.0491, 'grad_norm': 2.0485637187957764, 'learning_rate': 1.9994128528700583e-05, 'epoch': 0.04}
Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)
Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)
Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)
Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)
Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)
Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)
Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)

Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)
Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)
Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)
Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)
Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)

Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)
Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)
{'loss': 1.0437, 'grad_norm': 2.037684202194214, 'learning_rate': 1.9993913101530635e-05, 'epoch': 0.04}
{'loss': 1.16, 'grad_norm': 2.100424289703369, 'learning_rate': 1.9993693794379525e-05, 'epoch': 0.04}
{'loss': 1.0178, 'grad_norm': 1.9237256050109863, 'learning_rate': 1.9993470607332387e-05, 'epoch': 0.04}
{'loss': 1.0503, 'grad_norm': 1.5671967267990112, 'learning_rate': 1.999324354047588e-05, 'epoch': 0.04}
{'loss': 1.0568, 'grad_norm': 2.12188458442688, 'learning_rate': 1.9993012593898146e-05, 'epoch': 0.04}
Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)
Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)
Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)
Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)
Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)
Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)
Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)
Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)
Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)
Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)
Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)
Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)
Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)
Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)
Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)
Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)
{'loss': 1.0599, 'grad_norm': 1.9681603908538818, 'learning_rate': 1.9992777767688857e-05, 'epoch': 0.04}
{'loss': 1.0502, 'grad_norm': 1.8882967233657837, 'learning_rate': 1.9992539061939175e-05, 'epoch': 0.04}
{'loss': 1.111, 'grad_norm': 2.176253080368042, 'learning_rate': 1.999229647674178e-05, 'epoch': 0.04}
{'loss': 1.0543, 'grad_norm': 1.8833898305892944, 'learning_rate': 1.9992050012190845e-05, 'epoch': 0.04}
{'loss': 1.0568, 'grad_norm': 1.9013694524765015, 'learning_rate': 1.9991799668382058e-05, 'epoch': 0.04}
{'loss': 1.0993, 'grad_norm': 1.618804693222046, 'learning_rate': 1.9991545445412614e-05, 'epoch': 0.04}
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
{'loss': 1.0406, 'grad_norm': 1.9252206087112427, 'learning_rate': 1.9991287343381208e-05, 'epoch': 0.04}
{'loss': 1.1288, 'grad_norm': 1.985256552696228, 'learning_rate': 1.9991025362388044e-05, 'epoch': 0.04}
{'loss': 1.0248, 'grad_norm': 1.5270134210586548, 'learning_rate': 1.9990759502534835e-05, 'epoch': 0.04}
{'loss': 1.0201, 'grad_norm': 1.4401289224624634, 'learning_rate': 1.9990489763924796e-05, 'epoch': 0.04}
{'loss': 1.034, 'grad_norm': 1.8828151226043701, 'learning_rate': 1.9990216146662648e-05, 'epoch': 0.04}
Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)
Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)
Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)
Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)
Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)
Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)
Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)
Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)

Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)
Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)
Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)
Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)
Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)
Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)
Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)
{'loss': 1.0966, 'grad_norm': 2.0857949256896973, 'learning_rate': 1.9989938650854618e-05, 'epoch': 0.04}
{'loss': 1.1057, 'grad_norm': 1.9810420274734497, 'learning_rate': 1.998965727660844e-05, 'epoch': 0.04}
{'loss': 0.9977, 'grad_norm': 1.3908168077468872, 'learning_rate': 1.9989372024033352e-05, 'epoch': 0.04}
{'loss': 1.0202, 'grad_norm': 1.8879013061523438, 'learning_rate': 1.99890828932401e-05, 'epoch': 0.04}
{'loss': 1.033, 'grad_norm': 1.529113531112671, 'learning_rate': 1.9988789884340938e-05, 'epoch': 0.04}
Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)
Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)
Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)
Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)
Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)
Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)
Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)
Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)
Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)
Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)
Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)
Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)
Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)
Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)
Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)
Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)
{'loss': 1.0934, 'grad_norm': 1.9756476879119873, 'learning_rate': 1.9988492997449615e-05, 'epoch': 0.05}
{'loss': 1.0132, 'grad_norm': 1.5984811782836914, 'learning_rate': 1.9988192232681398e-05, 'epoch': 0.05}
{'loss': 1.0831, 'grad_norm': 2.045974016189575, 'learning_rate': 1.9987887590153055e-05, 'epoch': 0.05}
{'loss': 1.1411, 'grad_norm': 2.2115261554718018, 'learning_rate': 1.9987579069982856e-05, 'epoch': 0.05}
{'loss': 1.0576, 'grad_norm': 1.5231308937072754, 'learning_rate': 1.9987266672290577e-05, 'epoch': 0.05}
Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)
Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)
Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)
Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)
Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)
Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)
Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)
Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)

Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)
Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)
Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)
Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)
Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)
Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)
Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)
{'loss': 0.9985, 'grad_norm': 2.137526273727417, 'learning_rate': 1.9986950397197503e-05, 'epoch': 0.05}
{'loss': 1.1451, 'grad_norm': 2.100182294845581, 'learning_rate': 1.9986630244826425e-05, 'epoch': 0.05}
{'loss': 1.0924, 'grad_norm': 1.9021613597869873, 'learning_rate': 1.998630621530164e-05, 'epoch': 0.05}
{'loss': 1.0388, 'grad_norm': 2.0087625980377197, 'learning_rate': 1.998597830874894e-05, 'epoch': 0.05}
{'loss': 1.0089, 'grad_norm': 1.691898226737976, 'learning_rate': 1.9985646525295634e-05, 'epoch': 0.05}
Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)
Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)
Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)
Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)
Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)

Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)
Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)
Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)
Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)
Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)
Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)
Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)
Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)
Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)
Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)
{'loss': 1.0552, 'grad_norm': 1.8042941093444824, 'learning_rate': 1.998531086507053e-05, 'epoch': 0.05}
{'loss': 1.0154, 'grad_norm': 1.9087940454483032, 'learning_rate': 1.9984971328203945e-05, 'epoch': 0.05}
{'loss': 1.055, 'grad_norm': 1.847488522529602, 'learning_rate': 1.9984627914827698e-05, 'epoch': 0.05}
Error with image file is truncated (91 bytes not processed)
{'loss': 0.9627, 'grad_norm': 1.9069468975067139, 'learning_rate': 1.9984280625075115e-05, 'epoch': 0.05}
{'loss': 1.1205, 'grad_norm': 1.877274990081787, 'learning_rate': 1.9983929459081022e-05, 'epoch': 0.05}
{'loss': 1.1197, 'grad_norm': 1.9565407037734985, 'learning_rate': 1.998357441698176e-05, 'epoch': 0.05}
Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)
Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)
Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)
Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)
Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)
Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)
Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)
Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)
Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)
Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)
Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)
Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)
Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)
Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)
Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)
Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)
{'loss': 1.108, 'grad_norm': 2.202227830886841, 'learning_rate': 1.998321549891516e-05, 'epoch': 0.05}
{'loss': 1.0432, 'grad_norm': 1.902747392654419, 'learning_rate': 1.9982852705020572e-05, 'epoch': 0.05}
{'loss': 1.018, 'grad_norm': 2.090496778488159, 'learning_rate': 1.9982486035438848e-05, 'epoch': 0.05}
{'loss': 1.1138, 'grad_norm': 1.9884635210037231, 'learning_rate': 1.9982115490312334e-05, 'epoch': 0.05}
{'loss': 1.0804, 'grad_norm': 1.8760403394699097, 'learning_rate': 1.9981741069784894e-05, 'epoch': 0.05}
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
{'loss': 0.9702, 'grad_norm': 1.9484668970108032, 'learning_rate': 1.9981362774001886e-05, 'epoch': 0.05}
{'loss': 1.1733, 'grad_norm': 2.2266318798065186, 'learning_rate': 1.9980980603110185e-05, 'epoch': 0.05}
{'loss': 1.0153, 'grad_norm': 1.9277253150939941, 'learning_rate': 1.9980594557258158e-05, 'epoch': 0.05}
{'loss': 0.9998, 'grad_norm': 2.010301351547241, 'learning_rate': 1.9980204636595682e-05, 'epoch': 0.05}
{'loss': 1.0846, 'grad_norm': 1.888978362083435, 'learning_rate': 1.9979810841274135e-05, 'epoch': 0.05}
Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)
Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)
Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)
Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)
Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)
Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)
Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)
Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)

Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)
Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)
Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)
Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)
Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)
Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)
Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)
{'loss': 0.9973, 'grad_norm': 1.8444920778274536, 'learning_rate': 1.9979413171446403e-05, 'epoch': 0.05}
{'loss': 1.079, 'grad_norm': 1.9640154838562012, 'learning_rate': 1.9979011627266884e-05, 'epoch': 0.05}
{'loss': 1.0318, 'grad_norm': 2.0089218616485596, 'learning_rate': 1.997860620889146e-05, 'epoch': 0.05}
{'loss': 1.0243, 'grad_norm': 1.8361237049102783, 'learning_rate': 1.997819691647753e-05, 'epoch': 0.05}
{'loss': 1.0376, 'grad_norm': 1.5022350549697876, 'learning_rate': 1.9977783750184e-05, 'epoch': 0.05}
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
{'loss': 1.0401, 'grad_norm': 1.9998341798782349, 'learning_rate': 1.9977366710171274e-05, 'epoch': 0.05}
{'loss': 1.0184, 'grad_norm': 1.908202052116394, 'learning_rate': 1.9976945796601258e-05, 'epoch': 0.05}
{'loss': 1.0719, 'grad_norm': 2.066882848739624, 'learning_rate': 1.9976521009637366e-05, 'epoch': 0.05}
{'loss': 1.0663, 'grad_norm': 1.972237229347229, 'learning_rate': 1.997609234944452e-05, 'epoch': 0.05}
{'loss': 1.1035, 'grad_norm': 1.7905405759811401, 'learning_rate': 1.9975659816189137e-05, 'epoch': 0.05}
Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)
Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)
Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)
Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)
Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)
Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)
Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)
Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)
Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)
Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)
Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)
Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)
Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)
Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)
Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)
Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)
{'loss': 1.1279, 'grad_norm': 2.04242205619812, 'learning_rate': 1.997522341003914e-05, 'epoch': 0.05}
{'loss': 1.0903, 'grad_norm': 1.8581047058105469, 'learning_rate': 1.9974783131163957e-05, 'epoch': 0.05}
{'loss': 1.1132, 'grad_norm': 1.9862563610076904, 'learning_rate': 1.9974338979734523e-05, 'epoch': 0.05}
{'loss': 1.1269, 'grad_norm': 1.8636102676391602, 'learning_rate': 1.997389095592327e-05, 'epoch': 0.05}
{'loss': 1.0379, 'grad_norm': 1.8860360383987427, 'learning_rate': 1.9973439059904133e-05, 'epoch': 0.05}
{'loss': 1.0995, 'grad_norm': 1.8272238969802856, 'learning_rate': 1.9972983291852565e-05, 'epoch': 0.05}
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
{'loss': 1.0616, 'grad_norm': 1.8172085285186768, 'learning_rate': 1.9972523651945496e-05, 'epoch': 0.05}
{'loss': 1.1301, 'grad_norm': 2.158423900604248, 'learning_rate': 1.9972060140361384e-05, 'epoch': 0.05}
{'loss': 1.0051, 'grad_norm': 1.8030368089675903, 'learning_rate': 1.997159275728018e-05, 'epoch': 0.05}
{'loss': 1.0159, 'grad_norm': 1.5652694702148438, 'learning_rate': 1.9971121502883332e-05, 'epoch': 0.05}
{'loss': 1.0221, 'grad_norm': 1.8373386859893799, 'learning_rate': 1.9970646377353802e-05, 'epoch': 0.05}
Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)
Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)
Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)
Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)
Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)
Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)
Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)
Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)
Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)

Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)
Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)
Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)
Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)
Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)
Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)
{'loss': 1.0562, 'grad_norm': 1.7825496196746826, 'learning_rate': 1.997016738087605e-05, 'epoch': 0.05}
{'loss': 1.0228, 'grad_norm': 1.7079814672470093, 'learning_rate': 1.9969684513636035e-05, 'epoch': 0.05}
{'loss': 1.0775, 'grad_norm': 2.025540590286255, 'learning_rate': 1.9969197775821227e-05, 'epoch': 0.05}
{'loss': 0.9603, 'grad_norm': 1.8250815868377686, 'learning_rate': 1.9968707167620593e-05, 'epoch': 0.05}
{'loss': 1.077, 'grad_norm': 1.8728880882263184, 'learning_rate': 1.9968212689224603e-05, 'epoch': 0.05}
Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)
Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)

Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)
Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)
Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)
Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)
Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)
Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)
Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)
Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)
Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)
Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)

Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)
Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)
{'loss': 1.1664, 'grad_norm': 1.940574049949646, 'learning_rate': 1.996771434082523e-05, 'epoch': 0.06}
{'loss': 1.0124, 'grad_norm': 1.455222725868225, 'learning_rate': 1.9967212122615958e-05, 'epoch': 0.06}
{'loss': 1.1544, 'grad_norm': 1.8038727045059204, 'learning_rate': 1.9966706034791752e-05, 'epoch': 0.06}
{'loss': 1.1075, 'grad_norm': 1.8963258266448975, 'learning_rate': 1.9966196077549106e-05, 'epoch': 0.06}
{'loss': 1.0918, 'grad_norm': 2.105494260787964, 'learning_rate': 1.996568225108599e-05, 'epoch': 0.06}
Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)
Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)
Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)

Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)
Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)
Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)

Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)
Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)
Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)
Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)

Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)
Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)
Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)
{'loss': 1.1121, 'grad_norm': 1.9425053596496582, 'learning_rate': 1.99651645556019e-05, 'epoch': 0.06}
{'loss': 0.9621, 'grad_norm': 1.848116159439087, 'learning_rate': 1.9964642991297817e-05, 'epoch': 0.06}
{'loss': 1.0456, 'grad_norm': 1.8338088989257812, 'learning_rate': 1.996411755837623e-05, 'epoch': 0.06}
{'loss': 0.9738, 'grad_norm': 2.1286213397979736, 'learning_rate': 1.9963588257041137e-05, 'epoch': 0.06}
{'loss': 1.0633, 'grad_norm': 2.031278610229492, 'learning_rate': 1.996305508749802e-05, 'epoch': 0.06}
Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)
Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)
Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)
Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)
Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)
Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)
Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)
Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)
Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)
Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)
Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)
Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)
Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)
Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)
Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)
Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)
{'loss': 1.0928, 'grad_norm': 2.128774404525757, 'learning_rate': 1.9962518049953887e-05, 'epoch': 0.06}
{'loss': 1.1784, 'grad_norm': 2.032721757888794, 'learning_rate': 1.9961977144617225e-05, 'epoch': 0.06}
{'loss': 1.0669, 'grad_norm': 2.0687999725341797, 'learning_rate': 1.996143237169803e-05, 'epoch': 0.06}
{'loss': 1.0965, 'grad_norm': 1.9608389139175415, 'learning_rate': 1.996088373140781e-05, 'epoch': 0.06}
{'loss': 1.1249, 'grad_norm': 1.5857720375061035, 'learning_rate': 1.9960331223959564e-05, 'epoch': 0.06}
{'loss': 1.08, 'grad_norm': 1.9335402250289917, 'learning_rate': 1.995977484956779e-05, 'epoch': 0.06}
Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)
Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)
Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)

Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)
Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)
Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)
Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)
Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)
Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)
Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)
Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)
Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)

Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)
Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)
{'loss': 1.1346, 'grad_norm': 1.9929821491241455, 'learning_rate': 1.9959214608448495e-05, 'epoch': 0.06}
{'loss': 1.0432, 'grad_norm': 1.771497130393982, 'learning_rate': 1.9958650500819183e-05, 'epoch': 0.06}
{'loss': 1.0363, 'grad_norm': 1.812188982963562, 'learning_rate': 1.995808252689886e-05, 'epoch': 0.06}
{'loss': 1.0405, 'grad_norm': 1.795663833618164, 'learning_rate': 1.9957510686908034e-05, 'epoch': 0.06}
{'loss': 1.0957, 'grad_norm': 1.8121211528778076, 'learning_rate': 1.9956934981068713e-05, 'epoch': 0.06}
Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)
Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)
Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)
Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)

Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)
Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)

Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)
Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)
Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)
Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)
Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)
Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)
Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)
Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)
{'loss': 1.1437, 'grad_norm': 2.0116450786590576, 'learning_rate': 1.9956355409604402e-05, 'epoch': 0.06}
{'loss': 1.1398, 'grad_norm': 1.9516751766204834, 'learning_rate': 1.9955771972740118e-05, 'epoch': 0.06}
{'loss': 1.2271, 'grad_norm': 2.126250743865967, 'learning_rate': 1.9955184670702363e-05, 'epoch': 0.06}
{'loss': 1.0111, 'grad_norm': 1.7619121074676514, 'learning_rate': 1.995459350371915e-05, 'epoch': 0.06}
{'loss': 1.114, 'grad_norm': 1.9827897548675537, 'learning_rate': 1.9953998472019996e-05, 'epoch': 0.06}
Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)
Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)
Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)
Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)
Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)
Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)
Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)
Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)
Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)
Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)
Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)
Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)
Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)
Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)
Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)
Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)
{'loss': 1.1418, 'grad_norm': 2.024493455886841, 'learning_rate': 1.995339957583591e-05, 'epoch': 0.06}
{'loss': 1.0577, 'grad_norm': 1.497175931930542, 'learning_rate': 1.9952796815399403e-05, 'epoch': 0.06}
{'loss': 1.0781, 'grad_norm': 1.4902700185775757, 'learning_rate': 1.9952190190944484e-05, 'epoch': 0.06}
{'loss': 1.0095, 'grad_norm': 1.8330516815185547, 'learning_rate': 1.9951579702706668e-05, 'epoch': 0.06}
{'loss': 1.0798, 'grad_norm': 1.840100884437561, 'learning_rate': 1.9950965350922975e-05, 'epoch': 0.06}
Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)
Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)
Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)
Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)
Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)
Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)
Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)
Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)
Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)
Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)
Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)
Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)
Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)

Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)
Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)
{'loss': 1.0601, 'grad_norm': 1.8417084217071533, 'learning_rate': 1.9950347135831907e-05, 'epoch': 0.06}
{'loss': 1.0869, 'grad_norm': 2.0010879039764404, 'learning_rate': 1.994972505767348e-05, 'epoch': 0.06}
{'loss': 1.0416, 'grad_norm': 1.784494400024414, 'learning_rate': 1.994909911668921e-05, 'epoch': 0.06}
{'loss': 1.0288, 'grad_norm': 1.6319035291671753, 'learning_rate': 1.99484693131221e-05, 'epoch': 0.06}
{'loss': 1.0833, 'grad_norm': 1.8757425546646118, 'learning_rate': 1.994783564721667e-05, 'epoch': 0.06}
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
{'loss': 1.0836, 'grad_norm': 1.7285186052322388, 'learning_rate': 1.9947198119218924e-05, 'epoch': 0.06}
{'loss': 1.0699, 'grad_norm': 1.5834482908248901, 'learning_rate': 1.994655672937638e-05, 'epoch': 0.06}
{'loss': 1.0569, 'grad_norm': 2.016984224319458, 'learning_rate': 1.9945911477938044e-05, 'epoch': 0.06}
{'loss': 1.0745, 'grad_norm': 1.5066484212875366, 'learning_rate': 1.994526236515442e-05, 'epoch': 0.06}
{'loss': 1.1019, 'grad_norm': 1.7715644836425781, 'learning_rate': 1.994460939127753e-05, 'epoch': 0.06}
{'loss': 1.1204, 'grad_norm': 1.5854570865631104, 'learning_rate': 1.9943952556560863e-05, 'epoch': 0.06}
Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)
Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)
Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)
Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)
Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)
Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)
Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)
Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)
Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)
Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)
Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)

Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)
Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)
Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)
Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)
{'loss': 1.1281, 'grad_norm': 1.8995709419250488, 'learning_rate': 1.9943291861259433e-05, 'epoch': 0.06}
{'loss': 1.1461, 'grad_norm': 1.8645395040512085, 'learning_rate': 1.9942627305629747e-05, 'epoch': 0.06}
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
{'loss': 1.0893, 'grad_norm': 1.7690322399139404, 'learning_rate': 1.9941958889929808e-05, 'epoch': 0.06}
{'loss': 1.1372, 'grad_norm': 1.8394988775253296, 'learning_rate': 1.9941286614419113e-05, 'epoch': 0.06}
{'loss': 0.9888, 'grad_norm': 1.686012864112854, 'learning_rate': 1.994061047935867e-05, 'epoch': 0.06}
{'loss': 1.0087, 'grad_norm': 1.7378411293029785, 'learning_rate': 1.9939930485010968e-05, 'epoch': 0.06}
{'loss': 1.19, 'grad_norm': 1.8585559129714966, 'learning_rate': 1.9939246631640014e-05, 'epoch': 0.06}
Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)
Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)
Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)
Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)
Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)
Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)
Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)
Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)
Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)
Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)
Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)
Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)
Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)
Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)
Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)
Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)
{'loss': 1.0234, 'grad_norm': 1.6825724840164185, 'learning_rate': 1.99385589195113e-05, 'epoch': 0.06}
{'loss': 1.1259, 'grad_norm': 1.8534187078475952, 'learning_rate': 1.9937867348891815e-05, 'epoch': 0.06}
{'loss': 0.9867, 'grad_norm': 1.63921058177948, 'learning_rate': 1.9937171920050057e-05, 'epoch': 0.06}
{'loss': 1.1322, 'grad_norm': 1.8973331451416016, 'learning_rate': 1.9936472633256012e-05, 'epoch': 0.07}
{'loss': 1.1226, 'grad_norm': 1.7946430444717407, 'learning_rate': 1.9935769488781167e-05, 'epoch': 0.07}
{'loss': 1.1519, 'grad_norm': 1.9280909299850464, 'learning_rate': 1.993506248689851e-05, 'epoch': 0.07}
Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)
Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)
Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)
Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)
Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)
Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)
Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)
Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)
Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)
Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)
Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)
Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)
Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)
Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)
Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)
Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)
{'loss': 1.0721, 'grad_norm': 1.7945611476898193, 'learning_rate': 1.993435162788252e-05, 'epoch': 0.07}
{'loss': 1.112, 'grad_norm': 1.8639034032821655, 'learning_rate': 1.993363691200918e-05, 'epoch': 0.07}
{'loss': 1.0218, 'grad_norm': 1.783766746520996, 'learning_rate': 1.9932918339555965e-05, 'epoch': 0.07}
{'loss': 1.0869, 'grad_norm': 1.8423864841461182, 'learning_rate': 1.9932195910801848e-05, 'epoch': 0.07}
{'loss': 1.0502, 'grad_norm': 1.8464103937149048, 'learning_rate': 1.9931469626027305e-05, 'epoch': 0.07}
Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)
Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)
Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)
Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)

Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)
Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)
Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)
Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)
Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)
Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)
Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)
Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)
Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)
Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)
Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)
Error with image file is truncated (70 bytes not processed)
{'loss': 1.0697, 'grad_norm': 1.7144056558609009, 'learning_rate': 1.9930739485514304e-05, 'epoch': 0.07}
{'loss': 1.01, 'grad_norm': 1.6680433750152588, 'learning_rate': 1.9930005489546308e-05, 'epoch': 0.07}
{'loss': 1.0385, 'grad_norm': 1.962154746055603, 'learning_rate': 1.9929267638408277e-05, 'epoch': 0.07}
{'loss': 1.0964, 'grad_norm': 1.606225609779358, 'learning_rate': 1.9928525932386678e-05, 'epoch': 0.07}
{'loss': 1.0982, 'grad_norm': 1.5189716815948486, 'learning_rate': 1.9927780371769463e-05, 'epoch': 0.07}
Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)
Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)

Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)
Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)
Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)
Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)
Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)
Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)
Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)
Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)
Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)
Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)
Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)
Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)
Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)
{'loss': 1.1588, 'grad_norm': 1.952248454093933, 'learning_rate': 1.9927030956846083e-05, 'epoch': 0.07}
{'loss': 1.1028, 'grad_norm': 1.6733298301696777, 'learning_rate': 1.992627768790749e-05, 'epoch': 0.07}
{'loss': 1.0996, 'grad_norm': 1.8530000448226929, 'learning_rate': 1.9925520565246125e-05, 'epoch': 0.07}
{'loss': 1.0826, 'grad_norm': 1.5253276824951172, 'learning_rate': 1.9924759589155932e-05, 'epoch': 0.07}
{'loss': 1.1228, 'grad_norm': 1.776994228363037, 'learning_rate': 1.9923994759932344e-05, 'epoch': 0.07}
Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)
Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)
Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)
Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)

Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)

Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)
Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)
Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)
Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)
Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)
Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)
Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)
Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)
Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)
{'loss': 1.0748, 'grad_norm': 1.5022166967391968, 'learning_rate': 1.9923226077872296e-05, 'epoch': 0.07}
{'loss': 1.2054, 'grad_norm': 1.9195876121520996, 'learning_rate': 1.9922453543274223e-05, 'epoch': 0.07}
{'loss': 1.1002, 'grad_norm': 1.9350289106369019, 'learning_rate': 1.9921677156438044e-05, 'epoch': 0.07}
{'loss': 1.088, 'grad_norm': 1.84446382522583, 'learning_rate': 1.9920896917665178e-05, 'epoch': 0.07}
{'loss': 1.0086, 'grad_norm': 1.5588405132293701, 'learning_rate': 1.992011282725854e-05, 'epoch': 0.07}
Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)
Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)
Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)
Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)

Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)
Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)
Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)

Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)
Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)
Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)
Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)
Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)
Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)
Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)
WARNING: tokenization mismatch: 0 vs. 517. (ignored)
{'loss': 1.0773, 'grad_norm': 1.661045789718628, 'learning_rate': 1.9919324885522548e-05, 'epoch': 0.07}
{'loss': 1.0946, 'grad_norm': 1.8034794330596924, 'learning_rate': 1.99185330927631e-05, 'epoch': 0.07}
{'loss': 1.0723, 'grad_norm': 1.6835808753967285, 'learning_rate': 1.99177374492876e-05, 'epoch': 0.07}
{'loss': 1.0872, 'grad_norm': 1.8763264417648315, 'learning_rate': 1.991693795540494e-05, 'epoch': 0.07}
{'loss': 1.2004, 'grad_norm': 1.7126444578170776, 'learning_rate': 1.9916134611425522e-05, 'epoch': 0.07}
{'loss': 1.083, 'grad_norm': 1.8921947479248047, 'learning_rate': 1.9915327417661226e-05, 'epoch': 0.07}
Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)
Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)
Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)

Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)
Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)
Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)
Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)
Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)
Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)
Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)
Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)
Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)
Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)
Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)
Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)
{'loss': 1.1605, 'grad_norm': 2.1235992908477783, 'learning_rate': 1.991451637442543e-05, 'epoch': 0.07}
{'loss': 1.029, 'grad_norm': 1.8821810483932495, 'learning_rate': 1.9913701482033008e-05, 'epoch': 0.07}
{'loss': 1.1171, 'grad_norm': 1.7727760076522827, 'learning_rate': 1.9912882740800336e-05, 'epoch': 0.07}
{'loss': 1.0401, 'grad_norm': 1.4441946744918823, 'learning_rate': 1.9912060151045273e-05, 'epoch': 0.07}
{'loss': 1.0773, 'grad_norm': 1.8073910474777222, 'learning_rate': 1.9911233713087172e-05, 'epoch': 0.07}
Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)
Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)
Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)
Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)
Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)
Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)
Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)
Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)
Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)
Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)
Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)
Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)
Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)
Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)
Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)
Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)
{'loss': 1.1842, 'grad_norm': 1.9557212591171265, 'learning_rate': 1.9910403427246895e-05, 'epoch': 0.07}
{'loss': 1.1341, 'grad_norm': 1.7472413778305054, 'learning_rate': 1.990956929384678e-05, 'epoch': 0.07}
{'loss': 1.0917, 'grad_norm': 1.6846907138824463, 'learning_rate': 1.990873131321067e-05, 'epoch': 0.07}
{'loss': 1.0197, 'grad_norm': 1.6987171173095703, 'learning_rate': 1.9907889485663897e-05, 'epoch': 0.07}
{'loss': 1.0687, 'grad_norm': 1.7400058507919312, 'learning_rate': 1.9907043811533283e-05, 'epoch': 0.07}
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
{'loss': 1.1024, 'grad_norm': 1.826127290725708, 'learning_rate': 1.9906194291147155e-05, 'epoch': 0.07}
{'loss': 1.0498, 'grad_norm': 1.7255752086639404, 'learning_rate': 1.9905340924835322e-05, 'epoch': 0.07}
{'loss': 1.0749, 'grad_norm': 1.9940743446350098, 'learning_rate': 1.9904483712929094e-05, 'epoch': 0.07}
{'loss': 1.0198, 'grad_norm': 1.47284734249115, 'learning_rate': 1.9903622655761267e-05, 'epoch': 0.07}
{'loss': 1.1161, 'grad_norm': 1.7710096836090088, 'learning_rate': 1.990275775366613e-05, 'epoch': 0.07}
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
{'loss': 1.0831, 'grad_norm': 1.8323302268981934, 'learning_rate': 1.9901889006979473e-05, 'epoch': 0.07}
{'loss': 1.0881, 'grad_norm': 1.7680118083953857, 'learning_rate': 1.990101641603857e-05, 'epoch': 0.07}
{'loss': 1.1408, 'grad_norm': 1.846416711807251, 'learning_rate': 1.9900139981182193e-05, 'epoch': 0.07}
{'loss': 1.084, 'grad_norm': 1.75128972530365, 'learning_rate': 1.9899259702750604e-05, 'epoch': 0.07}
{'loss': 0.9958, 'grad_norm': 1.8915064334869385, 'learning_rate': 1.9898375581085555e-05, 'epoch': 0.07}
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
{'loss': 1.0539, 'grad_norm': 1.8851834535598755, 'learning_rate': 1.9897487616530296e-05, 'epoch': 0.07}
{'loss': 1.067, 'grad_norm': 1.8588217496871948, 'learning_rate': 1.9896595809429565e-05, 'epoch': 0.07}
{'loss': 1.0377, 'grad_norm': 1.8213260173797607, 'learning_rate': 1.9895700160129593e-05, 'epoch': 0.07}
{'loss': 1.0702, 'grad_norm': 1.70457124710083, 'learning_rate': 1.9894800668978095e-05, 'epoch': 0.08}
{'loss': 1.1439, 'grad_norm': 1.9513565301895142, 'learning_rate': 1.9893897336324292e-05, 'epoch': 0.08}
Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)
Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)
Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)

Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)
Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)
Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)
Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)
Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)
Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)

Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)
Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)
Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)
Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)
Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)
{'loss': 1.1494, 'grad_norm': 1.9489314556121826, 'learning_rate': 1.9892990162518884e-05, 'epoch': 0.08}
{'loss': 1.1072, 'grad_norm': 1.882545828819275, 'learning_rate': 1.9892079147914072e-05, 'epoch': 0.08}
{'loss': 1.0636, 'grad_norm': 1.7771310806274414, 'learning_rate': 1.9891164292863537e-05, 'epoch': 0.08}
{'loss': 1.0563, 'grad_norm': 1.5173730850219727, 'learning_rate': 1.9890245597722465e-05, 'epoch': 0.08}
{'loss': 1.0889, 'grad_norm': 1.8123664855957031, 'learning_rate': 1.9889323062847516e-05, 'epoch': 0.08}
{'loss': 1.0445, 'grad_norm': 1.9683890342712402, 'learning_rate': 1.988839668859686e-05, 'epoch': 0.08}
Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)
Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)
Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)
Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)
Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)
Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)
Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)
Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)
Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)
Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)
Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)
Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)
Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)
Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)
Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)
Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)
{'loss': 0.9968, 'grad_norm': 1.7653303146362305, 'learning_rate': 1.988746647533014e-05, 'epoch': 0.08}
{'loss': 1.085, 'grad_norm': 1.7484716176986694, 'learning_rate': 1.9886532423408495e-05, 'epoch': 0.08}
{'loss': 1.1505, 'grad_norm': 1.7886883020401, 'learning_rate': 1.9885594533194564e-05, 'epoch': 0.08}
{'loss': 1.0857, 'grad_norm': 1.7869987487792969, 'learning_rate': 1.9884652805052465e-05, 'epoch': 0.08}
{'loss': 1.0553, 'grad_norm': 1.7393865585327148, 'learning_rate': 1.9883707239347804e-05, 'epoch': 0.08}
Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)
Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)
Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)
Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)
Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)
Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)
Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)
Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)
Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)

Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)
Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)
Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)
Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)
Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)
Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)
{'loss': 1.0642, 'grad_norm': 1.8768706321716309, 'learning_rate': 1.988275783644769e-05, 'epoch': 0.08}
{'loss': 1.1395, 'grad_norm': 1.8233411312103271, 'learning_rate': 1.988180459672071e-05, 'epoch': 0.08}
{'loss': 1.0292, 'grad_norm': 1.784080147743225, 'learning_rate': 1.988084752053695e-05, 'epoch': 0.08}
{'loss': 1.1252, 'grad_norm': 1.8864237070083618, 'learning_rate': 1.9879886608267967e-05, 'epoch': 0.08}
{'loss': 1.0809, 'grad_norm': 1.7221320867538452, 'learning_rate': 1.9878921860286832e-05, 'epoch': 0.08}
Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)
Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)
Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)
Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)
Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)
Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)
Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)
Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)
Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)
Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)
Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)
Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)
Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)
Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)
Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)
Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)
{'loss': 1.0584, 'grad_norm': 1.7528502941131592, 'learning_rate': 1.9877953276968088e-05, 'epoch': 0.08}
{'loss': 1.0783, 'grad_norm': 1.7466181516647339, 'learning_rate': 1.9876980858687777e-05, 'epoch': 0.08}
{'loss': 1.0117, 'grad_norm': 1.7985111474990845, 'learning_rate': 1.9876004605823417e-05, 'epoch': 0.08}
{'loss': 1.1141, 'grad_norm': 1.7389378547668457, 'learning_rate': 1.987502451875403e-05, 'epoch': 0.08}
{'loss': 1.1308, 'grad_norm': 1.9155551195144653, 'learning_rate': 1.987404059786012e-05, 'epoch': 0.08}
Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)
Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)
Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)
Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)
Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)
Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)
Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)
Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)
Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)
Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)
Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)
Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)
Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)
Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)
Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)
Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)
{'loss': 1.0724, 'grad_norm': 1.7374615669250488, 'learning_rate': 1.9873052843523676e-05, 'epoch': 0.08}
{'loss': 1.155, 'grad_norm': 1.8722455501556396, 'learning_rate': 1.987206125612818e-05, 'epoch': 0.08}
{'loss': 1.0909, 'grad_norm': 1.8148421049118042, 'learning_rate': 1.98710658360586e-05, 'epoch': 0.08}
{'loss': 1.0616, 'grad_norm': 1.7568086385726929, 'learning_rate': 1.987006658370139e-05, 'epoch': 0.08}
{'loss': 1.1135, 'grad_norm': 1.5631790161132812, 'learning_rate': 1.9869063499444495e-05, 'epoch': 0.08}
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
{'loss': 1.0688, 'grad_norm': 1.7867317199707031, 'learning_rate': 1.9868056583677346e-05, 'epoch': 0.08}
{'loss': 1.019, 'grad_norm': 1.7509950399398804, 'learning_rate': 1.9867045836790867e-05, 'epoch': 0.08}
{'loss': 1.0953, 'grad_norm': 1.3983550071716309, 'learning_rate': 1.9866031259177463e-05, 'epoch': 0.08}
{'loss': 1.0596, 'grad_norm': 1.783445119857788, 'learning_rate': 1.9865012851231022e-05, 'epoch': 0.08}
{'loss': 1.1267, 'grad_norm': 1.9607702493667603, 'learning_rate': 1.9863990613346936e-05, 'epoch': 0.08}
{'loss': 1.164, 'grad_norm': 1.6963595151901245, 'learning_rate': 1.986296454592206e-05, 'epoch': 0.08}
Step 423: Updated gated ratio to 0.9837 (progress: 16.3%)
Step 423: Updated gated ratio to 0.9837 (progress: 16.3%)
Step 423: Updated gated ratio to 0.9837 (progress: 16.3%)
Step 423: Updated gated ratio to 0.9837 (progress: 16.3%)
Step 423: Updated gated ratio to 0.9837 (progress: 16.3%)
Step 423: Updated gated ratio to 0.9837 (progress: 16.3%)
Step 423: Updated gated ratio to 0.9837 (progress: 16.3%)
Step 423: Updated gated ratio to 0.9837 (progress: 16.3%)
Step 423: Updated gated ratio to 0.9837 (progress: 16.3%)
Step 423: Updated gated ratio to 0.9837 (progress: 16.3%)
Step 423: Updated gated ratio to 0.9837 (progress: 16.3%)
Step 423: Updated gated ratio to 0.9837 (progress: 16.3%)
Step 423: Updated gated ratio to 0.9837 (progress: 16.3%)
Step 423: Updated gated ratio to 0.9837 (progress: 16.3%)
Step 423: Updated gated ratio to 0.9837 (progress: 16.3%)
Step 423: Updated gated ratio to 0.9837 (progress: 16.3%)
{'loss': 1.071, 'grad_norm': 1.8192799091339111, 'learning_rate': 1.9861934649354763e-05, 'epoch': 0.08}
{'loss': 1.1318, 'grad_norm': 1.7990728616714478, 'learning_rate': 1.9860900924044873e-05, 'epoch': 0.08}
{'loss': 1.0191, 'grad_norm': 1.5190467834472656, 'learning_rate': 1.9859863370393726e-05, 'epoch': 0.08}
{'loss': 1.0474, 'grad_norm': 1.873589038848877, 'learning_rate': 1.9858821988804132e-05, 'epoch': 0.08}
{'loss': 1.096, 'grad_norm': 1.8615013360977173, 'learning_rate': 1.9857776779680393e-05, 'epoch': 0.08}
Step 428: Updated gated ratio to 0.9835 (progress: 16.5%)
Step 428: Updated gated ratio to 0.9835 (progress: 16.5%)
Step 428: Updated gated ratio to 0.9835 (progress: 16.5%)
Step 428: Updated gated ratio to 0.9835 (progress: 16.5%)
Step 428: Updated gated ratio to 0.9835 (progress: 16.5%)
Step 428: Updated gated ratio to 0.9835 (progress: 16.5%)
Step 428: Updated gated ratio to 0.9835 (progress: 16.5%)
Step 428: Updated gated ratio to 0.9835 (progress: 16.5%)
Step 428: Updated gated ratio to 0.9835 (progress: 16.5%)
Step 428: Updated gated ratio to 0.9835 (progress: 16.5%)
Step 428: Updated gated ratio to 0.9835 (progress: 16.5%)
Step 428: Updated gated ratio to 0.9835 (progress: 16.5%)
Step 428: Updated gated ratio to 0.9835 (progress: 16.5%)Step 428: Updated gated ratio to 0.9835 (progress: 16.5%)
Step 428: Updated gated ratio to 0.9835 (progress: 16.5%)

Step 428: Updated gated ratio to 0.9835 (progress: 16.5%)
{'loss': 1.1163, 'grad_norm': 1.7279707193374634, 'learning_rate': 1.98567277434283e-05, 'epoch': 0.08}
{'loss': 1.2015, 'grad_norm': 2.0038392543792725, 'learning_rate': 1.9855674880455115e-05, 'epoch': 0.08}
{'loss': 1.0966, 'grad_norm': 1.8001223802566528, 'learning_rate': 1.98546181911696e-05, 'epoch': 0.08}
{'loss': 1.0944, 'grad_norm': 1.8630706071853638, 'learning_rate': 1.9853557675982e-05, 'epoch': 0.08}
{'loss': 1.1833, 'grad_norm': 1.690690279006958, 'learning_rate': 1.985249333530404e-05, 'epoch': 0.08}
Step 433: Updated gated ratio to 0.9833 (progress: 16.7%)
Step 433: Updated gated ratio to 0.9833 (progress: 16.7%)
Step 433: Updated gated ratio to 0.9833 (progress: 16.7%)
Step 433: Updated gated ratio to 0.9833 (progress: 16.7%)
Step 433: Updated gated ratio to 0.9833 (progress: 16.7%)
Step 433: Updated gated ratio to 0.9833 (progress: 16.7%)
Step 433: Updated gated ratio to 0.9833 (progress: 16.7%)
Step 433: Updated gated ratio to 0.9833 (progress: 16.7%)
Step 433: Updated gated ratio to 0.9833 (progress: 16.7%)
Step 433: Updated gated ratio to 0.9833 (progress: 16.7%)
Step 433: Updated gated ratio to 0.9833 (progress: 16.7%)
Step 433: Updated gated ratio to 0.9833 (progress: 16.7%)
Step 433: Updated gated ratio to 0.9833 (progress: 16.7%)
Step 433: Updated gated ratio to 0.9833 (progress: 16.7%)
Step 433: Updated gated ratio to 0.9833 (progress: 16.7%)
Step 433: Updated gated ratio to 0.9833 (progress: 16.7%)
{'loss': 1.1121, 'grad_norm': 1.8651490211486816, 'learning_rate': 1.9851425169548938e-05, 'epoch': 0.08}
{'loss': 1.0942, 'grad_norm': 1.6615456342697144, 'learning_rate': 1.9850353179131392e-05, 'epoch': 0.08}
{'loss': 1.1628, 'grad_norm': 1.6517229080200195, 'learning_rate': 1.9849277364467585e-05, 'epoch': 0.08}
{'loss': 1.0413, 'grad_norm': 1.7657477855682373, 'learning_rate': 1.984819772597518e-05, 'epoch': 0.08}
{'loss': 1.0962, 'grad_norm': 1.7936774492263794, 'learning_rate': 1.9847114264073336e-05, 'epoch': 0.08}
Step 438: Updated gated ratio to 0.9831 (progress: 16.9%)
Step 438: Updated gated ratio to 0.9831 (progress: 16.9%)
Step 438: Updated gated ratio to 0.9831 (progress: 16.9%)
Step 438: Updated gated ratio to 0.9831 (progress: 16.9%)
Step 438: Updated gated ratio to 0.9831 (progress: 16.9%)
Step 438: Updated gated ratio to 0.9831 (progress: 16.9%)
Step 438: Updated gated ratio to 0.9831 (progress: 16.9%)
Step 438: Updated gated ratio to 0.9831 (progress: 16.9%)
Step 438: Updated gated ratio to 0.9831 (progress: 16.9%)
Step 438: Updated gated ratio to 0.9831 (progress: 16.9%)
Step 438: Updated gated ratio to 0.9831 (progress: 16.9%)
Step 438: Updated gated ratio to 0.9831 (progress: 16.9%)
Step 438: Updated gated ratio to 0.9831 (progress: 16.9%)
Step 438: Updated gated ratio to 0.9831 (progress: 16.9%)
Step 438: Updated gated ratio to 0.9831 (progress: 16.9%)
Step 438: Updated gated ratio to 0.9831 (progress: 16.9%)
{'loss': 1.159, 'grad_norm': 1.6773793697357178, 'learning_rate': 1.984602697918269e-05, 'epoch': 0.08}
{'loss': 1.0707, 'grad_norm': 1.7888520956039429, 'learning_rate': 1.9844935871725363e-05, 'epoch': 0.08}
{'loss': 1.1285, 'grad_norm': 1.766972541809082, 'learning_rate': 1.9843840942124956e-05, 'epoch': 0.08}
{'loss': 1.0879, 'grad_norm': 1.7545990943908691, 'learning_rate': 1.9842742190806566e-05, 'epoch': 0.09}
{'loss': 1.0847, 'grad_norm': 1.8007932901382446, 'learning_rate': 1.984163961819676e-05, 'epoch': 0.09}
Step 443: Updated gated ratio to 0.9829 (progress: 17.1%)
Step 443: Updated gated ratio to 0.9829 (progress: 17.1%)
Step 443: Updated gated ratio to 0.9829 (progress: 17.1%)
Step 443: Updated gated ratio to 0.9829 (progress: 17.1%)
Step 443: Updated gated ratio to 0.9829 (progress: 17.1%)
Step 443: Updated gated ratio to 0.9829 (progress: 17.1%)
Step 443: Updated gated ratio to 0.9829 (progress: 17.1%)
Step 443: Updated gated ratio to 0.9829 (progress: 17.1%)
Step 443: Updated gated ratio to 0.9829 (progress: 17.1%)
Step 443: Updated gated ratio to 0.9829 (progress: 17.1%)
Step 443: Updated gated ratio to 0.9829 (progress: 17.1%)
Step 443: Updated gated ratio to 0.9829 (progress: 17.1%)
Step 443: Updated gated ratio to 0.9829 (progress: 17.1%)
Step 443: Updated gated ratio to 0.9829 (progress: 17.1%)
Step 443: Updated gated ratio to 0.9829 (progress: 17.1%)
Step 443: Updated gated ratio to 0.9829 (progress: 17.1%)
{'loss': 1.0869, 'grad_norm': 1.8009737730026245, 'learning_rate': 1.9840533224723595e-05, 'epoch': 0.09}
{'loss': 1.1441, 'grad_norm': 1.8166393041610718, 'learning_rate': 1.9839423010816616e-05, 'epoch': 0.09}
{'loss': 1.0236, 'grad_norm': 1.8235331773757935, 'learning_rate': 1.983830897690684e-05, 'epoch': 0.09}
{'loss': 1.0471, 'grad_norm': 1.7355880737304688, 'learning_rate': 1.9837191123426777e-05, 'epoch': 0.09}
{'loss': 1.0916, 'grad_norm': 1.7884489297866821, 'learning_rate': 1.983606945081042e-05, 'epoch': 0.09}
{'loss': 1.1385, 'grad_norm': 1.855193018913269, 'learning_rate': 1.983494395949323e-05, 'epoch': 0.09}
Step 449: Updated gated ratio to 0.9827 (progress: 17.3%)
Step 449: Updated gated ratio to 0.9827 (progress: 17.3%)
Step 449: Updated gated ratio to 0.9827 (progress: 17.3%)
Step 449: Updated gated ratio to 0.9827 (progress: 17.3%)
Step 449: Updated gated ratio to 0.9827 (progress: 17.3%)
Step 449: Updated gated ratio to 0.9827 (progress: 17.3%)
Step 449: Updated gated ratio to 0.9827 (progress: 17.3%)
Step 449: Updated gated ratio to 0.9827 (progress: 17.3%)
Step 449: Updated gated ratio to 0.9827 (progress: 17.3%)
Step 449: Updated gated ratio to 0.9827 (progress: 17.3%)
Step 449: Updated gated ratio to 0.9827 (progress: 17.3%)Step 449: Updated gated ratio to 0.9827 (progress: 17.3%)

Step 449: Updated gated ratio to 0.9827 (progress: 17.3%)
Step 449: Updated gated ratio to 0.9827 (progress: 17.3%)
Step 449: Updated gated ratio to 0.9827 (progress: 17.3%)
Step 449: Updated gated ratio to 0.9827 (progress: 17.3%)
{'loss': 1.1989, 'grad_norm': 2.1432244777679443, 'learning_rate': 1.983381464991217e-05, 'epoch': 0.09}
{'loss': 1.1123, 'grad_norm': 1.9191781282424927, 'learning_rate': 1.9832681522505676e-05, 'epoch': 0.09}
{'loss': 1.1263, 'grad_norm': 2.0308456420898438, 'learning_rate': 1.9831544577713663e-05, 'epoch': 0.09}
{'loss': 1.0104, 'grad_norm': 1.585013508796692, 'learning_rate': 1.983040381597754e-05, 'epoch': 0.09}
{'loss': 1.0753, 'grad_norm': 1.5151150226593018, 'learning_rate': 1.982925923774018e-05, 'epoch': 0.09}
Step 454: Updated gated ratio to 0.9825 (progress: 17.5%)
Step 454: Updated gated ratio to 0.9825 (progress: 17.5%)
Step 454: Updated gated ratio to 0.9825 (progress: 17.5%)
Step 454: Updated gated ratio to 0.9825 (progress: 17.5%)
Step 454: Updated gated ratio to 0.9825 (progress: 17.5%)
Step 454: Updated gated ratio to 0.9825 (progress: 17.5%)
Step 454: Updated gated ratio to 0.9825 (progress: 17.5%)
Step 454: Updated gated ratio to 0.9825 (progress: 17.5%)
Step 454: Updated gated ratio to 0.9825 (progress: 17.5%)
Step 454: Updated gated ratio to 0.9825 (progress: 17.5%)
Step 454: Updated gated ratio to 0.9825 (progress: 17.5%)
Step 454: Updated gated ratio to 0.9825 (progress: 17.5%)
Step 454: Updated gated ratio to 0.9825 (progress: 17.5%)
Step 454: Updated gated ratio to 0.9825 (progress: 17.5%)
Step 454: Updated gated ratio to 0.9825 (progress: 17.5%)
Step 454: Updated gated ratio to 0.9825 (progress: 17.5%)
{'loss': 1.1259, 'grad_norm': 1.473719596862793, 'learning_rate': 1.9828110843445954e-05, 'epoch': 0.09}
{'loss': 1.0666, 'grad_norm': 1.3614354133605957, 'learning_rate': 1.982695863354071e-05, 'epoch': 0.09}
{'loss': 1.2083, 'grad_norm': 1.9932829141616821, 'learning_rate': 1.9825802608471767e-05, 'epoch': 0.09}
{'loss': 1.0469, 'grad_norm': 1.744410514831543, 'learning_rate': 1.982464276868794e-05, 'epoch': 0.09}
{'loss': 1.0766, 'grad_norm': 1.7285891771316528, 'learning_rate': 1.982347911463952e-05, 'epoch': 0.09}
Step 459: Updated gated ratio to 0.9823 (progress: 17.7%)
Step 459: Updated gated ratio to 0.9823 (progress: 17.7%)
Step 459: Updated gated ratio to 0.9823 (progress: 17.7%)
Step 459: Updated gated ratio to 0.9823 (progress: 17.7%)
Step 459: Updated gated ratio to 0.9823 (progress: 17.7%)Step 459: Updated gated ratio to 0.9823 (progress: 17.7%)
Step 459: Updated gated ratio to 0.9823 (progress: 17.7%)
Step 459: Updated gated ratio to 0.9823 (progress: 17.7%)
Step 459: Updated gated ratio to 0.9823 (progress: 17.7%)
Step 459: Updated gated ratio to 0.9823 (progress: 17.7%)
Step 459: Updated gated ratio to 0.9823 (progress: 17.7%)

Step 459: Updated gated ratio to 0.9823 (progress: 17.7%)
Step 459: Updated gated ratio to 0.9823 (progress: 17.7%)
Step 459: Updated gated ratio to 0.9823 (progress: 17.7%)
Step 459: Updated gated ratio to 0.9823 (progress: 17.7%)
Step 459: Updated gated ratio to 0.9823 (progress: 17.7%)
{'loss': 1.1344, 'grad_norm': 1.7138289213180542, 'learning_rate': 1.9822311646778277e-05, 'epoch': 0.09}
{'loss': 1.1028, 'grad_norm': 1.809149980545044, 'learning_rate': 1.982114036555746e-05, 'epoch': 0.09}
{'loss': 1.0327, 'grad_norm': 1.99359130859375, 'learning_rate': 1.9819965271431797e-05, 'epoch': 0.09}
{'loss': 1.1574, 'grad_norm': 1.9125185012817383, 'learning_rate': 1.9818786364857506e-05, 'epoch': 0.09}
{'loss': 1.0491, 'grad_norm': 1.8639599084854126, 'learning_rate': 1.9817603646292278e-05, 'epoch': 0.09}
Step 464: Updated gated ratio to 0.9821 (progress: 17.9%)
Step 464: Updated gated ratio to 0.9821 (progress: 17.9%)
Step 464: Updated gated ratio to 0.9821 (progress: 17.9%)
Step 464: Updated gated ratio to 0.9821 (progress: 17.9%)
Step 464: Updated gated ratio to 0.9821 (progress: 17.9%)
Step 464: Updated gated ratio to 0.9821 (progress: 17.9%)
Step 464: Updated gated ratio to 0.9821 (progress: 17.9%)
Step 464: Updated gated ratio to 0.9821 (progress: 17.9%)
Step 464: Updated gated ratio to 0.9821 (progress: 17.9%)
Step 464: Updated gated ratio to 0.9821 (progress: 17.9%)
Step 464: Updated gated ratio to 0.9821 (progress: 17.9%)
Step 464: Updated gated ratio to 0.9821 (progress: 17.9%)
Step 464: Updated gated ratio to 0.9821 (progress: 17.9%)
Step 464: Updated gated ratio to 0.9821 (progress: 17.9%)
Step 464: Updated gated ratio to 0.9821 (progress: 17.9%)
Step 464: Updated gated ratio to 0.9821 (progress: 17.9%)
{'loss': 0.9926, 'grad_norm': 1.8405050039291382, 'learning_rate': 1.9816417116195287e-05, 'epoch': 0.09}
{'loss': 1.1408, 'grad_norm': 2.027106761932373, 'learning_rate': 1.9815226775027182e-05, 'epoch': 0.09}
{'loss': 1.0484, 'grad_norm': 1.8292818069458008, 'learning_rate': 1.9814032623250093e-05, 'epoch': 0.09}
{'loss': 1.1993, 'grad_norm': 1.850074052810669, 'learning_rate': 1.9812834661327632e-05, 'epoch': 0.09}
{'loss': 1.0671, 'grad_norm': 1.7637660503387451, 'learning_rate': 1.9811632889724888e-05, 'epoch': 0.09}
Step 469: Updated gated ratio to 0.9819 (progress: 18.1%)
Step 469: Updated gated ratio to 0.9819 (progress: 18.1%)
Step 469: Updated gated ratio to 0.9819 (progress: 18.1%)
Step 469: Updated gated ratio to 0.9819 (progress: 18.1%)
Step 469: Updated gated ratio to 0.9819 (progress: 18.1%)
Step 469: Updated gated ratio to 0.9819 (progress: 18.1%)
Step 469: Updated gated ratio to 0.9819 (progress: 18.1%)
Step 469: Updated gated ratio to 0.9819 (progress: 18.1%)
Step 469: Updated gated ratio to 0.9819 (progress: 18.1%)
Step 469: Updated gated ratio to 0.9819 (progress: 18.1%)
Step 469: Updated gated ratio to 0.9819 (progress: 18.1%)
Step 469: Updated gated ratio to 0.9819 (progress: 18.1%)
Step 469: Updated gated ratio to 0.9819 (progress: 18.1%)
Step 469: Updated gated ratio to 0.9819 (progress: 18.1%)
Step 469: Updated gated ratio to 0.9819 (progress: 18.1%)
Step 469: Updated gated ratio to 0.9819 (progress: 18.1%)
{'loss': 1.0559, 'grad_norm': 1.694083571434021, 'learning_rate': 1.9810427308908437e-05, 'epoch': 0.09}
{'loss': 1.0748, 'grad_norm': 1.84430992603302, 'learning_rate': 1.9809217919346318e-05, 'epoch': 0.09}
{'loss': 1.1784, 'grad_norm': 1.8784505128860474, 'learning_rate': 1.980800472150806e-05, 'epoch': 0.09}
{'loss': 1.0566, 'grad_norm': 1.869990587234497, 'learning_rate': 1.9806787715864674e-05, 'epoch': 0.09}
{'loss': 1.0602, 'grad_norm': 1.8779710531234741, 'learning_rate': 1.9805566902888637e-05, 'epoch': 0.09}
{'loss': 1.1814, 'grad_norm': 1.930114507675171, 'learning_rate': 1.9804342283053916e-05, 'epoch': 0.09}
Step 475: Updated gated ratio to 0.9817 (progress: 18.3%)
Step 475: Updated gated ratio to 0.9817 (progress: 18.3%)
Step 475: Updated gated ratio to 0.9817 (progress: 18.3%)
Step 475: Updated gated ratio to 0.9817 (progress: 18.3%)
Step 475: Updated gated ratio to 0.9817 (progress: 18.3%)
Step 475: Updated gated ratio to 0.9817 (progress: 18.3%)
Step 475: Updated gated ratio to 0.9817 (progress: 18.3%)
Step 475: Updated gated ratio to 0.9817 (progress: 18.3%)
Step 475: Updated gated ratio to 0.9817 (progress: 18.3%)
Step 475: Updated gated ratio to 0.9817 (progress: 18.3%)
Step 475: Updated gated ratio to 0.9817 (progress: 18.3%)
Step 475: Updated gated ratio to 0.9817 (progress: 18.3%)
Step 475: Updated gated ratio to 0.9817 (progress: 18.3%)
Step 475: Updated gated ratio to 0.9817 (progress: 18.3%)
Step 475: Updated gated ratio to 0.9817 (progress: 18.3%)
Step 475: Updated gated ratio to 0.9817 (progress: 18.3%)
{'loss': 1.1316, 'grad_norm': 1.808586597442627, 'learning_rate': 1.980311385683594e-05, 'epoch': 0.09}
{'loss': 1.0519, 'grad_norm': 1.6560027599334717, 'learning_rate': 1.980188162471164e-05, 'epoch': 0.09}
{'loss': 1.127, 'grad_norm': 1.497851848602295, 'learning_rate': 1.98006455871594e-05, 'epoch': 0.09}
{'loss': 1.0891, 'grad_norm': 1.265552282333374, 'learning_rate': 1.97994057446591e-05, 'epoch': 0.09}
{'loss': 1.0706, 'grad_norm': 1.774156093597412, 'learning_rate': 1.979816209769209e-05, 'epoch': 0.09}
Step 480: Updated gated ratio to 0.9815 (progress: 18.5%)
Step 480: Updated gated ratio to 0.9815 (progress: 18.5%)
Step 480: Updated gated ratio to 0.9815 (progress: 18.5%)
Step 480: Updated gated ratio to 0.9815 (progress: 18.5%)
Step 480: Updated gated ratio to 0.9815 (progress: 18.5%)
Step 480: Updated gated ratio to 0.9815 (progress: 18.5%)
Step 480: Updated gated ratio to 0.9815 (progress: 18.5%)
Step 480: Updated gated ratio to 0.9815 (progress: 18.5%)
Step 480: Updated gated ratio to 0.9815 (progress: 18.5%)
Step 480: Updated gated ratio to 0.9815 (progress: 18.5%)
Step 480: Updated gated ratio to 0.9815 (progress: 18.5%)
Step 480: Updated gated ratio to 0.9815 (progress: 18.5%)
Step 480: Updated gated ratio to 0.9815 (progress: 18.5%)
Step 480: Updated gated ratio to 0.9815 (progress: 18.5%)
Step 480: Updated gated ratio to 0.9815 (progress: 18.5%)
Step 480: Updated gated ratio to 0.9815 (progress: 18.5%)
{'loss': 1.1153, 'grad_norm': 1.5342118740081787, 'learning_rate': 1.9796914646741187e-05, 'epoch': 0.09}
{'loss': 1.061, 'grad_norm': 1.8166069984436035, 'learning_rate': 1.9795663392290702e-05, 'epoch': 0.09}
{'loss': 1.1268, 'grad_norm': 1.9500129222869873, 'learning_rate': 1.9794408334826415e-05, 'epoch': 0.09}
{'loss': 1.0754, 'grad_norm': 1.7860276699066162, 'learning_rate': 1.979314947483558e-05, 'epoch': 0.09}
{'loss': 1.0721, 'grad_norm': 1.7147138118743896, 'learning_rate': 1.9791886812806932e-05, 'epoch': 0.09}
Step 485: Updated gated ratio to 0.9813 (progress: 18.7%)
Step 485: Updated gated ratio to 0.9813 (progress: 18.7%)
Step 485: Updated gated ratio to 0.9813 (progress: 18.7%)
Step 485: Updated gated ratio to 0.9813 (progress: 18.7%)
Step 485: Updated gated ratio to 0.9813 (progress: 18.7%)
Step 485: Updated gated ratio to 0.9813 (progress: 18.7%)
Step 485: Updated gated ratio to 0.9813 (progress: 18.7%)
Step 485: Updated gated ratio to 0.9813 (progress: 18.7%)
Step 485: Updated gated ratio to 0.9813 (progress: 18.7%)
Step 485: Updated gated ratio to 0.9813 (progress: 18.7%)
Step 485: Updated gated ratio to 0.9813 (progress: 18.7%)
Step 485: Updated gated ratio to 0.9813 (progress: 18.7%)
Step 485: Updated gated ratio to 0.9813 (progress: 18.7%)
Step 485: Updated gated ratio to 0.9813 (progress: 18.7%)
Step 485: Updated gated ratio to 0.9813 (progress: 18.7%)
Step 485: Updated gated ratio to 0.9813 (progress: 18.7%)
{'loss': 1.1333, 'grad_norm': 1.818840503692627, 'learning_rate': 1.9790620349230676e-05, 'epoch': 0.09}
{'loss': 1.0752, 'grad_norm': 1.679187536239624, 'learning_rate': 1.9789350084598504e-05, 'epoch': 0.09}
{'loss': 1.1795, 'grad_norm': 1.762177586555481, 'learning_rate': 1.9788076019403565e-05, 'epoch': 0.09}
{'loss': 1.1054, 'grad_norm': 1.7704269886016846, 'learning_rate': 1.9786798154140507e-05, 'epoch': 0.09}
{'loss': 1.0925, 'grad_norm': 1.7253469228744507, 'learning_rate': 1.9785516489305437e-05, 'epoch': 0.09}
Step 490: Updated gated ratio to 0.9811 (progress: 18.9%)
Step 490: Updated gated ratio to 0.9811 (progress: 18.9%)
Step 490: Updated gated ratio to 0.9811 (progress: 18.9%)
Step 490: Updated gated ratio to 0.9811 (progress: 18.9%)
Step 490: Updated gated ratio to 0.9811 (progress: 18.9%)Step 490: Updated gated ratio to 0.9811 (progress: 18.9%)

Step 490: Updated gated ratio to 0.9811 (progress: 18.9%)
Step 490: Updated gated ratio to 0.9811 (progress: 18.9%)
Step 490: Updated gated ratio to 0.9811 (progress: 18.9%)
Step 490: Updated gated ratio to 0.9811 (progress: 18.9%)
Step 490: Updated gated ratio to 0.9811 (progress: 18.9%)
Step 490: Updated gated ratio to 0.9811 (progress: 18.9%)
Step 490: Updated gated ratio to 0.9811 (progress: 18.9%)
Step 490: Updated gated ratio to 0.9811 (progress: 18.9%)
Step 490: Updated gated ratio to 0.9811 (progress: 18.9%)
Step 490: Updated gated ratio to 0.9811 (progress: 18.9%)
{'loss': 1.1474, 'grad_norm': 1.7727798223495483, 'learning_rate': 1.9784231025395936e-05, 'epoch': 0.09}
{'loss': 1.0688, 'grad_norm': 1.7738170623779297, 'learning_rate': 1.9782941762911075e-05, 'epoch': 0.09}
{'loss': 1.0607, 'grad_norm': 1.6715291738510132, 'learning_rate': 1.9781648702351383e-05, 'epoch': 0.09}
{'loss': 1.09, 'grad_norm': 1.7895336151123047, 'learning_rate': 1.9780351844218874e-05, 'epoch': 0.1}
{'loss': 1.1201, 'grad_norm': 1.8292937278747559, 'learning_rate': 1.977905118901703e-05, 'epoch': 0.1}
Step 495: Updated gated ratio to 0.9809 (progress: 19.1%)
Step 495: Updated gated ratio to 0.9809 (progress: 19.1%)
Step 495: Updated gated ratio to 0.9809 (progress: 19.1%)Step 495: Updated gated ratio to 0.9809 (progress: 19.1%)

Step 495: Updated gated ratio to 0.9809 (progress: 19.1%)
Step 495: Updated gated ratio to 0.9809 (progress: 19.1%)
Step 495: Updated gated ratio to 0.9809 (progress: 19.1%)
Step 495: Updated gated ratio to 0.9809 (progress: 19.1%)
Step 495: Updated gated ratio to 0.9809 (progress: 19.1%)
Step 495: Updated gated ratio to 0.9809 (progress: 19.1%)
Step 495: Updated gated ratio to 0.9809 (progress: 19.1%)
Step 495: Updated gated ratio to 0.9809 (progress: 19.1%)
Step 495: Updated gated ratio to 0.9809 (progress: 19.1%)
Step 495: Updated gated ratio to 0.9809 (progress: 19.1%)
Step 495: Updated gated ratio to 0.9809 (progress: 19.1%)
Step 495: Updated gated ratio to 0.9809 (progress: 19.1%)
{'loss': 1.138, 'grad_norm': 1.8977527618408203, 'learning_rate': 1.977774673725081e-05, 'epoch': 0.1}
{'loss': 1.0676, 'grad_norm': 1.6718358993530273, 'learning_rate': 1.977643848942665e-05, 'epoch': 0.1}
{'loss': 1.0988, 'grad_norm': 1.9134820699691772, 'learning_rate': 1.977512644605246e-05, 'epoch': 0.1}
{'loss': 1.0519, 'grad_norm': 1.6553171873092651, 'learning_rate': 1.9773810607637612e-05, 'epoch': 0.1}
{'loss': 1.1962, 'grad_norm': 1.8927206993103027, 'learning_rate': 1.9772490974692962e-05, 'epoch': 0.1}
{'loss': 1.0885, 'grad_norm': 1.7949838638305664, 'learning_rate': 1.9771167547730844e-05, 'epoch': 0.1}
Step 501: Updated gated ratio to 0.9807 (progress: 19.3%)
Step 501: Updated gated ratio to 0.9807 (progress: 19.3%)
Step 501: Updated gated ratio to 0.9807 (progress: 19.3%)
Step 501: Updated gated ratio to 0.9807 (progress: 19.3%)
Step 501: Updated gated ratio to 0.9807 (progress: 19.3%)
Step 501: Updated gated ratio to 0.9807 (progress: 19.3%)
Step 501: Updated gated ratio to 0.9807 (progress: 19.3%)
Step 501: Updated gated ratio to 0.9807 (progress: 19.3%)
Step 501: Updated gated ratio to 0.9807 (progress: 19.3%)
Step 501: Updated gated ratio to 0.9807 (progress: 19.3%)
Step 501: Updated gated ratio to 0.9807 (progress: 19.3%)
Step 501: Updated gated ratio to 0.9807 (progress: 19.3%)
Step 501: Updated gated ratio to 0.9807 (progress: 19.3%)Step 501: Updated gated ratio to 0.9807 (progress: 19.3%)

Step 501: Updated gated ratio to 0.9807 (progress: 19.3%)
Step 501: Updated gated ratio to 0.9807 (progress: 19.3%)
{'loss': 1.0987, 'grad_norm': 1.8868252038955688, 'learning_rate': 1.976984032726505e-05, 'epoch': 0.1}
{'loss': 1.0791, 'grad_norm': 1.828774094581604, 'learning_rate': 1.976850931381086e-05, 'epoch': 0.1}
{'loss': 1.1332, 'grad_norm': 1.807434320449829, 'learning_rate': 1.976717450788501e-05, 'epoch': 0.1}
{'loss': 1.0787, 'grad_norm': 1.5792584419250488, 'learning_rate': 1.9765835910005726e-05, 'epoch': 0.1}
{'loss': 1.0838, 'grad_norm': 1.9700138568878174, 'learning_rate': 1.9764493520692685e-05, 'epoch': 0.1}
Step 506: Updated gated ratio to 0.9805 (progress: 19.5%)
Step 506: Updated gated ratio to 0.9805 (progress: 19.5%)Step 506: Updated gated ratio to 0.9805 (progress: 19.5%)
Step 506: Updated gated ratio to 0.9805 (progress: 19.5%)Step 506: Updated gated ratio to 0.9805 (progress: 19.5%)

Step 506: Updated gated ratio to 0.9805 (progress: 19.5%)
Step 506: Updated gated ratio to 0.9805 (progress: 19.5%)

Step 506: Updated gated ratio to 0.9805 (progress: 19.5%)
Step 506: Updated gated ratio to 0.9805 (progress: 19.5%)
Step 506: Updated gated ratio to 0.9805 (progress: 19.5%)
Step 506: Updated gated ratio to 0.9805 (progress: 19.5%)
Step 506: Updated gated ratio to 0.9805 (progress: 19.5%)
Step 506: Updated gated ratio to 0.9805 (progress: 19.5%)
Step 506: Updated gated ratio to 0.9805 (progress: 19.5%)
Step 506: Updated gated ratio to 0.9805 (progress: 19.5%)
Step 506: Updated gated ratio to 0.9805 (progress: 19.5%)
{'loss': 1.0911, 'grad_norm': 2.0143184661865234, 'learning_rate': 1.9763147340467067e-05, 'epoch': 0.1}
{'loss': 1.0958, 'grad_norm': 1.616048812866211, 'learning_rate': 1.9761797369851498e-05, 'epoch': 0.1}
{'loss': 1.025, 'grad_norm': 1.8515307903289795, 'learning_rate': 1.9760443609370074e-05, 'epoch': 0.1}
{'loss': 1.1319, 'grad_norm': 1.7050501108169556, 'learning_rate': 1.975908605954838e-05, 'epoch': 0.1}
{'loss': 1.0815, 'grad_norm': 1.7949583530426025, 'learning_rate': 1.9757724720913466e-05, 'epoch': 0.1}
Step 511: Updated gated ratio to 0.9803 (progress: 19.7%)
Step 511: Updated gated ratio to 0.9803 (progress: 19.7%)
Step 511: Updated gated ratio to 0.9803 (progress: 19.7%)
Step 511: Updated gated ratio to 0.9803 (progress: 19.7%)
Step 511: Updated gated ratio to 0.9803 (progress: 19.7%)
Step 511: Updated gated ratio to 0.9803 (progress: 19.7%)
Step 511: Updated gated ratio to 0.9803 (progress: 19.7%)
Step 511: Updated gated ratio to 0.9803 (progress: 19.7%)
Step 511: Updated gated ratio to 0.9803 (progress: 19.7%)
Step 511: Updated gated ratio to 0.9803 (progress: 19.7%)
Step 511: Updated gated ratio to 0.9803 (progress: 19.7%)
Step 511: Updated gated ratio to 0.9803 (progress: 19.7%)
Step 511: Updated gated ratio to 0.9803 (progress: 19.7%)
Step 511: Updated gated ratio to 0.9803 (progress: 19.7%)
Step 511: Updated gated ratio to 0.9803 (progress: 19.7%)
Step 511: Updated gated ratio to 0.9803 (progress: 19.7%)
{'loss': 1.1514, 'grad_norm': 2.0011684894561768, 'learning_rate': 1.9756359593993845e-05, 'epoch': 0.1}
{'loss': 1.0579, 'grad_norm': 1.8006592988967896, 'learning_rate': 1.975499067931951e-05, 'epoch': 0.1}
{'loss': 1.0695, 'grad_norm': 1.797366738319397, 'learning_rate': 1.975361797742192e-05, 'epoch': 0.1}
{'loss': 1.1295, 'grad_norm': 1.8245689868927002, 'learning_rate': 1.9752241488834002e-05, 'epoch': 0.1}
{'loss': 1.2122, 'grad_norm': 1.812652826309204, 'learning_rate': 1.975086121409016e-05, 'epoch': 0.1}
Step 516: Updated gated ratio to 0.9801 (progress: 19.9%)
Step 516: Updated gated ratio to 0.9801 (progress: 19.9%)
Step 516: Updated gated ratio to 0.9801 (progress: 19.9%)Step 516: Updated gated ratio to 0.9801 (progress: 19.9%)
Step 516: Updated gated ratio to 0.9801 (progress: 19.9%)
Step 516: Updated gated ratio to 0.9801 (progress: 19.9%)
Step 516: Updated gated ratio to 0.9801 (progress: 19.9%)
Step 516: Updated gated ratio to 0.9801 (progress: 19.9%)
Step 516: Updated gated ratio to 0.9801 (progress: 19.9%)

Step 516: Updated gated ratio to 0.9801 (progress: 19.9%)
Step 516: Updated gated ratio to 0.9801 (progress: 19.9%)
Step 516: Updated gated ratio to 0.9801 (progress: 19.9%)
Step 516: Updated gated ratio to 0.9801 (progress: 19.9%)
Step 516: Updated gated ratio to 0.9801 (progress: 19.9%)
Step 516: Updated gated ratio to 0.9801 (progress: 19.9%)
Step 516: Updated gated ratio to 0.9801 (progress: 19.9%)
{'loss': 1.0457, 'grad_norm': 1.6894644498825073, 'learning_rate': 1.974947715372626e-05, 'epoch': 0.1}
{'loss': 0.9549, 'grad_norm': 1.8681565523147583, 'learning_rate': 1.974808930827965e-05, 'epoch': 0.1}
{'loss': 1.0464, 'grad_norm': 1.4195082187652588, 'learning_rate': 1.9746697678289128e-05, 'epoch': 0.1}
{'loss': 1.1241, 'grad_norm': 1.7552440166473389, 'learning_rate': 1.9745302264294982e-05, 'epoch': 0.1}
{'loss': 1.0705, 'grad_norm': 1.6978754997253418, 'learning_rate': 1.9743903066838954e-05, 'epoch': 0.1}
Step 521: Updated gated ratio to 0.9799 (progress: 20.1%)
Step 521: Updated gated ratio to 0.9799 (progress: 20.1%)
Step 521: Updated gated ratio to 0.9799 (progress: 20.1%)
Step 521: Updated gated ratio to 0.9799 (progress: 20.1%)
Step 521: Updated gated ratio to 0.9799 (progress: 20.1%)
Step 521: Updated gated ratio to 0.9799 (progress: 20.1%)
Step 521: Updated gated ratio to 0.9799 (progress: 20.1%)
Step 521: Updated gated ratio to 0.9799 (progress: 20.1%)
Step 521: Updated gated ratio to 0.9799 (progress: 20.1%)
Step 521: Updated gated ratio to 0.9799 (progress: 20.1%)
Step 521: Updated gated ratio to 0.9799 (progress: 20.1%)
Step 521: Updated gated ratio to 0.9799 (progress: 20.1%)
Step 521: Updated gated ratio to 0.9799 (progress: 20.1%)
Step 521: Updated gated ratio to 0.9799 (progress: 20.1%)
Step 521: Updated gated ratio to 0.9799 (progress: 20.1%)
Step 521: Updated gated ratio to 0.9799 (progress: 20.1%)
{'loss': 1.0949, 'grad_norm': 1.8674774169921875, 'learning_rate': 1.9742500086464266e-05, 'epoch': 0.1}
{'loss': 1.1073, 'grad_norm': 1.575642704963684, 'learning_rate': 1.9741093323715597e-05, 'epoch': 0.1}
{'loss': 1.0835, 'grad_norm': 1.6464868783950806, 'learning_rate': 1.9739682779139107e-05, 'epoch': 0.1}
{'loss': 1.1053, 'grad_norm': 1.6642539501190186, 'learning_rate': 1.9738268453282414e-05, 'epoch': 0.1}
{'loss': 1.0813, 'grad_norm': 1.7079684734344482, 'learning_rate': 1.9736850346694608e-05, 'epoch': 0.1}
{'loss': 1.1209, 'grad_norm': 1.8599064350128174, 'learning_rate': 1.973542845992625e-05, 'epoch': 0.1}
Step 527: Updated gated ratio to 0.9797 (progress: 20.3%)
Step 527: Updated gated ratio to 0.9797 (progress: 20.3%)
Step 527: Updated gated ratio to 0.9797 (progress: 20.3%)
Step 527: Updated gated ratio to 0.9797 (progress: 20.3%)
Step 527: Updated gated ratio to 0.9797 (progress: 20.3%)
Step 527: Updated gated ratio to 0.9797 (progress: 20.3%)
Step 527: Updated gated ratio to 0.9797 (progress: 20.3%)
Step 527: Updated gated ratio to 0.9797 (progress: 20.3%)
Step 527: Updated gated ratio to 0.9797 (progress: 20.3%)
Step 527: Updated gated ratio to 0.9797 (progress: 20.3%)
Step 527: Updated gated ratio to 0.9797 (progress: 20.3%)
Step 527: Updated gated ratio to 0.9797 (progress: 20.3%)
Step 527: Updated gated ratio to 0.9797 (progress: 20.3%)
Step 527: Updated gated ratio to 0.9797 (progress: 20.3%)
Step 527: Updated gated ratio to 0.9797 (progress: 20.3%)
Step 527: Updated gated ratio to 0.9797 (progress: 20.3%)
{'loss': 1.1165, 'grad_norm': 1.9238853454589844, 'learning_rate': 1.9734002793529362e-05, 'epoch': 0.1}
{'loss': 1.1279, 'grad_norm': 1.5478841066360474, 'learning_rate': 1.9732573348057437e-05, 'epoch': 0.1}
{'loss': 1.1721, 'grad_norm': 1.8528333902359009, 'learning_rate': 1.973114012406544e-05, 'epoch': 0.1}
{'loss': 1.1378, 'grad_norm': 1.9122315645217896, 'learning_rate': 1.9729703122109788e-05, 'epoch': 0.1}
{'loss': 1.0207, 'grad_norm': 1.801385521888733, 'learning_rate': 1.9728262342748384e-05, 'epoch': 0.1}
Step 532: Updated gated ratio to 0.9795 (progress: 20.5%)
Step 532: Updated gated ratio to 0.9795 (progress: 20.5%)
Step 532: Updated gated ratio to 0.9795 (progress: 20.5%)
Step 532: Updated gated ratio to 0.9795 (progress: 20.5%)
Step 532: Updated gated ratio to 0.9795 (progress: 20.5%)
Step 532: Updated gated ratio to 0.9795 (progress: 20.5%)
Step 532: Updated gated ratio to 0.9795 (progress: 20.5%)
Step 532: Updated gated ratio to 0.9795 (progress: 20.5%)
Step 532: Updated gated ratio to 0.9795 (progress: 20.5%)
Step 532: Updated gated ratio to 0.9795 (progress: 20.5%)
Step 532: Updated gated ratio to 0.9795 (progress: 20.5%)
Step 532: Updated gated ratio to 0.9795 (progress: 20.5%)
Step 532: Updated gated ratio to 0.9795 (progress: 20.5%)
Step 532: Updated gated ratio to 0.9795 (progress: 20.5%)
Step 532: Updated gated ratio to 0.9795 (progress: 20.5%)
Step 532: Updated gated ratio to 0.9795 (progress: 20.5%)
{'loss': 1.1643, 'grad_norm': 1.8240089416503906, 'learning_rate': 1.9726817786540584e-05, 'epoch': 0.1}
{'loss': 1.0076, 'grad_norm': 1.6844515800476074, 'learning_rate': 1.9725369454047215e-05, 'epoch': 0.1}
{'loss': 1.0598, 'grad_norm': 1.7319879531860352, 'learning_rate': 1.9723917345830568e-05, 'epoch': 0.1}
{'loss': 1.0654, 'grad_norm': 1.832393765449524, 'learning_rate': 1.9722461462454405e-05, 'epoch': 0.1}
{'loss': 1.0759, 'grad_norm': 1.8261831998825073, 'learning_rate': 1.9721001804483947e-05, 'epoch': 0.1}
Step 537: Updated gated ratio to 0.9793 (progress: 20.7%)
Step 537: Updated gated ratio to 0.9793 (progress: 20.7%)Step 537: Updated gated ratio to 0.9793 (progress: 20.7%)
Step 537: Updated gated ratio to 0.9793 (progress: 20.7%)
Step 537: Updated gated ratio to 0.9793 (progress: 20.7%)
Step 537: Updated gated ratio to 0.9793 (progress: 20.7%)
Step 537: Updated gated ratio to 0.9793 (progress: 20.7%)
Step 537: Updated gated ratio to 0.9793 (progress: 20.7%)
Step 537: Updated gated ratio to 0.9793 (progress: 20.7%)
Step 537: Updated gated ratio to 0.9793 (progress: 20.7%)
Step 537: Updated gated ratio to 0.9793 (progress: 20.7%)

Step 537: Updated gated ratio to 0.9793 (progress: 20.7%)
Step 537: Updated gated ratio to 0.9793 (progress: 20.7%)
Step 537: Updated gated ratio to 0.9793 (progress: 20.7%)
Step 537: Updated gated ratio to 0.9793 (progress: 20.7%)
Step 537: Updated gated ratio to 0.9793 (progress: 20.7%)
{'loss': 1.0784, 'grad_norm': 1.7901102304458618, 'learning_rate': 1.9719538372485887e-05, 'epoch': 0.1}
{'loss': 1.0829, 'grad_norm': 1.7248139381408691, 'learning_rate': 1.9718071167028376e-05, 'epoch': 0.1}
WARNING: tokenization mismatch: 0 vs. 1354. (ignored)
{'loss': 1.1876, 'grad_norm': 1.7550396919250488, 'learning_rate': 1.9716600188681038e-05, 'epoch': 0.1}
{'loss': 0.9703, 'grad_norm': 1.6820026636123657, 'learning_rate': 1.971512543801495e-05, 'epoch': 0.1}
{'loss': 1.1094, 'grad_norm': 1.7721235752105713, 'learning_rate': 1.9713646915602663e-05, 'epoch': 0.1}
Step 542: Updated gated ratio to 0.9791 (progress: 20.9%)
Step 542: Updated gated ratio to 0.9791 (progress: 20.9%)
Step 542: Updated gated ratio to 0.9791 (progress: 20.9%)
Step 542: Updated gated ratio to 0.9791 (progress: 20.9%)
Step 542: Updated gated ratio to 0.9791 (progress: 20.9%)Step 542: Updated gated ratio to 0.9791 (progress: 20.9%)

Step 542: Updated gated ratio to 0.9791 (progress: 20.9%)
Step 542: Updated gated ratio to 0.9791 (progress: 20.9%)
Step 542: Updated gated ratio to 0.9791 (progress: 20.9%)
Step 542: Updated gated ratio to 0.9791 (progress: 20.9%)
Step 542: Updated gated ratio to 0.9791 (progress: 20.9%)
Step 542: Updated gated ratio to 0.9791 (progress: 20.9%)
Step 542: Updated gated ratio to 0.9791 (progress: 20.9%)
Step 542: Updated gated ratio to 0.9791 (progress: 20.9%)
Step 542: Updated gated ratio to 0.9791 (progress: 20.9%)
Step 542: Updated gated ratio to 0.9791 (progress: 20.9%)
{'loss': 1.1055, 'grad_norm': 1.9122998714447021, 'learning_rate': 1.9712164622018197e-05, 'epoch': 0.1}
{'loss': 1.0719, 'grad_norm': 1.8618122339248657, 'learning_rate': 1.9710678557837024e-05, 'epoch': 0.1}
{'loss': 1.0829, 'grad_norm': 1.6862120628356934, 'learning_rate': 1.9709188723636088e-05, 'epoch': 0.1}
{'loss': 1.0516, 'grad_norm': 1.6804463863372803, 'learning_rate': 1.970769511999379e-05, 'epoch': 0.11}
{'loss': 1.0362, 'grad_norm': 1.6900814771652222, 'learning_rate': 1.9706197747490004e-05, 'epoch': 0.11}
Step 547: Updated gated ratio to 0.9789 (progress: 21.1%)
Step 547: Updated gated ratio to 0.9789 (progress: 21.1%)
Step 547: Updated gated ratio to 0.9789 (progress: 21.1%)Step 547: Updated gated ratio to 0.9789 (progress: 21.1%)
Step 547: Updated gated ratio to 0.9789 (progress: 21.1%)
Step 547: Updated gated ratio to 0.9789 (progress: 21.1%)
Step 547: Updated gated ratio to 0.9789 (progress: 21.1%)
Step 547: Updated gated ratio to 0.9789 (progress: 21.1%)
Step 547: Updated gated ratio to 0.9789 (progress: 21.1%)

Step 547: Updated gated ratio to 0.9789 (progress: 21.1%)
Step 547: Updated gated ratio to 0.9789 (progress: 21.1%)
Step 547: Updated gated ratio to 0.9789 (progress: 21.1%)
Step 547: Updated gated ratio to 0.9789 (progress: 21.1%)
Step 547: Updated gated ratio to 0.9789 (progress: 21.1%)
Step 547: Updated gated ratio to 0.9789 (progress: 21.1%)
Step 547: Updated gated ratio to 0.9789 (progress: 21.1%)
{'loss': 1.1334, 'grad_norm': 1.8450028896331787, 'learning_rate': 1.9704696606706055e-05, 'epoch': 0.11}
{'loss': 1.0699, 'grad_norm': 1.8472914695739746, 'learning_rate': 1.9703191698224742e-05, 'epoch': 0.11}
{'loss': 1.1403, 'grad_norm': 1.7090426683425903, 'learning_rate': 1.9701683022630323e-05, 'epoch': 0.11}
{'loss': 1.1482, 'grad_norm': 1.6826013326644897, 'learning_rate': 1.9700170580508514e-05, 'epoch': 0.11}
{'loss': 1.1397, 'grad_norm': 1.717836618423462, 'learning_rate': 1.9698654372446495e-05, 'epoch': 0.11}
{'loss': 1.0518, 'grad_norm': 1.6464546918869019, 'learning_rate': 1.969713439903292e-05, 'epoch': 0.11}
Step 553: Updated gated ratio to 0.9787 (progress: 21.3%)
Step 553: Updated gated ratio to 0.9787 (progress: 21.3%)
Step 553: Updated gated ratio to 0.9787 (progress: 21.3%)
Step 553: Updated gated ratio to 0.9787 (progress: 21.3%)
Step 553: Updated gated ratio to 0.9787 (progress: 21.3%)Step 553: Updated gated ratio to 0.9787 (progress: 21.3%)
Step 553: Updated gated ratio to 0.9787 (progress: 21.3%)
Step 553: Updated gated ratio to 0.9787 (progress: 21.3%)
Step 553: Updated gated ratio to 0.9787 (progress: 21.3%)
Step 553: Updated gated ratio to 0.9787 (progress: 21.3%)

Step 553: Updated gated ratio to 0.9787 (progress: 21.3%)
Step 553: Updated gated ratio to 0.9787 (progress: 21.3%)
Step 553: Updated gated ratio to 0.9787 (progress: 21.3%)
Step 553: Updated gated ratio to 0.9787 (progress: 21.3%)
Step 553: Updated gated ratio to 0.9787 (progress: 21.3%)
Step 553: Updated gated ratio to 0.9787 (progress: 21.3%)
{'loss': 1.0319, 'grad_norm': 1.7107312679290771, 'learning_rate': 1.9695610660857886e-05, 'epoch': 0.11}
{'loss': 1.0743, 'grad_norm': 1.4339178800582886, 'learning_rate': 1.9694083158512965e-05, 'epoch': 0.11}
{'loss': 1.0928, 'grad_norm': 1.8883183002471924, 'learning_rate': 1.9692551892591185e-05, 'epoch': 0.11}
{'loss': 1.125, 'grad_norm': 1.7698453664779663, 'learning_rate': 1.9691016863687037e-05, 'epoch': 0.11}
{'loss': 1.0894, 'grad_norm': 1.5049532651901245, 'learning_rate': 1.968947807239647e-05, 'epoch': 0.11}
Step 558: Updated gated ratio to 0.9785 (progress: 21.5%)
Step 558: Updated gated ratio to 0.9785 (progress: 21.5%)
Step 558: Updated gated ratio to 0.9785 (progress: 21.5%)
Step 558: Updated gated ratio to 0.9785 (progress: 21.5%)Step 558: Updated gated ratio to 0.9785 (progress: 21.5%)
Step 558: Updated gated ratio to 0.9785 (progress: 21.5%)Step 558: Updated gated ratio to 0.9785 (progress: 21.5%)

Step 558: Updated gated ratio to 0.9785 (progress: 21.5%)
Step 558: Updated gated ratio to 0.9785 (progress: 21.5%)
Step 558: Updated gated ratio to 0.9785 (progress: 21.5%)

Step 558: Updated gated ratio to 0.9785 (progress: 21.5%)
Step 558: Updated gated ratio to 0.9785 (progress: 21.5%)
Step 558: Updated gated ratio to 0.9785 (progress: 21.5%)
Step 558: Updated gated ratio to 0.9785 (progress: 21.5%)
Step 558: Updated gated ratio to 0.9785 (progress: 21.5%)
Step 558: Updated gated ratio to 0.9785 (progress: 21.5%)
{'loss': 1.0745, 'grad_norm': 1.744245171546936, 'learning_rate': 1.9687935519316897e-05, 'epoch': 0.11}
{'loss': 1.024, 'grad_norm': 1.9292460680007935, 'learning_rate': 1.9686389205047186e-05, 'epoch': 0.11}
{'loss': 1.0415, 'grad_norm': 1.7352036237716675, 'learning_rate': 1.9684839130187678e-05, 'epoch': 0.11}
{'loss': 1.0442, 'grad_norm': 1.6781213283538818, 'learning_rate': 1.968328529534016e-05, 'epoch': 0.11}
{'loss': 1.1192, 'grad_norm': 1.7055044174194336, 'learning_rate': 1.9681727701107885e-05, 'epoch': 0.11}
Step 563: Updated gated ratio to 0.9783 (progress: 21.7%)
Step 563: Updated gated ratio to 0.9783 (progress: 21.7%)
Step 563: Updated gated ratio to 0.9783 (progress: 21.7%)
Step 563: Updated gated ratio to 0.9783 (progress: 21.7%)Step 563: Updated gated ratio to 0.9783 (progress: 21.7%)
Step 563: Updated gated ratio to 0.9783 (progress: 21.7%)

Step 563: Updated gated ratio to 0.9783 (progress: 21.7%)
Step 563: Updated gated ratio to 0.9783 (progress: 21.7%)
Step 563: Updated gated ratio to 0.9783 (progress: 21.7%)
Step 563: Updated gated ratio to 0.9783 (progress: 21.7%)
Step 563: Updated gated ratio to 0.9783 (progress: 21.7%)
Step 563: Updated gated ratio to 0.9783 (progress: 21.7%)
Step 563: Updated gated ratio to 0.9783 (progress: 21.7%)
Step 563: Updated gated ratio to 0.9783 (progress: 21.7%)
Step 563: Updated gated ratio to 0.9783 (progress: 21.7%)
Step 563: Updated gated ratio to 0.9783 (progress: 21.7%)
{'loss': 1.1065, 'grad_norm': 1.9297899007797241, 'learning_rate': 1.9680166348095568e-05, 'epoch': 0.11}
{'loss': 1.1099, 'grad_norm': 1.744962453842163, 'learning_rate': 1.967860123690937e-05, 'epoch': 0.11}
{'loss': 1.0837, 'grad_norm': 1.7533645629882812, 'learning_rate': 1.9677032368156934e-05, 'epoch': 0.11}
{'loss': 1.1944, 'grad_norm': 1.6850438117980957, 'learning_rate': 1.967545974244734e-05, 'epoch': 0.11}
{'loss': 1.0886, 'grad_norm': 1.603971004486084, 'learning_rate': 1.9673883360391138e-05, 'epoch': 0.11}
Step 568: Updated gated ratio to 0.9781 (progress: 21.9%)
Step 568: Updated gated ratio to 0.9781 (progress: 21.9%)
Step 568: Updated gated ratio to 0.9781 (progress: 21.9%)
Step 568: Updated gated ratio to 0.9781 (progress: 21.9%)
Step 568: Updated gated ratio to 0.9781 (progress: 21.9%)
Step 568: Updated gated ratio to 0.9781 (progress: 21.9%)
Step 568: Updated gated ratio to 0.9781 (progress: 21.9%)
Step 568: Updated gated ratio to 0.9781 (progress: 21.9%)
Step 568: Updated gated ratio to 0.9781 (progress: 21.9%)
Step 568: Updated gated ratio to 0.9781 (progress: 21.9%)
Step 568: Updated gated ratio to 0.9781 (progress: 21.9%)
Step 568: Updated gated ratio to 0.9781 (progress: 21.9%)
Step 568: Updated gated ratio to 0.9781 (progress: 21.9%)Step 568: Updated gated ratio to 0.9781 (progress: 21.9%)
Step 568: Updated gated ratio to 0.9781 (progress: 21.9%)

Step 568: Updated gated ratio to 0.9781 (progress: 21.9%)
{'loss': 1.1359, 'grad_norm': 1.9952418804168701, 'learning_rate': 1.9672303222600333e-05, 'epoch': 0.11}
{'loss': 1.1507, 'grad_norm': 1.8004869222640991, 'learning_rate': 1.967071932968839e-05, 'epoch': 0.11}
{'loss': 1.0806, 'grad_norm': 1.5757616758346558, 'learning_rate': 1.9669131682270232e-05, 'epoch': 0.11}
{'loss': 1.1369, 'grad_norm': 1.8423824310302734, 'learning_rate': 1.9667540280962235e-05, 'epoch': 0.11}
{'loss': 1.0645, 'grad_norm': 1.5863465070724487, 'learning_rate': 1.966594512638224e-05, 'epoch': 0.11}
Step 573: Updated gated ratio to 0.9779 (progress: 22.1%)
Step 573: Updated gated ratio to 0.9779 (progress: 22.1%)
Step 573: Updated gated ratio to 0.9779 (progress: 22.1%)
Step 573: Updated gated ratio to 0.9779 (progress: 22.1%)
Step 573: Updated gated ratio to 0.9779 (progress: 22.1%)
Step 573: Updated gated ratio to 0.9779 (progress: 22.1%)
Step 573: Updated gated ratio to 0.9779 (progress: 22.1%)
Step 573: Updated gated ratio to 0.9779 (progress: 22.1%)
Step 573: Updated gated ratio to 0.9779 (progress: 22.1%)
Step 573: Updated gated ratio to 0.9779 (progress: 22.1%)
Step 573: Updated gated ratio to 0.9779 (progress: 22.1%)
Step 573: Updated gated ratio to 0.9779 (progress: 22.1%)
Step 573: Updated gated ratio to 0.9779 (progress: 22.1%)
Step 573: Updated gated ratio to 0.9779 (progress: 22.1%)
Step 573: Updated gated ratio to 0.9779 (progress: 22.1%)
Step 573: Updated gated ratio to 0.9779 (progress: 22.1%)
{'loss': 1.1205, 'grad_norm': 1.6667838096618652, 'learning_rate': 1.9664346219149538e-05, 'epoch': 0.11}
{'loss': 1.121, 'grad_norm': 1.6510809659957886, 'learning_rate': 1.966274355988488e-05, 'epoch': 0.11}
{'loss': 1.1766, 'grad_norm': 1.8586524724960327, 'learning_rate': 1.9661137149210473e-05, 'epoch': 0.11}
{'loss': 1.1313, 'grad_norm': 1.6238727569580078, 'learning_rate': 1.9659526987749987e-05, 'epoch': 0.11}
{'loss': 1.1008, 'grad_norm': 1.5694302320480347, 'learning_rate': 1.9657913076128532e-05, 'epoch': 0.11}
{'loss': 1.0846, 'grad_norm': 1.846935749053955, 'learning_rate': 1.965629541497269e-05, 'epoch': 0.11}
Step 579: Updated gated ratio to 0.9777 (progress: 22.3%)
Step 579: Updated gated ratio to 0.9777 (progress: 22.3%)
Step 579: Updated gated ratio to 0.9777 (progress: 22.3%)
Step 579: Updated gated ratio to 0.9777 (progress: 22.3%)
Step 579: Updated gated ratio to 0.9777 (progress: 22.3%)
Step 579: Updated gated ratio to 0.9777 (progress: 22.3%)
Step 579: Updated gated ratio to 0.9777 (progress: 22.3%)
Step 579: Updated gated ratio to 0.9777 (progress: 22.3%)
Step 579: Updated gated ratio to 0.9777 (progress: 22.3%)
Step 579: Updated gated ratio to 0.9777 (progress: 22.3%)
Step 579: Updated gated ratio to 0.9777 (progress: 22.3%)
Step 579: Updated gated ratio to 0.9777 (progress: 22.3%)
Step 579: Updated gated ratio to 0.9777 (progress: 22.3%)
Step 579: Updated gated ratio to 0.9777 (progress: 22.3%)
Step 579: Updated gated ratio to 0.9777 (progress: 22.3%)
Step 579: Updated gated ratio to 0.9777 (progress: 22.3%)
{'loss': 1.1392, 'grad_norm': 2.0763168334960938, 'learning_rate': 1.9654674004910493e-05, 'epoch': 0.11}
{'loss': 1.1585, 'grad_norm': 1.7040990591049194, 'learning_rate': 1.9653048846571427e-05, 'epoch': 0.11}
{'loss': 1.0119, 'grad_norm': 1.7801927328109741, 'learning_rate': 1.9651419940586437e-05, 'epoch': 0.11}
{'loss': 1.0553, 'grad_norm': 1.8560675382614136, 'learning_rate': 1.964978728758791e-05, 'epoch': 0.11}
{'loss': 1.1988, 'grad_norm': 1.7745147943496704, 'learning_rate': 1.9648150888209715e-05, 'epoch': 0.11}
Step 584: Updated gated ratio to 0.9775 (progress: 22.5%)
Step 584: Updated gated ratio to 0.9775 (progress: 22.5%)
Step 584: Updated gated ratio to 0.9775 (progress: 22.5%)
Step 584: Updated gated ratio to 0.9775 (progress: 22.5%)
Step 584: Updated gated ratio to 0.9775 (progress: 22.5%)
Step 584: Updated gated ratio to 0.9775 (progress: 22.5%)
Step 584: Updated gated ratio to 0.9775 (progress: 22.5%)
Step 584: Updated gated ratio to 0.9775 (progress: 22.5%)
Step 584: Updated gated ratio to 0.9775 (progress: 22.5%)
Step 584: Updated gated ratio to 0.9775 (progress: 22.5%)
Step 584: Updated gated ratio to 0.9775 (progress: 22.5%)
Step 584: Updated gated ratio to 0.9775 (progress: 22.5%)
Step 584: Updated gated ratio to 0.9775 (progress: 22.5%)
Step 584: Updated gated ratio to 0.9775 (progress: 22.5%)
Step 584: Updated gated ratio to 0.9775 (progress: 22.5%)
Step 584: Updated gated ratio to 0.9775 (progress: 22.5%)
{'loss': 1.0596, 'grad_norm': 1.717629313468933, 'learning_rate': 1.9646510743087144e-05, 'epoch': 0.11}
{'loss': 1.0935, 'grad_norm': 1.841332197189331, 'learning_rate': 1.964486685285697e-05, 'epoch': 0.11}
{'loss': 0.9853, 'grad_norm': 1.7942874431610107, 'learning_rate': 1.9643219218157395e-05, 'epoch': 0.11}
{'loss': 1.142, 'grad_norm': 1.7923738956451416, 'learning_rate': 1.9641567839628092e-05, 'epoch': 0.11}
{'loss': 1.0864, 'grad_norm': 1.6596431732177734, 'learning_rate': 1.963991271791019e-05, 'epoch': 0.11}
Step 589: Updated gated ratio to 0.9773 (progress: 22.7%)
Step 589: Updated gated ratio to 0.9773 (progress: 22.7%)
Step 589: Updated gated ratio to 0.9773 (progress: 22.7%)
Step 589: Updated gated ratio to 0.9773 (progress: 22.7%)Step 589: Updated gated ratio to 0.9773 (progress: 22.7%)

Step 589: Updated gated ratio to 0.9773 (progress: 22.7%)
Step 589: Updated gated ratio to 0.9773 (progress: 22.7%)
Step 589: Updated gated ratio to 0.9773 (progress: 22.7%)
Step 589: Updated gated ratio to 0.9773 (progress: 22.7%)
Step 589: Updated gated ratio to 0.9773 (progress: 22.7%)
Step 589: Updated gated ratio to 0.9773 (progress: 22.7%)
Step 589: Updated gated ratio to 0.9773 (progress: 22.7%)
Step 589: Updated gated ratio to 0.9773 (progress: 22.7%)
Step 589: Updated gated ratio to 0.9773 (progress: 22.7%)
Step 589: Updated gated ratio to 0.9773 (progress: 22.7%)
Step 589: Updated gated ratio to 0.9773 (progress: 22.7%)
{'loss': 1.1267, 'grad_norm': 1.7888842821121216, 'learning_rate': 1.9638253853646255e-05, 'epoch': 0.11}
{'loss': 1.085, 'grad_norm': 1.6699631214141846, 'learning_rate': 1.9636591247480323e-05, 'epoch': 0.11}
{'loss': 1.0382, 'grad_norm': 1.4921704530715942, 'learning_rate': 1.9634924900057867e-05, 'epoch': 0.11}
{'loss': 1.0663, 'grad_norm': 1.7626734972000122, 'learning_rate': 1.963325481202583e-05, 'epoch': 0.11}
{'loss': 1.0989, 'grad_norm': 1.5609428882598877, 'learning_rate': 1.963158098403259e-05, 'epoch': 0.11}
Step 594: Updated gated ratio to 0.9771 (progress: 22.9%)
Step 594: Updated gated ratio to 0.9771 (progress: 22.9%)
Step 594: Updated gated ratio to 0.9771 (progress: 22.9%)Step 594: Updated gated ratio to 0.9771 (progress: 22.9%)

Step 594: Updated gated ratio to 0.9771 (progress: 22.9%)
Step 594: Updated gated ratio to 0.9771 (progress: 22.9%)
Step 594: Updated gated ratio to 0.9771 (progress: 22.9%)
Step 594: Updated gated ratio to 0.9771 (progress: 22.9%)
Step 594: Updated gated ratio to 0.9771 (progress: 22.9%)
Step 594: Updated gated ratio to 0.9771 (progress: 22.9%)
Step 594: Updated gated ratio to 0.9771 (progress: 22.9%)
Step 594: Updated gated ratio to 0.9771 (progress: 22.9%)
Step 594: Updated gated ratio to 0.9771 (progress: 22.9%)
Step 594: Updated gated ratio to 0.9771 (progress: 22.9%)
Step 594: Updated gated ratio to 0.9771 (progress: 22.9%)
Step 594: Updated gated ratio to 0.9771 (progress: 22.9%)
{'loss': 1.12, 'grad_norm': 1.7345906496047974, 'learning_rate': 1.9629903416727987e-05, 'epoch': 0.11}
{'loss': 1.0906, 'grad_norm': 1.653485894203186, 'learning_rate': 1.962822211076331e-05, 'epoch': 0.11}
{'loss': 1.1167, 'grad_norm': 1.8453514575958252, 'learning_rate': 1.96265370667913e-05, 'epoch': 0.11}
{'loss': 1.0687, 'grad_norm': 1.5817722082138062, 'learning_rate': 1.9624848285466146e-05, 'epoch': 0.12}
{'loss': 1.1082, 'grad_norm': 1.7695093154907227, 'learning_rate': 1.9623155767443498e-05, 'epoch': 0.12}
Step 599: Updated gated ratio to 0.9769 (progress: 23.1%)
Step 599: Updated gated ratio to 0.9769 (progress: 23.1%)
Step 599: Updated gated ratio to 0.9769 (progress: 23.1%)
Step 599: Updated gated ratio to 0.9769 (progress: 23.1%)
Step 599: Updated gated ratio to 0.9769 (progress: 23.1%)Step 599: Updated gated ratio to 0.9769 (progress: 23.1%)
Step 599: Updated gated ratio to 0.9769 (progress: 23.1%)
Step 599: Updated gated ratio to 0.9769 (progress: 23.1%)
Step 599: Updated gated ratio to 0.9769 (progress: 23.1%)

Step 599: Updated gated ratio to 0.9769 (progress: 23.1%)
Step 599: Updated gated ratio to 0.9769 (progress: 23.1%)
Step 599: Updated gated ratio to 0.9769 (progress: 23.1%)
Step 599: Updated gated ratio to 0.9769 (progress: 23.1%)
Step 599: Updated gated ratio to 0.9769 (progress: 23.1%)
Step 599: Updated gated ratio to 0.9769 (progress: 23.1%)
Step 599: Updated gated ratio to 0.9769 (progress: 23.1%)
{'loss': 1.242, 'grad_norm': 1.8670494556427002, 'learning_rate': 1.9621459513380445e-05, 'epoch': 0.12}
{'loss': 1.1828, 'grad_norm': 1.928442120552063, 'learning_rate': 1.9619759523935532e-05, 'epoch': 0.12}
{'loss': 1.105, 'grad_norm': 1.9112318754196167, 'learning_rate': 1.9618055799768757e-05, 'epoch': 0.12}
{'loss': 1.1425, 'grad_norm': 1.7048583030700684, 'learning_rate': 1.961634834154156e-05, 'epoch': 0.12}
{'loss': 1.0737, 'grad_norm': 1.5239818096160889, 'learning_rate': 1.9614637149916834e-05, 'epoch': 0.12}
{'loss': 1.0333, 'grad_norm': 1.447243332862854, 'learning_rate': 1.9612922225558924e-05, 'epoch': 0.12}
Step 605: Updated gated ratio to 0.9767 (progress: 23.3%)
Step 605: Updated gated ratio to 0.9767 (progress: 23.3%)
Step 605: Updated gated ratio to 0.9767 (progress: 23.3%)
Step 605: Updated gated ratio to 0.9767 (progress: 23.3%)
Step 605: Updated gated ratio to 0.9767 (progress: 23.3%)
Step 605: Updated gated ratio to 0.9767 (progress: 23.3%)
Step 605: Updated gated ratio to 0.9767 (progress: 23.3%)
Step 605: Updated gated ratio to 0.9767 (progress: 23.3%)
Step 605: Updated gated ratio to 0.9767 (progress: 23.3%)
Step 605: Updated gated ratio to 0.9767 (progress: 23.3%)
Step 605: Updated gated ratio to 0.9767 (progress: 23.3%)
Step 605: Updated gated ratio to 0.9767 (progress: 23.3%)
Step 605: Updated gated ratio to 0.9767 (progress: 23.3%)
Step 605: Updated gated ratio to 0.9767 (progress: 23.3%)
Step 605: Updated gated ratio to 0.9767 (progress: 23.3%)
Step 605: Updated gated ratio to 0.9767 (progress: 23.3%)
{'loss': 1.0381, 'grad_norm': 1.7752283811569214, 'learning_rate': 1.961120356913363e-05, 'epoch': 0.12}
{'loss': 1.116, 'grad_norm': 1.6269357204437256, 'learning_rate': 1.960948118130818e-05, 'epoch': 0.12}
{'loss': 1.1425, 'grad_norm': 1.8217277526855469, 'learning_rate': 1.9607755062751273e-05, 'epoch': 0.12}
{'loss': 1.0792, 'grad_norm': 1.6681946516036987, 'learning_rate': 1.9606025214133046e-05, 'epoch': 0.12}
{'loss': 1.0903, 'grad_norm': 1.4035452604293823, 'learning_rate': 1.9604291636125084e-05, 'epoch': 0.12}
Step 610: Updated gated ratio to 0.9765 (progress: 23.5%)
Step 610: Updated gated ratio to 0.9765 (progress: 23.5%)
Step 610: Updated gated ratio to 0.9765 (progress: 23.5%)
Step 610: Updated gated ratio to 0.9765 (progress: 23.5%)
Step 610: Updated gated ratio to 0.9765 (progress: 23.5%)
Step 610: Updated gated ratio to 0.9765 (progress: 23.5%)
Step 610: Updated gated ratio to 0.9765 (progress: 23.5%)
Step 610: Updated gated ratio to 0.9765 (progress: 23.5%)
Step 610: Updated gated ratio to 0.9765 (progress: 23.5%)Step 610: Updated gated ratio to 0.9765 (progress: 23.5%)

Step 610: Updated gated ratio to 0.9765 (progress: 23.5%)
Step 610: Updated gated ratio to 0.9765 (progress: 23.5%)
Step 610: Updated gated ratio to 0.9765 (progress: 23.5%)
Step 610: Updated gated ratio to 0.9765 (progress: 23.5%)
Step 610: Updated gated ratio to 0.9765 (progress: 23.5%)
Step 610: Updated gated ratio to 0.9765 (progress: 23.5%)
{'loss': 1.0225, 'grad_norm': 1.5980591773986816, 'learning_rate': 1.960255432940043e-05, 'epoch': 0.12}
{'loss': 1.161, 'grad_norm': 1.8240869045257568, 'learning_rate': 1.9600813294633552e-05, 'epoch': 0.12}
{'loss': 1.1263, 'grad_norm': 1.8506991863250732, 'learning_rate': 1.9599068532500394e-05, 'epoch': 0.12}
{'loss': 1.1131, 'grad_norm': 1.85897958278656, 'learning_rate': 1.9597320043678322e-05, 'epoch': 0.12}
{'loss': 1.1836, 'grad_norm': 1.7581682205200195, 'learning_rate': 1.9595567828846166e-05, 'epoch': 0.12}
Step 615: Updated gated ratio to 0.9763 (progress: 23.7%)Step 615: Updated gated ratio to 0.9763 (progress: 23.7%)

Step 615: Updated gated ratio to 0.9763 (progress: 23.7%)Step 615: Updated gated ratio to 0.9763 (progress: 23.7%)
Step 615: Updated gated ratio to 0.9763 (progress: 23.7%)
Step 615: Updated gated ratio to 0.9763 (progress: 23.7%)
Step 615: Updated gated ratio to 0.9763 (progress: 23.7%)
Step 615: Updated gated ratio to 0.9763 (progress: 23.7%)
Step 615: Updated gated ratio to 0.9763 (progress: 23.7%)

Step 615: Updated gated ratio to 0.9763 (progress: 23.7%)
Step 615: Updated gated ratio to 0.9763 (progress: 23.7%)
Step 615: Updated gated ratio to 0.9763 (progress: 23.7%)
Step 615: Updated gated ratio to 0.9763 (progress: 23.7%)
Step 615: Updated gated ratio to 0.9763 (progress: 23.7%)
Step 615: Updated gated ratio to 0.9763 (progress: 23.7%)
Step 615: Updated gated ratio to 0.9763 (progress: 23.7%)
{'loss': 1.1429, 'grad_norm': 1.9313501119613647, 'learning_rate': 1.9593811888684192e-05, 'epoch': 0.12}
{'loss': 1.0756, 'grad_norm': 1.727918267250061, 'learning_rate': 1.9592052223874115e-05, 'epoch': 0.12}
{'loss': 1.0354, 'grad_norm': 1.5769892930984497, 'learning_rate': 1.959028883509911e-05, 'epoch': 0.12}
{'loss': 1.0622, 'grad_norm': 1.801505208015442, 'learning_rate': 1.9588521723043764e-05, 'epoch': 0.12}
{'loss': 1.1207, 'grad_norm': 1.570363163948059, 'learning_rate': 1.958675088839415e-05, 'epoch': 0.12}
Step 620: Updated gated ratio to 0.9761 (progress: 23.9%)
Step 620: Updated gated ratio to 0.9761 (progress: 23.9%)
Step 620: Updated gated ratio to 0.9761 (progress: 23.9%)
Step 620: Updated gated ratio to 0.9761 (progress: 23.9%)
Step 620: Updated gated ratio to 0.9761 (progress: 23.9%)
Step 620: Updated gated ratio to 0.9761 (progress: 23.9%)
Step 620: Updated gated ratio to 0.9761 (progress: 23.9%)
Step 620: Updated gated ratio to 0.9761 (progress: 23.9%)
Step 620: Updated gated ratio to 0.9761 (progress: 23.9%)
Step 620: Updated gated ratio to 0.9761 (progress: 23.9%)
Step 620: Updated gated ratio to 0.9761 (progress: 23.9%)
Step 620: Updated gated ratio to 0.9761 (progress: 23.9%)
Step 620: Updated gated ratio to 0.9761 (progress: 23.9%)
Step 620: Updated gated ratio to 0.9761 (progress: 23.9%)
Step 620: Updated gated ratio to 0.9761 (progress: 23.9%)
Step 620: Updated gated ratio to 0.9761 (progress: 23.9%)
{'loss': 1.0654, 'grad_norm': 1.6835834980010986, 'learning_rate': 1.9584976331837758e-05, 'epoch': 0.12}
{'loss': 1.0819, 'grad_norm': 1.3353488445281982, 'learning_rate': 1.9583198054063535e-05, 'epoch': 0.12}
{'loss': 1.1037, 'grad_norm': 1.7074640989303589, 'learning_rate': 1.9581416055761865e-05, 'epoch': 0.12}
{'loss': 1.0125, 'grad_norm': 1.606516718864441, 'learning_rate': 1.9579630337624585e-05, 'epoch': 0.12}
{'loss': 1.1754, 'grad_norm': 1.8659435510635376, 'learning_rate': 1.9577840900344974e-05, 'epoch': 0.12}
Step 625: Updated gated ratio to 0.9759 (progress: 24.1%)
Step 625: Updated gated ratio to 0.9759 (progress: 24.1%)
Step 625: Updated gated ratio to 0.9759 (progress: 24.1%)
Step 625: Updated gated ratio to 0.9759 (progress: 24.1%)
Step 625: Updated gated ratio to 0.9759 (progress: 24.1%)
Step 625: Updated gated ratio to 0.9759 (progress: 24.1%)Step 625: Updated gated ratio to 0.9759 (progress: 24.1%)

Step 625: Updated gated ratio to 0.9759 (progress: 24.1%)
Step 625: Updated gated ratio to 0.9759 (progress: 24.1%)
Step 625: Updated gated ratio to 0.9759 (progress: 24.1%)
Step 625: Updated gated ratio to 0.9759 (progress: 24.1%)
Step 625: Updated gated ratio to 0.9759 (progress: 24.1%)
Step 625: Updated gated ratio to 0.9759 (progress: 24.1%)
Step 625: Updated gated ratio to 0.9759 (progress: 24.1%)
Step 625: Updated gated ratio to 0.9759 (progress: 24.1%)
Step 625: Updated gated ratio to 0.9759 (progress: 24.1%)
{'loss': 1.087, 'grad_norm': 1.3816512823104858, 'learning_rate': 1.9576047744617752e-05, 'epoch': 0.12}
{'loss': 1.1611, 'grad_norm': 2.0225002765655518, 'learning_rate': 1.957425087113908e-05, 'epoch': 0.12}
{'loss': 1.0555, 'grad_norm': 1.8586716651916504, 'learning_rate': 1.9572450280606568e-05, 'epoch': 0.12}
{'loss': 1.1079, 'grad_norm': 1.797540307044983, 'learning_rate': 1.9570645973719273e-05, 'epoch': 0.12}
{'loss': 0.9977, 'grad_norm': 1.7822918891906738, 'learning_rate': 1.9568837951177677e-05, 'epoch': 0.12}
{'loss': 1.1738, 'grad_norm': 1.6789770126342773, 'learning_rate': 1.9567026213683728e-05, 'epoch': 0.12}
Step 631: Updated gated ratio to 0.9757 (progress: 24.3%)
Step 631: Updated gated ratio to 0.9757 (progress: 24.3%)
Step 631: Updated gated ratio to 0.9757 (progress: 24.3%)
Step 631: Updated gated ratio to 0.9757 (progress: 24.3%)
Step 631: Updated gated ratio to 0.9757 (progress: 24.3%)
Step 631: Updated gated ratio to 0.9757 (progress: 24.3%)
Step 631: Updated gated ratio to 0.9757 (progress: 24.3%)
Step 631: Updated gated ratio to 0.9757 (progress: 24.3%)
Step 631: Updated gated ratio to 0.9757 (progress: 24.3%)
Step 631: Updated gated ratio to 0.9757 (progress: 24.3%)
Step 631: Updated gated ratio to 0.9757 (progress: 24.3%)
Step 631: Updated gated ratio to 0.9757 (progress: 24.3%)
Step 631: Updated gated ratio to 0.9757 (progress: 24.3%)
Step 631: Updated gated ratio to 0.9757 (progress: 24.3%)
Step 631: Updated gated ratio to 0.9757 (progress: 24.3%)
Step 631: Updated gated ratio to 0.9757 (progress: 24.3%)
{'loss': 1.0602, 'grad_norm': 1.7300375699996948, 'learning_rate': 1.9565210761940798e-05, 'epoch': 0.12}
{'loss': 1.1473, 'grad_norm': 1.7298285961151123, 'learning_rate': 1.956339159665371e-05, 'epoch': 0.12}
{'loss': 1.0893, 'grad_norm': 1.6780316829681396, 'learning_rate': 1.956156871852873e-05, 'epoch': 0.12}
{'loss': 1.1351, 'grad_norm': 1.8372700214385986, 'learning_rate': 1.9559742128273558e-05, 'epoch': 0.12}
{'loss': 1.0163, 'grad_norm': 1.6971611976623535, 'learning_rate': 1.9557911826597337e-05, 'epoch': 0.12}
Step 636: Updated gated ratio to 0.9755 (progress: 24.5%)
Step 636: Updated gated ratio to 0.9755 (progress: 24.5%)
Step 636: Updated gated ratio to 0.9755 (progress: 24.5%)
Step 636: Updated gated ratio to 0.9755 (progress: 24.5%)
Step 636: Updated gated ratio to 0.9755 (progress: 24.5%)
Step 636: Updated gated ratio to 0.9755 (progress: 24.5%)
Step 636: Updated gated ratio to 0.9755 (progress: 24.5%)
Step 636: Updated gated ratio to 0.9755 (progress: 24.5%)
Step 636: Updated gated ratio to 0.9755 (progress: 24.5%)
Step 636: Updated gated ratio to 0.9755 (progress: 24.5%)
Step 636: Updated gated ratio to 0.9755 (progress: 24.5%)
Step 636: Updated gated ratio to 0.9755 (progress: 24.5%)
Step 636: Updated gated ratio to 0.9755 (progress: 24.5%)
Step 636: Updated gated ratio to 0.9755 (progress: 24.5%)
Step 636: Updated gated ratio to 0.9755 (progress: 24.5%)
Step 636: Updated gated ratio to 0.9755 (progress: 24.5%)
{'loss': 1.1373, 'grad_norm': 1.4868748188018799, 'learning_rate': 1.9556077814210662e-05, 'epoch': 0.12}
{'loss': 1.1653, 'grad_norm': 1.765802025794983, 'learning_rate': 1.955424009182555e-05, 'epoch': 0.12}
{'loss': 1.0884, 'grad_norm': 1.4615024328231812, 'learning_rate': 1.955239866015547e-05, 'epoch': 0.12}
{'loss': 1.1533, 'grad_norm': 1.731198787689209, 'learning_rate': 1.9550553519915335e-05, 'epoch': 0.12}
{'loss': 1.0885, 'grad_norm': 1.635805368423462, 'learning_rate': 1.954870467182149e-05, 'epoch': 0.12}
Step 641: Updated gated ratio to 0.9753 (progress: 24.7%)
Step 641: Updated gated ratio to 0.9753 (progress: 24.7%)
Step 641: Updated gated ratio to 0.9753 (progress: 24.7%)
Step 641: Updated gated ratio to 0.9753 (progress: 24.7%)
Step 641: Updated gated ratio to 0.9753 (progress: 24.7%)
Step 641: Updated gated ratio to 0.9753 (progress: 24.7%)
Step 641: Updated gated ratio to 0.9753 (progress: 24.7%)
Step 641: Updated gated ratio to 0.9753 (progress: 24.7%)
Step 641: Updated gated ratio to 0.9753 (progress: 24.7%)
Step 641: Updated gated ratio to 0.9753 (progress: 24.7%)
Step 641: Updated gated ratio to 0.9753 (progress: 24.7%)
Step 641: Updated gated ratio to 0.9753 (progress: 24.7%)
Step 641: Updated gated ratio to 0.9753 (progress: 24.7%)
Step 641: Updated gated ratio to 0.9753 (progress: 24.7%)
Step 641: Updated gated ratio to 0.9753 (progress: 24.7%)
Step 641: Updated gated ratio to 0.9753 (progress: 24.7%)
{'loss': 1.1143, 'grad_norm': 1.4051012992858887, 'learning_rate': 1.954685211659172e-05, 'epoch': 0.12}
{'loss': 1.1447, 'grad_norm': 1.724024772644043, 'learning_rate': 1.9544995854945248e-05, 'epoch': 0.12}
{'loss': 1.1269, 'grad_norm': 1.76564621925354, 'learning_rate': 1.954313588760274e-05, 'epoch': 0.12}
{'loss': 1.131, 'grad_norm': 1.8108515739440918, 'learning_rate': 1.9541272215286304e-05, 'epoch': 0.12}
{'loss': 1.1076, 'grad_norm': 1.8159654140472412, 'learning_rate': 1.9539404838719477e-05, 'epoch': 0.12}
Step 646: Updated gated ratio to 0.9751 (progress: 24.9%)
Step 646: Updated gated ratio to 0.9751 (progress: 24.9%)
Step 646: Updated gated ratio to 0.9751 (progress: 24.9%)
Step 646: Updated gated ratio to 0.9751 (progress: 24.9%)
Step 646: Updated gated ratio to 0.9751 (progress: 24.9%)
Step 646: Updated gated ratio to 0.9751 (progress: 24.9%)
Step 646: Updated gated ratio to 0.9751 (progress: 24.9%)
Step 646: Updated gated ratio to 0.9751 (progress: 24.9%)
Step 646: Updated gated ratio to 0.9751 (progress: 24.9%)
Step 646: Updated gated ratio to 0.9751 (progress: 24.9%)
Step 646: Updated gated ratio to 0.9751 (progress: 24.9%)
Step 646: Updated gated ratio to 0.9751 (progress: 24.9%)
Step 646: Updated gated ratio to 0.9751 (progress: 24.9%)
Step 646: Updated gated ratio to 0.9751 (progress: 24.9%)
Step 646: Updated gated ratio to 0.9751 (progress: 24.9%)
Step 646: Updated gated ratio to 0.9751 (progress: 24.9%)
{'loss': 1.0792, 'grad_norm': 1.787846565246582, 'learning_rate': 1.9537533758627242e-05, 'epoch': 0.12}
{'loss': 1.1122, 'grad_norm': 1.970176100730896, 'learning_rate': 1.953565897573601e-05, 'epoch': 0.12}
{'loss': 1.0621, 'grad_norm': 1.8079851865768433, 'learning_rate': 1.9533780490773645e-05, 'epoch': 0.12}
{'loss': 1.0531, 'grad_norm': 1.6836715936660767, 'learning_rate': 1.9531898304469435e-05, 'epoch': 0.13}
{'loss': 1.1119, 'grad_norm': 1.3773548603057861, 'learning_rate': 1.953001241755411e-05, 'epoch': 0.13}
Step 651: Updated gated ratio to 0.9749 (progress: 25.1%)
Step 651: Updated gated ratio to 0.9749 (progress: 25.1%)
Step 651: Updated gated ratio to 0.9749 (progress: 25.1%)
Step 651: Updated gated ratio to 0.9749 (progress: 25.1%)
Step 651: Updated gated ratio to 0.9749 (progress: 25.1%)Step 651: Updated gated ratio to 0.9749 (progress: 25.1%)

Step 651: Updated gated ratio to 0.9749 (progress: 25.1%)
Step 651: Updated gated ratio to 0.9749 (progress: 25.1%)
Step 651: Updated gated ratio to 0.9749 (progress: 25.1%)
Step 651: Updated gated ratio to 0.9749 (progress: 25.1%)
Step 651: Updated gated ratio to 0.9749 (progress: 25.1%)
Step 651: Updated gated ratio to 0.9749 (progress: 25.1%)
Step 651: Updated gated ratio to 0.9749 (progress: 25.1%)
Step 651: Updated gated ratio to 0.9749 (progress: 25.1%)
Step 651: Updated gated ratio to 0.9749 (progress: 25.1%)
Step 651: Updated gated ratio to 0.9749 (progress: 25.1%)
{'loss': 1.1813, 'grad_norm': 1.6753464937210083, 'learning_rate': 1.952812283075984e-05, 'epoch': 0.13}
{'loss': 1.0935, 'grad_norm': 1.6092661619186401, 'learning_rate': 1.952622954482022e-05, 'epoch': 0.13}
{'loss': 1.0573, 'grad_norm': 1.7015165090560913, 'learning_rate': 1.9524332560470293e-05, 'epoch': 0.13}
{'loss': 1.1051, 'grad_norm': 1.653366208076477, 'learning_rate': 1.9522431878446536e-05, 'epoch': 0.13}
{'loss': 1.1243, 'grad_norm': 1.7152756452560425, 'learning_rate': 1.9520527499486856e-05, 'epoch': 0.13}
Step 656: Updated gated ratio to 0.9747 (progress: 25.3%)
Step 656: Updated gated ratio to 0.9747 (progress: 25.3%)
Step 656: Updated gated ratio to 0.9747 (progress: 25.3%)
Step 656: Updated gated ratio to 0.9747 (progress: 25.3%)
Step 656: Updated gated ratio to 0.9747 (progress: 25.3%)
Step 656: Updated gated ratio to 0.9747 (progress: 25.3%)
Step 656: Updated gated ratio to 0.9747 (progress: 25.3%)
Step 656: Updated gated ratio to 0.9747 (progress: 25.3%)
Step 656: Updated gated ratio to 0.9747 (progress: 25.3%)
Step 656: Updated gated ratio to 0.9747 (progress: 25.3%)
Step 656: Updated gated ratio to 0.9747 (progress: 25.3%)
Step 656: Updated gated ratio to 0.9747 (progress: 25.3%)
Step 656: Updated gated ratio to 0.9747 (progress: 25.3%)
Step 656: Updated gated ratio to 0.9747 (progress: 25.3%)
Step 656: Updated gated ratio to 0.9747 (progress: 25.3%)
Step 656: Updated gated ratio to 0.9747 (progress: 25.3%)
{'loss': 1.0418, 'grad_norm': 1.7018526792526245, 'learning_rate': 1.95186194243306e-05, 'epoch': 0.13}
{'loss': 1.1276, 'grad_norm': 1.784907579421997, 'learning_rate': 1.9516707653718546e-05, 'epoch': 0.13}
{'loss': 1.1264, 'grad_norm': 1.606948971748352, 'learning_rate': 1.9514792188392914e-05, 'epoch': 0.13}
{'loss': 1.1219, 'grad_norm': 1.748266339302063, 'learning_rate': 1.9512873029097347e-05, 'epoch': 0.13}
{'loss': 1.0502, 'grad_norm': 1.7295467853546143, 'learning_rate': 1.9510950176576933e-05, 'epoch': 0.13}
{'loss': 1.0723, 'grad_norm': 1.8177173137664795, 'learning_rate': 1.950902363157819e-05, 'epoch': 0.13}
Step 662: Updated gated ratio to 0.9745 (progress: 25.5%)
Step 662: Updated gated ratio to 0.9745 (progress: 25.5%)Step 662: Updated gated ratio to 0.9745 (progress: 25.5%)

Step 662: Updated gated ratio to 0.9745 (progress: 25.5%)
Step 662: Updated gated ratio to 0.9745 (progress: 25.5%)
Step 662: Updated gated ratio to 0.9745 (progress: 25.5%)Step 662: Updated gated ratio to 0.9745 (progress: 25.5%)
Step 662: Updated gated ratio to 0.9745 (progress: 25.5%)
Step 662: Updated gated ratio to 0.9745 (progress: 25.5%)
Step 662: Updated gated ratio to 0.9745 (progress: 25.5%)

Step 662: Updated gated ratio to 0.9745 (progress: 25.5%)
Step 662: Updated gated ratio to 0.9745 (progress: 25.5%)
Step 662: Updated gated ratio to 0.9745 (progress: 25.5%)
Step 662: Updated gated ratio to 0.9745 (progress: 25.5%)
Step 662: Updated gated ratio to 0.9745 (progress: 25.5%)
Step 662: Updated gated ratio to 0.9745 (progress: 25.5%)
{'loss': 1.1718, 'grad_norm': 1.7615721225738525, 'learning_rate': 1.950709339484907e-05, 'epoch': 0.13}
{'loss': 1.1673, 'grad_norm': 1.9471399784088135, 'learning_rate': 1.9505159467138954e-05, 'epoch': 0.13}
{'loss': 1.111, 'grad_norm': 1.5295602083206177, 'learning_rate': 1.9503221849198655e-05, 'epoch': 0.13}
{'loss': 1.1011, 'grad_norm': 1.7837374210357666, 'learning_rate': 1.9501280541780435e-05, 'epoch': 0.13}
{'loss': 1.2038, 'grad_norm': 1.7644964456558228, 'learning_rate': 1.9499335545637968e-05, 'epoch': 0.13}
Step 667: Updated gated ratio to 0.9743 (progress: 25.7%)
Step 667: Updated gated ratio to 0.9743 (progress: 25.7%)Step 667: Updated gated ratio to 0.9743 (progress: 25.7%)
Step 667: Updated gated ratio to 0.9743 (progress: 25.7%)
Step 667: Updated gated ratio to 0.9743 (progress: 25.7%)Step 667: Updated gated ratio to 0.9743 (progress: 25.7%)
Step 667: Updated gated ratio to 0.9743 (progress: 25.7%)
Step 667: Updated gated ratio to 0.9743 (progress: 25.7%)
Step 667: Updated gated ratio to 0.9743 (progress: 25.7%)
Step 667: Updated gated ratio to 0.9743 (progress: 25.7%)


Step 667: Updated gated ratio to 0.9743 (progress: 25.7%)
Step 667: Updated gated ratio to 0.9743 (progress: 25.7%)
Step 667: Updated gated ratio to 0.9743 (progress: 25.7%)
Step 667: Updated gated ratio to 0.9743 (progress: 25.7%)
Step 667: Updated gated ratio to 0.9743 (progress: 25.7%)
Step 667: Updated gated ratio to 0.9743 (progress: 25.7%)
{'loss': 1.1205, 'grad_norm': 1.7082200050354004, 'learning_rate': 1.949738686152637e-05, 'epoch': 0.13}
{'loss': 1.1191, 'grad_norm': 1.7204618453979492, 'learning_rate': 1.9495434490202188e-05, 'epoch': 0.13}
Step 669: Updated gated ratio to 0.9742 (progress: 25.8%)
Step 669: Updated gated ratio to 0.9742 (progress: 25.8%)Step 669: Updated gated ratio to 0.9742 (progress: 25.8%)
Step 669: Updated gated ratio to 0.9742 (progress: 25.8%)
Step 669: Updated gated ratio to 0.9742 (progress: 25.8%)
Step 669: Updated gated ratio to 0.9742 (progress: 25.8%)

Step 669: Updated gated ratio to 0.9742 (progress: 25.8%)
Step 669: Updated gated ratio to 0.9742 (progress: 25.8%)
Step 669: Updated gated ratio to 0.9742 (progress: 25.8%)
Step 669: Updated gated ratio to 0.9742 (progress: 25.8%)
Step 669: Updated gated ratio to 0.9742 (progress: 25.8%)
Step 669: Updated gated ratio to 0.9742 (progress: 25.8%)
Step 669: Updated gated ratio to 0.9742 (progress: 25.8%)
Step 669: Updated gated ratio to 0.9742 (progress: 25.8%)
Step 669: Updated gated ratio to 0.9742 (progress: 25.8%)
Step 669: Updated gated ratio to 0.9742 (progress: 25.8%)
{'loss': 1.0386, 'grad_norm': 1.8336904048919678, 'learning_rate': 1.94934784324234e-05, 'epoch': 0.13}
{'loss': 1.0547, 'grad_norm': 1.7281808853149414, 'learning_rate': 1.9491518688949417e-05, 'epoch': 0.13}
{'loss': 1.0696, 'grad_norm': 1.851298451423645, 'learning_rate': 1.9489555260541074e-05, 'epoch': 0.13}
{'loss': 1.0496, 'grad_norm': 1.9178860187530518, 'learning_rate': 1.948758814796064e-05, 'epoch': 0.13}
{'loss': 1.1683, 'grad_norm': 1.8280549049377441, 'learning_rate': 1.9485617351971827e-05, 'epoch': 0.13}
{'loss': 1.1191, 'grad_norm': 1.7960431575775146, 'learning_rate': 1.9483642873339753e-05, 'epoch': 0.13}
Step 675: Updated gated ratio to 0.9740 (progress: 26.0%)
Step 675: Updated gated ratio to 0.9740 (progress: 26.0%)
Step 675: Updated gated ratio to 0.9740 (progress: 26.0%)
Step 675: Updated gated ratio to 0.9740 (progress: 26.0%)
Step 675: Updated gated ratio to 0.9740 (progress: 26.0%)
Step 675: Updated gated ratio to 0.9740 (progress: 26.0%)
Step 675: Updated gated ratio to 0.9740 (progress: 26.0%)
Step 675: Updated gated ratio to 0.9740 (progress: 26.0%)
Step 675: Updated gated ratio to 0.9740 (progress: 26.0%)
Step 675: Updated gated ratio to 0.9740 (progress: 26.0%)
Step 675: Updated gated ratio to 0.9740 (progress: 26.0%)
Step 675: Updated gated ratio to 0.9740 (progress: 26.0%)
Step 675: Updated gated ratio to 0.9740 (progress: 26.0%)
Step 675: Updated gated ratio to 0.9740 (progress: 26.0%)
Step 675: Updated gated ratio to 0.9740 (progress: 26.0%)
Step 675: Updated gated ratio to 0.9740 (progress: 26.0%)
{'loss': 1.1436, 'grad_norm': 1.749977469444275, 'learning_rate': 1.9481664712830987e-05, 'epoch': 0.13}
{'loss': 1.0344, 'grad_norm': 1.9093459844589233, 'learning_rate': 1.9479682871213515e-05, 'epoch': 0.13}
{'loss': 1.1527, 'grad_norm': 1.7520217895507812, 'learning_rate': 1.9477697349256756e-05, 'epoch': 0.13}
{'loss': 1.1132, 'grad_norm': 1.730644702911377, 'learning_rate': 1.947570814773156e-05, 'epoch': 0.13}
{'loss': 1.1351, 'grad_norm': 1.7722382545471191, 'learning_rate': 1.9473715267410206e-05, 'epoch': 0.13}
Step 680: Updated gated ratio to 0.9738 (progress: 26.2%)
Step 680: Updated gated ratio to 0.9738 (progress: 26.2%)
Step 680: Updated gated ratio to 0.9738 (progress: 26.2%)
Step 680: Updated gated ratio to 0.9738 (progress: 26.2%)
Step 680: Updated gated ratio to 0.9738 (progress: 26.2%)
Step 680: Updated gated ratio to 0.9738 (progress: 26.2%)
Step 680: Updated gated ratio to 0.9738 (progress: 26.2%)
Step 680: Updated gated ratio to 0.9738 (progress: 26.2%)
Step 680: Updated gated ratio to 0.9738 (progress: 26.2%)
Step 680: Updated gated ratio to 0.9738 (progress: 26.2%)
Step 680: Updated gated ratio to 0.9738 (progress: 26.2%)
Step 680: Updated gated ratio to 0.9738 (progress: 26.2%)
Step 680: Updated gated ratio to 0.9738 (progress: 26.2%)
Step 680: Updated gated ratio to 0.9738 (progress: 26.2%)
Step 680: Updated gated ratio to 0.9738 (progress: 26.2%)
Step 680: Updated gated ratio to 0.9738 (progress: 26.2%)
{'loss': 1.1539, 'grad_norm': 1.7275902032852173, 'learning_rate': 1.9471718709066392e-05, 'epoch': 0.13}
{'loss': 1.1451, 'grad_norm': 1.461826205253601, 'learning_rate': 1.9469718473475256e-05, 'epoch': 0.13}
{'loss': 1.0826, 'grad_norm': 1.7876356840133667, 'learning_rate': 1.9467714561413358e-05, 'epoch': 0.13}
{'loss': 1.1429, 'grad_norm': 1.7166532278060913, 'learning_rate': 1.9465706973658683e-05, 'epoch': 0.13}
{'loss': 1.0916, 'grad_norm': 1.6349784135818481, 'learning_rate': 1.9463695710990648e-05, 'epoch': 0.13}
Step 685: Updated gated ratio to 0.9736 (progress: 26.4%)
Step 685: Updated gated ratio to 0.9736 (progress: 26.4%)
Step 685: Updated gated ratio to 0.9736 (progress: 26.4%)
Step 685: Updated gated ratio to 0.9736 (progress: 26.4%)Step 685: Updated gated ratio to 0.9736 (progress: 26.4%)

Step 685: Updated gated ratio to 0.9736 (progress: 26.4%)
Step 685: Updated gated ratio to 0.9736 (progress: 26.4%)
Step 685: Updated gated ratio to 0.9736 (progress: 26.4%)
Step 685: Updated gated ratio to 0.9736 (progress: 26.4%)
Step 685: Updated gated ratio to 0.9736 (progress: 26.4%)
Step 685: Updated gated ratio to 0.9736 (progress: 26.4%)
Step 685: Updated gated ratio to 0.9736 (progress: 26.4%)
Step 685: Updated gated ratio to 0.9736 (progress: 26.4%)
Step 685: Updated gated ratio to 0.9736 (progress: 26.4%)
Step 685: Updated gated ratio to 0.9736 (progress: 26.4%)
Step 685: Updated gated ratio to 0.9736 (progress: 26.4%)
{'loss': 1.0869, 'grad_norm': 1.600128173828125, 'learning_rate': 1.946168077419009e-05, 'epoch': 0.13}
{'loss': 1.0314, 'grad_norm': 1.6286481618881226, 'learning_rate': 1.9459662164039283e-05, 'epoch': 0.13}
{'loss': 1.0561, 'grad_norm': 1.6863430738449097, 'learning_rate': 1.9457639881321917e-05, 'epoch': 0.13}
{'loss': 1.1976, 'grad_norm': 2.1006321907043457, 'learning_rate': 1.9455613926823115e-05, 'epoch': 0.13}
{'loss': 1.1366, 'grad_norm': 1.7243674993515015, 'learning_rate': 1.945358430132942e-05, 'epoch': 0.13}
Step 690: Updated gated ratio to 0.9734 (progress: 26.6%)
Step 690: Updated gated ratio to 0.9734 (progress: 26.6%)
Step 690: Updated gated ratio to 0.9734 (progress: 26.6%)
Step 690: Updated gated ratio to 0.9734 (progress: 26.6%)
Step 690: Updated gated ratio to 0.9734 (progress: 26.6%)Step 690: Updated gated ratio to 0.9734 (progress: 26.6%)

Step 690: Updated gated ratio to 0.9734 (progress: 26.6%)
Step 690: Updated gated ratio to 0.9734 (progress: 26.6%)
Step 690: Updated gated ratio to 0.9734 (progress: 26.6%)
Step 690: Updated gated ratio to 0.9734 (progress: 26.6%)
Step 690: Updated gated ratio to 0.9734 (progress: 26.6%)
Step 690: Updated gated ratio to 0.9734 (progress: 26.6%)
Step 690: Updated gated ratio to 0.9734 (progress: 26.6%)
Step 690: Updated gated ratio to 0.9734 (progress: 26.6%)
Step 690: Updated gated ratio to 0.9734 (progress: 26.6%)
Step 690: Updated gated ratio to 0.9734 (progress: 26.6%)
{'loss': 1.0664, 'grad_norm': 1.6441781520843506, 'learning_rate': 1.9451551005628803e-05, 'epoch': 0.13}
{'loss': 1.0758, 'grad_norm': 1.9109009504318237, 'learning_rate': 1.9449514040510654e-05, 'epoch': 0.13}
{'loss': 1.152, 'grad_norm': 1.441235899925232, 'learning_rate': 1.9447473406765803e-05, 'epoch': 0.13}
{'loss': 1.0946, 'grad_norm': 1.6143772602081299, 'learning_rate': 1.9445429105186487e-05, 'epoch': 0.13}
{'loss': 1.111, 'grad_norm': 1.8193902969360352, 'learning_rate': 1.9443381136566382e-05, 'epoch': 0.13}
Step 695: Updated gated ratio to 0.9732 (progress: 26.8%)
Step 695: Updated gated ratio to 0.9732 (progress: 26.8%)
Step 695: Updated gated ratio to 0.9732 (progress: 26.8%)
Step 695: Updated gated ratio to 0.9732 (progress: 26.8%)
Step 695: Updated gated ratio to 0.9732 (progress: 26.8%)
Step 695: Updated gated ratio to 0.9732 (progress: 26.8%)
Step 695: Updated gated ratio to 0.9732 (progress: 26.8%)
Step 695: Updated gated ratio to 0.9732 (progress: 26.8%)
Step 695: Updated gated ratio to 0.9732 (progress: 26.8%)
Step 695: Updated gated ratio to 0.9732 (progress: 26.8%)
Step 695: Updated gated ratio to 0.9732 (progress: 26.8%)
Step 695: Updated gated ratio to 0.9732 (progress: 26.8%)
Step 695: Updated gated ratio to 0.9732 (progress: 26.8%)
Step 695: Updated gated ratio to 0.9732 (progress: 26.8%)
Step 695: Updated gated ratio to 0.9732 (progress: 26.8%)
Step 695: Updated gated ratio to 0.9732 (progress: 26.8%)
{'loss': 1.0806, 'grad_norm': 1.6011948585510254, 'learning_rate': 1.9441329501700568e-05, 'epoch': 0.13}
{'loss': 1.1108, 'grad_norm': 1.847665786743164, 'learning_rate': 1.943927420138557e-05, 'epoch': 0.13}
{'loss': 1.1206, 'grad_norm': 1.7109858989715576, 'learning_rate': 1.9437215236419322e-05, 'epoch': 0.13}
{'loss': 1.1482, 'grad_norm': 1.659427523612976, 'learning_rate': 1.9435152607601187e-05, 'epoch': 0.13}
{'loss': 1.0544, 'grad_norm': 1.764284610748291, 'learning_rate': 1.943308631573195e-05, 'epoch': 0.13}
{'loss': 1.1254, 'grad_norm': 1.6946563720703125, 'learning_rate': 1.9431016361613816e-05, 'epoch': 0.13}
Step 701: Updated gated ratio to 0.9730 (progress: 27.0%)
Step 701: Updated gated ratio to 0.9730 (progress: 27.0%)
Step 701: Updated gated ratio to 0.9730 (progress: 27.0%)
Step 701: Updated gated ratio to 0.9730 (progress: 27.0%)
Step 701: Updated gated ratio to 0.9730 (progress: 27.0%)
Step 701: Updated gated ratio to 0.9730 (progress: 27.0%)
Step 701: Updated gated ratio to 0.9730 (progress: 27.0%)
Step 701: Updated gated ratio to 0.9730 (progress: 27.0%)
Step 701: Updated gated ratio to 0.9730 (progress: 27.0%)
Step 701: Updated gated ratio to 0.9730 (progress: 27.0%)
Step 701: Updated gated ratio to 0.9730 (progress: 27.0%)
Step 701: Updated gated ratio to 0.9730 (progress: 27.0%)
Step 701: Updated gated ratio to 0.9730 (progress: 27.0%)
Step 701: Updated gated ratio to 0.9730 (progress: 27.0%)
Step 701: Updated gated ratio to 0.9730 (progress: 27.0%)
Step 701: Updated gated ratio to 0.9730 (progress: 27.0%)
{'loss': 1.0812, 'grad_norm': 1.7170902490615845, 'learning_rate': 1.9428942746050406e-05, 'epoch': 0.14}
{'loss': 1.0521, 'grad_norm': 1.6980841159820557, 'learning_rate': 1.9426865469846773e-05, 'epoch': 0.14}
{'loss': 1.1477, 'grad_norm': 1.694219708442688, 'learning_rate': 1.9424784533809393e-05, 'epoch': 0.14}
{'loss': 1.1121, 'grad_norm': 1.769067645072937, 'learning_rate': 1.942269993874615e-05, 'epoch': 0.14}
{'loss': 1.1177, 'grad_norm': 1.7154667377471924, 'learning_rate': 1.9420611685466358e-05, 'epoch': 0.14}
Step 706: Updated gated ratio to 0.9728 (progress: 27.2%)
Step 706: Updated gated ratio to 0.9728 (progress: 27.2%)
Step 706: Updated gated ratio to 0.9728 (progress: 27.2%)
Step 706: Updated gated ratio to 0.9728 (progress: 27.2%)
Step 706: Updated gated ratio to 0.9728 (progress: 27.2%)
Step 706: Updated gated ratio to 0.9728 (progress: 27.2%)
Step 706: Updated gated ratio to 0.9728 (progress: 27.2%)
Step 706: Updated gated ratio to 0.9728 (progress: 27.2%)
Step 706: Updated gated ratio to 0.9728 (progress: 27.2%)
Step 706: Updated gated ratio to 0.9728 (progress: 27.2%)
Step 706: Updated gated ratio to 0.9728 (progress: 27.2%)
Step 706: Updated gated ratio to 0.9728 (progress: 27.2%)
Step 706: Updated gated ratio to 0.9728 (progress: 27.2%)
Step 706: Updated gated ratio to 0.9728 (progress: 27.2%)
Step 706: Updated gated ratio to 0.9728 (progress: 27.2%)
Step 706: Updated gated ratio to 0.9728 (progress: 27.2%)
{'loss': 1.0669, 'grad_norm': 1.3309166431427002, 'learning_rate': 1.9418519774780748e-05, 'epoch': 0.14}
{'loss': 1.177, 'grad_norm': 1.814917802810669, 'learning_rate': 1.9416424207501474e-05, 'epoch': 0.14}
{'loss': 1.0467, 'grad_norm': 1.7359073162078857, 'learning_rate': 1.9414324984442102e-05, 'epoch': 0.14}
{'loss': 1.0988, 'grad_norm': 1.835693359375, 'learning_rate': 1.9412222106417632e-05, 'epoch': 0.14}
{'loss': 1.0564, 'grad_norm': 1.6251288652420044, 'learning_rate': 1.9410115574244462e-05, 'epoch': 0.14}
Step 711: Updated gated ratio to 0.9726 (progress: 27.4%)
Step 711: Updated gated ratio to 0.9726 (progress: 27.4%)
Step 711: Updated gated ratio to 0.9726 (progress: 27.4%)
Step 711: Updated gated ratio to 0.9726 (progress: 27.4%)
Step 711: Updated gated ratio to 0.9726 (progress: 27.4%)
Step 711: Updated gated ratio to 0.9726 (progress: 27.4%)
Step 711: Updated gated ratio to 0.9726 (progress: 27.4%)
Step 711: Updated gated ratio to 0.9726 (progress: 27.4%)
Step 711: Updated gated ratio to 0.9726 (progress: 27.4%)
Step 711: Updated gated ratio to 0.9726 (progress: 27.4%)
Step 711: Updated gated ratio to 0.9726 (progress: 27.4%)
Step 711: Updated gated ratio to 0.9726 (progress: 27.4%)
Step 711: Updated gated ratio to 0.9726 (progress: 27.4%)
Step 711: Updated gated ratio to 0.9726 (progress: 27.4%)
Step 711: Updated gated ratio to 0.9726 (progress: 27.4%)
Step 711: Updated gated ratio to 0.9726 (progress: 27.4%)
{'loss': 1.0034, 'grad_norm': 1.7336913347244263, 'learning_rate': 1.9408005388740433e-05, 'epoch': 0.14}
Error with image file is truncated (46 bytes not processed)
{'loss': 1.005, 'grad_norm': 1.6538913249969482, 'learning_rate': 1.9405891550724778e-05, 'epoch': 0.14}
{'loss': 1.0806, 'grad_norm': 1.620322823524475, 'learning_rate': 1.940377406101817e-05, 'epoch': 0.14}
{'loss': 1.0279, 'grad_norm': 1.7154630422592163, 'learning_rate': 1.9401652920442694e-05, 'epoch': 0.14}
{'loss': 1.0413, 'grad_norm': 1.557413935661316, 'learning_rate': 1.9399528129821842e-05, 'epoch': 0.14}
Step 716: Updated gated ratio to 0.9724 (progress: 27.6%)
Step 716: Updated gated ratio to 0.9724 (progress: 27.6%)
Step 716: Updated gated ratio to 0.9724 (progress: 27.6%)
Step 716: Updated gated ratio to 0.9724 (progress: 27.6%)
Step 716: Updated gated ratio to 0.9724 (progress: 27.6%)
Step 716: Updated gated ratio to 0.9724 (progress: 27.6%)
Step 716: Updated gated ratio to 0.9724 (progress: 27.6%)
Step 716: Updated gated ratio to 0.9724 (progress: 27.6%)
Step 716: Updated gated ratio to 0.9724 (progress: 27.6%)
Step 716: Updated gated ratio to 0.9724 (progress: 27.6%)
Step 716: Updated gated ratio to 0.9724 (progress: 27.6%)
Step 716: Updated gated ratio to 0.9724 (progress: 27.6%)
Step 716: Updated gated ratio to 0.9724 (progress: 27.6%)
Step 716: Updated gated ratio to 0.9724 (progress: 27.6%)
Step 716: Updated gated ratio to 0.9724 (progress: 27.6%)
Step 716: Updated gated ratio to 0.9724 (progress: 27.6%)
{'loss': 1.164, 'grad_norm': 1.8187774419784546, 'learning_rate': 1.939739968998054e-05, 'epoch': 0.14}
{'loss': 1.088, 'grad_norm': 1.8423913717269897, 'learning_rate': 1.939526760174511e-05, 'epoch': 0.14}
{'loss': 1.0855, 'grad_norm': 1.7955859899520874, 'learning_rate': 1.939313186594331e-05, 'epoch': 0.14}
{'loss': 1.1139, 'grad_norm': 1.3591387271881104, 'learning_rate': 1.9390992483404308e-05, 'epoch': 0.14}
{'loss': 1.1591, 'grad_norm': 1.652376651763916, 'learning_rate': 1.938884945495868e-05, 'epoch': 0.14}
Step 721: Updated gated ratio to 0.9722 (progress: 27.8%)
Step 721: Updated gated ratio to 0.9722 (progress: 27.8%)
Step 721: Updated gated ratio to 0.9722 (progress: 27.8%)
Step 721: Updated gated ratio to 0.9722 (progress: 27.8%)
Step 721: Updated gated ratio to 0.9722 (progress: 27.8%)
Step 721: Updated gated ratio to 0.9722 (progress: 27.8%)
Step 721: Updated gated ratio to 0.9722 (progress: 27.8%)
Step 721: Updated gated ratio to 0.9722 (progress: 27.8%)
Step 721: Updated gated ratio to 0.9722 (progress: 27.8%)
Step 721: Updated gated ratio to 0.9722 (progress: 27.8%)
Step 721: Updated gated ratio to 0.9722 (progress: 27.8%)
Step 721: Updated gated ratio to 0.9722 (progress: 27.8%)
Step 721: Updated gated ratio to 0.9722 (progress: 27.8%)
Step 721: Updated gated ratio to 0.9722 (progress: 27.8%)
Step 721: Updated gated ratio to 0.9722 (progress: 27.8%)
Step 721: Updated gated ratio to 0.9722 (progress: 27.8%)
{'loss': 1.0511, 'grad_norm': 1.6847413778305054, 'learning_rate': 1.9386702781438425e-05, 'epoch': 0.14}
{'loss': 1.0874, 'grad_norm': 1.711714744567871, 'learning_rate': 1.938455246367696e-05, 'epoch': 0.14}
{'loss': 1.1089, 'grad_norm': 1.6855047941207886, 'learning_rate': 1.9382398502509107e-05, 'epoch': 0.14}
{'loss': 1.0672, 'grad_norm': 1.8212006092071533, 'learning_rate': 1.938024089877111e-05, 'epoch': 0.14}
{'loss': 1.1566, 'grad_norm': 1.781865119934082, 'learning_rate': 1.9378079653300624e-05, 'epoch': 0.14}
{'loss': 1.1258, 'grad_norm': 1.807300090789795, 'learning_rate': 1.9375914766936723e-05, 'epoch': 0.14}
Step 727: Updated gated ratio to 0.9720 (progress: 28.0%)
Step 727: Updated gated ratio to 0.9720 (progress: 28.0%)
Step 727: Updated gated ratio to 0.9720 (progress: 28.0%)
Step 727: Updated gated ratio to 0.9720 (progress: 28.0%)
Step 727: Updated gated ratio to 0.9720 (progress: 28.0%)
Step 727: Updated gated ratio to 0.9720 (progress: 28.0%)
Step 727: Updated gated ratio to 0.9720 (progress: 28.0%)
Step 727: Updated gated ratio to 0.9720 (progress: 28.0%)
Step 727: Updated gated ratio to 0.9720 (progress: 28.0%)
Step 727: Updated gated ratio to 0.9720 (progress: 28.0%)
Step 727: Updated gated ratio to 0.9720 (progress: 28.0%)
Step 727: Updated gated ratio to 0.9720 (progress: 28.0%)
Step 727: Updated gated ratio to 0.9720 (progress: 28.0%)
Step 727: Updated gated ratio to 0.9720 (progress: 28.0%)
Step 727: Updated gated ratio to 0.9720 (progress: 28.0%)
Step 727: Updated gated ratio to 0.9720 (progress: 28.0%)
{'loss': 1.098, 'grad_norm': 1.8732119798660278, 'learning_rate': 1.9373746240519884e-05, 'epoch': 0.14}
{'loss': 1.1457, 'grad_norm': 1.6954307556152344, 'learning_rate': 1.937157407489201e-05, 'epoch': 0.14}
{'loss': 1.168, 'grad_norm': 1.694170594215393, 'learning_rate': 1.9369398270896403e-05, 'epoch': 0.14}
{'loss': 1.1101, 'grad_norm': 1.7619974613189697, 'learning_rate': 1.936721882937779e-05, 'epoch': 0.14}
{'loss': 1.0471, 'grad_norm': 1.7101644277572632, 'learning_rate': 1.9365035751182307e-05, 'epoch': 0.14}
Step 732: Updated gated ratio to 0.9718 (progress: 28.2%)
Step 732: Updated gated ratio to 0.9718 (progress: 28.2%)
Step 732: Updated gated ratio to 0.9718 (progress: 28.2%)Step 732: Updated gated ratio to 0.9718 (progress: 28.2%)

Step 732: Updated gated ratio to 0.9718 (progress: 28.2%)Step 732: Updated gated ratio to 0.9718 (progress: 28.2%)

Step 732: Updated gated ratio to 0.9718 (progress: 28.2%)
Step 732: Updated gated ratio to 0.9718 (progress: 28.2%)Step 732: Updated gated ratio to 0.9718 (progress: 28.2%)
Step 732: Updated gated ratio to 0.9718 (progress: 28.2%)

Step 732: Updated gated ratio to 0.9718 (progress: 28.2%)
Step 732: Updated gated ratio to 0.9718 (progress: 28.2%)
Step 732: Updated gated ratio to 0.9718 (progress: 28.2%)
Step 732: Updated gated ratio to 0.9718 (progress: 28.2%)
Step 732: Updated gated ratio to 0.9718 (progress: 28.2%)
Step 732: Updated gated ratio to 0.9718 (progress: 28.2%)
{'loss': 1.1445, 'grad_norm': 1.3695446252822876, 'learning_rate': 1.93628490371575e-05, 'epoch': 0.14}
{'loss': 1.1045, 'grad_norm': 1.755738377571106, 'learning_rate': 1.9360658688152322e-05, 'epoch': 0.14}
{'loss': 1.1056, 'grad_norm': 1.6807833909988403, 'learning_rate': 1.9358464705017143e-05, 'epoch': 0.14}
{'loss': 1.0809, 'grad_norm': 1.333135962486267, 'learning_rate': 1.9356267088603745e-05, 'epoch': 0.14}
{'loss': 1.0835, 'grad_norm': 1.8483885526657104, 'learning_rate': 1.9354065839765316e-05, 'epoch': 0.14}
Step 737: Updated gated ratio to 0.9716 (progress: 28.4%)
Step 737: Updated gated ratio to 0.9716 (progress: 28.4%)
Step 737: Updated gated ratio to 0.9716 (progress: 28.4%)
Step 737: Updated gated ratio to 0.9716 (progress: 28.4%)
Step 737: Updated gated ratio to 0.9716 (progress: 28.4%)
Step 737: Updated gated ratio to 0.9716 (progress: 28.4%)
Step 737: Updated gated ratio to 0.9716 (progress: 28.4%)Step 737: Updated gated ratio to 0.9716 (progress: 28.4%)
Step 737: Updated gated ratio to 0.9716 (progress: 28.4%)

Step 737: Updated gated ratio to 0.9716 (progress: 28.4%)
Step 737: Updated gated ratio to 0.9716 (progress: 28.4%)
Step 737: Updated gated ratio to 0.9716 (progress: 28.4%)
Step 737: Updated gated ratio to 0.9716 (progress: 28.4%)
Step 737: Updated gated ratio to 0.9716 (progress: 28.4%)
Step 737: Updated gated ratio to 0.9716 (progress: 28.4%)
Step 737: Updated gated ratio to 0.9716 (progress: 28.4%)
{'loss': 1.0391, 'grad_norm': 1.589363932609558, 'learning_rate': 1.9351860959356462e-05, 'epoch': 0.14}
{'loss': 1.1312, 'grad_norm': 1.385894775390625, 'learning_rate': 1.9349652448233187e-05, 'epoch': 0.14}
{'loss': 1.0886, 'grad_norm': 1.6110854148864746, 'learning_rate': 1.934744030725291e-05, 'epoch': 0.14}
{'loss': 1.0959, 'grad_norm': 1.707585096359253, 'learning_rate': 1.934522453727447e-05, 'epoch': 0.14}
{'loss': 1.0413, 'grad_norm': 1.6033120155334473, 'learning_rate': 1.93430051391581e-05, 'epoch': 0.14}
Step 742: Updated gated ratio to 0.9714 (progress: 28.6%)
Step 742: Updated gated ratio to 0.9714 (progress: 28.6%)
Step 742: Updated gated ratio to 0.9714 (progress: 28.6%)
Step 742: Updated gated ratio to 0.9714 (progress: 28.6%)Step 742: Updated gated ratio to 0.9714 (progress: 28.6%)

Step 742: Updated gated ratio to 0.9714 (progress: 28.6%)
Step 742: Updated gated ratio to 0.9714 (progress: 28.6%)
Step 742: Updated gated ratio to 0.9714 (progress: 28.6%)
Step 742: Updated gated ratio to 0.9714 (progress: 28.6%)
Step 742: Updated gated ratio to 0.9714 (progress: 28.6%)
Step 742: Updated gated ratio to 0.9714 (progress: 28.6%)
Step 742: Updated gated ratio to 0.9714 (progress: 28.6%)
Step 742: Updated gated ratio to 0.9714 (progress: 28.6%)
Step 742: Updated gated ratio to 0.9714 (progress: 28.6%)
Step 742: Updated gated ratio to 0.9714 (progress: 28.6%)
Step 742: Updated gated ratio to 0.9714 (progress: 28.6%)
{'loss': 1.1102, 'grad_norm': 1.5851410627365112, 'learning_rate': 1.934078211376544e-05, 'epoch': 0.14}
{'loss': 1.0879, 'grad_norm': 1.5387778282165527, 'learning_rate': 1.9338555461959554e-05, 'epoch': 0.14}
{'loss': 1.0704, 'grad_norm': 1.6963889598846436, 'learning_rate': 1.93363251846049e-05, 'epoch': 0.14}
{'loss': 1.1972, 'grad_norm': 1.465646743774414, 'learning_rate': 1.9334091282567352e-05, 'epoch': 0.14}
{'loss': 1.04, 'grad_norm': 1.8309751749038696, 'learning_rate': 1.9331853756714185e-05, 'epoch': 0.14}
Step 747: Updated gated ratio to 0.9712 (progress: 28.8%)
Step 747: Updated gated ratio to 0.9712 (progress: 28.8%)
Step 747: Updated gated ratio to 0.9712 (progress: 28.8%)
Step 747: Updated gated ratio to 0.9712 (progress: 28.8%)
Step 747: Updated gated ratio to 0.9712 (progress: 28.8%)
Step 747: Updated gated ratio to 0.9712 (progress: 28.8%)
Step 747: Updated gated ratio to 0.9712 (progress: 28.8%)
Step 747: Updated gated ratio to 0.9712 (progress: 28.8%)
Step 747: Updated gated ratio to 0.9712 (progress: 28.8%)
Step 747: Updated gated ratio to 0.9712 (progress: 28.8%)
Step 747: Updated gated ratio to 0.9712 (progress: 28.8%)
Step 747: Updated gated ratio to 0.9712 (progress: 28.8%)
Step 747: Updated gated ratio to 0.9712 (progress: 28.8%)
Step 747: Updated gated ratio to 0.9712 (progress: 28.8%)
Step 747: Updated gated ratio to 0.9712 (progress: 28.8%)
Step 747: Updated gated ratio to 0.9712 (progress: 28.8%)
{'loss': 1.137, 'grad_norm': 1.7429323196411133, 'learning_rate': 1.9329612607914088e-05, 'epoch': 0.14}
{'loss': 1.1299, 'grad_norm': 1.7843672037124634, 'learning_rate': 1.9327367837037142e-05, 'epoch': 0.14}
{'loss': 1.0929, 'grad_norm': 1.7790157794952393, 'learning_rate': 1.9325119444954855e-05, 'epoch': 0.14}
{'loss': 1.1145, 'grad_norm': 1.5634374618530273, 'learning_rate': 1.9322867432540126e-05, 'epoch': 0.14}
{'loss': 1.0563, 'grad_norm': 1.7256321907043457, 'learning_rate': 1.9320611800667268e-05, 'epoch': 0.14}
{'loss': 1.123, 'grad_norm': 1.8393512964248657, 'learning_rate': 1.9318352550211986e-05, 'epoch': 0.14}
Step 753: Updated gated ratio to 0.9710 (progress: 29.0%)
Step 753: Updated gated ratio to 0.9710 (progress: 29.0%)
Step 753: Updated gated ratio to 0.9710 (progress: 29.0%)
Step 753: Updated gated ratio to 0.9710 (progress: 29.0%)Step 753: Updated gated ratio to 0.9710 (progress: 29.0%)

Step 753: Updated gated ratio to 0.9710 (progress: 29.0%)
Step 753: Updated gated ratio to 0.9710 (progress: 29.0%)
Step 753: Updated gated ratio to 0.9710 (progress: 29.0%)
Step 753: Updated gated ratio to 0.9710 (progress: 29.0%)
Step 753: Updated gated ratio to 0.9710 (progress: 29.0%)
Step 753: Updated gated ratio to 0.9710 (progress: 29.0%)
Step 753: Updated gated ratio to 0.9710 (progress: 29.0%)
Step 753: Updated gated ratio to 0.9710 (progress: 29.0%)
Step 753: Updated gated ratio to 0.9710 (progress: 29.0%)
Step 753: Updated gated ratio to 0.9710 (progress: 29.0%)
Step 753: Updated gated ratio to 0.9710 (progress: 29.0%)
{'loss': 1.1088, 'grad_norm': 1.4149394035339355, 'learning_rate': 1.9316089682051403e-05, 'epoch': 0.15}
{'loss': 1.1301, 'grad_norm': 1.583648443222046, 'learning_rate': 1.9313823197064042e-05, 'epoch': 0.15}
{'loss': 1.0869, 'grad_norm': 1.6261910200119019, 'learning_rate': 1.9311553096129835e-05, 'epoch': 0.15}
{'loss': 1.0556, 'grad_norm': 1.5389721393585205, 'learning_rate': 1.9309279380130112e-05, 'epoch': 0.15}
{'loss': 1.0866, 'grad_norm': 1.6599408388137817, 'learning_rate': 1.93070020499476e-05, 'epoch': 0.15}
Step 758: Updated gated ratio to 0.9708 (progress: 29.2%)Step 758: Updated gated ratio to 0.9708 (progress: 29.2%)
Step 758: Updated gated ratio to 0.9708 (progress: 29.2%)
Step 758: Updated gated ratio to 0.9708 (progress: 29.2%)Step 758: Updated gated ratio to 0.9708 (progress: 29.2%)

Step 758: Updated gated ratio to 0.9708 (progress: 29.2%)
Step 758: Updated gated ratio to 0.9708 (progress: 29.2%)
Step 758: Updated gated ratio to 0.9708 (progress: 29.2%)
Step 758: Updated gated ratio to 0.9708 (progress: 29.2%)

Step 758: Updated gated ratio to 0.9708 (progress: 29.2%)
Step 758: Updated gated ratio to 0.9708 (progress: 29.2%)
Step 758: Updated gated ratio to 0.9708 (progress: 29.2%)
Step 758: Updated gated ratio to 0.9708 (progress: 29.2%)
Step 758: Updated gated ratio to 0.9708 (progress: 29.2%)
Step 758: Updated gated ratio to 0.9708 (progress: 29.2%)
Step 758: Updated gated ratio to 0.9708 (progress: 29.2%)
{'loss': 1.1473, 'grad_norm': 1.8932220935821533, 'learning_rate': 1.930472110646645e-05, 'epoch': 0.15}
{'loss': 1.1686, 'grad_norm': 1.7991973161697388, 'learning_rate': 1.9302436550572187e-05, 'epoch': 0.15}
{'loss': 1.0182, 'grad_norm': 1.668371319770813, 'learning_rate': 1.930014838315177e-05, 'epoch': 0.15}
{'loss': 1.102, 'grad_norm': 1.7515255212783813, 'learning_rate': 1.9297856605093534e-05, 'epoch': 0.15}
{'loss': 1.0973, 'grad_norm': 1.6997551918029785, 'learning_rate': 1.9295561217287226e-05, 'epoch': 0.15}
Step 763: Updated gated ratio to 0.9706 (progress: 29.4%)
Step 763: Updated gated ratio to 0.9706 (progress: 29.4%)
Step 763: Updated gated ratio to 0.9706 (progress: 29.4%)
Step 763: Updated gated ratio to 0.9706 (progress: 29.4%)
Step 763: Updated gated ratio to 0.9706 (progress: 29.4%)
Step 763: Updated gated ratio to 0.9706 (progress: 29.4%)
Step 763: Updated gated ratio to 0.9706 (progress: 29.4%)
Step 763: Updated gated ratio to 0.9706 (progress: 29.4%)
Step 763: Updated gated ratio to 0.9706 (progress: 29.4%)Step 763: Updated gated ratio to 0.9706 (progress: 29.4%)
Step 763: Updated gated ratio to 0.9706 (progress: 29.4%)
Step 763: Updated gated ratio to 0.9706 (progress: 29.4%)

Step 763: Updated gated ratio to 0.9706 (progress: 29.4%)
Step 763: Updated gated ratio to 0.9706 (progress: 29.4%)
Step 763: Updated gated ratio to 0.9706 (progress: 29.4%)
Step 763: Updated gated ratio to 0.9706 (progress: 29.4%)
{'loss': 1.1569, 'grad_norm': 1.646425485610962, 'learning_rate': 1.9293262220624002e-05, 'epoch': 0.15}
{'loss': 1.1203, 'grad_norm': 1.7094258069992065, 'learning_rate': 1.9290959615996407e-05, 'epoch': 0.15}
{'loss': 1.0225, 'grad_norm': 1.7030450105667114, 'learning_rate': 1.9288653404298392e-05, 'epoch': 0.15}
{'loss': 1.043, 'grad_norm': 1.6286647319793701, 'learning_rate': 1.9286343586425307e-05, 'epoch': 0.15}
{'loss': 1.1303, 'grad_norm': 1.6659812927246094, 'learning_rate': 1.9284030163273907e-05, 'epoch': 0.15}
Step 768: Updated gated ratio to 0.9704 (progress: 29.6%)
Step 768: Updated gated ratio to 0.9704 (progress: 29.6%)
Step 768: Updated gated ratio to 0.9704 (progress: 29.6%)
Step 768: Updated gated ratio to 0.9704 (progress: 29.6%)
Step 768: Updated gated ratio to 0.9704 (progress: 29.6%)
Step 768: Updated gated ratio to 0.9704 (progress: 29.6%)
Step 768: Updated gated ratio to 0.9704 (progress: 29.6%)
Step 768: Updated gated ratio to 0.9704 (progress: 29.6%)
Step 768: Updated gated ratio to 0.9704 (progress: 29.6%)
Step 768: Updated gated ratio to 0.9704 (progress: 29.6%)
Step 768: Updated gated ratio to 0.9704 (progress: 29.6%)
Step 768: Updated gated ratio to 0.9704 (progress: 29.6%)
Step 768: Updated gated ratio to 0.9704 (progress: 29.6%)
Step 768: Updated gated ratio to 0.9704 (progress: 29.6%)
Step 768: Updated gated ratio to 0.9704 (progress: 29.6%)
Step 768: Updated gated ratio to 0.9704 (progress: 29.6%)
{'loss': 1.1334, 'grad_norm': 1.3995556831359863, 'learning_rate': 1.9281713135742333e-05, 'epoch': 0.15}
{'loss': 1.0654, 'grad_norm': 1.7599190473556519, 'learning_rate': 1.9279392504730147e-05, 'epoch': 0.15}
{'loss': 1.0466, 'grad_norm': 1.594394326210022, 'learning_rate': 1.9277068271138287e-05, 'epoch': 0.15}
{'loss': 1.1406, 'grad_norm': 1.5881422758102417, 'learning_rate': 1.9274740435869107e-05, 'epoch': 0.15}
{'loss': 1.1009, 'grad_norm': 1.774842381477356, 'learning_rate': 1.927240899982635e-05, 'epoch': 0.15}
Step 773: Updated gated ratio to 0.9702 (progress: 29.8%)
Step 773: Updated gated ratio to 0.9702 (progress: 29.8%)
Step 773: Updated gated ratio to 0.9702 (progress: 29.8%)
Step 773: Updated gated ratio to 0.9702 (progress: 29.8%)
Step 773: Updated gated ratio to 0.9702 (progress: 29.8%)
Step 773: Updated gated ratio to 0.9702 (progress: 29.8%)
Step 773: Updated gated ratio to 0.9702 (progress: 29.8%)
Step 773: Updated gated ratio to 0.9702 (progress: 29.8%)
Step 773: Updated gated ratio to 0.9702 (progress: 29.8%)
Step 773: Updated gated ratio to 0.9702 (progress: 29.8%)
Step 773: Updated gated ratio to 0.9702 (progress: 29.8%)
Step 773: Updated gated ratio to 0.9702 (progress: 29.8%)
Step 773: Updated gated ratio to 0.9702 (progress: 29.8%)
Step 773: Updated gated ratio to 0.9702 (progress: 29.8%)
Step 773: Updated gated ratio to 0.9702 (progress: 29.8%)
Step 773: Updated gated ratio to 0.9702 (progress: 29.8%)
Error with image file is truncated (16 bytes not processed)
{'loss': 1.1271, 'grad_norm': 1.859741449356079, 'learning_rate': 1.9270073963915162e-05, 'epoch': 0.15}
{'loss': 1.1496, 'grad_norm': 1.4001471996307373, 'learning_rate': 1.9267735329042086e-05, 'epoch': 0.15}
{'loss': 1.0906, 'grad_norm': 1.7131409645080566, 'learning_rate': 1.9265393096115056e-05, 'epoch': 0.15}
{'loss': 1.1663, 'grad_norm': 1.7217743396759033, 'learning_rate': 1.926304726604341e-05, 'epoch': 0.15}
{'loss': 1.1674, 'grad_norm': 1.7727645635604858, 'learning_rate': 1.9260697839737875e-05, 'epoch': 0.15}
{'loss': 1.1404, 'grad_norm': 1.6569546461105347, 'learning_rate': 1.925834481811059e-05, 'epoch': 0.15}
Step 779: Updated gated ratio to 0.9700 (progress: 30.0%)
Step 779: Updated gated ratio to 0.9700 (progress: 30.0%)
Step 779: Updated gated ratio to 0.9700 (progress: 30.0%)
Step 779: Updated gated ratio to 0.9700 (progress: 30.0%)
Step 779: Updated gated ratio to 0.9700 (progress: 30.0%)
Step 779: Updated gated ratio to 0.9700 (progress: 30.0%)
Step 779: Updated gated ratio to 0.9700 (progress: 30.0%)
Step 779: Updated gated ratio to 0.9700 (progress: 30.0%)
Step 779: Updated gated ratio to 0.9700 (progress: 30.0%)
Step 779: Updated gated ratio to 0.9700 (progress: 30.0%)
Step 779: Updated gated ratio to 0.9700 (progress: 30.0%)
Step 779: Updated gated ratio to 0.9700 (progress: 30.0%)
Step 779: Updated gated ratio to 0.9700 (progress: 30.0%)
Step 779: Updated gated ratio to 0.9700 (progress: 30.0%)
Step 779: Updated gated ratio to 0.9700 (progress: 30.0%)
Step 779: Updated gated ratio to 0.9700 (progress: 30.0%)
{'loss': 1.0994, 'grad_norm': 1.2774955034255981, 'learning_rate': 1.9255988202075065e-05, 'epoch': 0.15}
{'loss': 1.1366, 'grad_norm': 1.74733304977417, 'learning_rate': 1.925362799254623e-05, 'epoch': 0.15}
{'loss': 1.1423, 'grad_norm': 1.5791316032409668, 'learning_rate': 1.9251264190440398e-05, 'epoch': 0.15}
{'loss': 1.0911, 'grad_norm': 1.6082128286361694, 'learning_rate': 1.9248896796675277e-05, 'epoch': 0.15}
{'loss': 1.1171, 'grad_norm': 1.630563497543335, 'learning_rate': 1.924652581216997e-05, 'epoch': 0.15}
Step 784: Updated gated ratio to 0.9698 (progress: 30.2%)
Step 784: Updated gated ratio to 0.9698 (progress: 30.2%)
Step 784: Updated gated ratio to 0.9698 (progress: 30.2%)
Step 784: Updated gated ratio to 0.9698 (progress: 30.2%)
Step 784: Updated gated ratio to 0.9698 (progress: 30.2%)
Step 784: Updated gated ratio to 0.9698 (progress: 30.2%)
Step 784: Updated gated ratio to 0.9698 (progress: 30.2%)
Step 784: Updated gated ratio to 0.9698 (progress: 30.2%)
Step 784: Updated gated ratio to 0.9698 (progress: 30.2%)
Step 784: Updated gated ratio to 0.9698 (progress: 30.2%)
Step 784: Updated gated ratio to 0.9698 (progress: 30.2%)
Step 784: Updated gated ratio to 0.9698 (progress: 30.2%)
Step 784: Updated gated ratio to 0.9698 (progress: 30.2%)
Step 784: Updated gated ratio to 0.9698 (progress: 30.2%)
Step 784: Updated gated ratio to 0.9698 (progress: 30.2%)
Step 784: Updated gated ratio to 0.9698 (progress: 30.2%)
{'loss': 1.0778, 'grad_norm': 1.756668210029602, 'learning_rate': 1.9244151237844975e-05, 'epoch': 0.15}
{'loss': 1.0863, 'grad_norm': 1.9051539897918701, 'learning_rate': 1.9241773074622182e-05, 'epoch': 0.15}
{'loss': 1.0618, 'grad_norm': 1.6276943683624268, 'learning_rate': 1.923939132342488e-05, 'epoch': 0.15}
{'loss': 1.09, 'grad_norm': 1.7654523849487305, 'learning_rate': 1.923700598517775e-05, 'epoch': 0.15}
{'loss': 1.1226, 'grad_norm': 1.6515132188796997, 'learning_rate': 1.923461706080685e-05, 'epoch': 0.15}
Step 789: Updated gated ratio to 0.9696 (progress: 30.4%)
Step 789: Updated gated ratio to 0.9696 (progress: 30.4%)
Step 789: Updated gated ratio to 0.9696 (progress: 30.4%)
Step 789: Updated gated ratio to 0.9696 (progress: 30.4%)
Step 789: Updated gated ratio to 0.9696 (progress: 30.4%)
Step 789: Updated gated ratio to 0.9696 (progress: 30.4%)Step 789: Updated gated ratio to 0.9696 (progress: 30.4%)

Step 789: Updated gated ratio to 0.9696 (progress: 30.4%)
Step 789: Updated gated ratio to 0.9696 (progress: 30.4%)
Step 789: Updated gated ratio to 0.9696 (progress: 30.4%)
Step 789: Updated gated ratio to 0.9696 (progress: 30.4%)
Step 789: Updated gated ratio to 0.9696 (progress: 30.4%)
Step 789: Updated gated ratio to 0.9696 (progress: 30.4%)
Step 789: Updated gated ratio to 0.9696 (progress: 30.4%)
Step 789: Updated gated ratio to 0.9696 (progress: 30.4%)
Step 789: Updated gated ratio to 0.9696 (progress: 30.4%)
{'loss': 1.1187, 'grad_norm': 1.6598535776138306, 'learning_rate': 1.923222455123965e-05, 'epoch': 0.15}
{'loss': 1.0833, 'grad_norm': 2.017656087875366, 'learning_rate': 1.9229828457405005e-05, 'epoch': 0.15}
{'loss': 1.0423, 'grad_norm': 1.5943354368209839, 'learning_rate': 1.9227428780233162e-05, 'epoch': 0.15}
{'loss': 0.9851, 'grad_norm': 1.578521966934204, 'learning_rate': 1.922502552065576e-05, 'epoch': 0.15}
{'loss': 1.1368, 'grad_norm': 1.8451836109161377, 'learning_rate': 1.922261867960582e-05, 'epoch': 0.15}
Step 794: Updated gated ratio to 0.9694 (progress: 30.6%)
Step 794: Updated gated ratio to 0.9694 (progress: 30.6%)
Step 794: Updated gated ratio to 0.9694 (progress: 30.6%)
Step 794: Updated gated ratio to 0.9694 (progress: 30.6%)
Step 794: Updated gated ratio to 0.9694 (progress: 30.6%)
Step 794: Updated gated ratio to 0.9694 (progress: 30.6%)
Step 794: Updated gated ratio to 0.9694 (progress: 30.6%)
Step 794: Updated gated ratio to 0.9694 (progress: 30.6%)
Step 794: Updated gated ratio to 0.9694 (progress: 30.6%)
Step 794: Updated gated ratio to 0.9694 (progress: 30.6%)
Step 794: Updated gated ratio to 0.9694 (progress: 30.6%)
Step 794: Updated gated ratio to 0.9694 (progress: 30.6%)
Step 794: Updated gated ratio to 0.9694 (progress: 30.6%)
Step 794: Updated gated ratio to 0.9694 (progress: 30.6%)
Step 794: Updated gated ratio to 0.9694 (progress: 30.6%)
Step 794: Updated gated ratio to 0.9694 (progress: 30.6%)
{'loss': 1.0673, 'grad_norm': 1.7506383657455444, 'learning_rate': 1.9220208258017763e-05, 'epoch': 0.15}
{'loss': 1.0295, 'grad_norm': 1.9783817529678345, 'learning_rate': 1.92177942568274e-05, 'epoch': 0.15}
{'loss': 1.0782, 'grad_norm': 1.7265347242355347, 'learning_rate': 1.921537667697193e-05, 'epoch': 0.15}
{'loss': 1.154, 'grad_norm': 1.574768304824829, 'learning_rate': 1.9212955519389938e-05, 'epoch': 0.15}
{'loss': 1.145, 'grad_norm': 1.7634241580963135, 'learning_rate': 1.9210530785021405e-05, 'epoch': 0.15}
Step 799: Updated gated ratio to 0.9692 (progress: 30.8%)
Step 799: Updated gated ratio to 0.9692 (progress: 30.8%)
Step 799: Updated gated ratio to 0.9692 (progress: 30.8%)
Step 799: Updated gated ratio to 0.9692 (progress: 30.8%)
Step 799: Updated gated ratio to 0.9692 (progress: 30.8%)
Step 799: Updated gated ratio to 0.9692 (progress: 30.8%)
Step 799: Updated gated ratio to 0.9692 (progress: 30.8%)
Step 799: Updated gated ratio to 0.9692 (progress: 30.8%)
Step 799: Updated gated ratio to 0.9692 (progress: 30.8%)
Step 799: Updated gated ratio to 0.9692 (progress: 30.8%)
Step 799: Updated gated ratio to 0.9692 (progress: 30.8%)
Step 799: Updated gated ratio to 0.9692 (progress: 30.8%)
Step 799: Updated gated ratio to 0.9692 (progress: 30.8%)
Step 799: Updated gated ratio to 0.9692 (progress: 30.8%)
Step 799: Updated gated ratio to 0.9692 (progress: 30.8%)
Step 799: Updated gated ratio to 0.9692 (progress: 30.8%)
{'loss': 1.1913, 'grad_norm': 1.7350428104400635, 'learning_rate': 1.9208102474807692e-05, 'epoch': 0.15}
{'loss': 1.0351, 'grad_norm': 1.6397985219955444, 'learning_rate': 1.920567058969155e-05, 'epoch': 0.15}
{'loss': 1.0893, 'grad_norm': 1.8118239641189575, 'learning_rate': 1.920323513061713e-05, 'epoch': 0.15}
{'loss': 1.0793, 'grad_norm': 1.586098313331604, 'learning_rate': 1.9200796098529956e-05, 'epoch': 0.15}
{'loss': 1.0781, 'grad_norm': 1.5404460430145264, 'learning_rate': 1.919835349437694e-05, 'epoch': 0.15}
{'loss': 1.0756, 'grad_norm': 1.2375175952911377, 'learning_rate': 1.9195907319106394e-05, 'epoch': 0.15}
Step 805: Updated gated ratio to 0.9690 (progress: 31.0%)
Step 805: Updated gated ratio to 0.9690 (progress: 31.0%)
Step 805: Updated gated ratio to 0.9690 (progress: 31.0%)
Step 805: Updated gated ratio to 0.9690 (progress: 31.0%)
Step 805: Updated gated ratio to 0.9690 (progress: 31.0%)
Step 805: Updated gated ratio to 0.9690 (progress: 31.0%)
Step 805: Updated gated ratio to 0.9690 (progress: 31.0%)
Step 805: Updated gated ratio to 0.9690 (progress: 31.0%)
Step 805: Updated gated ratio to 0.9690 (progress: 31.0%)
Step 805: Updated gated ratio to 0.9690 (progress: 31.0%)
Step 805: Updated gated ratio to 0.9690 (progress: 31.0%)
Step 805: Updated gated ratio to 0.9690 (progress: 31.0%)
Step 805: Updated gated ratio to 0.9690 (progress: 31.0%)
Step 805: Updated gated ratio to 0.9690 (progress: 31.0%)
Step 805: Updated gated ratio to 0.9690 (progress: 31.0%)
Step 805: Updated gated ratio to 0.9690 (progress: 31.0%)
{'loss': 1.1603, 'grad_norm': 1.8482389450073242, 'learning_rate': 1.9193457573667996e-05, 'epoch': 0.16}
{'loss': 1.0631, 'grad_norm': 1.492139458656311, 'learning_rate': 1.919100425901283e-05, 'epoch': 0.16}
{'loss': 1.1448, 'grad_norm': 1.5250064134597778, 'learning_rate': 1.9188547376093355e-05, 'epoch': 0.16}
{'loss': 1.1531, 'grad_norm': 1.69367516040802, 'learning_rate': 1.918608692586342e-05, 'epoch': 0.16}
{'loss': 1.0871, 'grad_norm': 1.5989009141921997, 'learning_rate': 1.918362290927825e-05, 'epoch': 0.16}
Step 810: Updated gated ratio to 0.9688 (progress: 31.2%)
Step 810: Updated gated ratio to 0.9688 (progress: 31.2%)
Step 810: Updated gated ratio to 0.9688 (progress: 31.2%)
Step 810: Updated gated ratio to 0.9688 (progress: 31.2%)
Step 810: Updated gated ratio to 0.9688 (progress: 31.2%)
Step 810: Updated gated ratio to 0.9688 (progress: 31.2%)
Step 810: Updated gated ratio to 0.9688 (progress: 31.2%)
Step 810: Updated gated ratio to 0.9688 (progress: 31.2%)
Step 810: Updated gated ratio to 0.9688 (progress: 31.2%)
Step 810: Updated gated ratio to 0.9688 (progress: 31.2%)
Step 810: Updated gated ratio to 0.9688 (progress: 31.2%)
Step 810: Updated gated ratio to 0.9688 (progress: 31.2%)
Step 810: Updated gated ratio to 0.9688 (progress: 31.2%)
Step 810: Updated gated ratio to 0.9688 (progress: 31.2%)
Step 810: Updated gated ratio to 0.9688 (progress: 31.2%)
Step 810: Updated gated ratio to 0.9688 (progress: 31.2%)
{'loss': 1.1839, 'grad_norm': 1.9636638164520264, 'learning_rate': 1.9181155327294468e-05, 'epoch': 0.16}
{'loss': 1.039, 'grad_norm': 1.6652464866638184, 'learning_rate': 1.9178684180870072e-05, 'epoch': 0.16}
{'loss': 1.0288, 'grad_norm': 1.6890087127685547, 'learning_rate': 1.9176209470964446e-05, 'epoch': 0.16}
{'loss': 1.1144, 'grad_norm': 1.6980172395706177, 'learning_rate': 1.9173731198538354e-05, 'epoch': 0.16}
{'loss': 1.1716, 'grad_norm': 1.870735764503479, 'learning_rate': 1.9171249364553956e-05, 'epoch': 0.16}
Step 815: Updated gated ratio to 0.9686 (progress: 31.4%)
Step 815: Updated gated ratio to 0.9686 (progress: 31.4%)
Step 815: Updated gated ratio to 0.9686 (progress: 31.4%)Step 815: Updated gated ratio to 0.9686 (progress: 31.4%)
Step 815: Updated gated ratio to 0.9686 (progress: 31.4%)
Step 815: Updated gated ratio to 0.9686 (progress: 31.4%)

Step 815: Updated gated ratio to 0.9686 (progress: 31.4%)
Step 815: Updated gated ratio to 0.9686 (progress: 31.4%)
Step 815: Updated gated ratio to 0.9686 (progress: 31.4%)
Step 815: Updated gated ratio to 0.9686 (progress: 31.4%)
Step 815: Updated gated ratio to 0.9686 (progress: 31.4%)
Step 815: Updated gated ratio to 0.9686 (progress: 31.4%)
Step 815: Updated gated ratio to 0.9686 (progress: 31.4%)
Step 815: Updated gated ratio to 0.9686 (progress: 31.4%)
Step 815: Updated gated ratio to 0.9686 (progress: 31.4%)
Step 815: Updated gated ratio to 0.9686 (progress: 31.4%)
{'loss': 1.1157, 'grad_norm': 1.7466974258422852, 'learning_rate': 1.9168763969974773e-05, 'epoch': 0.16}
{'loss': 1.0781, 'grad_norm': 1.6070855855941772, 'learning_rate': 1.916627501576573e-05, 'epoch': 0.16}
{'loss': 1.1572, 'grad_norm': 1.689823865890503, 'learning_rate': 1.916378250289312e-05, 'epoch': 0.16}
{'loss': 1.0381, 'grad_norm': 1.6527236700057983, 'learning_rate': 1.9161286432324628e-05, 'epoch': 0.16}
{'loss': 1.1241, 'grad_norm': 1.6485730409622192, 'learning_rate': 1.9158786805029307e-05, 'epoch': 0.16}
Step 820: Updated gated ratio to 0.9684 (progress: 31.6%)
Step 820: Updated gated ratio to 0.9684 (progress: 31.6%)Step 820: Updated gated ratio to 0.9684 (progress: 31.6%)

Step 820: Updated gated ratio to 0.9684 (progress: 31.6%)
Step 820: Updated gated ratio to 0.9684 (progress: 31.6%)
Step 820: Updated gated ratio to 0.9684 (progress: 31.6%)
Step 820: Updated gated ratio to 0.9684 (progress: 31.6%)
Step 820: Updated gated ratio to 0.9684 (progress: 31.6%)
Step 820: Updated gated ratio to 0.9684 (progress: 31.6%)
Step 820: Updated gated ratio to 0.9684 (progress: 31.6%)
Step 820: Updated gated ratio to 0.9684 (progress: 31.6%)
Step 820: Updated gated ratio to 0.9684 (progress: 31.6%)
Step 820: Updated gated ratio to 0.9684 (progress: 31.6%)
Step 820: Updated gated ratio to 0.9684 (progress: 31.6%)
Step 820: Updated gated ratio to 0.9684 (progress: 31.6%)
Step 820: Updated gated ratio to 0.9684 (progress: 31.6%)
{'loss': 1.174, 'grad_norm': 1.6895384788513184, 'learning_rate': 1.9156283621977603e-05, 'epoch': 0.16}
{'loss': 1.0944, 'grad_norm': 1.7284135818481445, 'learning_rate': 1.9153776884141336e-05, 'epoch': 0.16}
{'loss': 0.9891, 'grad_norm': 1.6065224409103394, 'learning_rate': 1.915126659249371e-05, 'epoch': 0.16}
{'loss': 1.0501, 'grad_norm': 1.7209316492080688, 'learning_rate': 1.9148752748009304e-05, 'epoch': 0.16}
{'loss': 1.0629, 'grad_norm': 1.8621877431869507, 'learning_rate': 1.914623535166408e-05, 'epoch': 0.16}
Step 825: Updated gated ratio to 0.9682 (progress: 31.8%)
Step 825: Updated gated ratio to 0.9682 (progress: 31.8%)
Step 825: Updated gated ratio to 0.9682 (progress: 31.8%)
Step 825: Updated gated ratio to 0.9682 (progress: 31.8%)Step 825: Updated gated ratio to 0.9682 (progress: 31.8%)

Step 825: Updated gated ratio to 0.9682 (progress: 31.8%)
Step 825: Updated gated ratio to 0.9682 (progress: 31.8%)
Step 825: Updated gated ratio to 0.9682 (progress: 31.8%)
Step 825: Updated gated ratio to 0.9682 (progress: 31.8%)
Step 825: Updated gated ratio to 0.9682 (progress: 31.8%)
Step 825: Updated gated ratio to 0.9682 (progress: 31.8%)
Step 825: Updated gated ratio to 0.9682 (progress: 31.8%)
Step 825: Updated gated ratio to 0.9682 (progress: 31.8%)Step 825: Updated gated ratio to 0.9682 (progress: 31.8%)

Step 825: Updated gated ratio to 0.9682 (progress: 31.8%)
Step 825: Updated gated ratio to 0.9682 (progress: 31.8%)
{'loss': 1.0614, 'grad_norm': 1.6228750944137573, 'learning_rate': 1.9143714404435382e-05, 'epoch': 0.16}
{'loss': 1.0651, 'grad_norm': 1.5342882871627808, 'learning_rate': 1.9141189907301922e-05, 'epoch': 0.16}
{'loss': 1.0384, 'grad_norm': 1.5916887521743774, 'learning_rate': 1.9138661861243802e-05, 'epoch': 0.16}
{'loss': 1.2107, 'grad_norm': 1.7388052940368652, 'learning_rate': 1.913613026724249e-05, 'epoch': 0.16}
{'loss': 1.0369, 'grad_norm': 1.6373498439788818, 'learning_rate': 1.9133595126280848e-05, 'epoch': 0.16}
{'loss': 1.0919, 'grad_norm': 1.7301076650619507, 'learning_rate': 1.9131056439343095e-05, 'epoch': 0.16}
Step 831: Updated gated ratio to 0.9680 (progress: 32.0%)
Step 831: Updated gated ratio to 0.9680 (progress: 32.0%)
Step 831: Updated gated ratio to 0.9680 (progress: 32.0%)
Step 831: Updated gated ratio to 0.9680 (progress: 32.0%)
Step 831: Updated gated ratio to 0.9680 (progress: 32.0%)
Step 831: Updated gated ratio to 0.9680 (progress: 32.0%)
Step 831: Updated gated ratio to 0.9680 (progress: 32.0%)
Step 831: Updated gated ratio to 0.9680 (progress: 32.0%)
Step 831: Updated gated ratio to 0.9680 (progress: 32.0%)
Step 831: Updated gated ratio to 0.9680 (progress: 32.0%)
Step 831: Updated gated ratio to 0.9680 (progress: 32.0%)
Step 831: Updated gated ratio to 0.9680 (progress: 32.0%)
Step 831: Updated gated ratio to 0.9680 (progress: 32.0%)
Step 831: Updated gated ratio to 0.9680 (progress: 32.0%)
Step 831: Updated gated ratio to 0.9680 (progress: 32.0%)
Step 831: Updated gated ratio to 0.9680 (progress: 32.0%)
{'loss': 1.1042, 'grad_norm': 1.803579330444336, 'learning_rate': 1.9128514207414838e-05, 'epoch': 0.16}
{'loss': 1.1985, 'grad_norm': 2.0362906455993652, 'learning_rate': 1.9125968431483068e-05, 'epoch': 0.16}
{'loss': 1.1834, 'grad_norm': 1.566508412361145, 'learning_rate': 1.9123419112536132e-05, 'epoch': 0.16}
{'loss': 1.1238, 'grad_norm': 1.716843843460083, 'learning_rate': 1.912086625156377e-05, 'epoch': 0.16}
{'loss': 1.1871, 'grad_norm': 1.6768543720245361, 'learning_rate': 1.911830984955709e-05, 'epoch': 0.16}
Step 836: Updated gated ratio to 0.9678 (progress: 32.2%)
Step 836: Updated gated ratio to 0.9678 (progress: 32.2%)
Step 836: Updated gated ratio to 0.9678 (progress: 32.2%)Step 836: Updated gated ratio to 0.9678 (progress: 32.2%)

Step 836: Updated gated ratio to 0.9678 (progress: 32.2%)
Step 836: Updated gated ratio to 0.9678 (progress: 32.2%)
Step 836: Updated gated ratio to 0.9678 (progress: 32.2%)
Step 836: Updated gated ratio to 0.9678 (progress: 32.2%)
Step 836: Updated gated ratio to 0.9678 (progress: 32.2%)
Step 836: Updated gated ratio to 0.9678 (progress: 32.2%)Step 836: Updated gated ratio to 0.9678 (progress: 32.2%)

Step 836: Updated gated ratio to 0.9678 (progress: 32.2%)
Step 836: Updated gated ratio to 0.9678 (progress: 32.2%)
Step 836: Updated gated ratio to 0.9678 (progress: 32.2%)
Step 836: Updated gated ratio to 0.9678 (progress: 32.2%)
Step 836: Updated gated ratio to 0.9678 (progress: 32.2%)
{'loss': 1.1479, 'grad_norm': 1.4846882820129395, 'learning_rate': 1.911574990750857e-05, 'epoch': 0.16}
{'loss': 1.1766, 'grad_norm': 1.6932177543640137, 'learning_rate': 1.9113186426412073e-05, 'epoch': 0.16}
{'loss': 1.0524, 'grad_norm': 1.5999062061309814, 'learning_rate': 1.9110619407262828e-05, 'epoch': 0.16}
{'loss': 1.135, 'grad_norm': 1.6761739253997803, 'learning_rate': 1.9108048851057447e-05, 'epoch': 0.16}
{'loss': 1.0996, 'grad_norm': 1.6864385604858398, 'learning_rate': 1.9105474758793897e-05, 'epoch': 0.16}
Step 841: Updated gated ratio to 0.9676 (progress: 32.4%)
Step 841: Updated gated ratio to 0.9676 (progress: 32.4%)
Step 841: Updated gated ratio to 0.9676 (progress: 32.4%)
Step 841: Updated gated ratio to 0.9676 (progress: 32.4%)
Step 841: Updated gated ratio to 0.9676 (progress: 32.4%)
Step 841: Updated gated ratio to 0.9676 (progress: 32.4%)
Step 841: Updated gated ratio to 0.9676 (progress: 32.4%)
Step 841: Updated gated ratio to 0.9676 (progress: 32.4%)
Step 841: Updated gated ratio to 0.9676 (progress: 32.4%)
Step 841: Updated gated ratio to 0.9676 (progress: 32.4%)
Step 841: Updated gated ratio to 0.9676 (progress: 32.4%)
Step 841: Updated gated ratio to 0.9676 (progress: 32.4%)
Step 841: Updated gated ratio to 0.9676 (progress: 32.4%)
Step 841: Updated gated ratio to 0.9676 (progress: 32.4%)
Step 841: Updated gated ratio to 0.9676 (progress: 32.4%)
Step 841: Updated gated ratio to 0.9676 (progress: 32.4%)
{'loss': 1.1057, 'grad_norm': 1.740439772605896, 'learning_rate': 1.9102897131471536e-05, 'epoch': 0.16}
{'loss': 1.1277, 'grad_norm': 1.553410530090332, 'learning_rate': 1.9100315970091088e-05, 'epoch': 0.16}
{'loss': 1.0709, 'grad_norm': 1.5793780088424683, 'learning_rate': 1.9097731275654645e-05, 'epoch': 0.16}
{'loss': 1.2264, 'grad_norm': 1.8340801000595093, 'learning_rate': 1.909514304916568e-05, 'epoch': 0.16}
{'loss': 1.1426, 'grad_norm': 1.6103558540344238, 'learning_rate': 1.9092551291629026e-05, 'epoch': 0.16}
Step 846: Updated gated ratio to 0.9674 (progress: 32.6%)
Step 846: Updated gated ratio to 0.9674 (progress: 32.6%)
Step 846: Updated gated ratio to 0.9674 (progress: 32.6%)
Step 846: Updated gated ratio to 0.9674 (progress: 32.6%)
Step 846: Updated gated ratio to 0.9674 (progress: 32.6%)
Step 846: Updated gated ratio to 0.9674 (progress: 32.6%)
Step 846: Updated gated ratio to 0.9674 (progress: 32.6%)
Step 846: Updated gated ratio to 0.9674 (progress: 32.6%)
Step 846: Updated gated ratio to 0.9674 (progress: 32.6%)
Step 846: Updated gated ratio to 0.9674 (progress: 32.6%)
Step 846: Updated gated ratio to 0.9674 (progress: 32.6%)
Step 846: Updated gated ratio to 0.9674 (progress: 32.6%)
Step 846: Updated gated ratio to 0.9674 (progress: 32.6%)
Step 846: Updated gated ratio to 0.9674 (progress: 32.6%)
Step 846: Updated gated ratio to 0.9674 (progress: 32.6%)
Step 846: Updated gated ratio to 0.9674 (progress: 32.6%)
{'loss': 1.0868, 'grad_norm': 1.6387954950332642, 'learning_rate': 1.9089956004050893e-05, 'epoch': 0.16}
{'loss': 1.0651, 'grad_norm': 1.6494472026824951, 'learning_rate': 1.908735718743887e-05, 'epoch': 0.16}
{'loss': 1.0921, 'grad_norm': 1.855634331703186, 'learning_rate': 1.908475484280189e-05, 'epoch': 0.16}
{'loss': 1.0614, 'grad_norm': 1.6359567642211914, 'learning_rate': 1.908214897115029e-05, 'epoch': 0.16}
{'loss': 1.0548, 'grad_norm': 1.8314355611801147, 'learning_rate': 1.907953957349575e-05, 'epoch': 0.16}
Step 851: Updated gated ratio to 0.9672 (progress: 32.8%)
Step 851: Updated gated ratio to 0.9672 (progress: 32.8%)
Step 851: Updated gated ratio to 0.9672 (progress: 32.8%)
Step 851: Updated gated ratio to 0.9672 (progress: 32.8%)
Step 851: Updated gated ratio to 0.9672 (progress: 32.8%)
Step 851: Updated gated ratio to 0.9672 (progress: 32.8%)
Step 851: Updated gated ratio to 0.9672 (progress: 32.8%)
Step 851: Updated gated ratio to 0.9672 (progress: 32.8%)
Step 851: Updated gated ratio to 0.9672 (progress: 32.8%)
Step 851: Updated gated ratio to 0.9672 (progress: 32.8%)
Step 851: Updated gated ratio to 0.9672 (progress: 32.8%)
Step 851: Updated gated ratio to 0.9672 (progress: 32.8%)
Step 851: Updated gated ratio to 0.9672 (progress: 32.8%)
Step 851: Updated gated ratio to 0.9672 (progress: 32.8%)
Step 851: Updated gated ratio to 0.9672 (progress: 32.8%)
Step 851: Updated gated ratio to 0.9672 (progress: 32.8%)
{'loss': 1.1555, 'grad_norm': 1.6460617780685425, 'learning_rate': 1.907692665085133e-05, 'epoch': 0.16}
{'loss': 1.0737, 'grad_norm': 1.7627830505371094, 'learning_rate': 1.9074310204231457e-05, 'epoch': 0.16}
{'loss': 1.033, 'grad_norm': 1.678505539894104, 'learning_rate': 1.9071690234651923e-05, 'epoch': 0.16}
{'loss': 1.1199, 'grad_norm': 1.7896008491516113, 'learning_rate': 1.9069066743129893e-05, 'epoch': 0.16}
{'loss': 1.0928, 'grad_norm': 1.830924391746521, 'learning_rate': 1.90664397306839e-05, 'epoch': 0.16}
{'loss': 1.1177, 'grad_norm': 1.8510762453079224, 'learning_rate': 1.9063809198333832e-05, 'epoch': 0.16}
Step 857: Updated gated ratio to 0.9670 (progress: 33.0%)
Step 857: Updated gated ratio to 0.9670 (progress: 33.0%)
Step 857: Updated gated ratio to 0.9670 (progress: 33.0%)
Step 857: Updated gated ratio to 0.9670 (progress: 33.0%)
Step 857: Updated gated ratio to 0.9670 (progress: 33.0%)
Step 857: Updated gated ratio to 0.9670 (progress: 33.0%)
Step 857: Updated gated ratio to 0.9670 (progress: 33.0%)
Step 857: Updated gated ratio to 0.9670 (progress: 33.0%)
Step 857: Updated gated ratio to 0.9670 (progress: 33.0%)
Step 857: Updated gated ratio to 0.9670 (progress: 33.0%)
Step 857: Updated gated ratio to 0.9670 (progress: 33.0%)
Step 857: Updated gated ratio to 0.9670 (progress: 33.0%)
Step 857: Updated gated ratio to 0.9670 (progress: 33.0%)
Step 857: Updated gated ratio to 0.9670 (progress: 33.0%)
Step 857: Updated gated ratio to 0.9670 (progress: 33.0%)
Step 857: Updated gated ratio to 0.9670 (progress: 33.0%)
{'loss': 1.1654, 'grad_norm': 1.7092554569244385, 'learning_rate': 1.9061175147100957e-05, 'epoch': 0.17}
{'loss': 1.1123, 'grad_norm': 1.7117868661880493, 'learning_rate': 1.905853757800791e-05, 'epoch': 0.17}
{'loss': 1.1639, 'grad_norm': 1.7118910551071167, 'learning_rate': 1.9055896492078675e-05, 'epoch': 0.17}
{'loss': 1.0352, 'grad_norm': 1.693963646888733, 'learning_rate': 1.905325189033862e-05, 'epoch': 0.17}
{'loss': 1.197, 'grad_norm': 1.7013826370239258, 'learning_rate': 1.905060377381447e-05, 'epoch': 0.17}
Step 862: Updated gated ratio to 0.9668 (progress: 33.2%)
Step 862: Updated gated ratio to 0.9668 (progress: 33.2%)
Step 862: Updated gated ratio to 0.9668 (progress: 33.2%)
Step 862: Updated gated ratio to 0.9668 (progress: 33.2%)
Step 862: Updated gated ratio to 0.9668 (progress: 33.2%)
Step 862: Updated gated ratio to 0.9668 (progress: 33.2%)
Step 862: Updated gated ratio to 0.9668 (progress: 33.2%)
Step 862: Updated gated ratio to 0.9668 (progress: 33.2%)
Step 862: Updated gated ratio to 0.9668 (progress: 33.2%)
Step 862: Updated gated ratio to 0.9668 (progress: 33.2%)
Step 862: Updated gated ratio to 0.9668 (progress: 33.2%)
Step 862: Updated gated ratio to 0.9668 (progress: 33.2%)
Step 862: Updated gated ratio to 0.9668 (progress: 33.2%)
Step 862: Updated gated ratio to 0.9668 (progress: 33.2%)
Step 862: Updated gated ratio to 0.9668 (progress: 33.2%)
Step 862: Updated gated ratio to 0.9668 (progress: 33.2%)
{'loss': 1.0853, 'grad_norm': 1.726854920387268, 'learning_rate': 1.904795214353431e-05, 'epoch': 0.17}
{'loss': 1.1722, 'grad_norm': 1.6778531074523926, 'learning_rate': 1.90452970005276e-05, 'epoch': 0.17}
{'loss': 1.0871, 'grad_norm': 1.6807740926742554, 'learning_rate': 1.9042638345825155e-05, 'epoch': 0.17}
{'loss': 1.0712, 'grad_norm': 1.651093602180481, 'learning_rate': 1.9039976180459158e-05, 'epoch': 0.17}
{'loss': 1.1398, 'grad_norm': 1.8284839391708374, 'learning_rate': 1.9037310505463153e-05, 'epoch': 0.17}
Step 867: Updated gated ratio to 0.9666 (progress: 33.4%)Step 867: Updated gated ratio to 0.9666 (progress: 33.4%)
Step 867: Updated gated ratio to 0.9666 (progress: 33.4%)
Step 867: Updated gated ratio to 0.9666 (progress: 33.4%)
Step 867: Updated gated ratio to 0.9666 (progress: 33.4%)
Step 867: Updated gated ratio to 0.9666 (progress: 33.4%)
Step 867: Updated gated ratio to 0.9666 (progress: 33.4%)
Step 867: Updated gated ratio to 0.9666 (progress: 33.4%)
Step 867: Updated gated ratio to 0.9666 (progress: 33.4%)

Step 867: Updated gated ratio to 0.9666 (progress: 33.4%)
Step 867: Updated gated ratio to 0.9666 (progress: 33.4%)
Step 867: Updated gated ratio to 0.9666 (progress: 33.4%)
Step 867: Updated gated ratio to 0.9666 (progress: 33.4%)
Step 867: Updated gated ratio to 0.9666 (progress: 33.4%)
Step 867: Updated gated ratio to 0.9666 (progress: 33.4%)
Step 867: Updated gated ratio to 0.9666 (progress: 33.4%)
{'loss': 1.1188, 'grad_norm': 1.6293190717697144, 'learning_rate': 1.9034641321872043e-05, 'epoch': 0.17}
{'loss': 1.1146, 'grad_norm': 1.3870989084243774, 'learning_rate': 1.9031968630722104e-05, 'epoch': 0.17}
{'loss': 1.1608, 'grad_norm': 1.5920506715774536, 'learning_rate': 1.902929243305096e-05, 'epoch': 0.17}
{'loss': 1.1478, 'grad_norm': 1.7659893035888672, 'learning_rate': 1.902661272989761e-05, 'epoch': 0.17}
{'loss': 1.1268, 'grad_norm': 1.7424123287200928, 'learning_rate': 1.9023929522302394e-05, 'epoch': 0.17}
Step 872: Updated gated ratio to 0.9664 (progress: 33.6%)
Step 872: Updated gated ratio to 0.9664 (progress: 33.6%)
Step 872: Updated gated ratio to 0.9664 (progress: 33.6%)
Step 872: Updated gated ratio to 0.9664 (progress: 33.6%)
Step 872: Updated gated ratio to 0.9664 (progress: 33.6%)
Step 872: Updated gated ratio to 0.9664 (progress: 33.6%)
Step 872: Updated gated ratio to 0.9664 (progress: 33.6%)
Step 872: Updated gated ratio to 0.9664 (progress: 33.6%)
Step 872: Updated gated ratio to 0.9664 (progress: 33.6%)
Step 872: Updated gated ratio to 0.9664 (progress: 33.6%)
Step 872: Updated gated ratio to 0.9664 (progress: 33.6%)
Step 872: Updated gated ratio to 0.9664 (progress: 33.6%)
Step 872: Updated gated ratio to 0.9664 (progress: 33.6%)
Step 872: Updated gated ratio to 0.9664 (progress: 33.6%)
Step 872: Updated gated ratio to 0.9664 (progress: 33.6%)
Step 872: Updated gated ratio to 0.9664 (progress: 33.6%)
{'loss': 1.0976, 'grad_norm': 1.6170315742492676, 'learning_rate': 1.9021242811307044e-05, 'epoch': 0.17}
Error with image file is truncated (32 bytes not processed)
{'loss': 1.095, 'grad_norm': 1.631898283958435, 'learning_rate': 1.901855259795462e-05, 'epoch': 0.17}
{'loss': 1.0476, 'grad_norm': 1.6227556467056274, 'learning_rate': 1.9015858883289556e-05, 'epoch': 0.17}
{'loss': 1.1449, 'grad_norm': 1.6810729503631592, 'learning_rate': 1.9013161668357655e-05, 'epoch': 0.17}
{'loss': 1.1644, 'grad_norm': 1.7599233388900757, 'learning_rate': 1.901046095420606e-05, 'epoch': 0.17}
Step 877: Updated gated ratio to 0.9662 (progress: 33.8%)
Step 877: Updated gated ratio to 0.9662 (progress: 33.8%)
Step 877: Updated gated ratio to 0.9662 (progress: 33.8%)
Step 877: Updated gated ratio to 0.9662 (progress: 33.8%)
Step 877: Updated gated ratio to 0.9662 (progress: 33.8%)
Step 877: Updated gated ratio to 0.9662 (progress: 33.8%)
Step 877: Updated gated ratio to 0.9662 (progress: 33.8%)Step 877: Updated gated ratio to 0.9662 (progress: 33.8%)

Step 877: Updated gated ratio to 0.9662 (progress: 33.8%)
Step 877: Updated gated ratio to 0.9662 (progress: 33.8%)
Step 877: Updated gated ratio to 0.9662 (progress: 33.8%)
Step 877: Updated gated ratio to 0.9662 (progress: 33.8%)
Step 877: Updated gated ratio to 0.9662 (progress: 33.8%)
Step 877: Updated gated ratio to 0.9662 (progress: 33.8%)
Step 877: Updated gated ratio to 0.9662 (progress: 33.8%)
Step 877: Updated gated ratio to 0.9662 (progress: 33.8%)
{'loss': 1.1103, 'grad_norm': 1.6232675313949585, 'learning_rate': 1.9007756741883284e-05, 'epoch': 0.17}
{'loss': 1.0729, 'grad_norm': 1.5468345880508423, 'learning_rate': 1.9005049032439193e-05, 'epoch': 0.17}
{'loss': 1.1327, 'grad_norm': 1.6251298189163208, 'learning_rate': 1.9002337826925012e-05, 'epoch': 0.17}
{'loss': 0.9919, 'grad_norm': 1.540987491607666, 'learning_rate': 1.899962312639333e-05, 'epoch': 0.17}
{'loss': 1.1278, 'grad_norm': 1.3625743389129639, 'learning_rate': 1.8996904931898085e-05, 'epoch': 0.17}
{'loss': 1.1818, 'grad_norm': 1.7366505861282349, 'learning_rate': 1.899418324449457e-05, 'epoch': 0.17}
Step 883: Updated gated ratio to 0.9660 (progress: 34.0%)
Step 883: Updated gated ratio to 0.9660 (progress: 34.0%)
Step 883: Updated gated ratio to 0.9660 (progress: 34.0%)
Step 883: Updated gated ratio to 0.9660 (progress: 34.0%)
Step 883: Updated gated ratio to 0.9660 (progress: 34.0%)
Step 883: Updated gated ratio to 0.9660 (progress: 34.0%)
Step 883: Updated gated ratio to 0.9660 (progress: 34.0%)
Step 883: Updated gated ratio to 0.9660 (progress: 34.0%)
Step 883: Updated gated ratio to 0.9660 (progress: 34.0%)
Step 883: Updated gated ratio to 0.9660 (progress: 34.0%)
Step 883: Updated gated ratio to 0.9660 (progress: 34.0%)
Step 883: Updated gated ratio to 0.9660 (progress: 34.0%)
Step 883: Updated gated ratio to 0.9660 (progress: 34.0%)
Step 883: Updated gated ratio to 0.9660 (progress: 34.0%)
Step 883: Updated gated ratio to 0.9660 (progress: 34.0%)
Step 883: Updated gated ratio to 0.9660 (progress: 34.0%)
{'loss': 1.0819, 'grad_norm': 1.554276943206787, 'learning_rate': 1.8991458065239444e-05, 'epoch': 0.17}
{'loss': 1.1849, 'grad_norm': 1.259209156036377, 'learning_rate': 1.8988729395190712e-05, 'epoch': 0.17}
{'loss': 1.0973, 'grad_norm': 1.4574600458145142, 'learning_rate': 1.8985997235407735e-05, 'epoch': 0.17}
{'loss': 1.0465, 'grad_norm': 1.5680643320083618, 'learning_rate': 1.898326158695124e-05, 'epoch': 0.17}
{'loss': 1.1174, 'grad_norm': 1.642214298248291, 'learning_rate': 1.8980522450883287e-05, 'epoch': 0.17}
Step 888: Updated gated ratio to 0.9658 (progress: 34.2%)
Step 888: Updated gated ratio to 0.9658 (progress: 34.2%)
Step 888: Updated gated ratio to 0.9658 (progress: 34.2%)
Step 888: Updated gated ratio to 0.9658 (progress: 34.2%)
Step 888: Updated gated ratio to 0.9658 (progress: 34.2%)
Step 888: Updated gated ratio to 0.9658 (progress: 34.2%)
Step 888: Updated gated ratio to 0.9658 (progress: 34.2%)
Step 888: Updated gated ratio to 0.9658 (progress: 34.2%)
Step 888: Updated gated ratio to 0.9658 (progress: 34.2%)
Step 888: Updated gated ratio to 0.9658 (progress: 34.2%)
Step 888: Updated gated ratio to 0.9658 (progress: 34.2%)
Step 888: Updated gated ratio to 0.9658 (progress: 34.2%)
Step 888: Updated gated ratio to 0.9658 (progress: 34.2%)
Step 888: Updated gated ratio to 0.9658 (progress: 34.2%)
Step 888: Updated gated ratio to 0.9658 (progress: 34.2%)
Step 888: Updated gated ratio to 0.9658 (progress: 34.2%)
{'loss': 0.9866, 'grad_norm': 1.5256590843200684, 'learning_rate': 1.8977779828267314e-05, 'epoch': 0.17}
{'loss': 1.1907, 'grad_norm': 1.7861156463623047, 'learning_rate': 1.8975033720168094e-05, 'epoch': 0.17}
{'loss': 1.1171, 'grad_norm': 1.5898569822311401, 'learning_rate': 1.897228412765177e-05, 'epoch': 0.17}
{'loss': 1.041, 'grad_norm': 1.6900863647460938, 'learning_rate': 1.896953105178582e-05, 'epoch': 0.17}
{'loss': 1.0046, 'grad_norm': 1.606113314628601, 'learning_rate': 1.8966774493639084e-05, 'epoch': 0.17}
Step 893: Updated gated ratio to 0.9656 (progress: 34.4%)
Step 893: Updated gated ratio to 0.9656 (progress: 34.4%)
Step 893: Updated gated ratio to 0.9656 (progress: 34.4%)
Step 893: Updated gated ratio to 0.9656 (progress: 34.4%)
Step 893: Updated gated ratio to 0.9656 (progress: 34.4%)
Step 893: Updated gated ratio to 0.9656 (progress: 34.4%)
Step 893: Updated gated ratio to 0.9656 (progress: 34.4%)
Step 893: Updated gated ratio to 0.9656 (progress: 34.4%)
Step 893: Updated gated ratio to 0.9656 (progress: 34.4%)
Step 893: Updated gated ratio to 0.9656 (progress: 34.4%)
Step 893: Updated gated ratio to 0.9656 (progress: 34.4%)
Step 893: Updated gated ratio to 0.9656 (progress: 34.4%)
Step 893: Updated gated ratio to 0.9656 (progress: 34.4%)
Step 893: Updated gated ratio to 0.9656 (progress: 34.4%)
Step 893: Updated gated ratio to 0.9656 (progress: 34.4%)
Step 893: Updated gated ratio to 0.9656 (progress: 34.4%)
{'loss': 1.1363, 'grad_norm': 1.6315628290176392, 'learning_rate': 1.896401445428176e-05, 'epoch': 0.17}
{'loss': 1.1259, 'grad_norm': 1.6832150220870972, 'learning_rate': 1.896125093478538e-05, 'epoch': 0.17}
{'loss': 1.1729, 'grad_norm': 1.7984142303466797, 'learning_rate': 1.895848393622284e-05, 'epoch': 0.17}
{'loss': 0.9966, 'grad_norm': 1.6883957386016846, 'learning_rate': 1.895571345966839e-05, 'epoch': 0.17}
{'loss': 1.0953, 'grad_norm': 1.6030231714248657, 'learning_rate': 1.8952939506197622e-05, 'epoch': 0.17}
Step 898: Updated gated ratio to 0.9654 (progress: 34.6%)
Step 898: Updated gated ratio to 0.9654 (progress: 34.6%)
Step 898: Updated gated ratio to 0.9654 (progress: 34.6%)
Step 898: Updated gated ratio to 0.9654 (progress: 34.6%)
Step 898: Updated gated ratio to 0.9654 (progress: 34.6%)
Step 898: Updated gated ratio to 0.9654 (progress: 34.6%)
Step 898: Updated gated ratio to 0.9654 (progress: 34.6%)
Step 898: Updated gated ratio to 0.9654 (progress: 34.6%)
Step 898: Updated gated ratio to 0.9654 (progress: 34.6%)
Step 898: Updated gated ratio to 0.9654 (progress: 34.6%)
Step 898: Updated gated ratio to 0.9654 (progress: 34.6%)
Step 898: Updated gated ratio to 0.9654 (progress: 34.6%)
Step 898: Updated gated ratio to 0.9654 (progress: 34.6%)
Step 898: Updated gated ratio to 0.9654 (progress: 34.6%)
Step 898: Updated gated ratio to 0.9654 (progress: 34.6%)
Step 898: Updated gated ratio to 0.9654 (progress: 34.6%)
{'loss': 1.0325, 'grad_norm': 1.877792239189148, 'learning_rate': 1.8950162076887477e-05, 'epoch': 0.17}
{'loss': 1.147, 'grad_norm': 1.7532310485839844, 'learning_rate': 1.894738117281625e-05, 'epoch': 0.17}
{'loss': 1.0558, 'grad_norm': 1.7022943496704102, 'learning_rate': 1.8944596795063584e-05, 'epoch': 0.17}
{'loss': 1.0692, 'grad_norm': 1.5995745658874512, 'learning_rate': 1.894180894471047e-05, 'epoch': 0.17}
{'loss': 1.1577, 'grad_norm': 1.6999536752700806, 'learning_rate': 1.8939017622839253e-05, 'epoch': 0.17}
Step 903: Updated gated ratio to 0.9652 (progress: 34.8%)
Step 903: Updated gated ratio to 0.9652 (progress: 34.8%)
Step 903: Updated gated ratio to 0.9652 (progress: 34.8%)
Step 903: Updated gated ratio to 0.9652 (progress: 34.8%)
Step 903: Updated gated ratio to 0.9652 (progress: 34.8%)Step 903: Updated gated ratio to 0.9652 (progress: 34.8%)

Step 903: Updated gated ratio to 0.9652 (progress: 34.8%)
Step 903: Updated gated ratio to 0.9652 (progress: 34.8%)
Step 903: Updated gated ratio to 0.9652 (progress: 34.8%)
Step 903: Updated gated ratio to 0.9652 (progress: 34.8%)
Step 903: Updated gated ratio to 0.9652 (progress: 34.8%)
Step 903: Updated gated ratio to 0.9652 (progress: 34.8%)
Step 903: Updated gated ratio to 0.9652 (progress: 34.8%)
Step 903: Updated gated ratio to 0.9652 (progress: 34.8%)
Step 903: Updated gated ratio to 0.9652 (progress: 34.8%)
Step 903: Updated gated ratio to 0.9652 (progress: 34.8%)
{'loss': 1.1649, 'grad_norm': 1.542237401008606, 'learning_rate': 1.8936222830533613e-05, 'epoch': 0.17}
{'loss': 1.1346, 'grad_norm': 1.7214787006378174, 'learning_rate': 1.8933424568878586e-05, 'epoch': 0.17}
{'loss': 1.1925, 'grad_norm': 1.8164340257644653, 'learning_rate': 1.8930622838960555e-05, 'epoch': 0.17}
{'loss': 1.1555, 'grad_norm': 1.7168654203414917, 'learning_rate': 1.8927817641867244e-05, 'epoch': 0.17}
{'loss': 1.1265, 'grad_norm': 1.6576924324035645, 'learning_rate': 1.8925008978687737e-05, 'epoch': 0.17}
{'loss': 1.1889, 'grad_norm': 1.7178516387939453, 'learning_rate': 1.8922196850512446e-05, 'epoch': 0.17}
Step 909: Updated gated ratio to 0.9650 (progress: 35.0%)
Step 909: Updated gated ratio to 0.9650 (progress: 35.0%)
Step 909: Updated gated ratio to 0.9650 (progress: 35.0%)
Step 909: Updated gated ratio to 0.9650 (progress: 35.0%)
Step 909: Updated gated ratio to 0.9650 (progress: 35.0%)
Step 909: Updated gated ratio to 0.9650 (progress: 35.0%)
Step 909: Updated gated ratio to 0.9650 (progress: 35.0%)
Step 909: Updated gated ratio to 0.9650 (progress: 35.0%)
Step 909: Updated gated ratio to 0.9650 (progress: 35.0%)Step 909: Updated gated ratio to 0.9650 (progress: 35.0%)

Step 909: Updated gated ratio to 0.9650 (progress: 35.0%)
Step 909: Updated gated ratio to 0.9650 (progress: 35.0%)
Step 909: Updated gated ratio to 0.9650 (progress: 35.0%)
Step 909: Updated gated ratio to 0.9650 (progress: 35.0%)
Step 909: Updated gated ratio to 0.9650 (progress: 35.0%)
Step 909: Updated gated ratio to 0.9650 (progress: 35.0%)
{'loss': 1.1204, 'grad_norm': 1.5780256986618042, 'learning_rate': 1.8919381258433135e-05, 'epoch': 0.18}
{'loss': 1.135, 'grad_norm': 1.792486548423767, 'learning_rate': 1.8916562203542916e-05, 'epoch': 0.18}
{'loss': 1.1041, 'grad_norm': 1.697698950767517, 'learning_rate': 1.8913739686936244e-05, 'epoch': 0.18}
{'loss': 0.9915, 'grad_norm': 1.6161311864852905, 'learning_rate': 1.8910913709708918e-05, 'epoch': 0.18}
{'loss': 1.1012, 'grad_norm': 1.6569474935531616, 'learning_rate': 1.8908084272958077e-05, 'epoch': 0.18}
Step 914: Updated gated ratio to 0.9648 (progress: 35.2%)
Step 914: Updated gated ratio to 0.9648 (progress: 35.2%)
Step 914: Updated gated ratio to 0.9648 (progress: 35.2%)
Step 914: Updated gated ratio to 0.9648 (progress: 35.2%)
Step 914: Updated gated ratio to 0.9648 (progress: 35.2%)
Step 914: Updated gated ratio to 0.9648 (progress: 35.2%)
Step 914: Updated gated ratio to 0.9648 (progress: 35.2%)
Step 914: Updated gated ratio to 0.9648 (progress: 35.2%)
Step 914: Updated gated ratio to 0.9648 (progress: 35.2%)
Step 914: Updated gated ratio to 0.9648 (progress: 35.2%)
Step 914: Updated gated ratio to 0.9648 (progress: 35.2%)
Step 914: Updated gated ratio to 0.9648 (progress: 35.2%)
Step 914: Updated gated ratio to 0.9648 (progress: 35.2%)
Step 914: Updated gated ratio to 0.9648 (progress: 35.2%)
Step 914: Updated gated ratio to 0.9648 (progress: 35.2%)
Step 914: Updated gated ratio to 0.9648 (progress: 35.2%)
{'loss': 1.1903, 'grad_norm': 1.8789187669754028, 'learning_rate': 1.8905251377782206e-05, 'epoch': 0.18}
{'loss': 1.0893, 'grad_norm': 1.663341999053955, 'learning_rate': 1.8902415025281136e-05, 'epoch': 0.18}
{'loss': 1.1166, 'grad_norm': 1.7785032987594604, 'learning_rate': 1.889957521655603e-05, 'epoch': 0.18}
{'loss': 1.1199, 'grad_norm': 1.7046314477920532, 'learning_rate': 1.8896731952709408e-05, 'epoch': 0.18}
{'loss': 1.0911, 'grad_norm': 1.648173213005066, 'learning_rate': 1.8893885234845117e-05, 'epoch': 0.18}
Step 919: Updated gated ratio to 0.9646 (progress: 35.4%)
Step 919: Updated gated ratio to 0.9646 (progress: 35.4%)
Step 919: Updated gated ratio to 0.9646 (progress: 35.4%)
Step 919: Updated gated ratio to 0.9646 (progress: 35.4%)
Step 919: Updated gated ratio to 0.9646 (progress: 35.4%)
Step 919: Updated gated ratio to 0.9646 (progress: 35.4%)
Step 919: Updated gated ratio to 0.9646 (progress: 35.4%)
Step 919: Updated gated ratio to 0.9646 (progress: 35.4%)
Step 919: Updated gated ratio to 0.9646 (progress: 35.4%)
Step 919: Updated gated ratio to 0.9646 (progress: 35.4%)
Step 919: Updated gated ratio to 0.9646 (progress: 35.4%)
Step 919: Updated gated ratio to 0.9646 (progress: 35.4%)
Step 919: Updated gated ratio to 0.9646 (progress: 35.4%)
Step 919: Updated gated ratio to 0.9646 (progress: 35.4%)
Step 919: Updated gated ratio to 0.9646 (progress: 35.4%)
Step 919: Updated gated ratio to 0.9646 (progress: 35.4%)
{'loss': 1.2208, 'grad_norm': 1.8100426197052002, 'learning_rate': 1.8891035064068354e-05, 'epoch': 0.18}
{'loss': 1.0457, 'grad_norm': 1.7165535688400269, 'learning_rate': 1.888818144148565e-05, 'epoch': 0.18}
{'loss': 1.0789, 'grad_norm': 1.8626205921173096, 'learning_rate': 1.888532436820488e-05, 'epoch': 0.18}
{'loss': 1.1214, 'grad_norm': 1.7599622011184692, 'learning_rate': 1.8882463845335263e-05, 'epoch': 0.18}
{'loss': 1.0955, 'grad_norm': 1.6189180612564087, 'learning_rate': 1.8879599873987343e-05, 'epoch': 0.18}
Step 924: Updated gated ratio to 0.9644 (progress: 35.6%)
Step 924: Updated gated ratio to 0.9644 (progress: 35.6%)
Step 924: Updated gated ratio to 0.9644 (progress: 35.6%)
Step 924: Updated gated ratio to 0.9644 (progress: 35.6%)
Step 924: Updated gated ratio to 0.9644 (progress: 35.6%)
Step 924: Updated gated ratio to 0.9644 (progress: 35.6%)
Step 924: Updated gated ratio to 0.9644 (progress: 35.6%)
Step 924: Updated gated ratio to 0.9644 (progress: 35.6%)Step 924: Updated gated ratio to 0.9644 (progress: 35.6%)

Step 924: Updated gated ratio to 0.9644 (progress: 35.6%)
Step 924: Updated gated ratio to 0.9644 (progress: 35.6%)
Step 924: Updated gated ratio to 0.9644 (progress: 35.6%)
Step 924: Updated gated ratio to 0.9644 (progress: 35.6%)
Step 924: Updated gated ratio to 0.9644 (progress: 35.6%)
Step 924: Updated gated ratio to 0.9644 (progress: 35.6%)
Step 924: Updated gated ratio to 0.9644 (progress: 35.6%)
{'loss': 1.1114, 'grad_norm': 1.5509412288665771, 'learning_rate': 1.8876732455273022e-05, 'epoch': 0.18}
{'loss': 1.086, 'grad_norm': 1.6129436492919922, 'learning_rate': 1.8873861590305527e-05, 'epoch': 0.18}
{'loss': 1.1442, 'grad_norm': 1.7358237504959106, 'learning_rate': 1.8870987280199428e-05, 'epoch': 0.18}
{'loss': 1.0623, 'grad_norm': 1.6841861009597778, 'learning_rate': 1.886810952607063e-05, 'epoch': 0.18}
{'loss': 1.1413, 'grad_norm': 1.493238091468811, 'learning_rate': 1.8865228329036372e-05, 'epoch': 0.18}
Step 929: Updated gated ratio to 0.9642 (progress: 35.8%)
Step 929: Updated gated ratio to 0.9642 (progress: 35.8%)
Step 929: Updated gated ratio to 0.9642 (progress: 35.8%)Step 929: Updated gated ratio to 0.9642 (progress: 35.8%)
Step 929: Updated gated ratio to 0.9642 (progress: 35.8%)
Step 929: Updated gated ratio to 0.9642 (progress: 35.8%)
Step 929: Updated gated ratio to 0.9642 (progress: 35.8%)

Step 929: Updated gated ratio to 0.9642 (progress: 35.8%)
Step 929: Updated gated ratio to 0.9642 (progress: 35.8%)
Step 929: Updated gated ratio to 0.9642 (progress: 35.8%)
Step 929: Updated gated ratio to 0.9642 (progress: 35.8%)
Step 929: Updated gated ratio to 0.9642 (progress: 35.8%)
Step 929: Updated gated ratio to 0.9642 (progress: 35.8%)
Step 929: Updated gated ratio to 0.9642 (progress: 35.8%)
Step 929: Updated gated ratio to 0.9642 (progress: 35.8%)
Step 929: Updated gated ratio to 0.9642 (progress: 35.8%)
{'loss': 1.1178, 'grad_norm': 1.595374345779419, 'learning_rate': 1.886234369021524e-05, 'epoch': 0.18}
{'loss': 1.1323, 'grad_norm': 1.7555434703826904, 'learning_rate': 1.885945561072715e-05, 'epoch': 0.18}
{'loss': 1.1432, 'grad_norm': 1.8754287958145142, 'learning_rate': 1.885656409169335e-05, 'epoch': 0.18}
{'loss': 1.0092, 'grad_norm': 1.3427422046661377, 'learning_rate': 1.885366913423643e-05, 'epoch': 0.18}
{'loss': 1.1116, 'grad_norm': 1.6468465328216553, 'learning_rate': 1.8850770739480312e-05, 'epoch': 0.18}
Step 934: Updated gated ratio to 0.9640 (progress: 36.0%)
Step 934: Updated gated ratio to 0.9640 (progress: 36.0%)Step 934: Updated gated ratio to 0.9640 (progress: 36.0%)
Step 934: Updated gated ratio to 0.9640 (progress: 36.0%)
Step 934: Updated gated ratio to 0.9640 (progress: 36.0%)
Step 934: Updated gated ratio to 0.9640 (progress: 36.0%)
Step 934: Updated gated ratio to 0.9640 (progress: 36.0%)
Step 934: Updated gated ratio to 0.9640 (progress: 36.0%)

Step 934: Updated gated ratio to 0.9640 (progress: 36.0%)
Step 934: Updated gated ratio to 0.9640 (progress: 36.0%)
Step 934: Updated gated ratio to 0.9640 (progress: 36.0%)
Step 934: Updated gated ratio to 0.9640 (progress: 36.0%)
Step 934: Updated gated ratio to 0.9640 (progress: 36.0%)
Step 934: Updated gated ratio to 0.9640 (progress: 36.0%)
Step 934: Updated gated ratio to 0.9640 (progress: 36.0%)
Step 934: Updated gated ratio to 0.9640 (progress: 36.0%)
{'loss': 1.1599, 'grad_norm': 1.7200281620025635, 'learning_rate': 1.8847868908550252e-05, 'epoch': 0.18}
{'loss': 1.1139, 'grad_norm': 1.7065346240997314, 'learning_rate': 1.8844963642572837e-05, 'epoch': 0.18}
{'loss': 1.1139, 'grad_norm': 1.319970726966858, 'learning_rate': 1.8842054942676e-05, 'epoch': 0.18}
{'loss': 1.0525, 'grad_norm': 1.6691381931304932, 'learning_rate': 1.8839142809988987e-05, 'epoch': 0.18}
{'loss': 1.1265, 'grad_norm': 1.608758568763733, 'learning_rate': 1.88362272456424e-05, 'epoch': 0.18}
{'loss': 1.1696, 'grad_norm': 1.4125587940216064, 'learning_rate': 1.8833308250768153e-05, 'epoch': 0.18}
Step 940: Updated gated ratio to 0.9638 (progress: 36.2%)
Step 940: Updated gated ratio to 0.9638 (progress: 36.2%)
Step 940: Updated gated ratio to 0.9638 (progress: 36.2%)
Step 940: Updated gated ratio to 0.9638 (progress: 36.2%)
Step 940: Updated gated ratio to 0.9638 (progress: 36.2%)
Step 940: Updated gated ratio to 0.9638 (progress: 36.2%)Step 940: Updated gated ratio to 0.9638 (progress: 36.2%)

Step 940: Updated gated ratio to 0.9638 (progress: 36.2%)
Step 940: Updated gated ratio to 0.9638 (progress: 36.2%)
Step 940: Updated gated ratio to 0.9638 (progress: 36.2%)Step 940: Updated gated ratio to 0.9638 (progress: 36.2%)

Step 940: Updated gated ratio to 0.9638 (progress: 36.2%)
Step 940: Updated gated ratio to 0.9638 (progress: 36.2%)
Step 940: Updated gated ratio to 0.9638 (progress: 36.2%)
Step 940: Updated gated ratio to 0.9638 (progress: 36.2%)
Step 940: Updated gated ratio to 0.9638 (progress: 36.2%)
{'loss': 1.1319, 'grad_norm': 1.4133546352386475, 'learning_rate': 1.8830385826499507e-05, 'epoch': 0.18}
{'loss': 1.1788, 'grad_norm': 1.6715831756591797, 'learning_rate': 1.882745997397104e-05, 'epoch': 0.18}
{'loss': 1.082, 'grad_norm': 1.6569149494171143, 'learning_rate': 1.8824530694318675e-05, 'epoch': 0.18}
{'loss': 1.0919, 'grad_norm': 1.663253664970398, 'learning_rate': 1.882159798867966e-05, 'epoch': 0.18}
{'loss': 1.1781, 'grad_norm': 1.6130937337875366, 'learning_rate': 1.8818661858192562e-05, 'epoch': 0.18}
Step 945: Updated gated ratio to 0.9636 (progress: 36.4%)
Step 945: Updated gated ratio to 0.9636 (progress: 36.4%)
Step 945: Updated gated ratio to 0.9636 (progress: 36.4%)
Step 945: Updated gated ratio to 0.9636 (progress: 36.4%)
Step 945: Updated gated ratio to 0.9636 (progress: 36.4%)
Step 945: Updated gated ratio to 0.9636 (progress: 36.4%)
Step 945: Updated gated ratio to 0.9636 (progress: 36.4%)
Step 945: Updated gated ratio to 0.9636 (progress: 36.4%)
Step 945: Updated gated ratio to 0.9636 (progress: 36.4%)
Step 945: Updated gated ratio to 0.9636 (progress: 36.4%)
Step 945: Updated gated ratio to 0.9636 (progress: 36.4%)
Step 945: Updated gated ratio to 0.9636 (progress: 36.4%)
Step 945: Updated gated ratio to 0.9636 (progress: 36.4%)
Step 945: Updated gated ratio to 0.9636 (progress: 36.4%)
Step 945: Updated gated ratio to 0.9636 (progress: 36.4%)
Step 945: Updated gated ratio to 0.9636 (progress: 36.4%)
{'loss': 1.0892, 'grad_norm': 1.657436728477478, 'learning_rate': 1.88157223039973e-05, 'epoch': 0.18}
{'loss': 1.1954, 'grad_norm': 1.3674393892288208, 'learning_rate': 1.8812779327235106e-05, 'epoch': 0.18}
{'loss': 1.1312, 'grad_norm': 1.7243447303771973, 'learning_rate': 1.880983292904854e-05, 'epoch': 0.18}
{'loss': 1.1803, 'grad_norm': 1.6937884092330933, 'learning_rate': 1.88068831105815e-05, 'epoch': 0.18}
{'loss': 1.1828, 'grad_norm': 1.7242341041564941, 'learning_rate': 1.8803929872979214e-05, 'epoch': 0.18}
Step 950: Updated gated ratio to 0.9634 (progress: 36.6%)
Step 950: Updated gated ratio to 0.9634 (progress: 36.6%)
Step 950: Updated gated ratio to 0.9634 (progress: 36.6%)Step 950: Updated gated ratio to 0.9634 (progress: 36.6%)

Step 950: Updated gated ratio to 0.9634 (progress: 36.6%)
Step 950: Updated gated ratio to 0.9634 (progress: 36.6%)
Step 950: Updated gated ratio to 0.9634 (progress: 36.6%)
Step 950: Updated gated ratio to 0.9634 (progress: 36.6%)
Step 950: Updated gated ratio to 0.9634 (progress: 36.6%)
Step 950: Updated gated ratio to 0.9634 (progress: 36.6%)
Step 950: Updated gated ratio to 0.9634 (progress: 36.6%)
Step 950: Updated gated ratio to 0.9634 (progress: 36.6%)
Step 950: Updated gated ratio to 0.9634 (progress: 36.6%)
Step 950: Updated gated ratio to 0.9634 (progress: 36.6%)
Step 950: Updated gated ratio to 0.9634 (progress: 36.6%)
Step 950: Updated gated ratio to 0.9634 (progress: 36.6%)
{'loss': 1.1292, 'grad_norm': 1.5111165046691895, 'learning_rate': 1.8800973217388215e-05, 'epoch': 0.18}
{'loss': 1.1515, 'grad_norm': 1.7299041748046875, 'learning_rate': 1.879801314495639e-05, 'epoch': 0.18}
{'loss': 1.2013, 'grad_norm': 1.6747370958328247, 'learning_rate': 1.879504965683294e-05, 'epoch': 0.18}
{'loss': 1.0783, 'grad_norm': 1.7376112937927246, 'learning_rate': 1.8792082754168385e-05, 'epoch': 0.18}
{'loss': 1.0341, 'grad_norm': 1.7564623355865479, 'learning_rate': 1.878911243811459e-05, 'epoch': 0.18}
Step 955: Updated gated ratio to 0.9632 (progress: 36.8%)
Step 955: Updated gated ratio to 0.9632 (progress: 36.8%)
Step 955: Updated gated ratio to 0.9632 (progress: 36.8%)
Step 955: Updated gated ratio to 0.9632 (progress: 36.8%)
Step 955: Updated gated ratio to 0.9632 (progress: 36.8%)
Step 955: Updated gated ratio to 0.9632 (progress: 36.8%)
Step 955: Updated gated ratio to 0.9632 (progress: 36.8%)Step 955: Updated gated ratio to 0.9632 (progress: 36.8%)

Step 955: Updated gated ratio to 0.9632 (progress: 36.8%)Step 955: Updated gated ratio to 0.9632 (progress: 36.8%)

Step 955: Updated gated ratio to 0.9632 (progress: 36.8%)
Step 955: Updated gated ratio to 0.9632 (progress: 36.8%)
Step 955: Updated gated ratio to 0.9632 (progress: 36.8%)
Step 955: Updated gated ratio to 0.9632 (progress: 36.8%)
Step 955: Updated gated ratio to 0.9632 (progress: 36.8%)
Step 955: Updated gated ratio to 0.9632 (progress: 36.8%)
{'loss': 1.1634, 'grad_norm': 1.853687047958374, 'learning_rate': 1.8786138709824726e-05, 'epoch': 0.18}
{'loss': 0.9757, 'grad_norm': 1.521619439125061, 'learning_rate': 1.8783161570453295e-05, 'epoch': 0.18}
{'loss': 1.0982, 'grad_norm': 1.6570180654525757, 'learning_rate': 1.878018102115614e-05, 'epoch': 0.18}
{'loss': 1.1169, 'grad_norm': 1.6337662935256958, 'learning_rate': 1.8777197063090394e-05, 'epoch': 0.18}
{'loss': 1.1972, 'grad_norm': 1.7919983863830566, 'learning_rate': 1.877420969741454e-05, 'epoch': 0.18}
Step 960: Updated gated ratio to 0.9630 (progress: 37.0%)
Step 960: Updated gated ratio to 0.9630 (progress: 37.0%)
Step 960: Updated gated ratio to 0.9630 (progress: 37.0%)
Step 960: Updated gated ratio to 0.9630 (progress: 37.0%)
Step 960: Updated gated ratio to 0.9630 (progress: 37.0%)
Step 960: Updated gated ratio to 0.9630 (progress: 37.0%)Step 960: Updated gated ratio to 0.9630 (progress: 37.0%)

Step 960: Updated gated ratio to 0.9630 (progress: 37.0%)
Step 960: Updated gated ratio to 0.9630 (progress: 37.0%)
Step 960: Updated gated ratio to 0.9630 (progress: 37.0%)
Step 960: Updated gated ratio to 0.9630 (progress: 37.0%)
Step 960: Updated gated ratio to 0.9630 (progress: 37.0%)
Step 960: Updated gated ratio to 0.9630 (progress: 37.0%)
Step 960: Updated gated ratio to 0.9630 (progress: 37.0%)
Step 960: Updated gated ratio to 0.9630 (progress: 37.0%)
Step 960: Updated gated ratio to 0.9630 (progress: 37.0%)
{'loss': 1.1495, 'grad_norm': 1.5613112449645996, 'learning_rate': 1.877121892528838e-05, 'epoch': 0.18}
{'loss': 1.0918, 'grad_norm': 1.668089509010315, 'learning_rate': 1.876822474787303e-05, 'epoch': 0.19}
{'loss': 1.1782, 'grad_norm': 1.7345718145370483, 'learning_rate': 1.8765227166330933e-05, 'epoch': 0.19}
{'loss': 1.1313, 'grad_norm': 1.6952345371246338, 'learning_rate': 1.8762226181825857e-05, 'epoch': 0.19}
{'loss': 1.1843, 'grad_norm': 1.4248790740966797, 'learning_rate': 1.875922179552288e-05, 'epoch': 0.19}
{'loss': 1.1388, 'grad_norm': 1.6032191514968872, 'learning_rate': 1.875621400858842e-05, 'epoch': 0.19}
Step 966: Updated gated ratio to 0.9628 (progress: 37.2%)
Step 966: Updated gated ratio to 0.9628 (progress: 37.2%)
Step 966: Updated gated ratio to 0.9628 (progress: 37.2%)
Step 966: Updated gated ratio to 0.9628 (progress: 37.2%)
Step 966: Updated gated ratio to 0.9628 (progress: 37.2%)
Step 966: Updated gated ratio to 0.9628 (progress: 37.2%)
Step 966: Updated gated ratio to 0.9628 (progress: 37.2%)
Step 966: Updated gated ratio to 0.9628 (progress: 37.2%)
Step 966: Updated gated ratio to 0.9628 (progress: 37.2%)
Step 966: Updated gated ratio to 0.9628 (progress: 37.2%)
Step 966: Updated gated ratio to 0.9628 (progress: 37.2%)
Step 966: Updated gated ratio to 0.9628 (progress: 37.2%)
Step 966: Updated gated ratio to 0.9628 (progress: 37.2%)
Step 966: Updated gated ratio to 0.9628 (progress: 37.2%)
Step 966: Updated gated ratio to 0.9628 (progress: 37.2%)
Step 966: Updated gated ratio to 0.9628 (progress: 37.2%)
{'loss': 1.1289, 'grad_norm': 1.4624533653259277, 'learning_rate': 1.875320282219019e-05, 'epoch': 0.19}
{'loss': 1.09, 'grad_norm': 1.6487711668014526, 'learning_rate': 1.8750188237497247e-05, 'epoch': 0.19}
{'loss': 1.1942, 'grad_norm': 1.766231656074524, 'learning_rate': 1.874717025567995e-05, 'epoch': 0.19}
{'loss': 1.0826, 'grad_norm': 1.3662508726119995, 'learning_rate': 1.874414887790999e-05, 'epoch': 0.19}
{'loss': 1.0998, 'grad_norm': 1.7150956392288208, 'learning_rate': 1.8741124105360363e-05, 'epoch': 0.19}
Step 971: Updated gated ratio to 0.9626 (progress: 37.4%)
Step 971: Updated gated ratio to 0.9626 (progress: 37.4%)
Step 971: Updated gated ratio to 0.9626 (progress: 37.4%)
Step 971: Updated gated ratio to 0.9626 (progress: 37.4%)
Step 971: Updated gated ratio to 0.9626 (progress: 37.4%)
Step 971: Updated gated ratio to 0.9626 (progress: 37.4%)
Step 971: Updated gated ratio to 0.9626 (progress: 37.4%)
Step 971: Updated gated ratio to 0.9626 (progress: 37.4%)
Step 971: Updated gated ratio to 0.9626 (progress: 37.4%)
Step 971: Updated gated ratio to 0.9626 (progress: 37.4%)
Step 971: Updated gated ratio to 0.9626 (progress: 37.4%)
Step 971: Updated gated ratio to 0.9626 (progress: 37.4%)
Step 971: Updated gated ratio to 0.9626 (progress: 37.4%)
Step 971: Updated gated ratio to 0.9626 (progress: 37.4%)
Step 971: Updated gated ratio to 0.9626 (progress: 37.4%)
Step 971: Updated gated ratio to 0.9626 (progress: 37.4%)
{'loss': 1.0972, 'grad_norm': 1.5578644275665283, 'learning_rate': 1.873809593920539e-05, 'epoch': 0.19}
{'loss': 1.1527, 'grad_norm': 1.5315840244293213, 'learning_rate': 1.8735064380620717e-05, 'epoch': 0.19}
{'loss': 1.1272, 'grad_norm': 1.7178655862808228, 'learning_rate': 1.873202943078329e-05, 'epoch': 0.19}
{'loss': 1.1187, 'grad_norm': 1.398574709892273, 'learning_rate': 1.8728991090871387e-05, 'epoch': 0.19}
{'loss': 1.0339, 'grad_norm': 1.574659824371338, 'learning_rate': 1.8725949362064596e-05, 'epoch': 0.19}
Step 976: Updated gated ratio to 0.9624 (progress: 37.6%)
Step 976: Updated gated ratio to 0.9624 (progress: 37.6%)
Step 976: Updated gated ratio to 0.9624 (progress: 37.6%)
Step 976: Updated gated ratio to 0.9624 (progress: 37.6%)
Step 976: Updated gated ratio to 0.9624 (progress: 37.6%)
Step 976: Updated gated ratio to 0.9624 (progress: 37.6%)
Step 976: Updated gated ratio to 0.9624 (progress: 37.6%)
Step 976: Updated gated ratio to 0.9624 (progress: 37.6%)
Step 976: Updated gated ratio to 0.9624 (progress: 37.6%)
Step 976: Updated gated ratio to 0.9624 (progress: 37.6%)
Step 976: Updated gated ratio to 0.9624 (progress: 37.6%)
Step 976: Updated gated ratio to 0.9624 (progress: 37.6%)
Step 976: Updated gated ratio to 0.9624 (progress: 37.6%)
Step 976: Updated gated ratio to 0.9624 (progress: 37.6%)
Step 976: Updated gated ratio to 0.9624 (progress: 37.6%)
Step 976: Updated gated ratio to 0.9624 (progress: 37.6%)
{'loss': 1.151, 'grad_norm': 1.8871078491210938, 'learning_rate': 1.8722904245543817e-05, 'epoch': 0.19}
{'loss': 1.1528, 'grad_norm': 1.602389931678772, 'learning_rate': 1.871985574249127e-05, 'epoch': 0.19}
{'loss': 1.1229, 'grad_norm': 1.6986973285675049, 'learning_rate': 1.8716803854090495e-05, 'epoch': 0.19}
{'loss': 1.1736, 'grad_norm': 1.819550633430481, 'learning_rate': 1.8713748581526334e-05, 'epoch': 0.19}
{'loss': 1.0896, 'grad_norm': 1.7713570594787598, 'learning_rate': 1.871068992598495e-05, 'epoch': 0.19}
Step 981: Updated gated ratio to 0.9622 (progress: 37.8%)
Step 981: Updated gated ratio to 0.9622 (progress: 37.8%)
Step 981: Updated gated ratio to 0.9622 (progress: 37.8%)
Step 981: Updated gated ratio to 0.9622 (progress: 37.8%)
Step 981: Updated gated ratio to 0.9622 (progress: 37.8%)
Step 981: Updated gated ratio to 0.9622 (progress: 37.8%)Step 981: Updated gated ratio to 0.9622 (progress: 37.8%)

Step 981: Updated gated ratio to 0.9622 (progress: 37.8%)Step 981: Updated gated ratio to 0.9622 (progress: 37.8%)
Step 981: Updated gated ratio to 0.9622 (progress: 37.8%)
Step 981: Updated gated ratio to 0.9622 (progress: 37.8%)
Step 981: Updated gated ratio to 0.9622 (progress: 37.8%)

Step 981: Updated gated ratio to 0.9622 (progress: 37.8%)
Step 981: Updated gated ratio to 0.9622 (progress: 37.8%)
Step 981: Updated gated ratio to 0.9622 (progress: 37.8%)
Step 981: Updated gated ratio to 0.9622 (progress: 37.8%)
{'loss': 1.2025, 'grad_norm': 1.4243589639663696, 'learning_rate': 1.8707627888653816e-05, 'epoch': 0.19}
{'loss': 1.1193, 'grad_norm': 1.560625672340393, 'learning_rate': 1.8704562470721728e-05, 'epoch': 0.19}
{'loss': 1.0672, 'grad_norm': 1.6236664056777954, 'learning_rate': 1.870149367337878e-05, 'epoch': 0.19}
Step 984: Updated gated ratio to 0.9621 (progress: 37.9%)
Step 984: Updated gated ratio to 0.9621 (progress: 37.9%)
Step 984: Updated gated ratio to 0.9621 (progress: 37.9%)Step 984: Updated gated ratio to 0.9621 (progress: 37.9%)

Step 984: Updated gated ratio to 0.9621 (progress: 37.9%)
Step 984: Updated gated ratio to 0.9621 (progress: 37.9%)
Step 984: Updated gated ratio to 0.9621 (progress: 37.9%)
Step 984: Updated gated ratio to 0.9621 (progress: 37.9%)
Step 984: Updated gated ratio to 0.9621 (progress: 37.9%)
Step 984: Updated gated ratio to 0.9621 (progress: 37.9%)
Step 984: Updated gated ratio to 0.9621 (progress: 37.9%)
Step 984: Updated gated ratio to 0.9621 (progress: 37.9%)
Step 984: Updated gated ratio to 0.9621 (progress: 37.9%)
Step 984: Updated gated ratio to 0.9621 (progress: 37.9%)
Step 984: Updated gated ratio to 0.9621 (progress: 37.9%)
Step 984: Updated gated ratio to 0.9621 (progress: 37.9%)
{'loss': 1.2333, 'grad_norm': 1.570478081703186, 'learning_rate': 1.8698421497816386e-05, 'epoch': 0.19}
{'loss': 1.2321, 'grad_norm': 1.4340989589691162, 'learning_rate': 1.869534594522727e-05, 'epoch': 0.19}
{'loss': 1.1224, 'grad_norm': 1.6423795223236084, 'learning_rate': 1.8692267016805473e-05, 'epoch': 0.19}
{'loss': 1.0827, 'grad_norm': 1.5215821266174316, 'learning_rate': 1.8689184713746333e-05, 'epoch': 0.19}
{'loss': 1.0826, 'grad_norm': 1.6038175821304321, 'learning_rate': 1.868609903724651e-05, 'epoch': 0.19}
Step 989: Updated gated ratio to 0.9619 (progress: 38.1%)
Step 989: Updated gated ratio to 0.9619 (progress: 38.1%)
Step 989: Updated gated ratio to 0.9619 (progress: 38.1%)
Step 989: Updated gated ratio to 0.9619 (progress: 38.1%)
Step 989: Updated gated ratio to 0.9619 (progress: 38.1%)
Step 989: Updated gated ratio to 0.9619 (progress: 38.1%)
Step 989: Updated gated ratio to 0.9619 (progress: 38.1%)
Step 989: Updated gated ratio to 0.9619 (progress: 38.1%)
Step 989: Updated gated ratio to 0.9619 (progress: 38.1%)
Step 989: Updated gated ratio to 0.9619 (progress: 38.1%)
Step 989: Updated gated ratio to 0.9619 (progress: 38.1%)
Step 989: Updated gated ratio to 0.9619 (progress: 38.1%)
Step 989: Updated gated ratio to 0.9619 (progress: 38.1%)
Step 989: Updated gated ratio to 0.9619 (progress: 38.1%)
Step 989: Updated gated ratio to 0.9619 (progress: 38.1%)
Step 989: Updated gated ratio to 0.9619 (progress: 38.1%)
{'loss': 1.085, 'grad_norm': 1.6652235984802246, 'learning_rate': 1.8683009988503972e-05, 'epoch': 0.19}
{'loss': 1.2091, 'grad_norm': 1.9085438251495361, 'learning_rate': 1.867991756871799e-05, 'epoch': 0.19}
{'loss': 1.1226, 'grad_norm': 1.3924617767333984, 'learning_rate': 1.867682177908915e-05, 'epoch': 0.19}
{'loss': 1.1081, 'grad_norm': 1.655180811882019, 'learning_rate': 1.867372262081934e-05, 'epoch': 0.19}
{'loss': 1.1719, 'grad_norm': 1.7266559600830078, 'learning_rate': 1.8670620095111766e-05, 'epoch': 0.19}
Step 994: Updated gated ratio to 0.9617 (progress: 38.3%)
Step 994: Updated gated ratio to 0.9617 (progress: 38.3%)
Step 994: Updated gated ratio to 0.9617 (progress: 38.3%)
Step 994: Updated gated ratio to 0.9617 (progress: 38.3%)
Step 994: Updated gated ratio to 0.9617 (progress: 38.3%)
Step 994: Updated gated ratio to 0.9617 (progress: 38.3%)
Step 994: Updated gated ratio to 0.9617 (progress: 38.3%)
Step 994: Updated gated ratio to 0.9617 (progress: 38.3%)
Step 994: Updated gated ratio to 0.9617 (progress: 38.3%)
Step 994: Updated gated ratio to 0.9617 (progress: 38.3%)
Step 994: Updated gated ratio to 0.9617 (progress: 38.3%)
Step 994: Updated gated ratio to 0.9617 (progress: 38.3%)
Step 994: Updated gated ratio to 0.9617 (progress: 38.3%)
Step 994: Updated gated ratio to 0.9617 (progress: 38.3%)
Step 994: Updated gated ratio to 0.9617 (progress: 38.3%)
Step 994: Updated gated ratio to 0.9617 (progress: 38.3%)
{'loss': 1.0521, 'grad_norm': 1.590927243232727, 'learning_rate': 1.8667514203170934e-05, 'epoch': 0.19}
{'loss': 0.9881, 'grad_norm': 1.4395772218704224, 'learning_rate': 1.8664404946202658e-05, 'epoch': 0.19}
{'loss': 1.1286, 'grad_norm': 1.606163501739502, 'learning_rate': 1.8661292325414058e-05, 'epoch': 0.19}
{'loss': 1.1099, 'grad_norm': 1.6333764791488647, 'learning_rate': 1.865817634201356e-05, 'epoch': 0.19}
{'loss': 1.1151, 'grad_norm': 1.7429862022399902, 'learning_rate': 1.8655056997210893e-05, 'epoch': 0.19}
Step 999: Updated gated ratio to 0.9615 (progress: 38.5%)
Step 999: Updated gated ratio to 0.9615 (progress: 38.5%)
Step 999: Updated gated ratio to 0.9615 (progress: 38.5%)
Step 999: Updated gated ratio to 0.9615 (progress: 38.5%)
Step 999: Updated gated ratio to 0.9615 (progress: 38.5%)
Step 999: Updated gated ratio to 0.9615 (progress: 38.5%)Step 999: Updated gated ratio to 0.9615 (progress: 38.5%)

Step 999: Updated gated ratio to 0.9615 (progress: 38.5%)
Step 999: Updated gated ratio to 0.9615 (progress: 38.5%)
Step 999: Updated gated ratio to 0.9615 (progress: 38.5%)
Step 999: Updated gated ratio to 0.9615 (progress: 38.5%)
Step 999: Updated gated ratio to 0.9615 (progress: 38.5%)
Step 999: Updated gated ratio to 0.9615 (progress: 38.5%)
Step 999: Updated gated ratio to 0.9615 (progress: 38.5%)
Step 999: Updated gated ratio to 0.9615 (progress: 38.5%)
Step 999: Updated gated ratio to 0.9615 (progress: 38.5%)
{'loss': 1.101, 'grad_norm': 1.6515811681747437, 'learning_rate': 1.8651934292217097e-05, 'epoch': 0.19}
{'loss': 1.0821, 'grad_norm': 1.6942079067230225, 'learning_rate': 1.864880822824452e-05, 'epoch': 0.19}
{'loss': 1.0967, 'grad_norm': 1.726360559463501, 'learning_rate': 1.8645678806506795e-05, 'epoch': 0.19}
{'loss': 1.0871, 'grad_norm': 1.5669655799865723, 'learning_rate': 1.864254602821888e-05, 'epoch': 0.19}
{'loss': 1.0976, 'grad_norm': 1.5749173164367676, 'learning_rate': 1.8639409894597026e-05, 'epoch': 0.19}
{'loss': 1.1296, 'grad_norm': 1.6704261302947998, 'learning_rate': 1.8636270406858786e-05, 'epoch': 0.19}
Step 1005: Updated gated ratio to 0.9613 (progress: 38.7%)
Step 1005: Updated gated ratio to 0.9613 (progress: 38.7%)
Step 1005: Updated gated ratio to 0.9613 (progress: 38.7%)
Step 1005: Updated gated ratio to 0.9613 (progress: 38.7%)
Step 1005: Updated gated ratio to 0.9613 (progress: 38.7%)Step 1005: Updated gated ratio to 0.9613 (progress: 38.7%)
Step 1005: Updated gated ratio to 0.9613 (progress: 38.7%)
Step 1005: Updated gated ratio to 0.9613 (progress: 38.7%)
Step 1005: Updated gated ratio to 0.9613 (progress: 38.7%)
Step 1005: Updated gated ratio to 0.9613 (progress: 38.7%)
Step 1005: Updated gated ratio to 0.9613 (progress: 38.7%)
Step 1005: Updated gated ratio to 0.9613 (progress: 38.7%)
Step 1005: Updated gated ratio to 0.9613 (progress: 38.7%)

Step 1005: Updated gated ratio to 0.9613 (progress: 38.7%)
Step 1005: Updated gated ratio to 0.9613 (progress: 38.7%)
Step 1005: Updated gated ratio to 0.9613 (progress: 38.7%)
{'loss': 1.1936, 'grad_norm': 1.6750664710998535, 'learning_rate': 1.8633127566223023e-05, 'epoch': 0.19}
{'loss': 1.1592, 'grad_norm': 1.6685526371002197, 'learning_rate': 1.862998137390989e-05, 'epoch': 0.19}
{'loss': 1.0533, 'grad_norm': 1.6642814874649048, 'learning_rate': 1.8626831831140845e-05, 'epoch': 0.19}
{'loss': 1.0342, 'grad_norm': 1.7488484382629395, 'learning_rate': 1.8623678939138652e-05, 'epoch': 0.19}
{'loss': 1.1086, 'grad_norm': 1.4395604133605957, 'learning_rate': 1.8620522699127374e-05, 'epoch': 0.19}
Step 1010: Updated gated ratio to 0.9611 (progress: 38.9%)
Step 1010: Updated gated ratio to 0.9611 (progress: 38.9%)
Step 1010: Updated gated ratio to 0.9611 (progress: 38.9%)
Step 1010: Updated gated ratio to 0.9611 (progress: 38.9%)
Step 1010: Updated gated ratio to 0.9611 (progress: 38.9%)
Step 1010: Updated gated ratio to 0.9611 (progress: 38.9%)
Step 1010: Updated gated ratio to 0.9611 (progress: 38.9%)
Step 1010: Updated gated ratio to 0.9611 (progress: 38.9%)
Step 1010: Updated gated ratio to 0.9611 (progress: 38.9%)
Step 1010: Updated gated ratio to 0.9611 (progress: 38.9%)
Step 1010: Updated gated ratio to 0.9611 (progress: 38.9%)
Step 1010: Updated gated ratio to 0.9611 (progress: 38.9%)
Step 1010: Updated gated ratio to 0.9611 (progress: 38.9%)
Step 1010: Updated gated ratio to 0.9611 (progress: 38.9%)
Step 1010: Updated gated ratio to 0.9611 (progress: 38.9%)
Step 1010: Updated gated ratio to 0.9611 (progress: 38.9%)
{'loss': 0.9945, 'grad_norm': 1.5917876958847046, 'learning_rate': 1.8617363112332376e-05, 'epoch': 0.19}
{'loss': 1.1497, 'grad_norm': 1.7982333898544312, 'learning_rate': 1.8614200179980307e-05, 'epoch': 0.19}
{'loss': 1.1696, 'grad_norm': 1.836512804031372, 'learning_rate': 1.8611033903299136e-05, 'epoch': 0.19}
{'loss': 1.0519, 'grad_norm': 1.7268716096878052, 'learning_rate': 1.8607864283518116e-05, 'epoch': 0.2}
{'loss': 1.1746, 'grad_norm': 1.4257899522781372, 'learning_rate': 1.8604691321867804e-05, 'epoch': 0.2}
Step 1015: Updated gated ratio to 0.9609 (progress: 39.1%)
Step 1015: Updated gated ratio to 0.9609 (progress: 39.1%)
Step 1015: Updated gated ratio to 0.9609 (progress: 39.1%)
Step 1015: Updated gated ratio to 0.9609 (progress: 39.1%)
Step 1015: Updated gated ratio to 0.9609 (progress: 39.1%)
Step 1015: Updated gated ratio to 0.9609 (progress: 39.1%)
Step 1015: Updated gated ratio to 0.9609 (progress: 39.1%)
Step 1015: Updated gated ratio to 0.9609 (progress: 39.1%)
Step 1015: Updated gated ratio to 0.9609 (progress: 39.1%)
Step 1015: Updated gated ratio to 0.9609 (progress: 39.1%)
Step 1015: Updated gated ratio to 0.9609 (progress: 39.1%)
Step 1015: Updated gated ratio to 0.9609 (progress: 39.1%)
Step 1015: Updated gated ratio to 0.9609 (progress: 39.1%)
Step 1015: Updated gated ratio to 0.9609 (progress: 39.1%)
Step 1015: Updated gated ratio to 0.9609 (progress: 39.1%)
Step 1015: Updated gated ratio to 0.9609 (progress: 39.1%)
{'loss': 1.0912, 'grad_norm': 1.7084616422653198, 'learning_rate': 1.8601515019580053e-05, 'epoch': 0.2}
{'loss': 1.0884, 'grad_norm': 1.5839449167251587, 'learning_rate': 1.8598335377888012e-05, 'epoch': 0.2}
{'loss': 1.0976, 'grad_norm': 1.748047947883606, 'learning_rate': 1.8595152398026128e-05, 'epoch': 0.2}
{'loss': 1.0855, 'grad_norm': 1.747301697731018, 'learning_rate': 1.8591966081230142e-05, 'epoch': 0.2}
{'loss': 1.0567, 'grad_norm': 1.7019678354263306, 'learning_rate': 1.8588776428737095e-05, 'epoch': 0.2}
Step 1020: Updated gated ratio to 0.9607 (progress: 39.3%)
Step 1020: Updated gated ratio to 0.9607 (progress: 39.3%)
Step 1020: Updated gated ratio to 0.9607 (progress: 39.3%)
Step 1020: Updated gated ratio to 0.9607 (progress: 39.3%)
Step 1020: Updated gated ratio to 0.9607 (progress: 39.3%)Step 1020: Updated gated ratio to 0.9607 (progress: 39.3%)
Step 1020: Updated gated ratio to 0.9607 (progress: 39.3%)
Step 1020: Updated gated ratio to 0.9607 (progress: 39.3%)Step 1020: Updated gated ratio to 0.9607 (progress: 39.3%)

Step 1020: Updated gated ratio to 0.9607 (progress: 39.3%)
Step 1020: Updated gated ratio to 0.9607 (progress: 39.3%)

Step 1020: Updated gated ratio to 0.9607 (progress: 39.3%)
Step 1020: Updated gated ratio to 0.9607 (progress: 39.3%)
Step 1020: Updated gated ratio to 0.9607 (progress: 39.3%)
Step 1020: Updated gated ratio to 0.9607 (progress: 39.3%)
Step 1020: Updated gated ratio to 0.9607 (progress: 39.3%)
{'loss': 1.1086, 'grad_norm': 1.7545424699783325, 'learning_rate': 1.858558344178532e-05, 'epoch': 0.2}
{'loss': 1.0076, 'grad_norm': 1.7260503768920898, 'learning_rate': 1.8582387121614437e-05, 'epoch': 0.2}
{'loss': 1.1247, 'grad_norm': 1.4103540182113647, 'learning_rate': 1.857918746946538e-05, 'epoch': 0.2}
{'loss': 1.0979, 'grad_norm': 1.9164531230926514, 'learning_rate': 1.8575984486580353e-05, 'epoch': 0.2}
{'loss': 1.2082, 'grad_norm': 1.8451672792434692, 'learning_rate': 1.857277817420287e-05, 'epoch': 0.2}
Step 1025: Updated gated ratio to 0.9605 (progress: 39.5%)
Step 1025: Updated gated ratio to 0.9605 (progress: 39.5%)Step 1025: Updated gated ratio to 0.9605 (progress: 39.5%)
Step 1025: Updated gated ratio to 0.9605 (progress: 39.5%)
Step 1025: Updated gated ratio to 0.9605 (progress: 39.5%)
Step 1025: Updated gated ratio to 0.9605 (progress: 39.5%)
Step 1025: Updated gated ratio to 0.9605 (progress: 39.5%)
Step 1025: Updated gated ratio to 0.9605 (progress: 39.5%)
Step 1025: Updated gated ratio to 0.9605 (progress: 39.5%)
Step 1025: Updated gated ratio to 0.9605 (progress: 39.5%)
Step 1025: Updated gated ratio to 0.9605 (progress: 39.5%)
Step 1025: Updated gated ratio to 0.9605 (progress: 39.5%)

Step 1025: Updated gated ratio to 0.9605 (progress: 39.5%)
Step 1025: Updated gated ratio to 0.9605 (progress: 39.5%)
Step 1025: Updated gated ratio to 0.9605 (progress: 39.5%)
Step 1025: Updated gated ratio to 0.9605 (progress: 39.5%)
{'loss': 1.0338, 'grad_norm': 1.560058355331421, 'learning_rate': 1.8569568533577727e-05, 'epoch': 0.2}
{'loss': 1.1604, 'grad_norm': 1.7751691341400146, 'learning_rate': 1.8566355565951023e-05, 'epoch': 0.2}
{'loss': 1.0635, 'grad_norm': 1.515498399734497, 'learning_rate': 1.8563139272570142e-05, 'epoch': 0.2}
{'loss': 1.1243, 'grad_norm': 1.8396598100662231, 'learning_rate': 1.8559919654683756e-05, 'epoch': 0.2}
{'loss': 1.2014, 'grad_norm': 1.7095130681991577, 'learning_rate': 1.8556696713541833e-05, 'epoch': 0.2}
{'loss': 1.1868, 'grad_norm': 1.7395148277282715, 'learning_rate': 1.855347045039563e-05, 'epoch': 0.2}
Step 1031: Updated gated ratio to 0.9603 (progress: 39.7%)
Step 1031: Updated gated ratio to 0.9603 (progress: 39.7%)
Step 1031: Updated gated ratio to 0.9603 (progress: 39.7%)
Step 1031: Updated gated ratio to 0.9603 (progress: 39.7%)
Step 1031: Updated gated ratio to 0.9603 (progress: 39.7%)
Step 1031: Updated gated ratio to 0.9603 (progress: 39.7%)
Step 1031: Updated gated ratio to 0.9603 (progress: 39.7%)
Step 1031: Updated gated ratio to 0.9603 (progress: 39.7%)
Step 1031: Updated gated ratio to 0.9603 (progress: 39.7%)
Step 1031: Updated gated ratio to 0.9603 (progress: 39.7%)
Step 1031: Updated gated ratio to 0.9603 (progress: 39.7%)
Step 1031: Updated gated ratio to 0.9603 (progress: 39.7%)
Step 1031: Updated gated ratio to 0.9603 (progress: 39.7%)
Step 1031: Updated gated ratio to 0.9603 (progress: 39.7%)
Step 1031: Updated gated ratio to 0.9603 (progress: 39.7%)
Step 1031: Updated gated ratio to 0.9603 (progress: 39.7%)
{'loss': 1.1256, 'grad_norm': 1.7388312816619873, 'learning_rate': 1.8550240866497697e-05, 'epoch': 0.2}
{'loss': 1.0511, 'grad_norm': 1.6976313591003418, 'learning_rate': 1.854700796310186e-05, 'epoch': 0.2}
{'loss': 1.0209, 'grad_norm': 1.609642505645752, 'learning_rate': 1.8543771741463254e-05, 'epoch': 0.2}
{'loss': 1.1462, 'grad_norm': 1.597680687904358, 'learning_rate': 1.8540532202838286e-05, 'epoch': 0.2}
{'loss': 1.0889, 'grad_norm': 1.5969926118850708, 'learning_rate': 1.8537289348484658e-05, 'epoch': 0.2}
Step 1036: Updated gated ratio to 0.9601 (progress: 39.9%)
Step 1036: Updated gated ratio to 0.9601 (progress: 39.9%)
Step 1036: Updated gated ratio to 0.9601 (progress: 39.9%)
Step 1036: Updated gated ratio to 0.9601 (progress: 39.9%)
Step 1036: Updated gated ratio to 0.9601 (progress: 39.9%)
Step 1036: Updated gated ratio to 0.9601 (progress: 39.9%)
Step 1036: Updated gated ratio to 0.9601 (progress: 39.9%)
Step 1036: Updated gated ratio to 0.9601 (progress: 39.9%)
Step 1036: Updated gated ratio to 0.9601 (progress: 39.9%)
Step 1036: Updated gated ratio to 0.9601 (progress: 39.9%)
Step 1036: Updated gated ratio to 0.9601 (progress: 39.9%)
Step 1036: Updated gated ratio to 0.9601 (progress: 39.9%)
Step 1036: Updated gated ratio to 0.9601 (progress: 39.9%)
Step 1036: Updated gated ratio to 0.9601 (progress: 39.9%)
Step 1036: Updated gated ratio to 0.9601 (progress: 39.9%)
Step 1036: Updated gated ratio to 0.9601 (progress: 39.9%)
{'loss': 1.2095, 'grad_norm': 1.8508174419403076, 'learning_rate': 1.8534043179661357e-05, 'epoch': 0.2}
{'loss': 1.1742, 'grad_norm': 1.7730178833007812, 'learning_rate': 1.8530793697628658e-05, 'epoch': 0.2}
{'loss': 1.0071, 'grad_norm': 1.7040410041809082, 'learning_rate': 1.8527540903648122e-05, 'epoch': 0.2}
{'loss': 1.0462, 'grad_norm': 1.685325026512146, 'learning_rate': 1.8524284798982595e-05, 'epoch': 0.2}
{'loss': 1.1317, 'grad_norm': 1.6401253938674927, 'learning_rate': 1.852102538489621e-05, 'epoch': 0.2}
Step 1041: Updated gated ratio to 0.9599 (progress: 40.1%)
Step 1041: Updated gated ratio to 0.9599 (progress: 40.1%)
Step 1041: Updated gated ratio to 0.9599 (progress: 40.1%)
Step 1041: Updated gated ratio to 0.9599 (progress: 40.1%)
Step 1041: Updated gated ratio to 0.9599 (progress: 40.1%)
Step 1041: Updated gated ratio to 0.9599 (progress: 40.1%)
Step 1041: Updated gated ratio to 0.9599 (progress: 40.1%)
Step 1041: Updated gated ratio to 0.9599 (progress: 40.1%)Step 1041: Updated gated ratio to 0.9599 (progress: 40.1%)

Step 1041: Updated gated ratio to 0.9599 (progress: 40.1%)
Step 1041: Updated gated ratio to 0.9599 (progress: 40.1%)
Step 1041: Updated gated ratio to 0.9599 (progress: 40.1%)
Step 1041: Updated gated ratio to 0.9599 (progress: 40.1%)
Step 1041: Updated gated ratio to 0.9599 (progress: 40.1%)
Step 1041: Updated gated ratio to 0.9599 (progress: 40.1%)
Step 1041: Updated gated ratio to 0.9599 (progress: 40.1%)
{'loss': 1.1553, 'grad_norm': 1.4038615226745605, 'learning_rate': 1.8517762662654383e-05, 'epoch': 0.2}
{'loss': 1.1778, 'grad_norm': 1.7675913572311401, 'learning_rate': 1.851449663352381e-05, 'epoch': 0.2}
{'loss': 1.0836, 'grad_norm': 1.576005220413208, 'learning_rate': 1.851122729877249e-05, 'epoch': 0.2}
{'loss': 1.1383, 'grad_norm': 1.436442494392395, 'learning_rate': 1.8507954659669677e-05, 'epoch': 0.2}
Step 1046: Updated gated ratio to 0.9597 (progress: 40.3%)
Step 1046: Updated gated ratio to 0.9597 (progress: 40.3%)
Step 1046: Updated gated ratio to 0.9597 (progress: 40.3%)
Step 1046: Updated gated ratio to 0.9597 (progress: 40.3%)
Step 1046: Updated gated ratio to 0.9597 (progress: 40.3%)
Step 1046: Updated gated ratio to 0.9597 (progress: 40.3%)
Step 1046: Updated gated ratio to 0.9597 (progress: 40.3%)
Step 1046: Updated gated ratio to 0.9597 (progress: 40.3%)
Step 1046: Updated gated ratio to 0.9597 (progress: 40.3%)
Step 1046: Updated gated ratio to 0.9597 (progress: 40.3%)
Step 1046: Updated gated ratio to 0.9597 (progress: 40.3%)
Step 1046: Updated gated ratio to 0.9597 (progress: 40.3%)
Step 1046: Updated gated ratio to 0.9597 (progress: 40.3%)
Step 1046: Updated gated ratio to 0.9597 (progress: 40.3%)
Step 1046: Updated gated ratio to 0.9597 (progress: 40.3%)
{'loss': 1.0927, 'grad_norm': 1.6918379068374634, 'learning_rate': 1.850467871748593e-05, 'epoch': 0.2}
Step 1046: Updated gated ratio to 0.9597 (progress: 40.3%)
{'loss': 1.1774, 'grad_norm': 1.3377429246902466, 'learning_rate': 1.850139947349308e-05, 'epoch': 0.2}
{'loss': 1.1532, 'grad_norm': 1.731475591659546, 'learning_rate': 1.8498116928964244e-05, 'epoch': 0.2}
{'loss': 1.0892, 'grad_norm': 1.5096995830535889, 'learning_rate': 1.849483108517381e-05, 'epoch': 0.2}
{'loss': 1.1139, 'grad_norm': 1.9194552898406982, 'learning_rate': 1.849154194339747e-05, 'epoch': 0.2}
{'loss': 1.1072, 'grad_norm': 1.6654878854751587, 'learning_rate': 1.8488249504912173e-05, 'epoch': 0.2}
Step 1051: Updated gated ratio to 0.9595 (progress: 40.5%)
Step 1051: Updated gated ratio to 0.9595 (progress: 40.5%)
Step 1051: Updated gated ratio to 0.9595 (progress: 40.5%)
Step 1051: Updated gated ratio to 0.9595 (progress: 40.5%)
Step 1051: Updated gated ratio to 0.9595 (progress: 40.5%)
Step 1051: Updated gated ratio to 0.9595 (progress: 40.5%)
Step 1051: Updated gated ratio to 0.9595 (progress: 40.5%)
Step 1051: Updated gated ratio to 0.9595 (progress: 40.5%)
Step 1051: Updated gated ratio to 0.9595 (progress: 40.5%)
Step 1051: Updated gated ratio to 0.9595 (progress: 40.5%)
Step 1051: Updated gated ratio to 0.9595 (progress: 40.5%)
Step 1051: Updated gated ratio to 0.9595 (progress: 40.5%)
Step 1051: Updated gated ratio to 0.9595 (progress: 40.5%)
Step 1051: Updated gated ratio to 0.9595 (progress: 40.5%)
Step 1051: Updated gated ratio to 0.9595 (progress: 40.5%)
Step 1051: Updated gated ratio to 0.9595 (progress: 40.5%)
{'loss': 1.116, 'grad_norm': 1.7705366611480713, 'learning_rate': 1.8484953770996163e-05, 'epoch': 0.2}
{'loss': 1.1088, 'grad_norm': 1.6221083402633667, 'learning_rate': 1.848165474292895e-05, 'epoch': 0.2}
{'loss': 1.0247, 'grad_norm': 1.5402635335922241, 'learning_rate': 1.8478352421991334e-05, 'epoch': 0.2}
{'loss': 1.0936, 'grad_norm': 1.5449714660644531, 'learning_rate': 1.847504680946539e-05, 'epoch': 0.2}
{'loss': 1.1336, 'grad_norm': 1.7673922777175903, 'learning_rate': 1.847173790663447e-05, 'epoch': 0.2}
{'loss': 1.136, 'grad_norm': 1.6627695560455322, 'learning_rate': 1.8468425714783206e-05, 'epoch': 0.2}
Step 1057: Updated gated ratio to 0.9593 (progress: 40.7%)
Step 1057: Updated gated ratio to 0.9593 (progress: 40.7%)
Step 1057: Updated gated ratio to 0.9593 (progress: 40.7%)
Step 1057: Updated gated ratio to 0.9593 (progress: 40.7%)
Step 1057: Updated gated ratio to 0.9593 (progress: 40.7%)
Step 1057: Updated gated ratio to 0.9593 (progress: 40.7%)
Step 1057: Updated gated ratio to 0.9593 (progress: 40.7%)
Step 1057: Updated gated ratio to 0.9593 (progress: 40.7%)
Step 1057: Updated gated ratio to 0.9593 (progress: 40.7%)
Step 1057: Updated gated ratio to 0.9593 (progress: 40.7%)Step 1057: Updated gated ratio to 0.9593 (progress: 40.7%)

Step 1057: Updated gated ratio to 0.9593 (progress: 40.7%)
Step 1057: Updated gated ratio to 0.9593 (progress: 40.7%)
Step 1057: Updated gated ratio to 0.9593 (progress: 40.7%)
Step 1057: Updated gated ratio to 0.9593 (progress: 40.7%)
Step 1057: Updated gated ratio to 0.9593 (progress: 40.7%)
{'loss': 1.1584, 'grad_norm': 1.7022666931152344, 'learning_rate': 1.84651102351975e-05, 'epoch': 0.2}
{'loss': 1.1424, 'grad_norm': 1.3491629362106323, 'learning_rate': 1.846179146916454e-05, 'epoch': 0.2}
{'loss': 1.0791, 'grad_norm': 1.3694097995758057, 'learning_rate': 1.8458469417972783e-05, 'epoch': 0.2}
{'loss': 1.1188, 'grad_norm': 1.616645336151123, 'learning_rate': 1.8455144082911965e-05, 'epoch': 0.2}
{'loss': 1.0921, 'grad_norm': 1.6048308610916138, 'learning_rate': 1.8451815465273097e-05, 'epoch': 0.2}
Step 1062: Updated gated ratio to 0.9591 (progress: 40.9%)
Step 1062: Updated gated ratio to 0.9591 (progress: 40.9%)
Step 1062: Updated gated ratio to 0.9591 (progress: 40.9%)
Step 1062: Updated gated ratio to 0.9591 (progress: 40.9%)
Step 1062: Updated gated ratio to 0.9591 (progress: 40.9%)
Step 1062: Updated gated ratio to 0.9591 (progress: 40.9%)
Step 1062: Updated gated ratio to 0.9591 (progress: 40.9%)
Step 1062: Updated gated ratio to 0.9591 (progress: 40.9%)
Step 1062: Updated gated ratio to 0.9591 (progress: 40.9%)
Step 1062: Updated gated ratio to 0.9591 (progress: 40.9%)
Step 1062: Updated gated ratio to 0.9591 (progress: 40.9%)
Step 1062: Updated gated ratio to 0.9591 (progress: 40.9%)
Step 1062: Updated gated ratio to 0.9591 (progress: 40.9%)
Step 1062: Updated gated ratio to 0.9591 (progress: 40.9%)
Step 1062: Updated gated ratio to 0.9591 (progress: 40.9%)
Step 1062: Updated gated ratio to 0.9591 (progress: 40.9%)
{'loss': 1.1133, 'grad_norm': 1.4756367206573486, 'learning_rate': 1.8448483566348456e-05, 'epoch': 0.2}
{'loss': 1.0918, 'grad_norm': 1.7445076704025269, 'learning_rate': 1.8445148387431605e-05, 'epoch': 0.2}
{'loss': 1.1751, 'grad_norm': 1.6471478939056396, 'learning_rate': 1.8441809929817382e-05, 'epoch': 0.2}
{'loss': 1.0528, 'grad_norm': 1.6859629154205322, 'learning_rate': 1.8438468194801876e-05, 'epoch': 0.21}
{'loss': 1.1894, 'grad_norm': 1.7108513116836548, 'learning_rate': 1.8435123183682475e-05, 'epoch': 0.21}
Step 1067: Updated gated ratio to 0.9589 (progress: 41.1%)
Step 1067: Updated gated ratio to 0.9589 (progress: 41.1%)
Step 1067: Updated gated ratio to 0.9589 (progress: 41.1%)
Step 1067: Updated gated ratio to 0.9589 (progress: 41.1%)
Step 1067: Updated gated ratio to 0.9589 (progress: 41.1%)
Step 1067: Updated gated ratio to 0.9589 (progress: 41.1%)
Step 1067: Updated gated ratio to 0.9589 (progress: 41.1%)
Step 1067: Updated gated ratio to 0.9589 (progress: 41.1%)
Step 1067: Updated gated ratio to 0.9589 (progress: 41.1%)
Step 1067: Updated gated ratio to 0.9589 (progress: 41.1%)
Step 1067: Updated gated ratio to 0.9589 (progress: 41.1%)
Step 1067: Updated gated ratio to 0.9589 (progress: 41.1%)
Step 1067: Updated gated ratio to 0.9589 (progress: 41.1%)
Step 1067: Updated gated ratio to 0.9589 (progress: 41.1%)
Step 1067: Updated gated ratio to 0.9589 (progress: 41.1%)
Step 1067: Updated gated ratio to 0.9589 (progress: 41.1%)
{'loss': 1.1358, 'grad_norm': 1.6286718845367432, 'learning_rate': 1.8431774897757824e-05, 'epoch': 0.21}
{'loss': 1.2041, 'grad_norm': 1.4467291831970215, 'learning_rate': 1.8428423338327847e-05, 'epoch': 0.21}
{'loss': 1.2262, 'grad_norm': 1.7013931274414062, 'learning_rate': 1.8425068506693727e-05, 'epoch': 0.21}
{'loss': 1.1313, 'grad_norm': 1.7914127111434937, 'learning_rate': 1.842171040415793e-05, 'epoch': 0.21}
{'loss': 1.133, 'grad_norm': 1.5948481559753418, 'learning_rate': 1.8418349032024185e-05, 'epoch': 0.21}
Step 1072: Updated gated ratio to 0.9587 (progress: 41.3%)
Step 1072: Updated gated ratio to 0.9587 (progress: 41.3%)
Step 1072: Updated gated ratio to 0.9587 (progress: 41.3%)
Step 1072: Updated gated ratio to 0.9587 (progress: 41.3%)
Step 1072: Updated gated ratio to 0.9587 (progress: 41.3%)
Step 1072: Updated gated ratio to 0.9587 (progress: 41.3%)Step 1072: Updated gated ratio to 0.9587 (progress: 41.3%)

Step 1072: Updated gated ratio to 0.9587 (progress: 41.3%)
Step 1072: Updated gated ratio to 0.9587 (progress: 41.3%)
Step 1072: Updated gated ratio to 0.9587 (progress: 41.3%)
Step 1072: Updated gated ratio to 0.9587 (progress: 41.3%)
Step 1072: Updated gated ratio to 0.9587 (progress: 41.3%)
Step 1072: Updated gated ratio to 0.9587 (progress: 41.3%)
Step 1072: Updated gated ratio to 0.9587 (progress: 41.3%)
Step 1072: Updated gated ratio to 0.9587 (progress: 41.3%)
Step 1072: Updated gated ratio to 0.9587 (progress: 41.3%)
{'loss': 1.1383, 'grad_norm': 1.6130070686340332, 'learning_rate': 1.8414984391597492e-05, 'epoch': 0.21}
{'loss': 1.1506, 'grad_norm': 1.6516938209533691, 'learning_rate': 1.8411616484184126e-05, 'epoch': 0.21}
{'loss': 1.1257, 'grad_norm': 1.591596245765686, 'learning_rate': 1.8408245311091618e-05, 'epoch': 0.21}
{'loss': 1.087, 'grad_norm': 1.5656147003173828, 'learning_rate': 1.8404870873628774e-05, 'epoch': 0.21}
{'loss': 1.1822, 'grad_norm': 1.413805603981018, 'learning_rate': 1.8401493173105675e-05, 'epoch': 0.21}
Step 1077: Updated gated ratio to 0.9585 (progress: 41.5%)
Step 1077: Updated gated ratio to 0.9585 (progress: 41.5%)
Step 1077: Updated gated ratio to 0.9585 (progress: 41.5%)
Step 1077: Updated gated ratio to 0.9585 (progress: 41.5%)
Step 1077: Updated gated ratio to 0.9585 (progress: 41.5%)
Step 1077: Updated gated ratio to 0.9585 (progress: 41.5%)
Step 1077: Updated gated ratio to 0.9585 (progress: 41.5%)
Step 1077: Updated gated ratio to 0.9585 (progress: 41.5%)
Step 1077: Updated gated ratio to 0.9585 (progress: 41.5%)Step 1077: Updated gated ratio to 0.9585 (progress: 41.5%)
Step 1077: Updated gated ratio to 0.9585 (progress: 41.5%)
Step 1077: Updated gated ratio to 0.9585 (progress: 41.5%)

Step 1077: Updated gated ratio to 0.9585 (progress: 41.5%)
Step 1077: Updated gated ratio to 0.9585 (progress: 41.5%)
Step 1077: Updated gated ratio to 0.9585 (progress: 41.5%)
Step 1077: Updated gated ratio to 0.9585 (progress: 41.5%)
{'loss': 1.0946, 'grad_norm': 1.7102552652359009, 'learning_rate': 1.8398112210833648e-05, 'epoch': 0.21}
{'loss': 1.1405, 'grad_norm': 1.600937843322754, 'learning_rate': 1.8394727988125308e-05, 'epoch': 0.21}
{'loss': 1.1219, 'grad_norm': 1.6506937742233276, 'learning_rate': 1.8391340506294524e-05, 'epoch': 0.21}
{'loss': 1.1734, 'grad_norm': 1.6643881797790527, 'learning_rate': 1.8387949766656434e-05, 'epoch': 0.21}
{'loss': 1.1504, 'grad_norm': 1.4266903400421143, 'learning_rate': 1.8384555770527438e-05, 'epoch': 0.21}
{'loss': 1.095, 'grad_norm': 1.6909668445587158, 'learning_rate': 1.8381158519225204e-05, 'epoch': 0.21}
Step 1083: Updated gated ratio to 0.9583 (progress: 41.7%)
Step 1083: Updated gated ratio to 0.9583 (progress: 41.7%)
Step 1083: Updated gated ratio to 0.9583 (progress: 41.7%)
Step 1083: Updated gated ratio to 0.9583 (progress: 41.7%)
Step 1083: Updated gated ratio to 0.9583 (progress: 41.7%)Step 1083: Updated gated ratio to 0.9583 (progress: 41.7%)

Step 1083: Updated gated ratio to 0.9583 (progress: 41.7%)
Step 1083: Updated gated ratio to 0.9583 (progress: 41.7%)
Step 1083: Updated gated ratio to 0.9583 (progress: 41.7%)
Step 1083: Updated gated ratio to 0.9583 (progress: 41.7%)
Step 1083: Updated gated ratio to 0.9583 (progress: 41.7%)Step 1083: Updated gated ratio to 0.9583 (progress: 41.7%)

Step 1083: Updated gated ratio to 0.9583 (progress: 41.7%)
Step 1083: Updated gated ratio to 0.9583 (progress: 41.7%)
Step 1083: Updated gated ratio to 0.9583 (progress: 41.7%)
Step 1083: Updated gated ratio to 0.9583 (progress: 41.7%)
{'loss': 1.1277, 'grad_norm': 1.4890987873077393, 'learning_rate': 1.8377758014068662e-05, 'epoch': 0.21}
{'loss': 1.1291, 'grad_norm': 1.7763134241104126, 'learning_rate': 1.8374354256378e-05, 'epoch': 0.21}
{'loss': 1.004, 'grad_norm': 1.5800856351852417, 'learning_rate': 1.837094724747468e-05, 'epoch': 0.21}
{'loss': 1.1312, 'grad_norm': 1.7056264877319336, 'learning_rate': 1.8367536988681422e-05, 'epoch': 0.21}
{'loss': 1.135, 'grad_norm': 1.3529736995697021, 'learning_rate': 1.83641234813222e-05, 'epoch': 0.21}
Step 1088: Updated gated ratio to 0.9581 (progress: 41.9%)
Step 1088: Updated gated ratio to 0.9581 (progress: 41.9%)
Step 1088: Updated gated ratio to 0.9581 (progress: 41.9%)
Step 1088: Updated gated ratio to 0.9581 (progress: 41.9%)
Step 1088: Updated gated ratio to 0.9581 (progress: 41.9%)
Step 1088: Updated gated ratio to 0.9581 (progress: 41.9%)Step 1088: Updated gated ratio to 0.9581 (progress: 41.9%)
Step 1088: Updated gated ratio to 0.9581 (progress: 41.9%)
Step 1088: Updated gated ratio to 0.9581 (progress: 41.9%)
Step 1088: Updated gated ratio to 0.9581 (progress: 41.9%)

Step 1088: Updated gated ratio to 0.9581 (progress: 41.9%)
Step 1088: Updated gated ratio to 0.9581 (progress: 41.9%)
Step 1088: Updated gated ratio to 0.9581 (progress: 41.9%)
Step 1088: Updated gated ratio to 0.9581 (progress: 41.9%)
Step 1088: Updated gated ratio to 0.9581 (progress: 41.9%)
Step 1088: Updated gated ratio to 0.9581 (progress: 41.9%)
{'loss': 1.1699, 'grad_norm': 1.5600100755691528, 'learning_rate': 1.8360706726722253e-05, 'epoch': 0.21}
{'loss': 1.0791, 'grad_norm': 1.5637236833572388, 'learning_rate': 1.835728672620809e-05, 'epoch': 0.21}
{'loss': 1.2475, 'grad_norm': 1.4475595951080322, 'learning_rate': 1.8353863481107473e-05, 'epoch': 0.21}
{'loss': 1.0509, 'grad_norm': 1.5483777523040771, 'learning_rate': 1.835043699274942e-05, 'epoch': 0.21}
{'loss': 1.0968, 'grad_norm': 1.548158884048462, 'learning_rate': 1.8347007262464206e-05, 'epoch': 0.21}
Step 1093: Updated gated ratio to 0.9579 (progress: 42.1%)
Step 1093: Updated gated ratio to 0.9579 (progress: 42.1%)
Step 1093: Updated gated ratio to 0.9579 (progress: 42.1%)
Step 1093: Updated gated ratio to 0.9579 (progress: 42.1%)Step 1093: Updated gated ratio to 0.9579 (progress: 42.1%)

Step 1093: Updated gated ratio to 0.9579 (progress: 42.1%)
Step 1093: Updated gated ratio to 0.9579 (progress: 42.1%)
Step 1093: Updated gated ratio to 0.9579 (progress: 42.1%)
Step 1093: Updated gated ratio to 0.9579 (progress: 42.1%)Step 1093: Updated gated ratio to 0.9579 (progress: 42.1%)
Step 1093: Updated gated ratio to 0.9579 (progress: 42.1%)
Step 1093: Updated gated ratio to 0.9579 (progress: 42.1%)
Step 1093: Updated gated ratio to 0.9579 (progress: 42.1%)
Step 1093: Updated gated ratio to 0.9579 (progress: 42.1%)

Step 1093: Updated gated ratio to 0.9579 (progress: 42.1%)
Step 1093: Updated gated ratio to 0.9579 (progress: 42.1%)
{'loss': 1.1646, 'grad_norm': 1.523252248764038, 'learning_rate': 1.8343574291583385e-05, 'epoch': 0.21}
{'loss': 1.1119, 'grad_norm': 1.5334231853485107, 'learning_rate': 1.8340138081439743e-05, 'epoch': 0.21}
{'loss': 1.1871, 'grad_norm': 1.3982722759246826, 'learning_rate': 1.833669863336734e-05, 'epoch': 0.21}
{'loss': 1.1764, 'grad_norm': 1.7030268907546997, 'learning_rate': 1.833325594870148e-05, 'epoch': 0.21}
{'loss': 1.1703, 'grad_norm': 1.6202213764190674, 'learning_rate': 1.8329810028778747e-05, 'epoch': 0.21}
Step 1098: Updated gated ratio to 0.9577 (progress: 42.3%)
Step 1098: Updated gated ratio to 0.9577 (progress: 42.3%)
Step 1098: Updated gated ratio to 0.9577 (progress: 42.3%)
Step 1098: Updated gated ratio to 0.9577 (progress: 42.3%)
Step 1098: Updated gated ratio to 0.9577 (progress: 42.3%)
Step 1098: Updated gated ratio to 0.9577 (progress: 42.3%)
Step 1098: Updated gated ratio to 0.9577 (progress: 42.3%)
Step 1098: Updated gated ratio to 0.9577 (progress: 42.3%)
Step 1098: Updated gated ratio to 0.9577 (progress: 42.3%)
Step 1098: Updated gated ratio to 0.9577 (progress: 42.3%)
Step 1098: Updated gated ratio to 0.9577 (progress: 42.3%)
Step 1098: Updated gated ratio to 0.9577 (progress: 42.3%)
Step 1098: Updated gated ratio to 0.9577 (progress: 42.3%)
Step 1098: Updated gated ratio to 0.9577 (progress: 42.3%)
Step 1098: Updated gated ratio to 0.9577 (progress: 42.3%)
Step 1098: Updated gated ratio to 0.9577 (progress: 42.3%)
{'loss': 1.0964, 'grad_norm': 1.4853076934814453, 'learning_rate': 1.8326360874936952e-05, 'epoch': 0.21}
{'loss': 1.1547, 'grad_norm': 1.6580034494400024, 'learning_rate': 1.8322908488515182e-05, 'epoch': 0.21}
{'loss': 1.0626, 'grad_norm': 1.5054317712783813, 'learning_rate': 1.8319452870853772e-05, 'epoch': 0.21}
{'loss': 1.1714, 'grad_norm': 1.7228906154632568, 'learning_rate': 1.8315994023294306e-05, 'epoch': 0.21}
{'loss': 1.0392, 'grad_norm': 1.5842128992080688, 'learning_rate': 1.8312531947179634e-05, 'epoch': 0.21}
Step 1103: Updated gated ratio to 0.9575 (progress: 42.5%)
Step 1103: Updated gated ratio to 0.9575 (progress: 42.5%)
Step 1103: Updated gated ratio to 0.9575 (progress: 42.5%)
Step 1103: Updated gated ratio to 0.9575 (progress: 42.5%)
Step 1103: Updated gated ratio to 0.9575 (progress: 42.5%)
Step 1103: Updated gated ratio to 0.9575 (progress: 42.5%)
Step 1103: Updated gated ratio to 0.9575 (progress: 42.5%)
Step 1103: Updated gated ratio to 0.9575 (progress: 42.5%)
Step 1103: Updated gated ratio to 0.9575 (progress: 42.5%)
Step 1103: Updated gated ratio to 0.9575 (progress: 42.5%)
Step 1103: Updated gated ratio to 0.9575 (progress: 42.5%)Step 1103: Updated gated ratio to 0.9575 (progress: 42.5%)
Step 1103: Updated gated ratio to 0.9575 (progress: 42.5%)

Step 1103: Updated gated ratio to 0.9575 (progress: 42.5%)
Step 1103: Updated gated ratio to 0.9575 (progress: 42.5%)
Step 1103: Updated gated ratio to 0.9575 (progress: 42.5%)
{'loss': 1.1169, 'grad_norm': 1.6307804584503174, 'learning_rate': 1.8309066643853854e-05, 'epoch': 0.21}
{'loss': 1.0729, 'grad_norm': 1.567643165588379, 'learning_rate': 1.8305598114662312e-05, 'epoch': 0.21}
{'loss': 0.9952, 'grad_norm': 1.646483302116394, 'learning_rate': 1.830212636095161e-05, 'epoch': 0.21}
{'loss': 1.1283, 'grad_norm': 1.5864022970199585, 'learning_rate': 1.8298651384069605e-05, 'epoch': 0.21}
{'loss': 1.0773, 'grad_norm': 1.6695340871810913, 'learning_rate': 1.8295173185365405e-05, 'epoch': 0.21}
{'loss': 1.072, 'grad_norm': 1.5169153213500977, 'learning_rate': 1.829169176618936e-05, 'epoch': 0.21}
Step 1109: Updated gated ratio to 0.9573 (progress: 42.7%)
Step 1109: Updated gated ratio to 0.9573 (progress: 42.7%)Step 1109: Updated gated ratio to 0.9573 (progress: 42.7%)
Step 1109: Updated gated ratio to 0.9573 (progress: 42.7%)
Step 1109: Updated gated ratio to 0.9573 (progress: 42.7%)
Step 1109: Updated gated ratio to 0.9573 (progress: 42.7%)
Step 1109: Updated gated ratio to 0.9573 (progress: 42.7%)
Step 1109: Updated gated ratio to 0.9573 (progress: 42.7%)

Step 1109: Updated gated ratio to 0.9573 (progress: 42.7%)
Step 1109: Updated gated ratio to 0.9573 (progress: 42.7%)
Step 1109: Updated gated ratio to 0.9573 (progress: 42.7%)
Step 1109: Updated gated ratio to 0.9573 (progress: 42.7%)Step 1109: Updated gated ratio to 0.9573 (progress: 42.7%)

Step 1109: Updated gated ratio to 0.9573 (progress: 42.7%)
Step 1109: Updated gated ratio to 0.9573 (progress: 42.7%)
Step 1109: Updated gated ratio to 0.9573 (progress: 42.7%)
{'loss': 1.0632, 'grad_norm': 1.7442306280136108, 'learning_rate': 1.828820712789308e-05, 'epoch': 0.21}
{'loss': 1.2516, 'grad_norm': 1.7561746835708618, 'learning_rate': 1.828471927182942e-05, 'epoch': 0.21}
{'loss': 1.1551, 'grad_norm': 1.4610404968261719, 'learning_rate': 1.828122819935249e-05, 'epoch': 0.21}
{'loss': 1.2204, 'grad_norm': 1.802275538444519, 'learning_rate': 1.8277733911817642e-05, 'epoch': 0.21}
{'loss': 1.0601, 'grad_norm': 1.6189141273498535, 'learning_rate': 1.8274236410581478e-05, 'epoch': 0.21}
Step 1114: Updated gated ratio to 0.9571 (progress: 42.9%)
Step 1114: Updated gated ratio to 0.9571 (progress: 42.9%)
Step 1114: Updated gated ratio to 0.9571 (progress: 42.9%)
Step 1114: Updated gated ratio to 0.9571 (progress: 42.9%)
Step 1114: Updated gated ratio to 0.9571 (progress: 42.9%)
Step 1114: Updated gated ratio to 0.9571 (progress: 42.9%)
Step 1114: Updated gated ratio to 0.9571 (progress: 42.9%)
Step 1114: Updated gated ratio to 0.9571 (progress: 42.9%)
Step 1114: Updated gated ratio to 0.9571 (progress: 42.9%)
Step 1114: Updated gated ratio to 0.9571 (progress: 42.9%)
Step 1114: Updated gated ratio to 0.9571 (progress: 42.9%)
Step 1114: Updated gated ratio to 0.9571 (progress: 42.9%)
Step 1114: Updated gated ratio to 0.9571 (progress: 42.9%)
Step 1114: Updated gated ratio to 0.9571 (progress: 42.9%)
Step 1114: Updated gated ratio to 0.9571 (progress: 42.9%)
Step 1114: Updated gated ratio to 0.9571 (progress: 42.9%)
{'loss': 1.184, 'grad_norm': 1.7012529373168945, 'learning_rate': 1.827073569700185e-05, 'epoch': 0.21}
{'loss': 1.125, 'grad_norm': 1.6644573211669922, 'learning_rate': 1.8267231772437854e-05, 'epoch': 0.21}
{'loss': 1.1842, 'grad_norm': 1.4050720930099487, 'learning_rate': 1.8263724638249834e-05, 'epoch': 0.21}
{'loss': 1.0732, 'grad_norm': 1.7003936767578125, 'learning_rate': 1.8260214295799382e-05, 'epoch': 0.22}
{'loss': 1.0933, 'grad_norm': 1.5760204792022705, 'learning_rate': 1.825670074644933e-05, 'epoch': 0.22}
Step 1119: Updated gated ratio to 0.9569 (progress: 43.1%)
Step 1119: Updated gated ratio to 0.9569 (progress: 43.1%)
Step 1119: Updated gated ratio to 0.9569 (progress: 43.1%)
Step 1119: Updated gated ratio to 0.9569 (progress: 43.1%)
Step 1119: Updated gated ratio to 0.9569 (progress: 43.1%)
Step 1119: Updated gated ratio to 0.9569 (progress: 43.1%)
Step 1119: Updated gated ratio to 0.9569 (progress: 43.1%)
Step 1119: Updated gated ratio to 0.9569 (progress: 43.1%)
Step 1119: Updated gated ratio to 0.9569 (progress: 43.1%)
Step 1119: Updated gated ratio to 0.9569 (progress: 43.1%)
Step 1119: Updated gated ratio to 0.9569 (progress: 43.1%)
Step 1119: Updated gated ratio to 0.9569 (progress: 43.1%)
Step 1119: Updated gated ratio to 0.9569 (progress: 43.1%)
Step 1119: Updated gated ratio to 0.9569 (progress: 43.1%)
Step 1119: Updated gated ratio to 0.9569 (progress: 43.1%)
Step 1119: Updated gated ratio to 0.9569 (progress: 43.1%)
{'loss': 1.0518, 'grad_norm': 1.6881669759750366, 'learning_rate': 1.8253183991563768e-05, 'epoch': 0.22}
{'loss': 1.0777, 'grad_norm': 1.6211483478546143, 'learning_rate': 1.824966403250801e-05, 'epoch': 0.22}
{'loss': 1.0995, 'grad_norm': 1.7056505680084229, 'learning_rate': 1.8246140870648633e-05, 'epoch': 0.22}
{'loss': 1.1282, 'grad_norm': 1.5148879289627075, 'learning_rate': 1.8242614507353446e-05, 'epoch': 0.22}
{'loss': 1.1599, 'grad_norm': 1.6443829536437988, 'learning_rate': 1.8239084943991507e-05, 'epoch': 0.22}
Step 1124: Updated gated ratio to 0.9567 (progress: 43.3%)
Step 1124: Updated gated ratio to 0.9567 (progress: 43.3%)
Step 1124: Updated gated ratio to 0.9567 (progress: 43.3%)
Step 1124: Updated gated ratio to 0.9567 (progress: 43.3%)
Step 1124: Updated gated ratio to 0.9567 (progress: 43.3%)
Step 1124: Updated gated ratio to 0.9567 (progress: 43.3%)
Step 1124: Updated gated ratio to 0.9567 (progress: 43.3%)
Step 1124: Updated gated ratio to 0.9567 (progress: 43.3%)
Step 1124: Updated gated ratio to 0.9567 (progress: 43.3%)
Step 1124: Updated gated ratio to 0.9567 (progress: 43.3%)
Step 1124: Updated gated ratio to 0.9567 (progress: 43.3%)
Step 1124: Updated gated ratio to 0.9567 (progress: 43.3%)
Step 1124: Updated gated ratio to 0.9567 (progress: 43.3%)
Step 1124: Updated gated ratio to 0.9567 (progress: 43.3%)
Step 1124: Updated gated ratio to 0.9567 (progress: 43.3%)
Step 1124: Updated gated ratio to 0.9567 (progress: 43.3%)
{'loss': 1.0878, 'grad_norm': 1.6255288124084473, 'learning_rate': 1.823555218193311e-05, 'epoch': 0.22}
{'loss': 1.1042, 'grad_norm': 1.6990140676498413, 'learning_rate': 1.8232016222549797e-05, 'epoch': 0.22}
{'loss': 1.2336, 'grad_norm': 1.828716516494751, 'learning_rate': 1.8228477067214352e-05, 'epoch': 0.22}
{'loss': 1.0993, 'grad_norm': 1.7125823497772217, 'learning_rate': 1.8224934717300794e-05, 'epoch': 0.22}
{'loss': 1.1366, 'grad_norm': 1.4370346069335938, 'learning_rate': 1.8221389174184385e-05, 'epoch': 0.22}
Step 1129: Updated gated ratio to 0.9565 (progress: 43.5%)
Step 1129: Updated gated ratio to 0.9565 (progress: 43.5%)
Step 1129: Updated gated ratio to 0.9565 (progress: 43.5%)
Step 1129: Updated gated ratio to 0.9565 (progress: 43.5%)Step 1129: Updated gated ratio to 0.9565 (progress: 43.5%)

Step 1129: Updated gated ratio to 0.9565 (progress: 43.5%)
Step 1129: Updated gated ratio to 0.9565 (progress: 43.5%)
Step 1129: Updated gated ratio to 0.9565 (progress: 43.5%)
Step 1129: Updated gated ratio to 0.9565 (progress: 43.5%)
Step 1129: Updated gated ratio to 0.9565 (progress: 43.5%)
Step 1129: Updated gated ratio to 0.9565 (progress: 43.5%)
Step 1129: Updated gated ratio to 0.9565 (progress: 43.5%)
Step 1129: Updated gated ratio to 0.9565 (progress: 43.5%)
Step 1129: Updated gated ratio to 0.9565 (progress: 43.5%)
Step 1129: Updated gated ratio to 0.9565 (progress: 43.5%)
Step 1129: Updated gated ratio to 0.9565 (progress: 43.5%)
{'loss': 1.1178, 'grad_norm': 1.4760314226150513, 'learning_rate': 1.8217840439241633e-05, 'epoch': 0.22}
{'loss': 1.1111, 'grad_norm': 1.6864123344421387, 'learning_rate': 1.8214288513850267e-05, 'epoch': 0.22}
{'loss': 1.0799, 'grad_norm': 1.6327548027038574, 'learning_rate': 1.8210733399389277e-05, 'epoch': 0.22}
{'loss': 1.1669, 'grad_norm': 1.5474311113357544, 'learning_rate': 1.820717509723888e-05, 'epoch': 0.22}
{'loss': 0.9065, 'grad_norm': 1.5469216108322144, 'learning_rate': 1.8203613608780525e-05, 'epoch': 0.22}
{'loss': 1.1032, 'grad_norm': 1.7916631698608398, 'learning_rate': 1.8200048935396908e-05, 'epoch': 0.22}
Step 1135: Updated gated ratio to 0.9563 (progress: 43.7%)
Step 1135: Updated gated ratio to 0.9563 (progress: 43.7%)
Step 1135: Updated gated ratio to 0.9563 (progress: 43.7%)
Step 1135: Updated gated ratio to 0.9563 (progress: 43.7%)Step 1135: Updated gated ratio to 0.9563 (progress: 43.7%)
Step 1135: Updated gated ratio to 0.9563 (progress: 43.7%)
Step 1135: Updated gated ratio to 0.9563 (progress: 43.7%)
Step 1135: Updated gated ratio to 0.9563 (progress: 43.7%)

Step 1135: Updated gated ratio to 0.9563 (progress: 43.7%)
Step 1135: Updated gated ratio to 0.9563 (progress: 43.7%)
Step 1135: Updated gated ratio to 0.9563 (progress: 43.7%)
Step 1135: Updated gated ratio to 0.9563 (progress: 43.7%)
Step 1135: Updated gated ratio to 0.9563 (progress: 43.7%)
Step 1135: Updated gated ratio to 0.9563 (progress: 43.7%)
Step 1135: Updated gated ratio to 0.9563 (progress: 43.7%)
Step 1135: Updated gated ratio to 0.9563 (progress: 43.7%)
{'loss': 1.1314, 'grad_norm': 1.664096713066101, 'learning_rate': 1.819648107847196e-05, 'epoch': 0.22}
{'loss': 1.0517, 'grad_norm': 1.4742580652236938, 'learning_rate': 1.8192910039390844e-05, 'epoch': 0.22}
{'loss': 1.0526, 'grad_norm': 1.3897414207458496, 'learning_rate': 1.8189335819539963e-05, 'epoch': 0.22}
{'loss': 1.1143, 'grad_norm': 1.643092393875122, 'learning_rate': 1.8185758420306947e-05, 'epoch': 0.22}
{'loss': 1.1031, 'grad_norm': 1.4058574438095093, 'learning_rate': 1.818217784308067e-05, 'epoch': 0.22}
Step 1140: Updated gated ratio to 0.9561 (progress: 43.9%)
Step 1140: Updated gated ratio to 0.9561 (progress: 43.9%)
Step 1140: Updated gated ratio to 0.9561 (progress: 43.9%)
Step 1140: Updated gated ratio to 0.9561 (progress: 43.9%)
Step 1140: Updated gated ratio to 0.9561 (progress: 43.9%)
Step 1140: Updated gated ratio to 0.9561 (progress: 43.9%)
Step 1140: Updated gated ratio to 0.9561 (progress: 43.9%)
Step 1140: Updated gated ratio to 0.9561 (progress: 43.9%)
Step 1140: Updated gated ratio to 0.9561 (progress: 43.9%)
Step 1140: Updated gated ratio to 0.9561 (progress: 43.9%)
Step 1140: Updated gated ratio to 0.9561 (progress: 43.9%)
Step 1140: Updated gated ratio to 0.9561 (progress: 43.9%)
Step 1140: Updated gated ratio to 0.9561 (progress: 43.9%)
Step 1140: Updated gated ratio to 0.9561 (progress: 43.9%)
Step 1140: Updated gated ratio to 0.9561 (progress: 43.9%)
Step 1140: Updated gated ratio to 0.9561 (progress: 43.9%)
{'loss': 1.1693, 'grad_norm': 1.6039260625839233, 'learning_rate': 1.817859408925123e-05, 'epoch': 0.22}
{'loss': 1.1013, 'grad_norm': 1.7057133913040161, 'learning_rate': 1.817500716020997e-05, 'epoch': 0.22}
{'loss': 1.0205, 'grad_norm': 1.6572935581207275, 'learning_rate': 1.8171417057349457e-05, 'epoch': 0.22}
{'loss': 1.0128, 'grad_norm': 1.5293185710906982, 'learning_rate': 1.816782378206349e-05, 'epoch': 0.22}
{'loss': 1.1798, 'grad_norm': 1.3691855669021606, 'learning_rate': 1.8164227335747108e-05, 'epoch': 0.22}
Step 1145: Updated gated ratio to 0.9559 (progress: 44.1%)
Step 1145: Updated gated ratio to 0.9559 (progress: 44.1%)
Step 1145: Updated gated ratio to 0.9559 (progress: 44.1%)
Step 1145: Updated gated ratio to 0.9559 (progress: 44.1%)
Step 1145: Updated gated ratio to 0.9559 (progress: 44.1%)
Step 1145: Updated gated ratio to 0.9559 (progress: 44.1%)
Step 1145: Updated gated ratio to 0.9559 (progress: 44.1%)
Step 1145: Updated gated ratio to 0.9559 (progress: 44.1%)
Step 1145: Updated gated ratio to 0.9559 (progress: 44.1%)
Step 1145: Updated gated ratio to 0.9559 (progress: 44.1%)
Step 1145: Updated gated ratio to 0.9559 (progress: 44.1%)Step 1145: Updated gated ratio to 0.9559 (progress: 44.1%)

Step 1145: Updated gated ratio to 0.9559 (progress: 44.1%)
Step 1145: Updated gated ratio to 0.9559 (progress: 44.1%)
Step 1145: Updated gated ratio to 0.9559 (progress: 44.1%)
Step 1145: Updated gated ratio to 0.9559 (progress: 44.1%)
{'loss': 1.0108, 'grad_norm': 1.5797981023788452, 'learning_rate': 1.8160627719796568e-05, 'epoch': 0.22}
{'loss': 1.0429, 'grad_norm': 1.6505285501480103, 'learning_rate': 1.815702493560937e-05, 'epoch': 0.22}
{'loss': 1.0059, 'grad_norm': 1.6264426708221436, 'learning_rate': 1.8153418984584238e-05, 'epoch': 0.22}
{'loss': 1.0878, 'grad_norm': 1.4426988363265991, 'learning_rate': 1.8149809868121125e-05, 'epoch': 0.22}
{'loss': 1.1467, 'grad_norm': 1.6421575546264648, 'learning_rate': 1.8146197587621217e-05, 'epoch': 0.22}
Step 1150: Updated gated ratio to 0.9557 (progress: 44.3%)
Step 1150: Updated gated ratio to 0.9557 (progress: 44.3%)
Step 1150: Updated gated ratio to 0.9557 (progress: 44.3%)
Step 1150: Updated gated ratio to 0.9557 (progress: 44.3%)
Step 1150: Updated gated ratio to 0.9557 (progress: 44.3%)
Step 1150: Updated gated ratio to 0.9557 (progress: 44.3%)
Step 1150: Updated gated ratio to 0.9557 (progress: 44.3%)
Step 1150: Updated gated ratio to 0.9557 (progress: 44.3%)
Step 1150: Updated gated ratio to 0.9557 (progress: 44.3%)
Step 1150: Updated gated ratio to 0.9557 (progress: 44.3%)
Step 1150: Updated gated ratio to 0.9557 (progress: 44.3%)
Step 1150: Updated gated ratio to 0.9557 (progress: 44.3%)
Step 1150: Updated gated ratio to 0.9557 (progress: 44.3%)
Step 1150: Updated gated ratio to 0.9557 (progress: 44.3%)
Step 1150: Updated gated ratio to 0.9557 (progress: 44.3%)
Step 1150: Updated gated ratio to 0.9557 (progress: 44.3%)
{'loss': 1.1336, 'grad_norm': 1.6673235893249512, 'learning_rate': 1.814258214448692e-05, 'epoch': 0.22}
{'loss': 1.0708, 'grad_norm': 1.6023683547973633, 'learning_rate': 1.8138963540121878e-05, 'epoch': 0.22}
{'loss': 1.1466, 'grad_norm': 1.5408495664596558, 'learning_rate': 1.813534177593096e-05, 'epoch': 0.22}
{'loss': 1.0475, 'grad_norm': 1.544838309288025, 'learning_rate': 1.8131716853320254e-05, 'epoch': 0.22}
{'loss': 1.0697, 'grad_norm': 1.7228608131408691, 'learning_rate': 1.8128088773697086e-05, 'epoch': 0.22}
Step 1155: Updated gated ratio to 0.9555 (progress: 44.5%)
Step 1155: Updated gated ratio to 0.9555 (progress: 44.5%)
Step 1155: Updated gated ratio to 0.9555 (progress: 44.5%)Step 1155: Updated gated ratio to 0.9555 (progress: 44.5%)

Step 1155: Updated gated ratio to 0.9555 (progress: 44.5%)Step 1155: Updated gated ratio to 0.9555 (progress: 44.5%)
Step 1155: Updated gated ratio to 0.9555 (progress: 44.5%)
Step 1155: Updated gated ratio to 0.9555 (progress: 44.5%)
Step 1155: Updated gated ratio to 0.9555 (progress: 44.5%)
Step 1155: Updated gated ratio to 0.9555 (progress: 44.5%)

Step 1155: Updated gated ratio to 0.9555 (progress: 44.5%)
Step 1155: Updated gated ratio to 0.9555 (progress: 44.5%)
Step 1155: Updated gated ratio to 0.9555 (progress: 44.5%)
Step 1155: Updated gated ratio to 0.9555 (progress: 44.5%)
Step 1155: Updated gated ratio to 0.9555 (progress: 44.5%)
Step 1155: Updated gated ratio to 0.9555 (progress: 44.5%)
{'loss': 1.211, 'grad_norm': 1.3800342082977295, 'learning_rate': 1.8124457538469996e-05, 'epoch': 0.22}
{'loss': 1.1508, 'grad_norm': 1.71634042263031, 'learning_rate': 1.8120823149048753e-05, 'epoch': 0.22}
{'loss': 1.092, 'grad_norm': 1.7192531824111938, 'learning_rate': 1.811718560684436e-05, 'epoch': 0.22}
{'loss': 1.1761, 'grad_norm': 1.5464273691177368, 'learning_rate': 1.8113544913269025e-05, 'epoch': 0.22}
{'loss': 1.1037, 'grad_norm': 1.6213500499725342, 'learning_rate': 1.8109901069736202e-05, 'epoch': 0.22}
{'loss': 1.1614, 'grad_norm': 1.6304551362991333, 'learning_rate': 1.8106254077660552e-05, 'epoch': 0.22}
Step 1161: Updated gated ratio to 0.9553 (progress: 44.7%)
Step 1161: Updated gated ratio to 0.9553 (progress: 44.7%)
Step 1161: Updated gated ratio to 0.9553 (progress: 44.7%)
Step 1161: Updated gated ratio to 0.9553 (progress: 44.7%)
Step 1161: Updated gated ratio to 0.9553 (progress: 44.7%)
Step 1161: Updated gated ratio to 0.9553 (progress: 44.7%)
Step 1161: Updated gated ratio to 0.9553 (progress: 44.7%)
Step 1161: Updated gated ratio to 0.9553 (progress: 44.7%)
Step 1161: Updated gated ratio to 0.9553 (progress: 44.7%)
Step 1161: Updated gated ratio to 0.9553 (progress: 44.7%)
Step 1161: Updated gated ratio to 0.9553 (progress: 44.7%)
Step 1161: Updated gated ratio to 0.9553 (progress: 44.7%)
Step 1161: Updated gated ratio to 0.9553 (progress: 44.7%)
Step 1161: Updated gated ratio to 0.9553 (progress: 44.7%)
Step 1161: Updated gated ratio to 0.9553 (progress: 44.7%)
Step 1161: Updated gated ratio to 0.9553 (progress: 44.7%)
{'loss': 1.0028, 'grad_norm': 1.5883121490478516, 'learning_rate': 1.810260393845796e-05, 'epoch': 0.22}
{'loss': 1.1606, 'grad_norm': 1.772580623626709, 'learning_rate': 1.809895065354554e-05, 'epoch': 0.22}
{'loss': 1.1514, 'grad_norm': 1.8743646144866943, 'learning_rate': 1.8095294224341622e-05, 'epoch': 0.22}
{'loss': 1.1289, 'grad_norm': 1.7317062616348267, 'learning_rate': 1.8091634652265755e-05, 'epoch': 0.22}
{'loss': 1.1811, 'grad_norm': 1.5944627523422241, 'learning_rate': 1.8087971938738715e-05, 'epoch': 0.22}
Step 1166: Updated gated ratio to 0.9551 (progress: 44.9%)
Step 1166: Updated gated ratio to 0.9551 (progress: 44.9%)
Step 1166: Updated gated ratio to 0.9551 (progress: 44.9%)Step 1166: Updated gated ratio to 0.9551 (progress: 44.9%)
Step 1166: Updated gated ratio to 0.9551 (progress: 44.9%)
Step 1166: Updated gated ratio to 0.9551 (progress: 44.9%)
Step 1166: Updated gated ratio to 0.9551 (progress: 44.9%)

Step 1166: Updated gated ratio to 0.9551 (progress: 44.9%)
Step 1166: Updated gated ratio to 0.9551 (progress: 44.9%)
Step 1166: Updated gated ratio to 0.9551 (progress: 44.9%)
Step 1166: Updated gated ratio to 0.9551 (progress: 44.9%)
Step 1166: Updated gated ratio to 0.9551 (progress: 44.9%)
Step 1166: Updated gated ratio to 0.9551 (progress: 44.9%)
Step 1166: Updated gated ratio to 0.9551 (progress: 44.9%)
Step 1166: Updated gated ratio to 0.9551 (progress: 44.9%)
Step 1166: Updated gated ratio to 0.9551 (progress: 44.9%)
{'loss': 1.1343, 'grad_norm': 1.6254260540008545, 'learning_rate': 1.808430608518249e-05, 'epoch': 0.22}
{'loss': 1.088, 'grad_norm': 1.5878740549087524, 'learning_rate': 1.808063709302029e-05, 'epoch': 0.22}
{'loss': 1.16, 'grad_norm': 1.5699851512908936, 'learning_rate': 1.807696496367655e-05, 'epoch': 0.22}
{'loss': 1.161, 'grad_norm': 1.42153000831604, 'learning_rate': 1.8073289698576913e-05, 'epoch': 0.23}
{'loss': 1.1603, 'grad_norm': 1.774155616760254, 'learning_rate': 1.8069611299148236e-05, 'epoch': 0.23}
Step 1171: Updated gated ratio to 0.9549 (progress: 45.1%)
Step 1171: Updated gated ratio to 0.9549 (progress: 45.1%)
Step 1171: Updated gated ratio to 0.9549 (progress: 45.1%)
Step 1171: Updated gated ratio to 0.9549 (progress: 45.1%)
Step 1171: Updated gated ratio to 0.9549 (progress: 45.1%)
Step 1171: Updated gated ratio to 0.9549 (progress: 45.1%)
Step 1171: Updated gated ratio to 0.9549 (progress: 45.1%)
Step 1171: Updated gated ratio to 0.9549 (progress: 45.1%)
Step 1171: Updated gated ratio to 0.9549 (progress: 45.1%)
Step 1171: Updated gated ratio to 0.9549 (progress: 45.1%)
Step 1171: Updated gated ratio to 0.9549 (progress: 45.1%)
Step 1171: Updated gated ratio to 0.9549 (progress: 45.1%)
Step 1171: Updated gated ratio to 0.9549 (progress: 45.1%)
Step 1171: Updated gated ratio to 0.9549 (progress: 45.1%)
Step 1171: Updated gated ratio to 0.9549 (progress: 45.1%)
Step 1171: Updated gated ratio to 0.9549 (progress: 45.1%)
{'loss': 1.0157, 'grad_norm': 1.764254093170166, 'learning_rate': 1.8065929766818617e-05, 'epoch': 0.23}
{'loss': 1.1022, 'grad_norm': 1.6006836891174316, 'learning_rate': 1.806224510301734e-05, 'epoch': 0.23}
{'loss': 1.083, 'grad_norm': 1.557618260383606, 'learning_rate': 1.8058557309174926e-05, 'epoch': 0.23}
{'loss': 1.1572, 'grad_norm': 1.5636765956878662, 'learning_rate': 1.8054866386723096e-05, 'epoch': 0.23}
{'loss': 1.1668, 'grad_norm': 1.5992265939712524, 'learning_rate': 1.80511723370948e-05, 'epoch': 0.23}
Step 1176: Updated gated ratio to 0.9547 (progress: 45.3%)
Step 1176: Updated gated ratio to 0.9547 (progress: 45.3%)
Step 1176: Updated gated ratio to 0.9547 (progress: 45.3%)
Step 1176: Updated gated ratio to 0.9547 (progress: 45.3%)
Step 1176: Updated gated ratio to 0.9547 (progress: 45.3%)
Step 1176: Updated gated ratio to 0.9547 (progress: 45.3%)
Step 1176: Updated gated ratio to 0.9547 (progress: 45.3%)
Step 1176: Updated gated ratio to 0.9547 (progress: 45.3%)
Step 1176: Updated gated ratio to 0.9547 (progress: 45.3%)
Step 1176: Updated gated ratio to 0.9547 (progress: 45.3%)
Step 1176: Updated gated ratio to 0.9547 (progress: 45.3%)
Step 1176: Updated gated ratio to 0.9547 (progress: 45.3%)
Step 1176: Updated gated ratio to 0.9547 (progress: 45.3%)
Step 1176: Updated gated ratio to 0.9547 (progress: 45.3%)
Step 1176: Updated gated ratio to 0.9547 (progress: 45.3%)
Step 1176: Updated gated ratio to 0.9547 (progress: 45.3%)
{'loss': 1.0317, 'grad_norm': 1.605838656425476, 'learning_rate': 1.804747516172419e-05, 'epoch': 0.23}
{'loss': 1.1602, 'grad_norm': 1.604962944984436, 'learning_rate': 1.8043774862046644e-05, 'epoch': 0.23}
{'loss': 1.0948, 'grad_norm': 1.6594771146774292, 'learning_rate': 1.804007143949874e-05, 'epoch': 0.23}
{'loss': 1.149, 'grad_norm': 1.60783851146698, 'learning_rate': 1.8036364895518272e-05, 'epoch': 0.23}
{'loss': 1.1256, 'grad_norm': 1.8318977355957031, 'learning_rate': 1.8032655231544253e-05, 'epoch': 0.23}
Step 1181: Updated gated ratio to 0.9545 (progress: 45.5%)
Step 1181: Updated gated ratio to 0.9545 (progress: 45.5%)Step 1181: Updated gated ratio to 0.9545 (progress: 45.5%)

Step 1181: Updated gated ratio to 0.9545 (progress: 45.5%)
Step 1181: Updated gated ratio to 0.9545 (progress: 45.5%)
Step 1181: Updated gated ratio to 0.9545 (progress: 45.5%)
Step 1181: Updated gated ratio to 0.9545 (progress: 45.5%)
Step 1181: Updated gated ratio to 0.9545 (progress: 45.5%)
Step 1181: Updated gated ratio to 0.9545 (progress: 45.5%)
Step 1181: Updated gated ratio to 0.9545 (progress: 45.5%)Step 1181: Updated gated ratio to 0.9545 (progress: 45.5%)
Step 1181: Updated gated ratio to 0.9545 (progress: 45.5%)
Step 1181: Updated gated ratio to 0.9545 (progress: 45.5%)
Step 1181: Updated gated ratio to 0.9545 (progress: 45.5%)

Step 1181: Updated gated ratio to 0.9545 (progress: 45.5%)
Step 1181: Updated gated ratio to 0.9545 (progress: 45.5%)
{'loss': 1.0895, 'grad_norm': 1.6174403429031372, 'learning_rate': 1.8028942449016903e-05, 'epoch': 0.23}
{'loss': 1.1335, 'grad_norm': 1.7432860136032104, 'learning_rate': 1.8025226549377647e-05, 'epoch': 0.23}
{'loss': 1.1678, 'grad_norm': 1.775368094444275, 'learning_rate': 1.8021507534069133e-05, 'epoch': 0.23}
Error with image file is truncated (74 bytes not processed)
{'loss': 1.1829, 'grad_norm': 1.4461642503738403, 'learning_rate': 1.8017785404535198e-05, 'epoch': 0.23}
{'loss': 1.0507, 'grad_norm': 1.4969689846038818, 'learning_rate': 1.8014060162220916e-05, 'epoch': 0.23}
Step 1186: Updated gated ratio to 0.9543 (progress: 45.7%)
Step 1186: Updated gated ratio to 0.9543 (progress: 45.7%)
Step 1186: Updated gated ratio to 0.9543 (progress: 45.7%)
Step 1186: Updated gated ratio to 0.9543 (progress: 45.7%)
Step 1186: Updated gated ratio to 0.9543 (progress: 45.7%)
Step 1186: Updated gated ratio to 0.9543 (progress: 45.7%)
Step 1186: Updated gated ratio to 0.9543 (progress: 45.7%)
Step 1186: Updated gated ratio to 0.9543 (progress: 45.7%)
Step 1186: Updated gated ratio to 0.9543 (progress: 45.7%)Step 1186: Updated gated ratio to 0.9543 (progress: 45.7%)

Step 1186: Updated gated ratio to 0.9543 (progress: 45.7%)
Step 1186: Updated gated ratio to 0.9543 (progress: 45.7%)
Step 1186: Updated gated ratio to 0.9543 (progress: 45.7%)
Step 1186: Updated gated ratio to 0.9543 (progress: 45.7%)
Step 1186: Updated gated ratio to 0.9543 (progress: 45.7%)
Step 1186: Updated gated ratio to 0.9543 (progress: 45.7%)
{'loss': 1.0378, 'grad_norm': 1.7540333271026611, 'learning_rate': 1.801033180857254e-05, 'epoch': 0.23}
{'loss': 1.0908, 'grad_norm': 1.6704760789871216, 'learning_rate': 1.8006600345037558e-05, 'epoch': 0.23}
{'loss': 1.0042, 'grad_norm': 1.5525730848312378, 'learning_rate': 1.8002865773064644e-05, 'epoch': 0.23}
{'loss': 1.0475, 'grad_norm': 1.676119327545166, 'learning_rate': 1.799912809410369e-05, 'epoch': 0.23}
{'loss': 1.1232, 'grad_norm': 1.6045249700546265, 'learning_rate': 1.799538730960579e-05, 'epoch': 0.23}
{'loss': 1.128, 'grad_norm': 1.7414088249206543, 'learning_rate': 1.799164342102325e-05, 'epoch': 0.23}
Step 1192: Updated gated ratio to 0.9541 (progress: 45.9%)
Step 1192: Updated gated ratio to 0.9541 (progress: 45.9%)
Step 1192: Updated gated ratio to 0.9541 (progress: 45.9%)
Step 1192: Updated gated ratio to 0.9541 (progress: 45.9%)
Step 1192: Updated gated ratio to 0.9541 (progress: 45.9%)
Step 1192: Updated gated ratio to 0.9541 (progress: 45.9%)
Step 1192: Updated gated ratio to 0.9541 (progress: 45.9%)
Step 1192: Updated gated ratio to 0.9541 (progress: 45.9%)
Step 1192: Updated gated ratio to 0.9541 (progress: 45.9%)
Step 1192: Updated gated ratio to 0.9541 (progress: 45.9%)
Step 1192: Updated gated ratio to 0.9541 (progress: 45.9%)
Step 1192: Updated gated ratio to 0.9541 (progress: 45.9%)
Step 1192: Updated gated ratio to 0.9541 (progress: 45.9%)Step 1192: Updated gated ratio to 0.9541 (progress: 45.9%)

Step 1192: Updated gated ratio to 0.9541 (progress: 45.9%)
Step 1192: Updated gated ratio to 0.9541 (progress: 45.9%)
{'loss': 1.1244, 'grad_norm': 1.8630822896957397, 'learning_rate': 1.7987896429809573e-05, 'epoch': 0.23}
{'loss': 1.1236, 'grad_norm': 1.6222648620605469, 'learning_rate': 1.798414633741947e-05, 'epoch': 0.23}
{'loss': 1.2551, 'grad_norm': 1.5641536712646484, 'learning_rate': 1.7980393145308857e-05, 'epoch': 0.23}
{'loss': 1.107, 'grad_norm': 1.6878682374954224, 'learning_rate': 1.797663685493485e-05, 'epoch': 0.23}
{'loss': 1.1149, 'grad_norm': 1.6983596086502075, 'learning_rate': 1.7972877467755777e-05, 'epoch': 0.23}
Step 1197: Updated gated ratio to 0.9539 (progress: 46.1%)
Step 1197: Updated gated ratio to 0.9539 (progress: 46.1%)
Step 1197: Updated gated ratio to 0.9539 (progress: 46.1%)
Step 1197: Updated gated ratio to 0.9539 (progress: 46.1%)
Step 1197: Updated gated ratio to 0.9539 (progress: 46.1%)
Step 1197: Updated gated ratio to 0.9539 (progress: 46.1%)
Step 1197: Updated gated ratio to 0.9539 (progress: 46.1%)
Step 1197: Updated gated ratio to 0.9539 (progress: 46.1%)
Step 1197: Updated gated ratio to 0.9539 (progress: 46.1%)
Step 1197: Updated gated ratio to 0.9539 (progress: 46.1%)
Step 1197: Updated gated ratio to 0.9539 (progress: 46.1%)
Step 1197: Updated gated ratio to 0.9539 (progress: 46.1%)
Step 1197: Updated gated ratio to 0.9539 (progress: 46.1%)
Step 1197: Updated gated ratio to 0.9539 (progress: 46.1%)
Step 1197: Updated gated ratio to 0.9539 (progress: 46.1%)
Step 1197: Updated gated ratio to 0.9539 (progress: 46.1%)
{'loss': 1.188, 'grad_norm': 1.642663598060608, 'learning_rate': 1.7969114985231152e-05, 'epoch': 0.23}
{'loss': 1.1037, 'grad_norm': 1.5145379304885864, 'learning_rate': 1.796534940882171e-05, 'epoch': 0.23}
{'loss': 1.1125, 'grad_norm': 1.5391322374343872, 'learning_rate': 1.7961580739989365e-05, 'epoch': 0.23}
{'loss': 1.1272, 'grad_norm': 1.694573998451233, 'learning_rate': 1.795780898019726e-05, 'epoch': 0.23}
{'loss': 1.1418, 'grad_norm': 1.3941305875778198, 'learning_rate': 1.795403413090971e-05, 'epoch': 0.23}
Step 1202: Updated gated ratio to 0.9537 (progress: 46.3%)
Step 1202: Updated gated ratio to 0.9537 (progress: 46.3%)
Step 1202: Updated gated ratio to 0.9537 (progress: 46.3%)
Step 1202: Updated gated ratio to 0.9537 (progress: 46.3%)
Step 1202: Updated gated ratio to 0.9537 (progress: 46.3%)
Step 1202: Updated gated ratio to 0.9537 (progress: 46.3%)
Step 1202: Updated gated ratio to 0.9537 (progress: 46.3%)
Step 1202: Updated gated ratio to 0.9537 (progress: 46.3%)
Step 1202: Updated gated ratio to 0.9537 (progress: 46.3%)
Step 1202: Updated gated ratio to 0.9537 (progress: 46.3%)
Step 1202: Updated gated ratio to 0.9537 (progress: 46.3%)
Step 1202: Updated gated ratio to 0.9537 (progress: 46.3%)
Step 1202: Updated gated ratio to 0.9537 (progress: 46.3%)
Step 1202: Updated gated ratio to 0.9537 (progress: 46.3%)Step 1202: Updated gated ratio to 0.9537 (progress: 46.3%)

Step 1202: Updated gated ratio to 0.9537 (progress: 46.3%)
{'loss': 1.1504, 'grad_norm': 1.8498224020004272, 'learning_rate': 1.7950256193592243e-05, 'epoch': 0.23}
{'loss': 1.1271, 'grad_norm': 1.7427799701690674, 'learning_rate': 1.794647516971159e-05, 'epoch': 0.23}
{'loss': 1.1206, 'grad_norm': 1.6740082502365112, 'learning_rate': 1.7942691060735666e-05, 'epoch': 0.23}
{'loss': 1.0578, 'grad_norm': 1.6515185832977295, 'learning_rate': 1.79389038681336e-05, 'epoch': 0.23}
{'loss': 1.101, 'grad_norm': 1.669446349143982, 'learning_rate': 1.7935113593375707e-05, 'epoch': 0.23}
Step 1207: Updated gated ratio to 0.9535 (progress: 46.5%)
Step 1207: Updated gated ratio to 0.9535 (progress: 46.5%)
Step 1207: Updated gated ratio to 0.9535 (progress: 46.5%)
Step 1207: Updated gated ratio to 0.9535 (progress: 46.5%)Step 1207: Updated gated ratio to 0.9535 (progress: 46.5%)

Step 1207: Updated gated ratio to 0.9535 (progress: 46.5%)
Step 1207: Updated gated ratio to 0.9535 (progress: 46.5%)
Step 1207: Updated gated ratio to 0.9535 (progress: 46.5%)
Step 1207: Updated gated ratio to 0.9535 (progress: 46.5%)
Step 1207: Updated gated ratio to 0.9535 (progress: 46.5%)
Step 1207: Updated gated ratio to 0.9535 (progress: 46.5%)
Step 1207: Updated gated ratio to 0.9535 (progress: 46.5%)
Step 1207: Updated gated ratio to 0.9535 (progress: 46.5%)
Step 1207: Updated gated ratio to 0.9535 (progress: 46.5%)
Step 1207: Updated gated ratio to 0.9535 (progress: 46.5%)
Step 1207: Updated gated ratio to 0.9535 (progress: 46.5%)
{'loss': 1.1233, 'grad_norm': 1.280529260635376, 'learning_rate': 1.7931320237933503e-05, 'epoch': 0.23}
{'loss': 1.1659, 'grad_norm': 1.8468060493469238, 'learning_rate': 1.79275238032797e-05, 'epoch': 0.23}
{'loss': 1.0812, 'grad_norm': 1.5057663917541504, 'learning_rate': 1.7923724290888205e-05, 'epoch': 0.23}
{'loss': 1.078, 'grad_norm': 1.6027144193649292, 'learning_rate': 1.791992170223412e-05, 'epoch': 0.23}
{'loss': 1.125, 'grad_norm': 1.37041437625885, 'learning_rate': 1.791611603879374e-05, 'epoch': 0.23}
Step 1212: Updated gated ratio to 0.9533 (progress: 46.7%)
Step 1212: Updated gated ratio to 0.9533 (progress: 46.7%)Step 1212: Updated gated ratio to 0.9533 (progress: 46.7%)

Step 1212: Updated gated ratio to 0.9533 (progress: 46.7%)
Step 1212: Updated gated ratio to 0.9533 (progress: 46.7%)
Step 1212: Updated gated ratio to 0.9533 (progress: 46.7%)
Step 1212: Updated gated ratio to 0.9533 (progress: 46.7%)Step 1212: Updated gated ratio to 0.9533 (progress: 46.7%)

Step 1212: Updated gated ratio to 0.9533 (progress: 46.7%)
Step 1212: Updated gated ratio to 0.9533 (progress: 46.7%)
Step 1212: Updated gated ratio to 0.9533 (progress: 46.7%)
Step 1212: Updated gated ratio to 0.9533 (progress: 46.7%)
Step 1212: Updated gated ratio to 0.9533 (progress: 46.7%)
Step 1212: Updated gated ratio to 0.9533 (progress: 46.7%)
Step 1212: Updated gated ratio to 0.9533 (progress: 46.7%)
Step 1212: Updated gated ratio to 0.9533 (progress: 46.7%)
{'loss': 1.0947, 'grad_norm': 1.852842092514038, 'learning_rate': 1.791230730204455e-05, 'epoch': 0.23}
{'loss': 1.1548, 'grad_norm': 1.6195935010910034, 'learning_rate': 1.7908495493465236e-05, 'epoch': 0.23}
{'loss': 1.135, 'grad_norm': 1.7106391191482544, 'learning_rate': 1.7904680614535675e-05, 'epoch': 0.23}
{'loss': 1.1351, 'grad_norm': 1.5998200178146362, 'learning_rate': 1.7900862666736935e-05, 'epoch': 0.23}
{'loss': 1.0141, 'grad_norm': 1.7889316082000732, 'learning_rate': 1.789704165155127e-05, 'epoch': 0.23}
{'loss': 1.2516, 'grad_norm': 1.845044493675232, 'learning_rate': 1.7893217570462134e-05, 'epoch': 0.23}
Step 1218: Updated gated ratio to 0.9531 (progress: 46.9%)
Step 1218: Updated gated ratio to 0.9531 (progress: 46.9%)
Step 1218: Updated gated ratio to 0.9531 (progress: 46.9%)Step 1218: Updated gated ratio to 0.9531 (progress: 46.9%)
Step 1218: Updated gated ratio to 0.9531 (progress: 46.9%)
Step 1218: Updated gated ratio to 0.9531 (progress: 46.9%)
Step 1218: Updated gated ratio to 0.9531 (progress: 46.9%)

Step 1218: Updated gated ratio to 0.9531 (progress: 46.9%)
Step 1218: Updated gated ratio to 0.9531 (progress: 46.9%)
Step 1218: Updated gated ratio to 0.9531 (progress: 46.9%)
Step 1218: Updated gated ratio to 0.9531 (progress: 46.9%)
Step 1218: Updated gated ratio to 0.9531 (progress: 46.9%)
Step 1218: Updated gated ratio to 0.9531 (progress: 46.9%)
Step 1218: Updated gated ratio to 0.9531 (progress: 46.9%)
Step 1218: Updated gated ratio to 0.9531 (progress: 46.9%)
Step 1218: Updated gated ratio to 0.9531 (progress: 46.9%)
{'loss': 1.1022, 'grad_norm': 1.6493024826049805, 'learning_rate': 1.7889390424954168e-05, 'epoch': 0.23}
{'loss': 1.1306, 'grad_norm': 1.5484970808029175, 'learning_rate': 1.78855602165132e-05, 'epoch': 0.23}
{'loss': 1.1596, 'grad_norm': 1.6517523527145386, 'learning_rate': 1.7881726946626244e-05, 'epoch': 0.23}
{'loss': 1.0161, 'grad_norm': 1.5185202360153198, 'learning_rate': 1.787789061678151e-05, 'epoch': 0.24}
{'loss': 1.1353, 'grad_norm': 1.5208832025527954, 'learning_rate': 1.78740512284684e-05, 'epoch': 0.24}
Step 1223: Updated gated ratio to 0.9529 (progress: 47.1%)
Step 1223: Updated gated ratio to 0.9529 (progress: 47.1%)
Step 1223: Updated gated ratio to 0.9529 (progress: 47.1%)
Step 1223: Updated gated ratio to 0.9529 (progress: 47.1%)
Step 1223: Updated gated ratio to 0.9529 (progress: 47.1%)
Step 1223: Updated gated ratio to 0.9529 (progress: 47.1%)
Step 1223: Updated gated ratio to 0.9529 (progress: 47.1%)Step 1223: Updated gated ratio to 0.9529 (progress: 47.1%)
Step 1223: Updated gated ratio to 0.9529 (progress: 47.1%)
Step 1223: Updated gated ratio to 0.9529 (progress: 47.1%)
Step 1223: Updated gated ratio to 0.9529 (progress: 47.1%)
Step 1223: Updated gated ratio to 0.9529 (progress: 47.1%)
Step 1223: Updated gated ratio to 0.9529 (progress: 47.1%)

Step 1223: Updated gated ratio to 0.9529 (progress: 47.1%)
Step 1223: Updated gated ratio to 0.9529 (progress: 47.1%)
Step 1223: Updated gated ratio to 0.9529 (progress: 47.1%)
{'loss': 1.1538, 'grad_norm': 1.3550469875335693, 'learning_rate': 1.787020878317749e-05, 'epoch': 0.24}
{'loss': 1.0639, 'grad_norm': 1.7907246351242065, 'learning_rate': 1.7866363282400555e-05, 'epoch': 0.24}
{'loss': 1.1353, 'grad_norm': 1.669893741607666, 'learning_rate': 1.7862514727630543e-05, 'epoch': 0.24}
{'loss': 1.2153, 'grad_norm': 1.6887075901031494, 'learning_rate': 1.7858663120361597e-05, 'epoch': 0.24}
{'loss': 1.1154, 'grad_norm': 1.6060688495635986, 'learning_rate': 1.785480846208905e-05, 'epoch': 0.24}
Step 1228: Updated gated ratio to 0.9527 (progress: 47.3%)
Step 1228: Updated gated ratio to 0.9527 (progress: 47.3%)
Step 1228: Updated gated ratio to 0.9527 (progress: 47.3%)
Step 1228: Updated gated ratio to 0.9527 (progress: 47.3%)
Step 1228: Updated gated ratio to 0.9527 (progress: 47.3%)
Step 1228: Updated gated ratio to 0.9527 (progress: 47.3%)
Step 1228: Updated gated ratio to 0.9527 (progress: 47.3%)
Step 1228: Updated gated ratio to 0.9527 (progress: 47.3%)
Step 1228: Updated gated ratio to 0.9527 (progress: 47.3%)
Step 1228: Updated gated ratio to 0.9527 (progress: 47.3%)
Step 1228: Updated gated ratio to 0.9527 (progress: 47.3%)
Step 1228: Updated gated ratio to 0.9527 (progress: 47.3%)
Step 1228: Updated gated ratio to 0.9527 (progress: 47.3%)
Step 1228: Updated gated ratio to 0.9527 (progress: 47.3%)
Step 1228: Updated gated ratio to 0.9527 (progress: 47.3%)
Step 1228: Updated gated ratio to 0.9527 (progress: 47.3%)
{'loss': 0.9887, 'grad_norm': 1.573575496673584, 'learning_rate': 1.7850950754309405e-05, 'epoch': 0.24}
{'loss': 1.1892, 'grad_norm': 1.6908780336380005, 'learning_rate': 1.7847089998520365e-05, 'epoch': 0.24}
{'loss': 1.0639, 'grad_norm': 1.6254842281341553, 'learning_rate': 1.7843226196220803e-05, 'epoch': 0.24}
{'loss': 1.1784, 'grad_norm': 1.6452473402023315, 'learning_rate': 1.783935934891078e-05, 'epoch': 0.24}
{'loss': 1.132, 'grad_norm': 1.2807247638702393, 'learning_rate': 1.7835489458091544e-05, 'epoch': 0.24}
Step 1233: Updated gated ratio to 0.9525 (progress: 47.5%)
Step 1233: Updated gated ratio to 0.9525 (progress: 47.5%)
Step 1233: Updated gated ratio to 0.9525 (progress: 47.5%)
Step 1233: Updated gated ratio to 0.9525 (progress: 47.5%)
Step 1233: Updated gated ratio to 0.9525 (progress: 47.5%)
Step 1233: Updated gated ratio to 0.9525 (progress: 47.5%)
Step 1233: Updated gated ratio to 0.9525 (progress: 47.5%)
Step 1233: Updated gated ratio to 0.9525 (progress: 47.5%)
Step 1233: Updated gated ratio to 0.9525 (progress: 47.5%)
Step 1233: Updated gated ratio to 0.9525 (progress: 47.5%)
Step 1233: Updated gated ratio to 0.9525 (progress: 47.5%)
Step 1233: Updated gated ratio to 0.9525 (progress: 47.5%)
Step 1233: Updated gated ratio to 0.9525 (progress: 47.5%)
Step 1233: Updated gated ratio to 0.9525 (progress: 47.5%)
Step 1233: Updated gated ratio to 0.9525 (progress: 47.5%)
Step 1233: Updated gated ratio to 0.9525 (progress: 47.5%)
{'loss': 1.1292, 'grad_norm': 1.760223388671875, 'learning_rate': 1.7831616525265515e-05, 'epoch': 0.24}
{'loss': 1.0364, 'grad_norm': 1.5785932540893555, 'learning_rate': 1.7827740551936296e-05, 'epoch': 0.24}
{'loss': 1.1172, 'grad_norm': 1.6469180583953857, 'learning_rate': 1.7823861539608686e-05, 'epoch': 0.24}
{'loss': 1.0692, 'grad_norm': 1.442168116569519, 'learning_rate': 1.7819979489788638e-05, 'epoch': 0.24}
{'loss': 1.1048, 'grad_norm': 1.5808957815170288, 'learning_rate': 1.7816094403983298e-05, 'epoch': 0.24}
Step 1238: Updated gated ratio to 0.9523 (progress: 47.7%)
Step 1238: Updated gated ratio to 0.9523 (progress: 47.7%)
Step 1238: Updated gated ratio to 0.9523 (progress: 47.7%)
Step 1238: Updated gated ratio to 0.9523 (progress: 47.7%)
Step 1238: Updated gated ratio to 0.9523 (progress: 47.7%)
Step 1238: Updated gated ratio to 0.9523 (progress: 47.7%)
Step 1238: Updated gated ratio to 0.9523 (progress: 47.7%)
Step 1238: Updated gated ratio to 0.9523 (progress: 47.7%)
Step 1238: Updated gated ratio to 0.9523 (progress: 47.7%)
Step 1238: Updated gated ratio to 0.9523 (progress: 47.7%)
Step 1238: Updated gated ratio to 0.9523 (progress: 47.7%)
Step 1238: Updated gated ratio to 0.9523 (progress: 47.7%)
Step 1238: Updated gated ratio to 0.9523 (progress: 47.7%)
Step 1238: Updated gated ratio to 0.9523 (progress: 47.7%)
Step 1238: Updated gated ratio to 0.9523 (progress: 47.7%)
Step 1238: Updated gated ratio to 0.9523 (progress: 47.7%)
{'loss': 1.1188, 'grad_norm': 1.6092966794967651, 'learning_rate': 1.7812206283701002e-05, 'epoch': 0.24}
{'loss': 1.1268, 'grad_norm': 1.488319993019104, 'learning_rate': 1.7808315130451244e-05, 'epoch': 0.24}
{'loss': 1.069, 'grad_norm': 1.5663596391677856, 'learning_rate': 1.78044209457447e-05, 'epoch': 0.24}
{'loss': 1.1563, 'grad_norm': 1.716457486152649, 'learning_rate': 1.7800523731093232e-05, 'epoch': 0.24}
{'loss': 1.1933, 'grad_norm': 1.7834818363189697, 'learning_rate': 1.7796623488009875e-05, 'epoch': 0.24}
{'loss': 1.1046, 'grad_norm': 1.698501706123352, 'learning_rate': 1.7792720218008826e-05, 'epoch': 0.24}
Step 1244: Updated gated ratio to 0.9521 (progress: 47.9%)
Step 1244: Updated gated ratio to 0.9521 (progress: 47.9%)
Step 1244: Updated gated ratio to 0.9521 (progress: 47.9%)
Step 1244: Updated gated ratio to 0.9521 (progress: 47.9%)
Step 1244: Updated gated ratio to 0.9521 (progress: 47.9%)
Step 1244: Updated gated ratio to 0.9521 (progress: 47.9%)
Step 1244: Updated gated ratio to 0.9521 (progress: 47.9%)
Step 1244: Updated gated ratio to 0.9521 (progress: 47.9%)
Step 1244: Updated gated ratio to 0.9521 (progress: 47.9%)
Step 1244: Updated gated ratio to 0.9521 (progress: 47.9%)
Step 1244: Updated gated ratio to 0.9521 (progress: 47.9%)
Step 1244: Updated gated ratio to 0.9521 (progress: 47.9%)
Step 1244: Updated gated ratio to 0.9521 (progress: 47.9%)
Step 1244: Updated gated ratio to 0.9521 (progress: 47.9%)
Step 1244: Updated gated ratio to 0.9521 (progress: 47.9%)
Step 1244: Updated gated ratio to 0.9521 (progress: 47.9%)
{'loss': 1.1706, 'grad_norm': 1.5862516164779663, 'learning_rate': 1.7788813922605488e-05, 'epoch': 0.24}
{'loss': 1.2075, 'grad_norm': 1.7241320610046387, 'learning_rate': 1.7784904603316402e-05, 'epoch': 0.24}
{'loss': 1.1534, 'grad_norm': 1.648519515991211, 'learning_rate': 1.7780992261659305e-05, 'epoch': 0.24}
{'loss': 1.1644, 'grad_norm': 1.5522562265396118, 'learning_rate': 1.777707689915311e-05, 'epoch': 0.24}
{'loss': 1.0668, 'grad_norm': 1.5343248844146729, 'learning_rate': 1.777315851731789e-05, 'epoch': 0.24}
Step 1249: Updated gated ratio to 0.9519 (progress: 48.1%)
Step 1249: Updated gated ratio to 0.9519 (progress: 48.1%)
Step 1249: Updated gated ratio to 0.9519 (progress: 48.1%)
Step 1249: Updated gated ratio to 0.9519 (progress: 48.1%)
Step 1249: Updated gated ratio to 0.9519 (progress: 48.1%)
Step 1249: Updated gated ratio to 0.9519 (progress: 48.1%)Step 1249: Updated gated ratio to 0.9519 (progress: 48.1%)

Step 1249: Updated gated ratio to 0.9519 (progress: 48.1%)
Step 1249: Updated gated ratio to 0.9519 (progress: 48.1%)
Step 1249: Updated gated ratio to 0.9519 (progress: 48.1%)
Step 1249: Updated gated ratio to 0.9519 (progress: 48.1%)
Step 1249: Updated gated ratio to 0.9519 (progress: 48.1%)
Step 1249: Updated gated ratio to 0.9519 (progress: 48.1%)
Step 1249: Updated gated ratio to 0.9519 (progress: 48.1%)
Step 1249: Updated gated ratio to 0.9519 (progress: 48.1%)
Step 1249: Updated gated ratio to 0.9519 (progress: 48.1%)
{'loss': 1.0581, 'grad_norm': 1.6970371007919312, 'learning_rate': 1.7769237117674893e-05, 'epoch': 0.24}
{'loss': 1.0487, 'grad_norm': 1.6184508800506592, 'learning_rate': 1.7765312701746543e-05, 'epoch': 0.24}
{'loss': 1.2012, 'grad_norm': 1.3711556196212769, 'learning_rate': 1.7761385271056436e-05, 'epoch': 0.24}
{'loss': 1.1469, 'grad_norm': 1.6294187307357788, 'learning_rate': 1.7757454827129338e-05, 'epoch': 0.24}
{'loss': 1.0926, 'grad_norm': 1.718733549118042, 'learning_rate': 1.7753521371491174e-05, 'epoch': 0.24}
Step 1254: Updated gated ratio to 0.9517 (progress: 48.3%)
Step 1254: Updated gated ratio to 0.9517 (progress: 48.3%)
Step 1254: Updated gated ratio to 0.9517 (progress: 48.3%)
Step 1254: Updated gated ratio to 0.9517 (progress: 48.3%)
Step 1254: Updated gated ratio to 0.9517 (progress: 48.3%)
Step 1254: Updated gated ratio to 0.9517 (progress: 48.3%)
Step 1254: Updated gated ratio to 0.9517 (progress: 48.3%)
Step 1254: Updated gated ratio to 0.9517 (progress: 48.3%)
Step 1254: Updated gated ratio to 0.9517 (progress: 48.3%)
Step 1254: Updated gated ratio to 0.9517 (progress: 48.3%)
Step 1254: Updated gated ratio to 0.9517 (progress: 48.3%)
Step 1254: Updated gated ratio to 0.9517 (progress: 48.3%)
Step 1254: Updated gated ratio to 0.9517 (progress: 48.3%)
Step 1254: Updated gated ratio to 0.9517 (progress: 48.3%)
Step 1254: Updated gated ratio to 0.9517 (progress: 48.3%)
Step 1254: Updated gated ratio to 0.9517 (progress: 48.3%)
{'loss': 1.0269, 'grad_norm': 1.6672431230545044, 'learning_rate': 1.7749584905669057e-05, 'epoch': 0.24}
{'loss': 1.0866, 'grad_norm': 1.5658034086227417, 'learning_rate': 1.774564543119125e-05, 'epoch': 0.24}
{'loss': 1.1254, 'grad_norm': 1.6396898031234741, 'learning_rate': 1.7741702949587196e-05, 'epoch': 0.24}
{'loss': 1.0614, 'grad_norm': 1.5893622636795044, 'learning_rate': 1.7737757462387507e-05, 'epoch': 0.24}
{'loss': 1.1034, 'grad_norm': 1.6197973489761353, 'learning_rate': 1.7733808971123946e-05, 'epoch': 0.24}
Step 1259: Updated gated ratio to 0.9515 (progress: 48.5%)
Step 1259: Updated gated ratio to 0.9515 (progress: 48.5%)
Step 1259: Updated gated ratio to 0.9515 (progress: 48.5%)Step 1259: Updated gated ratio to 0.9515 (progress: 48.5%)

Step 1259: Updated gated ratio to 0.9515 (progress: 48.5%)
Step 1259: Updated gated ratio to 0.9515 (progress: 48.5%)
Step 1259: Updated gated ratio to 0.9515 (progress: 48.5%)
Step 1259: Updated gated ratio to 0.9515 (progress: 48.5%)Step 1259: Updated gated ratio to 0.9515 (progress: 48.5%)

Step 1259: Updated gated ratio to 0.9515 (progress: 48.5%)
Step 1259: Updated gated ratio to 0.9515 (progress: 48.5%)
Step 1259: Updated gated ratio to 0.9515 (progress: 48.5%)
Step 1259: Updated gated ratio to 0.9515 (progress: 48.5%)
Step 1259: Updated gated ratio to 0.9515 (progress: 48.5%)
Step 1259: Updated gated ratio to 0.9515 (progress: 48.5%)
Step 1259: Updated gated ratio to 0.9515 (progress: 48.5%)
{'loss': 1.1508, 'grad_norm': 1.782218098640442, 'learning_rate': 1.7729857477329463e-05, 'epoch': 0.24}
{'loss': 1.1581, 'grad_norm': 1.8181977272033691, 'learning_rate': 1.7725902982538162e-05, 'epoch': 0.24}
{'loss': 0.9645, 'grad_norm': 1.6362289190292358, 'learning_rate': 1.772194548828531e-05, 'epoch': 0.24}
{'loss': 1.082, 'grad_norm': 1.5796113014221191, 'learning_rate': 1.7717984996107346e-05, 'epoch': 0.24}
{'loss': 1.1218, 'grad_norm': 1.5545903444290161, 'learning_rate': 1.771402150754187e-05, 'epoch': 0.24}
Step 1264: Updated gated ratio to 0.9513 (progress: 48.7%)
Step 1264: Updated gated ratio to 0.9513 (progress: 48.7%)
Step 1264: Updated gated ratio to 0.9513 (progress: 48.7%)
Step 1264: Updated gated ratio to 0.9513 (progress: 48.7%)
Step 1264: Updated gated ratio to 0.9513 (progress: 48.7%)
Step 1264: Updated gated ratio to 0.9513 (progress: 48.7%)
Step 1264: Updated gated ratio to 0.9513 (progress: 48.7%)
Step 1264: Updated gated ratio to 0.9513 (progress: 48.7%)
Step 1264: Updated gated ratio to 0.9513 (progress: 48.7%)
Step 1264: Updated gated ratio to 0.9513 (progress: 48.7%)
Step 1264: Updated gated ratio to 0.9513 (progress: 48.7%)
Step 1264: Updated gated ratio to 0.9513 (progress: 48.7%)
Step 1264: Updated gated ratio to 0.9513 (progress: 48.7%)
Step 1264: Updated gated ratio to 0.9513 (progress: 48.7%)
Step 1264: Updated gated ratio to 0.9513 (progress: 48.7%)
Step 1264: Updated gated ratio to 0.9513 (progress: 48.7%)
{'loss': 1.1175, 'grad_norm': 1.6518256664276123, 'learning_rate': 1.7710055024127637e-05, 'epoch': 0.24}
{'loss': 1.1334, 'grad_norm': 1.3282767534255981, 'learning_rate': 1.7706085547404582e-05, 'epoch': 0.24}
{'loss': 1.2414, 'grad_norm': 1.646515130996704, 'learning_rate': 1.770211307891379e-05, 'epoch': 0.24}
{'loss': 1.1773, 'grad_norm': 1.7003262042999268, 'learning_rate': 1.769813762019751e-05, 'epoch': 0.24}
{'loss': 1.1463, 'grad_norm': 1.6087977886199951, 'learning_rate': 1.769415917279915e-05, 'epoch': 0.24}
{'loss': 1.1414, 'grad_norm': 1.4834929704666138, 'learning_rate': 1.7690177738263284e-05, 'epoch': 0.24}
Step 1270: Updated gated ratio to 0.9511 (progress: 48.9%)
Step 1270: Updated gated ratio to 0.9511 (progress: 48.9%)
Step 1270: Updated gated ratio to 0.9511 (progress: 48.9%)
Step 1270: Updated gated ratio to 0.9511 (progress: 48.9%)
Step 1270: Updated gated ratio to 0.9511 (progress: 48.9%)
Step 1270: Updated gated ratio to 0.9511 (progress: 48.9%)
Step 1270: Updated gated ratio to 0.9511 (progress: 48.9%)
Step 1270: Updated gated ratio to 0.9511 (progress: 48.9%)
Step 1270: Updated gated ratio to 0.9511 (progress: 48.9%)
Step 1270: Updated gated ratio to 0.9511 (progress: 48.9%)
Step 1270: Updated gated ratio to 0.9511 (progress: 48.9%)
Step 1270: Updated gated ratio to 0.9511 (progress: 48.9%)
Step 1270: Updated gated ratio to 0.9511 (progress: 48.9%)
Step 1270: Updated gated ratio to 0.9511 (progress: 48.9%)
Step 1270: Updated gated ratio to 0.9511 (progress: 48.9%)
Step 1270: Updated gated ratio to 0.9511 (progress: 48.9%)
{'loss': 1.2276, 'grad_norm': 1.3363417387008667, 'learning_rate': 1.7686193318135635e-05, 'epoch': 0.24}
{'loss': 1.1083, 'grad_norm': 1.53267240524292, 'learning_rate': 1.76822059139631e-05, 'epoch': 0.24}
{'loss': 1.0403, 'grad_norm': 1.5788300037384033, 'learning_rate': 1.7678215527293724e-05, 'epoch': 0.24}
{'loss': 1.1534, 'grad_norm': 1.3691734075546265, 'learning_rate': 1.767422215967671e-05, 'epoch': 0.25}
{'loss': 1.0492, 'grad_norm': 1.466860055923462, 'learning_rate': 1.767022581266242e-05, 'epoch': 0.25}
Step 1275: Updated gated ratio to 0.9509 (progress: 49.1%)
Step 1275: Updated gated ratio to 0.9509 (progress: 49.1%)
Step 1275: Updated gated ratio to 0.9509 (progress: 49.1%)Step 1275: Updated gated ratio to 0.9509 (progress: 49.1%)
Step 1275: Updated gated ratio to 0.9509 (progress: 49.1%)
Step 1275: Updated gated ratio to 0.9509 (progress: 49.1%)
Step 1275: Updated gated ratio to 0.9509 (progress: 49.1%)

Step 1275: Updated gated ratio to 0.9509 (progress: 49.1%)
Step 1275: Updated gated ratio to 0.9509 (progress: 49.1%)
Step 1275: Updated gated ratio to 0.9509 (progress: 49.1%)
Step 1275: Updated gated ratio to 0.9509 (progress: 49.1%)
Step 1275: Updated gated ratio to 0.9509 (progress: 49.1%)
Step 1275: Updated gated ratio to 0.9509 (progress: 49.1%)
Step 1275: Updated gated ratio to 0.9509 (progress: 49.1%)
Step 1275: Updated gated ratio to 0.9509 (progress: 49.1%)
Step 1275: Updated gated ratio to 0.9509 (progress: 49.1%)
{'loss': 1.1178, 'grad_norm': 1.6385313272476196, 'learning_rate': 1.766622648780238e-05, 'epoch': 0.25}
{'loss': 1.1489, 'grad_norm': 1.6425195932388306, 'learning_rate': 1.766222418664926e-05, 'epoch': 0.25}
{'loss': 1.1365, 'grad_norm': 1.5092886686325073, 'learning_rate': 1.765821891075689e-05, 'epoch': 0.25}
{'loss': 1.0789, 'grad_norm': 1.4872660636901855, 'learning_rate': 1.7654210661680263e-05, 'epoch': 0.25}
{'loss': 1.1396, 'grad_norm': 1.6551015377044678, 'learning_rate': 1.765019944097551e-05, 'epoch': 0.25}
Step 1280: Updated gated ratio to 0.9507 (progress: 49.3%)
Step 1280: Updated gated ratio to 0.9507 (progress: 49.3%)
Step 1280: Updated gated ratio to 0.9507 (progress: 49.3%)
Step 1280: Updated gated ratio to 0.9507 (progress: 49.3%)
Step 1280: Updated gated ratio to 0.9507 (progress: 49.3%)
Step 1280: Updated gated ratio to 0.9507 (progress: 49.3%)
Step 1280: Updated gated ratio to 0.9507 (progress: 49.3%)
Step 1280: Updated gated ratio to 0.9507 (progress: 49.3%)
Step 1280: Updated gated ratio to 0.9507 (progress: 49.3%)
Step 1280: Updated gated ratio to 0.9507 (progress: 49.3%)
Step 1280: Updated gated ratio to 0.9507 (progress: 49.3%)
Step 1280: Updated gated ratio to 0.9507 (progress: 49.3%)
Step 1280: Updated gated ratio to 0.9507 (progress: 49.3%)
Step 1280: Updated gated ratio to 0.9507 (progress: 49.3%)
Step 1280: Updated gated ratio to 0.9507 (progress: 49.3%)
Step 1280: Updated gated ratio to 0.9507 (progress: 49.3%)
{'loss': 1.0451, 'grad_norm': 1.6869487762451172, 'learning_rate': 1.7646185250199936e-05, 'epoch': 0.25}
{'loss': 1.156, 'grad_norm': 1.7280807495117188, 'learning_rate': 1.7642168090911976e-05, 'epoch': 0.25}
{'loss': 1.1406, 'grad_norm': 1.8383405208587646, 'learning_rate': 1.763814796467124e-05, 'epoch': 0.25}
{'loss': 1.0422, 'grad_norm': 1.5996812582015991, 'learning_rate': 1.763412487303847e-05, 'epoch': 0.25}
{'loss': 1.1517, 'grad_norm': 1.6482858657836914, 'learning_rate': 1.7630098817575578e-05, 'epoch': 0.25}
Step 1285: Updated gated ratio to 0.9505 (progress: 49.5%)
Step 1285: Updated gated ratio to 0.9505 (progress: 49.5%)
Step 1285: Updated gated ratio to 0.9505 (progress: 49.5%)
Step 1285: Updated gated ratio to 0.9505 (progress: 49.5%)
Step 1285: Updated gated ratio to 0.9505 (progress: 49.5%)
Step 1285: Updated gated ratio to 0.9505 (progress: 49.5%)
Step 1285: Updated gated ratio to 0.9505 (progress: 49.5%)
Step 1285: Updated gated ratio to 0.9505 (progress: 49.5%)
Step 1285: Updated gated ratio to 0.9505 (progress: 49.5%)
Step 1285: Updated gated ratio to 0.9505 (progress: 49.5%)
Step 1285: Updated gated ratio to 0.9505 (progress: 49.5%)
Step 1285: Updated gated ratio to 0.9505 (progress: 49.5%)
Step 1285: Updated gated ratio to 0.9505 (progress: 49.5%)
Step 1285: Updated gated ratio to 0.9505 (progress: 49.5%)
Step 1285: Updated gated ratio to 0.9505 (progress: 49.5%)
Step 1285: Updated gated ratio to 0.9505 (progress: 49.5%)
{'loss': 1.1174, 'grad_norm': 1.5370649099349976, 'learning_rate': 1.762606979984561e-05, 'epoch': 0.25}
{'loss': 1.0818, 'grad_norm': 1.6855274438858032, 'learning_rate': 1.7622037821412775e-05, 'epoch': 0.25}
{'loss': 1.1347, 'grad_norm': 1.6569838523864746, 'learning_rate': 1.7618002883842426e-05, 'epoch': 0.25}
{'loss': 1.1378, 'grad_norm': 1.7416568994522095, 'learning_rate': 1.7613964988701057e-05, 'epoch': 0.25}
{'loss': 1.1451, 'grad_norm': 1.800737977027893, 'learning_rate': 1.7609924137556326e-05, 'epoch': 0.25}
Step 1290: Updated gated ratio to 0.9503 (progress: 49.7%)
Step 1290: Updated gated ratio to 0.9503 (progress: 49.7%)
Step 1290: Updated gated ratio to 0.9503 (progress: 49.7%)
Step 1290: Updated gated ratio to 0.9503 (progress: 49.7%)
Step 1290: Updated gated ratio to 0.9503 (progress: 49.7%)Step 1290: Updated gated ratio to 0.9503 (progress: 49.7%)
Step 1290: Updated gated ratio to 0.9503 (progress: 49.7%)
Step 1290: Updated gated ratio to 0.9503 (progress: 49.7%)
Step 1290: Updated gated ratio to 0.9503 (progress: 49.7%)
Step 1290: Updated gated ratio to 0.9503 (progress: 49.7%)
Step 1290: Updated gated ratio to 0.9503 (progress: 49.7%)
Step 1290: Updated gated ratio to 0.9503 (progress: 49.7%)

Step 1290: Updated gated ratio to 0.9503 (progress: 49.7%)
Step 1290: Updated gated ratio to 0.9503 (progress: 49.7%)
Step 1290: Updated gated ratio to 0.9503 (progress: 49.7%)
Step 1290: Updated gated ratio to 0.9503 (progress: 49.7%)
{'loss': 1.196, 'grad_norm': 1.3288222551345825, 'learning_rate': 1.7605880331977022e-05, 'epoch': 0.25}
{'loss': 1.0852, 'grad_norm': 1.5959711074829102, 'learning_rate': 1.76018335735331e-05, 'epoch': 0.25}
{'loss': 1.1018, 'grad_norm': 1.1868778467178345, 'learning_rate': 1.7597783863795644e-05, 'epoch': 0.25}
{'loss': 1.0416, 'grad_norm': 1.7054232358932495, 'learning_rate': 1.7593731204336895e-05, 'epoch': 0.25}
{'loss': 1.1725, 'grad_norm': 1.5604819059371948, 'learning_rate': 1.7589675596730233e-05, 'epoch': 0.25}
{'loss': 1.1103, 'grad_norm': 1.885151743888855, 'learning_rate': 1.758561704255018e-05, 'epoch': 0.25}
Step 1296: Updated gated ratio to 0.9501 (progress: 49.9%)
Step 1296: Updated gated ratio to 0.9501 (progress: 49.9%)
Step 1296: Updated gated ratio to 0.9501 (progress: 49.9%)
Step 1296: Updated gated ratio to 0.9501 (progress: 49.9%)Step 1296: Updated gated ratio to 0.9501 (progress: 49.9%)

Step 1296: Updated gated ratio to 0.9501 (progress: 49.9%)
Step 1296: Updated gated ratio to 0.9501 (progress: 49.9%)
Step 1296: Updated gated ratio to 0.9501 (progress: 49.9%)
Step 1296: Updated gated ratio to 0.9501 (progress: 49.9%)
Step 1296: Updated gated ratio to 0.9501 (progress: 49.9%)
Step 1296: Updated gated ratio to 0.9501 (progress: 49.9%)
Step 1296: Updated gated ratio to 0.9501 (progress: 49.9%)
Step 1296: Updated gated ratio to 0.9501 (progress: 49.9%)
Step 1296: Updated gated ratio to 0.9501 (progress: 49.9%)
Step 1296: Updated gated ratio to 0.9501 (progress: 49.9%)
Step 1296: Updated gated ratio to 0.9501 (progress: 49.9%)
{'loss': 1.1625, 'grad_norm': 1.6804313659667969, 'learning_rate': 1.7581555543372413e-05, 'epoch': 0.25}
{'loss': 1.0752, 'grad_norm': 1.6319383382797241, 'learning_rate': 1.7577491100773744e-05, 'epoch': 0.25}
{'loss': 1.1357, 'grad_norm': 1.5719431638717651, 'learning_rate': 1.7573423716332128e-05, 'epoch': 0.25}
{'loss': 1.0971, 'grad_norm': 1.5913158655166626, 'learning_rate': 1.7569353391626665e-05, 'epoch': 0.25}
{'loss': 1.1665, 'grad_norm': 1.7235596179962158, 'learning_rate': 1.7565280128237595e-05, 'epoch': 0.25}
Step 1301: Updated gated ratio to 0.9499 (progress: 50.1%)
Step 1301: Updated gated ratio to 0.9499 (progress: 50.1%)
Step 1301: Updated gated ratio to 0.9499 (progress: 50.1%)
Step 1301: Updated gated ratio to 0.9499 (progress: 50.1%)
Step 1301: Updated gated ratio to 0.9499 (progress: 50.1%)
Step 1301: Updated gated ratio to 0.9499 (progress: 50.1%)
Step 1301: Updated gated ratio to 0.9499 (progress: 50.1%)
Step 1301: Updated gated ratio to 0.9499 (progress: 50.1%)
Step 1301: Updated gated ratio to 0.9499 (progress: 50.1%)
Step 1301: Updated gated ratio to 0.9499 (progress: 50.1%)
Step 1301: Updated gated ratio to 0.9499 (progress: 50.1%)
Step 1301: Updated gated ratio to 0.9499 (progress: 50.1%)
Step 1301: Updated gated ratio to 0.9499 (progress: 50.1%)
Step 1301: Updated gated ratio to 0.9499 (progress: 50.1%)
Step 1301: Updated gated ratio to 0.9499 (progress: 50.1%)
Step 1301: Updated gated ratio to 0.9499 (progress: 50.1%)
{'loss': 1.1292, 'grad_norm': 1.3218426704406738, 'learning_rate': 1.75612039277463e-05, 'epoch': 0.25}
{'loss': 1.0715, 'grad_norm': 1.6857157945632935, 'learning_rate': 1.75571247917353e-05, 'epoch': 0.25}
{'loss': 1.2316, 'grad_norm': 1.8276978731155396, 'learning_rate': 1.7553042721788255e-05, 'epoch': 0.25}
{'loss': 1.1728, 'grad_norm': 1.7489538192749023, 'learning_rate': 1.754895771948997e-05, 'epoch': 0.25}
{'loss': 1.1585, 'grad_norm': 1.5661790370941162, 'learning_rate': 1.754486978642637e-05, 'epoch': 0.25}
Step 1306: Updated gated ratio to 0.9497 (progress: 50.3%)
Step 1306: Updated gated ratio to 0.9497 (progress: 50.3%)
Step 1306: Updated gated ratio to 0.9497 (progress: 50.3%)
Step 1306: Updated gated ratio to 0.9497 (progress: 50.3%)
Step 1306: Updated gated ratio to 0.9497 (progress: 50.3%)Step 1306: Updated gated ratio to 0.9497 (progress: 50.3%)

Step 1306: Updated gated ratio to 0.9497 (progress: 50.3%)
Step 1306: Updated gated ratio to 0.9497 (progress: 50.3%)
Step 1306: Updated gated ratio to 0.9497 (progress: 50.3%)
Step 1306: Updated gated ratio to 0.9497 (progress: 50.3%)
Step 1306: Updated gated ratio to 0.9497 (progress: 50.3%)
Step 1306: Updated gated ratio to 0.9497 (progress: 50.3%)
Step 1306: Updated gated ratio to 0.9497 (progress: 50.3%)
Step 1306: Updated gated ratio to 0.9497 (progress: 50.3%)
Step 1306: Updated gated ratio to 0.9497 (progress: 50.3%)
Step 1306: Updated gated ratio to 0.9497 (progress: 50.3%)
{'loss': 1.1205, 'grad_norm': 1.6960304975509644, 'learning_rate': 1.7540778924184553e-05, 'epoch': 0.25}
{'loss': 1.1011, 'grad_norm': 1.7331491708755493, 'learning_rate': 1.7536685134352717e-05, 'epoch': 0.25}
{'loss': 1.065, 'grad_norm': 1.57825767993927, 'learning_rate': 1.7532588418520215e-05, 'epoch': 0.25}
{'loss': 1.1302, 'grad_norm': 1.444847822189331, 'learning_rate': 1.7528488778277535e-05, 'epoch': 0.25}
{'loss': 1.1981, 'grad_norm': 1.3598183393478394, 'learning_rate': 1.75243862152163e-05, 'epoch': 0.25}
Step 1311: Updated gated ratio to 0.9495 (progress: 50.5%)
Step 1311: Updated gated ratio to 0.9495 (progress: 50.5%)
Step 1311: Updated gated ratio to 0.9495 (progress: 50.5%)
Step 1311: Updated gated ratio to 0.9495 (progress: 50.5%)
Step 1311: Updated gated ratio to 0.9495 (progress: 50.5%)
Step 1311: Updated gated ratio to 0.9495 (progress: 50.5%)
Step 1311: Updated gated ratio to 0.9495 (progress: 50.5%)
Step 1311: Updated gated ratio to 0.9495 (progress: 50.5%)
Step 1311: Updated gated ratio to 0.9495 (progress: 50.5%)
Step 1311: Updated gated ratio to 0.9495 (progress: 50.5%)
Step 1311: Updated gated ratio to 0.9495 (progress: 50.5%)
Step 1311: Updated gated ratio to 0.9495 (progress: 50.5%)
Step 1311: Updated gated ratio to 0.9495 (progress: 50.5%)
Step 1311: Updated gated ratio to 0.9495 (progress: 50.5%)
Step 1311: Updated gated ratio to 0.9495 (progress: 50.5%)
Step 1311: Updated gated ratio to 0.9495 (progress: 50.5%)
{'loss': 1.1802, 'grad_norm': 1.6071351766586304, 'learning_rate': 1.752028073092926e-05, 'epoch': 0.25}
{'loss': 1.1565, 'grad_norm': 1.6980578899383545, 'learning_rate': 1.7516172327010314e-05, 'epoch': 0.25}
{'loss': 1.1736, 'grad_norm': 1.6095679998397827, 'learning_rate': 1.751206100505448e-05, 'epoch': 0.25}
{'loss': 1.0945, 'grad_norm': 1.5154484510421753, 'learning_rate': 1.7507946766657914e-05, 'epoch': 0.25}
{'loss': 1.1324, 'grad_norm': 1.7695509195327759, 'learning_rate': 1.7503829613417905e-05, 'epoch': 0.25}
Step 1316: Updated gated ratio to 0.9493 (progress: 50.7%)
Step 1316: Updated gated ratio to 0.9493 (progress: 50.7%)
Step 1316: Updated gated ratio to 0.9493 (progress: 50.7%)
Step 1316: Updated gated ratio to 0.9493 (progress: 50.7%)Step 1316: Updated gated ratio to 0.9493 (progress: 50.7%)

Step 1316: Updated gated ratio to 0.9493 (progress: 50.7%)
Step 1316: Updated gated ratio to 0.9493 (progress: 50.7%)
Step 1316: Updated gated ratio to 0.9493 (progress: 50.7%)
Step 1316: Updated gated ratio to 0.9493 (progress: 50.7%)
Step 1316: Updated gated ratio to 0.9493 (progress: 50.7%)
Step 1316: Updated gated ratio to 0.9493 (progress: 50.7%)
Step 1316: Updated gated ratio to 0.9493 (progress: 50.7%)
Step 1316: Updated gated ratio to 0.9493 (progress: 50.7%)
Step 1316: Updated gated ratio to 0.9493 (progress: 50.7%)
Step 1316: Updated gated ratio to 0.9493 (progress: 50.7%)
Step 1316: Updated gated ratio to 0.9493 (progress: 50.7%)
{'loss': 1.1248, 'grad_norm': 1.799251914024353, 'learning_rate': 1.749970954693288e-05, 'epoch': 0.25}
{'loss': 1.2029, 'grad_norm': 1.6952418088912964, 'learning_rate': 1.7495586568802384e-05, 'epoch': 0.25}
{'loss': 1.1178, 'grad_norm': 1.6294060945510864, 'learning_rate': 1.7491460680627105e-05, 'epoch': 0.25}
{'loss': 1.1519, 'grad_norm': 1.7727981805801392, 'learning_rate': 1.7487331884008845e-05, 'epoch': 0.25}
{'loss': 1.1581, 'grad_norm': 1.7503294944763184, 'learning_rate': 1.7483200180550554e-05, 'epoch': 0.25}
{'loss': 1.1197, 'grad_norm': 1.6327214241027832, 'learning_rate': 1.74790655718563e-05, 'epoch': 0.25}
Step 1322: Updated gated ratio to 0.9491 (progress: 50.9%)
Step 1322: Updated gated ratio to 0.9491 (progress: 50.9%)
Step 1322: Updated gated ratio to 0.9491 (progress: 50.9%)
Step 1322: Updated gated ratio to 0.9491 (progress: 50.9%)
Step 1322: Updated gated ratio to 0.9491 (progress: 50.9%)
Step 1322: Updated gated ratio to 0.9491 (progress: 50.9%)
Step 1322: Updated gated ratio to 0.9491 (progress: 50.9%)
Step 1322: Updated gated ratio to 0.9491 (progress: 50.9%)
Step 1322: Updated gated ratio to 0.9491 (progress: 50.9%)
Step 1322: Updated gated ratio to 0.9491 (progress: 50.9%)
Step 1322: Updated gated ratio to 0.9491 (progress: 50.9%)
Step 1322: Updated gated ratio to 0.9491 (progress: 50.9%)Step 1322: Updated gated ratio to 0.9491 (progress: 50.9%)

Step 1322: Updated gated ratio to 0.9491 (progress: 50.9%)
Step 1322: Updated gated ratio to 0.9491 (progress: 50.9%)
Step 1322: Updated gated ratio to 0.9491 (progress: 50.9%)
{'loss': 1.1186, 'grad_norm': 1.5833288431167603, 'learning_rate': 1.747492805953128e-05, 'epoch': 0.25}
{'loss': 1.1803, 'grad_norm': 1.6951336860656738, 'learning_rate': 1.7470787645181818e-05, 'epoch': 0.25}
Step 1324: Updated gated ratio to 0.9490 (progress: 51.0%)
Step 1324: Updated gated ratio to 0.9490 (progress: 51.0%)
Step 1324: Updated gated ratio to 0.9490 (progress: 51.0%)
Step 1324: Updated gated ratio to 0.9490 (progress: 51.0%)
Step 1324: Updated gated ratio to 0.9490 (progress: 51.0%)
Step 1324: Updated gated ratio to 0.9490 (progress: 51.0%)
Step 1324: Updated gated ratio to 0.9490 (progress: 51.0%)
Step 1324: Updated gated ratio to 0.9490 (progress: 51.0%)
Step 1324: Updated gated ratio to 0.9490 (progress: 51.0%)
Step 1324: Updated gated ratio to 0.9490 (progress: 51.0%)
Step 1324: Updated gated ratio to 0.9490 (progress: 51.0%)
Step 1324: Updated gated ratio to 0.9490 (progress: 51.0%)
Step 1324: Updated gated ratio to 0.9490 (progress: 51.0%)
Step 1324: Updated gated ratio to 0.9490 (progress: 51.0%)
Step 1324: Updated gated ratio to 0.9490 (progress: 51.0%)
Step 1324: Updated gated ratio to 0.9490 (progress: 51.0%)
{'loss': 1.0236, 'grad_norm': 1.5302577018737793, 'learning_rate': 1.7466644330415362e-05, 'epoch': 0.25}
{'loss': 1.0859, 'grad_norm': 1.5468724966049194, 'learning_rate': 1.7462498116840496e-05, 'epoch': 0.26}
{'loss': 1.1295, 'grad_norm': 1.5050417184829712, 'learning_rate': 1.745834900606692e-05, 'epoch': 0.26}
{'loss': 1.1345, 'grad_norm': 1.4847873449325562, 'learning_rate': 1.7454196999705458e-05, 'epoch': 0.26}
{'loss': 1.0557, 'grad_norm': 1.5982565879821777, 'learning_rate': 1.7450042099368066e-05, 'epoch': 0.26}
Step 1329: Updated gated ratio to 0.9488 (progress: 51.2%)
Step 1329: Updated gated ratio to 0.9488 (progress: 51.2%)
Step 1329: Updated gated ratio to 0.9488 (progress: 51.2%)
Step 1329: Updated gated ratio to 0.9488 (progress: 51.2%)
Step 1329: Updated gated ratio to 0.9488 (progress: 51.2%)
Step 1329: Updated gated ratio to 0.9488 (progress: 51.2%)
Step 1329: Updated gated ratio to 0.9488 (progress: 51.2%)
Step 1329: Updated gated ratio to 0.9488 (progress: 51.2%)
Step 1329: Updated gated ratio to 0.9488 (progress: 51.2%)
Step 1329: Updated gated ratio to 0.9488 (progress: 51.2%)
Step 1329: Updated gated ratio to 0.9488 (progress: 51.2%)
Step 1329: Updated gated ratio to 0.9488 (progress: 51.2%)
Step 1329: Updated gated ratio to 0.9488 (progress: 51.2%)
Step 1329: Updated gated ratio to 0.9488 (progress: 51.2%)
Step 1329: Updated gated ratio to 0.9488 (progress: 51.2%)
Step 1329: Updated gated ratio to 0.9488 (progress: 51.2%)
{'loss': 1.073, 'grad_norm': 1.488369107246399, 'learning_rate': 1.7445884306667823e-05, 'epoch': 0.26}
{'loss': 1.0993, 'grad_norm': 1.2544556856155396, 'learning_rate': 1.7441723623218917e-05, 'epoch': 0.26}
{'loss': 1.2181, 'grad_norm': 1.47685968875885, 'learning_rate': 1.7437560050636678e-05, 'epoch': 0.26}
{'loss': 1.0835, 'grad_norm': 1.5588500499725342, 'learning_rate': 1.7433393590537543e-05, 'epoch': 0.26}
{'loss': 1.0953, 'grad_norm': 1.604859709739685, 'learning_rate': 1.7429224244539077e-05, 'epoch': 0.26}
{'loss': 1.0701, 'grad_norm': 1.5807251930236816, 'learning_rate': 1.7425052014259965e-05, 'epoch': 0.26}
Step 1335: Updated gated ratio to 0.9486 (progress: 51.4%)
Step 1335: Updated gated ratio to 0.9486 (progress: 51.4%)Step 1335: Updated gated ratio to 0.9486 (progress: 51.4%)

Step 1335: Updated gated ratio to 0.9486 (progress: 51.4%)
Step 1335: Updated gated ratio to 0.9486 (progress: 51.4%)
Step 1335: Updated gated ratio to 0.9486 (progress: 51.4%)
Step 1335: Updated gated ratio to 0.9486 (progress: 51.4%)
Step 1335: Updated gated ratio to 0.9486 (progress: 51.4%)
Step 1335: Updated gated ratio to 0.9486 (progress: 51.4%)
Step 1335: Updated gated ratio to 0.9486 (progress: 51.4%)
Step 1335: Updated gated ratio to 0.9486 (progress: 51.4%)
Step 1335: Updated gated ratio to 0.9486 (progress: 51.4%)
Step 1335: Updated gated ratio to 0.9486 (progress: 51.4%)
Step 1335: Updated gated ratio to 0.9486 (progress: 51.4%)
Step 1335: Updated gated ratio to 0.9486 (progress: 51.4%)
Step 1335: Updated gated ratio to 0.9486 (progress: 51.4%)
{'loss': 1.1743, 'grad_norm': 1.3168503046035767, 'learning_rate': 1.7420876901320006e-05, 'epoch': 0.26}
{'loss': 1.1296, 'grad_norm': 1.4518744945526123, 'learning_rate': 1.7416698907340128e-05, 'epoch': 0.26}
{'loss': 1.1108, 'grad_norm': 1.6112034320831299, 'learning_rate': 1.741251803394237e-05, 'epoch': 0.26}
{'loss': 1.104, 'grad_norm': 1.7367480993270874, 'learning_rate': 1.740833428274989e-05, 'epoch': 0.26}
{'loss': 1.1027, 'grad_norm': 1.5328480005264282, 'learning_rate': 1.7404147655386966e-05, 'epoch': 0.26}
Step 1340: Updated gated ratio to 0.9484 (progress: 51.6%)
Step 1340: Updated gated ratio to 0.9484 (progress: 51.6%)
Step 1340: Updated gated ratio to 0.9484 (progress: 51.6%)
Step 1340: Updated gated ratio to 0.9484 (progress: 51.6%)Step 1340: Updated gated ratio to 0.9484 (progress: 51.6%)
Step 1340: Updated gated ratio to 0.9484 (progress: 51.6%)
Step 1340: Updated gated ratio to 0.9484 (progress: 51.6%)
Step 1340: Updated gated ratio to 0.9484 (progress: 51.6%)

Step 1340: Updated gated ratio to 0.9484 (progress: 51.6%)
Step 1340: Updated gated ratio to 0.9484 (progress: 51.6%)
Step 1340: Updated gated ratio to 0.9484 (progress: 51.6%)
Step 1340: Updated gated ratio to 0.9484 (progress: 51.6%)
Step 1340: Updated gated ratio to 0.9484 (progress: 51.6%)
Step 1340: Updated gated ratio to 0.9484 (progress: 51.6%)
Step 1340: Updated gated ratio to 0.9484 (progress: 51.6%)
Step 1340: Updated gated ratio to 0.9484 (progress: 51.6%)
{'loss': 1.1464, 'grad_norm': 1.7249107360839844, 'learning_rate': 1.739995815347899e-05, 'epoch': 0.26}
{'loss': 1.0925, 'grad_norm': 1.5471000671386719, 'learning_rate': 1.739576577865247e-05, 'epoch': 0.26}
{'loss': 1.053, 'grad_norm': 1.430656909942627, 'learning_rate': 1.739157053253503e-05, 'epoch': 0.26}
{'loss': 1.1275, 'grad_norm': 1.2059783935546875, 'learning_rate': 1.738737241675541e-05, 'epoch': 0.26}
{'loss': 1.1327, 'grad_norm': 1.5384478569030762, 'learning_rate': 1.7383171432943466e-05, 'epoch': 0.26}
Step 1345: Updated gated ratio to 0.9482 (progress: 51.8%)
Step 1345: Updated gated ratio to 0.9482 (progress: 51.8%)
Step 1345: Updated gated ratio to 0.9482 (progress: 51.8%)Step 1345: Updated gated ratio to 0.9482 (progress: 51.8%)
Step 1345: Updated gated ratio to 0.9482 (progress: 51.8%)
Step 1345: Updated gated ratio to 0.9482 (progress: 51.8%)
Step 1345: Updated gated ratio to 0.9482 (progress: 51.8%)
Step 1345: Updated gated ratio to 0.9482 (progress: 51.8%)
Step 1345: Updated gated ratio to 0.9482 (progress: 51.8%)

Step 1345: Updated gated ratio to 0.9482 (progress: 51.8%)
Step 1345: Updated gated ratio to 0.9482 (progress: 51.8%)
Step 1345: Updated gated ratio to 0.9482 (progress: 51.8%)
Step 1345: Updated gated ratio to 0.9482 (progress: 51.8%)
Step 1345: Updated gated ratio to 0.9482 (progress: 51.8%)
Step 1345: Updated gated ratio to 0.9482 (progress: 51.8%)
Step 1345: Updated gated ratio to 0.9482 (progress: 51.8%)
{'loss': 1.0348, 'grad_norm': 1.60125732421875, 'learning_rate': 1.737896758273016e-05, 'epoch': 0.26}
{'loss': 1.0701, 'grad_norm': 1.5864838361740112, 'learning_rate': 1.7374760867747574e-05, 'epoch': 0.26}
{'loss': 1.1126, 'grad_norm': 1.627639651298523, 'learning_rate': 1.7370551289628895e-05, 'epoch': 0.26}
{'loss': 0.9416, 'grad_norm': 1.5640780925750732, 'learning_rate': 1.7366338850008432e-05, 'epoch': 0.26}
{'loss': 1.0779, 'grad_norm': 1.541867971420288, 'learning_rate': 1.73621235505216e-05, 'epoch': 0.26}
Step 1350: Updated gated ratio to 0.9480 (progress: 52.0%)
Step 1350: Updated gated ratio to 0.9480 (progress: 52.0%)
Step 1350: Updated gated ratio to 0.9480 (progress: 52.0%)
Step 1350: Updated gated ratio to 0.9480 (progress: 52.0%)
Step 1350: Updated gated ratio to 0.9480 (progress: 52.0%)
Step 1350: Updated gated ratio to 0.9480 (progress: 52.0%)
Step 1350: Updated gated ratio to 0.9480 (progress: 52.0%)
Step 1350: Updated gated ratio to 0.9480 (progress: 52.0%)Step 1350: Updated gated ratio to 0.9480 (progress: 52.0%)

Step 1350: Updated gated ratio to 0.9480 (progress: 52.0%)
Step 1350: Updated gated ratio to 0.9480 (progress: 52.0%)
Step 1350: Updated gated ratio to 0.9480 (progress: 52.0%)
Step 1350: Updated gated ratio to 0.9480 (progress: 52.0%)
Step 1350: Updated gated ratio to 0.9480 (progress: 52.0%)
Step 1350: Updated gated ratio to 0.9480 (progress: 52.0%)
Step 1350: Updated gated ratio to 0.9480 (progress: 52.0%)
{'loss': 1.0496, 'grad_norm': 1.5724005699157715, 'learning_rate': 1.7357905392804918e-05, 'epoch': 0.26}
{'loss': 1.1641, 'grad_norm': 1.5838072299957275, 'learning_rate': 1.735368437849602e-05, 'epoch': 0.26}
{'loss': 1.1863, 'grad_norm': 1.7880223989486694, 'learning_rate': 1.7349460509233654e-05, 'epoch': 0.26}
{'loss': 1.1263, 'grad_norm': 1.6188853979110718, 'learning_rate': 1.734523378665767e-05, 'epoch': 0.26}
{'loss': 1.1598, 'grad_norm': 1.539512038230896, 'learning_rate': 1.7341004212409026e-05, 'epoch': 0.26}
Step 1355: Updated gated ratio to 0.9478 (progress: 52.2%)
Step 1355: Updated gated ratio to 0.9478 (progress: 52.2%)
Step 1355: Updated gated ratio to 0.9478 (progress: 52.2%)
Step 1355: Updated gated ratio to 0.9478 (progress: 52.2%)
Step 1355: Updated gated ratio to 0.9478 (progress: 52.2%)
Step 1355: Updated gated ratio to 0.9478 (progress: 52.2%)Step 1355: Updated gated ratio to 0.9478 (progress: 52.2%)

Step 1355: Updated gated ratio to 0.9478 (progress: 52.2%)
Step 1355: Updated gated ratio to 0.9478 (progress: 52.2%)
Step 1355: Updated gated ratio to 0.9478 (progress: 52.2%)
Step 1355: Updated gated ratio to 0.9478 (progress: 52.2%)
Step 1355: Updated gated ratio to 0.9478 (progress: 52.2%)
Step 1355: Updated gated ratio to 0.9478 (progress: 52.2%)
Step 1355: Updated gated ratio to 0.9478 (progress: 52.2%)
Step 1355: Updated gated ratio to 0.9478 (progress: 52.2%)
Step 1355: Updated gated ratio to 0.9478 (progress: 52.2%)
{'loss': 1.1458, 'grad_norm': 1.3394745588302612, 'learning_rate': 1.7336771788129785e-05, 'epoch': 0.26}
{'loss': 1.0834, 'grad_norm': 1.4747363328933716, 'learning_rate': 1.7332536515463126e-05, 'epoch': 0.26}
{'loss': 1.0558, 'grad_norm': 1.6851515769958496, 'learning_rate': 1.7328298396053324e-05, 'epoch': 0.26}
{'loss': 1.0374, 'grad_norm': 1.601590633392334, 'learning_rate': 1.7324057431545768e-05, 'epoch': 0.26}
{'loss': 1.1584, 'grad_norm': 1.722424030303955, 'learning_rate': 1.7319813623586935e-05, 'epoch': 0.26}
{'loss': 1.1676, 'grad_norm': 1.7363137006759644, 'learning_rate': 1.7315566973824433e-05, 'epoch': 0.26}
Step 1361: Updated gated ratio to 0.9476 (progress: 52.4%)
Step 1361: Updated gated ratio to 0.9476 (progress: 52.4%)
Step 1361: Updated gated ratio to 0.9476 (progress: 52.4%)
Step 1361: Updated gated ratio to 0.9476 (progress: 52.4%)
Step 1361: Updated gated ratio to 0.9476 (progress: 52.4%)
Step 1361: Updated gated ratio to 0.9476 (progress: 52.4%)
Step 1361: Updated gated ratio to 0.9476 (progress: 52.4%)
Step 1361: Updated gated ratio to 0.9476 (progress: 52.4%)
Step 1361: Updated gated ratio to 0.9476 (progress: 52.4%)
Step 1361: Updated gated ratio to 0.9476 (progress: 52.4%)
Step 1361: Updated gated ratio to 0.9476 (progress: 52.4%)
Step 1361: Updated gated ratio to 0.9476 (progress: 52.4%)
Step 1361: Updated gated ratio to 0.9476 (progress: 52.4%)
Step 1361: Updated gated ratio to 0.9476 (progress: 52.4%)
Step 1361: Updated gated ratio to 0.9476 (progress: 52.4%)
Step 1361: Updated gated ratio to 0.9476 (progress: 52.4%)
{'loss': 1.0946, 'grad_norm': 1.6377869844436646, 'learning_rate': 1.7311317483906946e-05, 'epoch': 0.26}
{'loss': 1.1049, 'grad_norm': 1.5736159086227417, 'learning_rate': 1.730706515548427e-05, 'epoch': 0.26}
{'loss': 1.1714, 'grad_norm': 1.5662611722946167, 'learning_rate': 1.730280999020732e-05, 'epoch': 0.26}
{'loss': 1.1584, 'grad_norm': 1.4264769554138184, 'learning_rate': 1.729855198972808e-05, 'epoch': 0.26}
{'loss': 1.1059, 'grad_norm': 1.5915051698684692, 'learning_rate': 1.729429115569967e-05, 'epoch': 0.26}
Step 1366: Updated gated ratio to 0.9474 (progress: 52.6%)
Step 1366: Updated gated ratio to 0.9474 (progress: 52.6%)
Step 1366: Updated gated ratio to 0.9474 (progress: 52.6%)
Step 1366: Updated gated ratio to 0.9474 (progress: 52.6%)
Step 1366: Updated gated ratio to 0.9474 (progress: 52.6%)
Step 1366: Updated gated ratio to 0.9474 (progress: 52.6%)
Step 1366: Updated gated ratio to 0.9474 (progress: 52.6%)
Step 1366: Updated gated ratio to 0.9474 (progress: 52.6%)
Step 1366: Updated gated ratio to 0.9474 (progress: 52.6%)
Step 1366: Updated gated ratio to 0.9474 (progress: 52.6%)
Step 1366: Updated gated ratio to 0.9474 (progress: 52.6%)
Step 1366: Updated gated ratio to 0.9474 (progress: 52.6%)
Step 1366: Updated gated ratio to 0.9474 (progress: 52.6%)
Step 1366: Updated gated ratio to 0.9474 (progress: 52.6%)
Step 1366: Updated gated ratio to 0.9474 (progress: 52.6%)
Step 1366: Updated gated ratio to 0.9474 (progress: 52.6%)
{'loss': 1.0716, 'grad_norm': 1.659688115119934, 'learning_rate': 1.729002748977628e-05, 'epoch': 0.26}
{'loss': 1.1901, 'grad_norm': 1.732343316078186, 'learning_rate': 1.7285760993613215e-05, 'epoch': 0.26}
{'loss': 1.1483, 'grad_norm': 1.594888687133789, 'learning_rate': 1.7281491668866874e-05, 'epoch': 0.26}
{'loss': 1.1762, 'grad_norm': 1.7564362287521362, 'learning_rate': 1.727721951719476e-05, 'epoch': 0.26}
{'loss': 1.0772, 'grad_norm': 1.667208194732666, 'learning_rate': 1.7272944540255468e-05, 'epoch': 0.26}
Step 1371: Updated gated ratio to 0.9472 (progress: 52.8%)
Step 1371: Updated gated ratio to 0.9472 (progress: 52.8%)
Step 1371: Updated gated ratio to 0.9472 (progress: 52.8%)
Step 1371: Updated gated ratio to 0.9472 (progress: 52.8%)
Step 1371: Updated gated ratio to 0.9472 (progress: 52.8%)
Step 1371: Updated gated ratio to 0.9472 (progress: 52.8%)
Step 1371: Updated gated ratio to 0.9472 (progress: 52.8%)
Step 1371: Updated gated ratio to 0.9472 (progress: 52.8%)
Step 1371: Updated gated ratio to 0.9472 (progress: 52.8%)
Step 1371: Updated gated ratio to 0.9472 (progress: 52.8%)
Step 1371: Updated gated ratio to 0.9472 (progress: 52.8%)
Step 1371: Updated gated ratio to 0.9472 (progress: 52.8%)
Step 1371: Updated gated ratio to 0.9472 (progress: 52.8%)
Step 1371: Updated gated ratio to 0.9472 (progress: 52.8%)
Step 1371: Updated gated ratio to 0.9472 (progress: 52.8%)
Step 1371: Updated gated ratio to 0.9472 (progress: 52.8%)
{'loss': 1.1357, 'grad_norm': 1.5209957361221313, 'learning_rate': 1.726866673970869e-05, 'epoch': 0.26}
{'loss': 1.0358, 'grad_norm': 1.5553767681121826, 'learning_rate': 1.7264386117215216e-05, 'epoch': 0.26}
{'loss': 1.1371, 'grad_norm': 1.5354514122009277, 'learning_rate': 1.7260102674436933e-05, 'epoch': 0.26}
{'loss': 1.1927, 'grad_norm': 1.4352390766143799, 'learning_rate': 1.7255816413036818e-05, 'epoch': 0.26}
{'loss': 1.0806, 'grad_norm': 1.555928111076355, 'learning_rate': 1.7251527334678946e-05, 'epoch': 0.26}
Step 1376: Updated gated ratio to 0.9470 (progress: 53.0%)
Step 1376: Updated gated ratio to 0.9470 (progress: 53.0%)
Step 1376: Updated gated ratio to 0.9470 (progress: 53.0%)Step 1376: Updated gated ratio to 0.9470 (progress: 53.0%)

Step 1376: Updated gated ratio to 0.9470 (progress: 53.0%)
Step 1376: Updated gated ratio to 0.9470 (progress: 53.0%)
Step 1376: Updated gated ratio to 0.9470 (progress: 53.0%)
Step 1376: Updated gated ratio to 0.9470 (progress: 53.0%)
Step 1376: Updated gated ratio to 0.9470 (progress: 53.0%)
Step 1376: Updated gated ratio to 0.9470 (progress: 53.0%)
Step 1376: Updated gated ratio to 0.9470 (progress: 53.0%)
Step 1376: Updated gated ratio to 0.9470 (progress: 53.0%)
Step 1376: Updated gated ratio to 0.9470 (progress: 53.0%)Step 1376: Updated gated ratio to 0.9470 (progress: 53.0%)

Step 1376: Updated gated ratio to 0.9470 (progress: 53.0%)
Step 1376: Updated gated ratio to 0.9470 (progress: 53.0%)
{'loss': 1.1485, 'grad_norm': 1.4845538139343262, 'learning_rate': 1.7247235441028486e-05, 'epoch': 0.26}
{'loss': 1.1427, 'grad_norm': 1.5775508880615234, 'learning_rate': 1.7242940733751696e-05, 'epoch': 0.27}
{'loss': 1.0741, 'grad_norm': 1.5814193487167358, 'learning_rate': 1.7238643214515934e-05, 'epoch': 0.27}
{'loss': 1.1426, 'grad_norm': 1.6278654336929321, 'learning_rate': 1.7234342884989642e-05, 'epoch': 0.27}
{'loss': 1.1944, 'grad_norm': 1.7131234407424927, 'learning_rate': 1.7230039746842352e-05, 'epoch': 0.27}
Step 1381: Updated gated ratio to 0.9468 (progress: 53.2%)
Step 1381: Updated gated ratio to 0.9468 (progress: 53.2%)
Step 1381: Updated gated ratio to 0.9468 (progress: 53.2%)
Step 1381: Updated gated ratio to 0.9468 (progress: 53.2%)
Step 1381: Updated gated ratio to 0.9468 (progress: 53.2%)
Step 1381: Updated gated ratio to 0.9468 (progress: 53.2%)
Step 1381: Updated gated ratio to 0.9468 (progress: 53.2%)
Step 1381: Updated gated ratio to 0.9468 (progress: 53.2%)
Step 1381: Updated gated ratio to 0.9468 (progress: 53.2%)
Step 1381: Updated gated ratio to 0.9468 (progress: 53.2%)
Step 1381: Updated gated ratio to 0.9468 (progress: 53.2%)
Step 1381: Updated gated ratio to 0.9468 (progress: 53.2%)
Step 1381: Updated gated ratio to 0.9468 (progress: 53.2%)
Step 1381: Updated gated ratio to 0.9468 (progress: 53.2%)
Step 1381: Updated gated ratio to 0.9468 (progress: 53.2%)
Step 1381: Updated gated ratio to 0.9468 (progress: 53.2%)
{'loss': 1.1461, 'grad_norm': 1.5592195987701416, 'learning_rate': 1.7225733801744698e-05, 'epoch': 0.27}
{'loss': 1.0935, 'grad_norm': 1.6913350820541382, 'learning_rate': 1.7221425051368394e-05, 'epoch': 0.27}
{'loss': 1.188, 'grad_norm': 1.6383657455444336, 'learning_rate': 1.7217113497386245e-05, 'epoch': 0.27}
{'loss': 1.207, 'grad_norm': 1.6763259172439575, 'learning_rate': 1.721279914147214e-05, 'epoch': 0.27}
{'loss': 1.217, 'grad_norm': 1.3900736570358276, 'learning_rate': 1.7208481985301065e-05, 'epoch': 0.27}
{'loss': 1.1501, 'grad_norm': 1.6387664079666138, 'learning_rate': 1.7204162030549093e-05, 'epoch': 0.27}
Step 1387: Updated gated ratio to 0.9466 (progress: 53.4%)
Step 1387: Updated gated ratio to 0.9466 (progress: 53.4%)
Step 1387: Updated gated ratio to 0.9466 (progress: 53.4%)
Step 1387: Updated gated ratio to 0.9466 (progress: 53.4%)Step 1387: Updated gated ratio to 0.9466 (progress: 53.4%)

Step 1387: Updated gated ratio to 0.9466 (progress: 53.4%)
Step 1387: Updated gated ratio to 0.9466 (progress: 53.4%)
Step 1387: Updated gated ratio to 0.9466 (progress: 53.4%)
Step 1387: Updated gated ratio to 0.9466 (progress: 53.4%)
Step 1387: Updated gated ratio to 0.9466 (progress: 53.4%)
Step 1387: Updated gated ratio to 0.9466 (progress: 53.4%)
Step 1387: Updated gated ratio to 0.9466 (progress: 53.4%)
Step 1387: Updated gated ratio to 0.9466 (progress: 53.4%)
Step 1387: Updated gated ratio to 0.9466 (progress: 53.4%)
Step 1387: Updated gated ratio to 0.9466 (progress: 53.4%)
Step 1387: Updated gated ratio to 0.9466 (progress: 53.4%)
{'loss': 1.0885, 'grad_norm': 1.6728540658950806, 'learning_rate': 1.7199839278893368e-05, 'epoch': 0.27}
Error with image file is truncated (67 bytes not processed)
{'loss': 1.2407, 'grad_norm': 1.3690012693405151, 'learning_rate': 1.719551373201214e-05, 'epoch': 0.27}
{'loss': 1.1786, 'grad_norm': 1.7242413759231567, 'learning_rate': 1.7191185391584736e-05, 'epoch': 0.27}
{'loss': 1.068, 'grad_norm': 1.582416296005249, 'learning_rate': 1.7186854259291558e-05, 'epoch': 0.27}
{'loss': 1.1697, 'grad_norm': 1.6406683921813965, 'learning_rate': 1.7182520336814105e-05, 'epoch': 0.27}
Step 1392: Updated gated ratio to 0.9464 (progress: 53.6%)
Step 1392: Updated gated ratio to 0.9464 (progress: 53.6%)
Step 1392: Updated gated ratio to 0.9464 (progress: 53.6%)
Step 1392: Updated gated ratio to 0.9464 (progress: 53.6%)Step 1392: Updated gated ratio to 0.9464 (progress: 53.6%)
Step 1392: Updated gated ratio to 0.9464 (progress: 53.6%)
Step 1392: Updated gated ratio to 0.9464 (progress: 53.6%)
Step 1392: Updated gated ratio to 0.9464 (progress: 53.6%)
Step 1392: Updated gated ratio to 0.9464 (progress: 53.6%)
Step 1392: Updated gated ratio to 0.9464 (progress: 53.6%)
Step 1392: Updated gated ratio to 0.9464 (progress: 53.6%)
Step 1392: Updated gated ratio to 0.9464 (progress: 53.6%)
Step 1392: Updated gated ratio to 0.9464 (progress: 53.6%)
Step 1392: Updated gated ratio to 0.9464 (progress: 53.6%)

Step 1392: Updated gated ratio to 0.9464 (progress: 53.6%)
Step 1392: Updated gated ratio to 0.9464 (progress: 53.6%)
{'loss': 1.1167, 'grad_norm': 1.6216646432876587, 'learning_rate': 1.717818362583496e-05, 'epoch': 0.27}
{'loss': 1.1401, 'grad_norm': 1.3291183710098267, 'learning_rate': 1.7173844128037777e-05, 'epoch': 0.27}
{'loss': 1.1563, 'grad_norm': 1.2791327238082886, 'learning_rate': 1.71695018451073e-05, 'epoch': 0.27}
{'loss': 1.2346, 'grad_norm': 1.6489073038101196, 'learning_rate': 1.7165156778729355e-05, 'epoch': 0.27}
{'loss': 1.156, 'grad_norm': 1.7074280977249146, 'learning_rate': 1.7160808930590845e-05, 'epoch': 0.27}
Step 1397: Updated gated ratio to 0.9462 (progress: 53.8%)
Step 1397: Updated gated ratio to 0.9462 (progress: 53.8%)
Step 1397: Updated gated ratio to 0.9462 (progress: 53.8%)
Step 1397: Updated gated ratio to 0.9462 (progress: 53.8%)
Step 1397: Updated gated ratio to 0.9462 (progress: 53.8%)Step 1397: Updated gated ratio to 0.9462 (progress: 53.8%)
Step 1397: Updated gated ratio to 0.9462 (progress: 53.8%)
Step 1397: Updated gated ratio to 0.9462 (progress: 53.8%)
Step 1397: Updated gated ratio to 0.9462 (progress: 53.8%)

Step 1397: Updated gated ratio to 0.9462 (progress: 53.8%)
Step 1397: Updated gated ratio to 0.9462 (progress: 53.8%)
Step 1397: Updated gated ratio to 0.9462 (progress: 53.8%)
Step 1397: Updated gated ratio to 0.9462 (progress: 53.8%)
Step 1397: Updated gated ratio to 0.9462 (progress: 53.8%)
Step 1397: Updated gated ratio to 0.9462 (progress: 53.8%)
Step 1397: Updated gated ratio to 0.9462 (progress: 53.8%)
{'loss': 1.109, 'grad_norm': 1.5783872604370117, 'learning_rate': 1.7156458302379753e-05, 'epoch': 0.27}
{'loss': 1.0828, 'grad_norm': 1.7228479385375977, 'learning_rate': 1.7152104895785147e-05, 'epoch': 0.27}
{'loss': 1.1182, 'grad_norm': 1.6026159524917603, 'learning_rate': 1.7147748712497162e-05, 'epoch': 0.27}
{'loss': 1.1472, 'grad_norm': 1.3156630992889404, 'learning_rate': 1.7143389754207026e-05, 'epoch': 0.27}
{'loss': 1.0489, 'grad_norm': 1.3022600412368774, 'learning_rate': 1.713902802260703e-05, 'epoch': 0.27}
Step 1402: Updated gated ratio to 0.9460 (progress: 54.0%)
Step 1402: Updated gated ratio to 0.9460 (progress: 54.0%)
Step 1402: Updated gated ratio to 0.9460 (progress: 54.0%)
Step 1402: Updated gated ratio to 0.9460 (progress: 54.0%)
Step 1402: Updated gated ratio to 0.9460 (progress: 54.0%)
Step 1402: Updated gated ratio to 0.9460 (progress: 54.0%)
Step 1402: Updated gated ratio to 0.9460 (progress: 54.0%)
Step 1402: Updated gated ratio to 0.9460 (progress: 54.0%)
Step 1402: Updated gated ratio to 0.9460 (progress: 54.0%)
Step 1402: Updated gated ratio to 0.9460 (progress: 54.0%)
Step 1402: Updated gated ratio to 0.9460 (progress: 54.0%)
Step 1402: Updated gated ratio to 0.9460 (progress: 54.0%)
Step 1402: Updated gated ratio to 0.9460 (progress: 54.0%)
Step 1402: Updated gated ratio to 0.9460 (progress: 54.0%)
Step 1402: Updated gated ratio to 0.9460 (progress: 54.0%)
Step 1402: Updated gated ratio to 0.9460 (progress: 54.0%)
{'loss': 1.1322, 'grad_norm': 1.7261091470718384, 'learning_rate': 1.7134663519390557e-05, 'epoch': 0.27}
{'loss': 1.1515, 'grad_norm': 1.7076927423477173, 'learning_rate': 1.7130296246252048e-05, 'epoch': 0.27}
{'loss': 1.1783, 'grad_norm': 1.4472975730895996, 'learning_rate': 1.7125926204887034e-05, 'epoch': 0.27}
{'loss': 1.142, 'grad_norm': 1.741977572441101, 'learning_rate': 1.712155339699211e-05, 'epoch': 0.27}
{'loss': 1.0588, 'grad_norm': 1.5276185274124146, 'learning_rate': 1.7117177824264962e-05, 'epoch': 0.27}
Step 1407: Updated gated ratio to 0.9458 (progress: 54.2%)
Step 1407: Updated gated ratio to 0.9458 (progress: 54.2%)
Step 1407: Updated gated ratio to 0.9458 (progress: 54.2%)
Step 1407: Updated gated ratio to 0.9458 (progress: 54.2%)
Step 1407: Updated gated ratio to 0.9458 (progress: 54.2%)
Step 1407: Updated gated ratio to 0.9458 (progress: 54.2%)Step 1407: Updated gated ratio to 0.9458 (progress: 54.2%)

Step 1407: Updated gated ratio to 0.9458 (progress: 54.2%)
Step 1407: Updated gated ratio to 0.9458 (progress: 54.2%)
Step 1407: Updated gated ratio to 0.9458 (progress: 54.2%)
Step 1407: Updated gated ratio to 0.9458 (progress: 54.2%)
Step 1407: Updated gated ratio to 0.9458 (progress: 54.2%)
Step 1407: Updated gated ratio to 0.9458 (progress: 54.2%)
Step 1407: Updated gated ratio to 0.9458 (progress: 54.2%)
Step 1407: Updated gated ratio to 0.9458 (progress: 54.2%)
Step 1407: Updated gated ratio to 0.9458 (progress: 54.2%)
{'loss': 1.1522, 'grad_norm': 1.6325969696044922, 'learning_rate': 1.7112799488404327e-05, 'epoch': 0.27}
{'loss': 1.0206, 'grad_norm': 1.5077009201049805, 'learning_rate': 1.7108418391110033e-05, 'epoch': 0.27}
{'loss': 1.1867, 'grad_norm': 1.342629075050354, 'learning_rate': 1.7104034534082968e-05, 'epoch': 0.27}
{'loss': 1.1318, 'grad_norm': 1.6317839622497559, 'learning_rate': 1.7099647919025096e-05, 'epoch': 0.27}
{'loss': 1.132, 'grad_norm': 1.539487600326538, 'learning_rate': 1.7095258547639456e-05, 'epoch': 0.27}
{'loss': 1.052, 'grad_norm': 1.6487417221069336, 'learning_rate': 1.709086642163015e-05, 'epoch': 0.27}
Step 1413: Updated gated ratio to 0.9456 (progress: 54.4%)
Step 1413: Updated gated ratio to 0.9456 (progress: 54.4%)
Step 1413: Updated gated ratio to 0.9456 (progress: 54.4%)
Step 1413: Updated gated ratio to 0.9456 (progress: 54.4%)
Step 1413: Updated gated ratio to 0.9456 (progress: 54.4%)
Step 1413: Updated gated ratio to 0.9456 (progress: 54.4%)Step 1413: Updated gated ratio to 0.9456 (progress: 54.4%)
Step 1413: Updated gated ratio to 0.9456 (progress: 54.4%)
Step 1413: Updated gated ratio to 0.9456 (progress: 54.4%)
Step 1413: Updated gated ratio to 0.9456 (progress: 54.4%)
Step 1413: Updated gated ratio to 0.9456 (progress: 54.4%)
Step 1413: Updated gated ratio to 0.9456 (progress: 54.4%)

Step 1413: Updated gated ratio to 0.9456 (progress: 54.4%)
Step 1413: Updated gated ratio to 0.9456 (progress: 54.4%)
Step 1413: Updated gated ratio to 0.9456 (progress: 54.4%)
Step 1413: Updated gated ratio to 0.9456 (progress: 54.4%)
{'loss': 1.1879, 'grad_norm': 1.5513098239898682, 'learning_rate': 1.7086471542702355e-05, 'epoch': 0.27}
{'loss': 1.0467, 'grad_norm': 1.6232943534851074, 'learning_rate': 1.708207391256231e-05, 'epoch': 0.27}
{'loss': 1.0659, 'grad_norm': 1.5501981973648071, 'learning_rate': 1.707767353291733e-05, 'epoch': 0.27}
{'loss': 1.1464, 'grad_norm': 1.6758546829223633, 'learning_rate': 1.7073270405475796e-05, 'epoch': 0.27}
{'loss': 1.1004, 'grad_norm': 1.6230812072753906, 'learning_rate': 1.7068864531947147e-05, 'epoch': 0.27}
Step 1418: Updated gated ratio to 0.9454 (progress: 54.6%)
Step 1418: Updated gated ratio to 0.9454 (progress: 54.6%)
Step 1418: Updated gated ratio to 0.9454 (progress: 54.6%)Step 1418: Updated gated ratio to 0.9454 (progress: 54.6%)
Step 1418: Updated gated ratio to 0.9454 (progress: 54.6%)
Step 1418: Updated gated ratio to 0.9454 (progress: 54.6%)

Step 1418: Updated gated ratio to 0.9454 (progress: 54.6%)
Step 1418: Updated gated ratio to 0.9454 (progress: 54.6%)
Step 1418: Updated gated ratio to 0.9454 (progress: 54.6%)
Step 1418: Updated gated ratio to 0.9454 (progress: 54.6%)
Step 1418: Updated gated ratio to 0.9454 (progress: 54.6%)
Step 1418: Updated gated ratio to 0.9454 (progress: 54.6%)
Step 1418: Updated gated ratio to 0.9454 (progress: 54.6%)
Step 1418: Updated gated ratio to 0.9454 (progress: 54.6%)
Step 1418: Updated gated ratio to 0.9454 (progress: 54.6%)
Step 1418: Updated gated ratio to 0.9454 (progress: 54.6%)
{'loss': 1.1464, 'grad_norm': 1.6457438468933105, 'learning_rate': 1.70644559140419e-05, 'epoch': 0.27}
{'loss': 1.1742, 'grad_norm': 1.3939154148101807, 'learning_rate': 1.706004455347163e-05, 'epoch': 0.27}
{'loss': 1.1916, 'grad_norm': 1.3481026887893677, 'learning_rate': 1.705563045194898e-05, 'epoch': 0.27}
{'loss': 1.1886, 'grad_norm': 1.6181002855300903, 'learning_rate': 1.7051213611187657e-05, 'epoch': 0.27}
{'loss': 1.0444, 'grad_norm': 1.7132033109664917, 'learning_rate': 1.704679403290243e-05, 'epoch': 0.27}
Step 1423: Updated gated ratio to 0.9452 (progress: 54.8%)
Step 1423: Updated gated ratio to 0.9452 (progress: 54.8%)
Step 1423: Updated gated ratio to 0.9452 (progress: 54.8%)
Step 1423: Updated gated ratio to 0.9452 (progress: 54.8%)
Step 1423: Updated gated ratio to 0.9452 (progress: 54.8%)
Step 1423: Updated gated ratio to 0.9452 (progress: 54.8%)
Step 1423: Updated gated ratio to 0.9452 (progress: 54.8%)
Step 1423: Updated gated ratio to 0.9452 (progress: 54.8%)
Step 1423: Updated gated ratio to 0.9452 (progress: 54.8%)
Step 1423: Updated gated ratio to 0.9452 (progress: 54.8%)
Step 1423: Updated gated ratio to 0.9452 (progress: 54.8%)
Step 1423: Updated gated ratio to 0.9452 (progress: 54.8%)
Step 1423: Updated gated ratio to 0.9452 (progress: 54.8%)
Step 1423: Updated gated ratio to 0.9452 (progress: 54.8%)
Step 1423: Updated gated ratio to 0.9452 (progress: 54.8%)
Step 1423: Updated gated ratio to 0.9452 (progress: 54.8%)
{'loss': 1.1605, 'grad_norm': 1.2203350067138672, 'learning_rate': 1.7042371718809132e-05, 'epoch': 0.27}
{'loss': 1.0803, 'grad_norm': 1.5733177661895752, 'learning_rate': 1.7037946670624652e-05, 'epoch': 0.27}
{'loss': 1.1163, 'grad_norm': 1.5198808908462524, 'learning_rate': 1.7033518890066956e-05, 'epoch': 0.27}
{'loss': 1.0777, 'grad_norm': 1.5593241453170776, 'learning_rate': 1.7029088378855055e-05, 'epoch': 0.27}
{'loss': 1.1311, 'grad_norm': 1.7154656648635864, 'learning_rate': 1.7024655138709025e-05, 'epoch': 0.27}
Step 1428: Updated gated ratio to 0.9450 (progress: 55.0%)
Step 1428: Updated gated ratio to 0.9450 (progress: 55.0%)Step 1428: Updated gated ratio to 0.9450 (progress: 55.0%)
Step 1428: Updated gated ratio to 0.9450 (progress: 55.0%)

Step 1428: Updated gated ratio to 0.9450 (progress: 55.0%)
Step 1428: Updated gated ratio to 0.9450 (progress: 55.0%)Step 1428: Updated gated ratio to 0.9450 (progress: 55.0%)
Step 1428: Updated gated ratio to 0.9450 (progress: 55.0%)
Step 1428: Updated gated ratio to 0.9450 (progress: 55.0%)
Step 1428: Updated gated ratio to 0.9450 (progress: 55.0%)

Step 1428: Updated gated ratio to 0.9450 (progress: 55.0%)
Step 1428: Updated gated ratio to 0.9450 (progress: 55.0%)
Step 1428: Updated gated ratio to 0.9450 (progress: 55.0%)
Step 1428: Updated gated ratio to 0.9450 (progress: 55.0%)
Step 1428: Updated gated ratio to 0.9450 (progress: 55.0%)
Step 1428: Updated gated ratio to 0.9450 (progress: 55.0%)
{'loss': 1.1062, 'grad_norm': 1.6056854724884033, 'learning_rate': 1.7020219171350004e-05, 'epoch': 0.27}
{'loss': 1.122, 'grad_norm': 1.6833206415176392, 'learning_rate': 1.7015780478500187e-05, 'epoch': 0.28}
{'loss': 1.132, 'grad_norm': 1.6707288026809692, 'learning_rate': 1.701133906188283e-05, 'epoch': 0.28}
{'loss': 1.1136, 'grad_norm': 1.729716181755066, 'learning_rate': 1.700689492322224e-05, 'epoch': 0.28}
{'loss': 1.1999, 'grad_norm': 1.8234784603118896, 'learning_rate': 1.700244806424379e-05, 'epoch': 0.28}
Step 1433: Updated gated ratio to 0.9448 (progress: 55.2%)
Step 1433: Updated gated ratio to 0.9448 (progress: 55.2%)
Step 1433: Updated gated ratio to 0.9448 (progress: 55.2%)
Step 1433: Updated gated ratio to 0.9448 (progress: 55.2%)
Step 1433: Updated gated ratio to 0.9448 (progress: 55.2%)
Step 1433: Updated gated ratio to 0.9448 (progress: 55.2%)
Step 1433: Updated gated ratio to 0.9448 (progress: 55.2%)
Step 1433: Updated gated ratio to 0.9448 (progress: 55.2%)
Step 1433: Updated gated ratio to 0.9448 (progress: 55.2%)
Step 1433: Updated gated ratio to 0.9448 (progress: 55.2%)
Step 1433: Updated gated ratio to 0.9448 (progress: 55.2%)
Step 1433: Updated gated ratio to 0.9448 (progress: 55.2%)
Step 1433: Updated gated ratio to 0.9448 (progress: 55.2%)
Step 1433: Updated gated ratio to 0.9448 (progress: 55.2%)
Step 1433: Updated gated ratio to 0.9448 (progress: 55.2%)
Step 1433: Updated gated ratio to 0.9448 (progress: 55.2%)
{'loss': 1.1247, 'grad_norm': 1.513875126838684, 'learning_rate': 1.6997998486673893e-05, 'epoch': 0.28}
{'loss': 1.1166, 'grad_norm': 1.5165294408798218, 'learning_rate': 1.699354619224004e-05, 'epoch': 0.28}
{'loss': 1.1106, 'grad_norm': 1.8602628707885742, 'learning_rate': 1.698909118267076e-05, 'epoch': 0.28}
{'loss': 1.1234, 'grad_norm': 1.6852660179138184, 'learning_rate': 1.6984633459695646e-05, 'epoch': 0.28}
{'loss': 1.0776, 'grad_norm': 1.4704035520553589, 'learning_rate': 1.6980173025045328e-05, 'epoch': 0.28}
Step 1438: Updated gated ratio to 0.9446 (progress: 55.4%)
Step 1438: Updated gated ratio to 0.9446 (progress: 55.4%)
Step 1438: Updated gated ratio to 0.9446 (progress: 55.4%)
Step 1438: Updated gated ratio to 0.9446 (progress: 55.4%)
Step 1438: Updated gated ratio to 0.9446 (progress: 55.4%)
Step 1438: Updated gated ratio to 0.9446 (progress: 55.4%)
Step 1438: Updated gated ratio to 0.9446 (progress: 55.4%)
Step 1438: Updated gated ratio to 0.9446 (progress: 55.4%)
Step 1438: Updated gated ratio to 0.9446 (progress: 55.4%)
Step 1438: Updated gated ratio to 0.9446 (progress: 55.4%)
Step 1438: Updated gated ratio to 0.9446 (progress: 55.4%)
Step 1438: Updated gated ratio to 0.9446 (progress: 55.4%)
Step 1438: Updated gated ratio to 0.9446 (progress: 55.4%)
Step 1438: Updated gated ratio to 0.9446 (progress: 55.4%)
Step 1438: Updated gated ratio to 0.9446 (progress: 55.4%)
Step 1438: Updated gated ratio to 0.9446 (progress: 55.4%)
{'loss': 1.1222, 'grad_norm': 1.6117782592773438, 'learning_rate': 1.697570988045151e-05, 'epoch': 0.28}
{'loss': 1.0806, 'grad_norm': 1.6160662174224854, 'learning_rate': 1.6971244027646937e-05, 'epoch': 0.28}
{'loss': 1.0844, 'grad_norm': 1.8225277662277222, 'learning_rate': 1.69667754683654e-05, 'epoch': 0.28}
{'loss': 1.088, 'grad_norm': 1.6582608222961426, 'learning_rate': 1.6962304204341758e-05, 'epoch': 0.28}
{'loss': 1.1566, 'grad_norm': 1.7120906114578247, 'learning_rate': 1.6957830237311904e-05, 'epoch': 0.28}
{'loss': 1.1484, 'grad_norm': 1.5737367868423462, 'learning_rate': 1.6953353569012784e-05, 'epoch': 0.28}
Step 1444: Updated gated ratio to 0.9444 (progress: 55.6%)
Step 1444: Updated gated ratio to 0.9444 (progress: 55.6%)
Step 1444: Updated gated ratio to 0.9444 (progress: 55.6%)
Step 1444: Updated gated ratio to 0.9444 (progress: 55.6%)
Step 1444: Updated gated ratio to 0.9444 (progress: 55.6%)
Step 1444: Updated gated ratio to 0.9444 (progress: 55.6%)
Step 1444: Updated gated ratio to 0.9444 (progress: 55.6%)
Step 1444: Updated gated ratio to 0.9444 (progress: 55.6%)
Step 1444: Updated gated ratio to 0.9444 (progress: 55.6%)
Step 1444: Updated gated ratio to 0.9444 (progress: 55.6%)
Step 1444: Updated gated ratio to 0.9444 (progress: 55.6%)
Step 1444: Updated gated ratio to 0.9444 (progress: 55.6%)
Step 1444: Updated gated ratio to 0.9444 (progress: 55.6%)
Step 1444: Updated gated ratio to 0.9444 (progress: 55.6%)
Step 1444: Updated gated ratio to 0.9444 (progress: 55.6%)
Step 1444: Updated gated ratio to 0.9444 (progress: 55.6%)
{'loss': 1.0376, 'grad_norm': 1.5539580583572388, 'learning_rate': 1.6948874201182402e-05, 'epoch': 0.28}
{'loss': 1.0323, 'grad_norm': 1.5883102416992188, 'learning_rate': 1.6944392135559798e-05, 'epoch': 0.28}
{'loss': 1.1186, 'grad_norm': 1.5567693710327148, 'learning_rate': 1.6939907373885062e-05, 'epoch': 0.28}
{'loss': 1.164, 'grad_norm': 1.3891459703445435, 'learning_rate': 1.6935419917899335e-05, 'epoch': 0.28}
{'loss': 1.1261, 'grad_norm': 1.2229360342025757, 'learning_rate': 1.6930929769344807e-05, 'epoch': 0.28}
Step 1449: Updated gated ratio to 0.9442 (progress: 55.8%)
Step 1449: Updated gated ratio to 0.9442 (progress: 55.8%)
Step 1449: Updated gated ratio to 0.9442 (progress: 55.8%)
Step 1449: Updated gated ratio to 0.9442 (progress: 55.8%)
Step 1449: Updated gated ratio to 0.9442 (progress: 55.8%)
Step 1449: Updated gated ratio to 0.9442 (progress: 55.8%)
Step 1449: Updated gated ratio to 0.9442 (progress: 55.8%)
Step 1449: Updated gated ratio to 0.9442 (progress: 55.8%)
Step 1449: Updated gated ratio to 0.9442 (progress: 55.8%)
Step 1449: Updated gated ratio to 0.9442 (progress: 55.8%)
Step 1449: Updated gated ratio to 0.9442 (progress: 55.8%)
Step 1449: Updated gated ratio to 0.9442 (progress: 55.8%)
Step 1449: Updated gated ratio to 0.9442 (progress: 55.8%)
Step 1449: Updated gated ratio to 0.9442 (progress: 55.8%)
Step 1449: Updated gated ratio to 0.9442 (progress: 55.8%)
Step 1449: Updated gated ratio to 0.9442 (progress: 55.8%)
{'loss': 1.1987, 'grad_norm': 1.7227625846862793, 'learning_rate': 1.69264369299647e-05, 'epoch': 0.28}
{'loss': 1.2344, 'grad_norm': 1.709898591041565, 'learning_rate': 1.692194140150329e-05, 'epoch': 0.28}
{'loss': 1.1485, 'grad_norm': 1.6354621648788452, 'learning_rate': 1.69174431857059e-05, 'epoch': 0.28}
{'loss': 1.1716, 'grad_norm': 1.3449722528457642, 'learning_rate': 1.6912942284318898e-05, 'epoch': 0.28}
{'loss': 1.2332, 'grad_norm': 1.023598074913025, 'learning_rate': 1.6908438699089674e-05, 'epoch': 0.28}
Step 1454: Updated gated ratio to 0.9440 (progress: 56.0%)
Step 1454: Updated gated ratio to 0.9440 (progress: 56.0%)
Step 1454: Updated gated ratio to 0.9440 (progress: 56.0%)
Step 1454: Updated gated ratio to 0.9440 (progress: 56.0%)Step 1454: Updated gated ratio to 0.9440 (progress: 56.0%)
Step 1454: Updated gated ratio to 0.9440 (progress: 56.0%)
Step 1454: Updated gated ratio to 0.9440 (progress: 56.0%)Step 1454: Updated gated ratio to 0.9440 (progress: 56.0%)

Step 1454: Updated gated ratio to 0.9440 (progress: 56.0%)
Step 1454: Updated gated ratio to 0.9440 (progress: 56.0%)
Step 1454: Updated gated ratio to 0.9440 (progress: 56.0%)

Step 1454: Updated gated ratio to 0.9440 (progress: 56.0%)
Step 1454: Updated gated ratio to 0.9440 (progress: 56.0%)
Step 1454: Updated gated ratio to 0.9440 (progress: 56.0%)
Step 1454: Updated gated ratio to 0.9440 (progress: 56.0%)
Step 1454: Updated gated ratio to 0.9440 (progress: 56.0%)
{'loss': 1.1209, 'grad_norm': 1.6289359331130981, 'learning_rate': 1.690393243176668e-05, 'epoch': 0.28}
{'loss': 0.9856, 'grad_norm': 1.5052303075790405, 'learning_rate': 1.6899423484099413e-05, 'epoch': 0.28}
{'loss': 1.2355, 'grad_norm': 1.7124751806259155, 'learning_rate': 1.6894911857838394e-05, 'epoch': 0.28}
{'loss': 1.1022, 'grad_norm': 1.7235721349716187, 'learning_rate': 1.689039755473519e-05, 'epoch': 0.28}
{'loss': 1.1204, 'grad_norm': 1.4824621677398682, 'learning_rate': 1.6885880576542417e-05, 'epoch': 0.28}
Step 1459: Updated gated ratio to 0.9438 (progress: 56.2%)
Step 1459: Updated gated ratio to 0.9438 (progress: 56.2%)
Step 1459: Updated gated ratio to 0.9438 (progress: 56.2%)
Step 1459: Updated gated ratio to 0.9438 (progress: 56.2%)
Step 1459: Updated gated ratio to 0.9438 (progress: 56.2%)
Step 1459: Updated gated ratio to 0.9438 (progress: 56.2%)
Step 1459: Updated gated ratio to 0.9438 (progress: 56.2%)
Step 1459: Updated gated ratio to 0.9438 (progress: 56.2%)
Step 1459: Updated gated ratio to 0.9438 (progress: 56.2%)
Step 1459: Updated gated ratio to 0.9438 (progress: 56.2%)
Step 1459: Updated gated ratio to 0.9438 (progress: 56.2%)
Step 1459: Updated gated ratio to 0.9438 (progress: 56.2%)
Step 1459: Updated gated ratio to 0.9438 (progress: 56.2%)
Step 1459: Updated gated ratio to 0.9438 (progress: 56.2%)
Step 1459: Updated gated ratio to 0.9438 (progress: 56.2%)
Step 1459: Updated gated ratio to 0.9438 (progress: 56.2%)
{'loss': 1.1629, 'grad_norm': 1.3164838552474976, 'learning_rate': 1.6881360925013712e-05, 'epoch': 0.28}
{'loss': 1.1671, 'grad_norm': 1.600669503211975, 'learning_rate': 1.6876838601903765e-05, 'epoch': 0.28}
{'loss': 1.1963, 'grad_norm': 1.3525203466415405, 'learning_rate': 1.6872313608968296e-05, 'epoch': 0.28}
{'loss': 1.0639, 'grad_norm': 1.4378993511199951, 'learning_rate': 1.6867785947964065e-05, 'epoch': 0.28}
{'loss': 1.1675, 'grad_norm': 1.6458828449249268, 'learning_rate': 1.6863255620648866e-05, 'epoch': 0.28}
Step 1464: Updated gated ratio to 0.9436 (progress: 56.4%)
Step 1464: Updated gated ratio to 0.9436 (progress: 56.4%)
Step 1464: Updated gated ratio to 0.9436 (progress: 56.4%)
Step 1464: Updated gated ratio to 0.9436 (progress: 56.4%)Step 1464: Updated gated ratio to 0.9436 (progress: 56.4%)
Step 1464: Updated gated ratio to 0.9436 (progress: 56.4%)
Step 1464: Updated gated ratio to 0.9436 (progress: 56.4%)
Step 1464: Updated gated ratio to 0.9436 (progress: 56.4%)
Step 1464: Updated gated ratio to 0.9436 (progress: 56.4%)
Step 1464: Updated gated ratio to 0.9436 (progress: 56.4%)
Step 1464: Updated gated ratio to 0.9436 (progress: 56.4%)
Step 1464: Updated gated ratio to 0.9436 (progress: 56.4%)
Step 1464: Updated gated ratio to 0.9436 (progress: 56.4%)

Step 1464: Updated gated ratio to 0.9436 (progress: 56.4%)
Step 1464: Updated gated ratio to 0.9436 (progress: 56.4%)
Step 1464: Updated gated ratio to 0.9436 (progress: 56.4%)
{'loss': 1.1392, 'grad_norm': 1.752731204032898, 'learning_rate': 1.685872262878152e-05, 'epoch': 0.28}
{'loss': 1.0476, 'grad_norm': 1.496062159538269, 'learning_rate': 1.6854186974121903e-05, 'epoch': 0.28}
{'loss': 1.1419, 'grad_norm': 1.5189803838729858, 'learning_rate': 1.68496486584309e-05, 'epoch': 0.28}
{'loss': 1.1406, 'grad_norm': 1.461300253868103, 'learning_rate': 1.6845107683470453e-05, 'epoch': 0.28}
{'loss': 1.0826, 'grad_norm': 1.5991756916046143, 'learning_rate': 1.6840564051003517e-05, 'epoch': 0.28}
{'loss': 1.0966, 'grad_norm': 1.214059591293335, 'learning_rate': 1.6836017762794087e-05, 'epoch': 0.28}
Step 1470: Updated gated ratio to 0.9434 (progress: 56.6%)
Step 1470: Updated gated ratio to 0.9434 (progress: 56.6%)
Step 1470: Updated gated ratio to 0.9434 (progress: 56.6%)
Step 1470: Updated gated ratio to 0.9434 (progress: 56.6%)
Step 1470: Updated gated ratio to 0.9434 (progress: 56.6%)
Step 1470: Updated gated ratio to 0.9434 (progress: 56.6%)
Step 1470: Updated gated ratio to 0.9434 (progress: 56.6%)
Step 1470: Updated gated ratio to 0.9434 (progress: 56.6%)
Step 1470: Updated gated ratio to 0.9434 (progress: 56.6%)
Step 1470: Updated gated ratio to 0.9434 (progress: 56.6%)
Step 1470: Updated gated ratio to 0.9434 (progress: 56.6%)
Step 1470: Updated gated ratio to 0.9434 (progress: 56.6%)
Step 1470: Updated gated ratio to 0.9434 (progress: 56.6%)
Step 1470: Updated gated ratio to 0.9434 (progress: 56.6%)
Step 1470: Updated gated ratio to 0.9434 (progress: 56.6%)
Step 1470: Updated gated ratio to 0.9434 (progress: 56.6%)
{'loss': 1.1033, 'grad_norm': 1.566915512084961, 'learning_rate': 1.6831468820607192e-05, 'epoch': 0.28}
{'loss': 1.1464, 'grad_norm': 1.6419146060943604, 'learning_rate': 1.6826917226208886e-05, 'epoch': 0.28}
{'loss': 1.1718, 'grad_norm': 1.6721539497375488, 'learning_rate': 1.6822362981366257e-05, 'epoch': 0.28}
{'loss': 1.0435, 'grad_norm': 1.4898613691329956, 'learning_rate': 1.6817806087847417e-05, 'epoch': 0.28}
{'loss': 1.1323, 'grad_norm': 1.5586309432983398, 'learning_rate': 1.681324654742151e-05, 'epoch': 0.28}
Step 1475: Updated gated ratio to 0.9432 (progress: 56.8%)
Step 1475: Updated gated ratio to 0.9432 (progress: 56.8%)
Step 1475: Updated gated ratio to 0.9432 (progress: 56.8%)
Step 1475: Updated gated ratio to 0.9432 (progress: 56.8%)
Step 1475: Updated gated ratio to 0.9432 (progress: 56.8%)
Step 1475: Updated gated ratio to 0.9432 (progress: 56.8%)
Step 1475: Updated gated ratio to 0.9432 (progress: 56.8%)
Step 1475: Updated gated ratio to 0.9432 (progress: 56.8%)
Step 1475: Updated gated ratio to 0.9432 (progress: 56.8%)
Step 1475: Updated gated ratio to 0.9432 (progress: 56.8%)
Step 1475: Updated gated ratio to 0.9432 (progress: 56.8%)
Step 1475: Updated gated ratio to 0.9432 (progress: 56.8%)
Step 1475: Updated gated ratio to 0.9432 (progress: 56.8%)
Step 1475: Updated gated ratio to 0.9432 (progress: 56.8%)
Step 1475: Updated gated ratio to 0.9432 (progress: 56.8%)
Step 1475: Updated gated ratio to 0.9432 (progress: 56.8%)
{'loss': 1.1736, 'grad_norm': 1.346004605293274, 'learning_rate': 1.6808684361858706e-05, 'epoch': 0.28}
{'loss': 1.0345, 'grad_norm': 1.4508370161056519, 'learning_rate': 1.6804119532930202e-05, 'epoch': 0.28}
{'loss': 1.181, 'grad_norm': 1.6056452989578247, 'learning_rate': 1.6799552062408225e-05, 'epoch': 0.28}
{'loss': 1.0534, 'grad_norm': 1.7403850555419922, 'learning_rate': 1.6794981952066018e-05, 'epoch': 0.28}
{'loss': 1.1228, 'grad_norm': 1.452185869216919, 'learning_rate': 1.6790409203677863e-05, 'epoch': 0.28}
Step 1480: Updated gated ratio to 0.9430 (progress: 57.0%)
Step 1480: Updated gated ratio to 0.9430 (progress: 57.0%)Step 1480: Updated gated ratio to 0.9430 (progress: 57.0%)

Step 1480: Updated gated ratio to 0.9430 (progress: 57.0%)
Step 1480: Updated gated ratio to 0.9430 (progress: 57.0%)
Step 1480: Updated gated ratio to 0.9430 (progress: 57.0%)
Step 1480: Updated gated ratio to 0.9430 (progress: 57.0%)
Step 1480: Updated gated ratio to 0.9430 (progress: 57.0%)
Step 1480: Updated gated ratio to 0.9430 (progress: 57.0%)
Step 1480: Updated gated ratio to 0.9430 (progress: 57.0%)
Step 1480: Updated gated ratio to 0.9430 (progress: 57.0%)
Step 1480: Updated gated ratio to 0.9430 (progress: 57.0%)
Step 1480: Updated gated ratio to 0.9430 (progress: 57.0%)
Step 1480: Updated gated ratio to 0.9430 (progress: 57.0%)
Step 1480: Updated gated ratio to 0.9430 (progress: 57.0%)
Step 1480: Updated gated ratio to 0.9430 (progress: 57.0%)
{'loss': 1.1675, 'grad_norm': 1.5813288688659668, 'learning_rate': 1.6785833819019052e-05, 'epoch': 0.28}
{'loss': 1.092, 'grad_norm': 1.631012201309204, 'learning_rate': 1.678125579986591e-05, 'epoch': 0.29}
{'loss': 1.1558, 'grad_norm': 1.5170077085494995, 'learning_rate': 1.677667514799578e-05, 'epoch': 0.29}
{'loss': 1.1498, 'grad_norm': 1.6393824815750122, 'learning_rate': 1.6772091865187032e-05, 'epoch': 0.29}
{'loss': 1.1731, 'grad_norm': 1.2895691394805908, 'learning_rate': 1.676750595321905e-05, 'epoch': 0.29}
Step 1485: Updated gated ratio to 0.9428 (progress: 57.2%)
Step 1485: Updated gated ratio to 0.9428 (progress: 57.2%)
Step 1485: Updated gated ratio to 0.9428 (progress: 57.2%)
Step 1485: Updated gated ratio to 0.9428 (progress: 57.2%)
Step 1485: Updated gated ratio to 0.9428 (progress: 57.2%)
Step 1485: Updated gated ratio to 0.9428 (progress: 57.2%)
Step 1485: Updated gated ratio to 0.9428 (progress: 57.2%)
Step 1485: Updated gated ratio to 0.9428 (progress: 57.2%)
Step 1485: Updated gated ratio to 0.9428 (progress: 57.2%)
Step 1485: Updated gated ratio to 0.9428 (progress: 57.2%)
Step 1485: Updated gated ratio to 0.9428 (progress: 57.2%)
Step 1485: Updated gated ratio to 0.9428 (progress: 57.2%)
Step 1485: Updated gated ratio to 0.9428 (progress: 57.2%)
Step 1485: Updated gated ratio to 0.9428 (progress: 57.2%)
Step 1485: Updated gated ratio to 0.9428 (progress: 57.2%)
Step 1485: Updated gated ratio to 0.9428 (progress: 57.2%)
{'loss': 1.088, 'grad_norm': 1.6563875675201416, 'learning_rate': 1.6762917413872246e-05, 'epoch': 0.29}
{'loss': 1.1368, 'grad_norm': 1.6236828565597534, 'learning_rate': 1.675832624892805e-05, 'epoch': 0.29}
{'loss': 1.2199, 'grad_norm': 1.6357150077819824, 'learning_rate': 1.6753732460168907e-05, 'epoch': 0.29}
{'loss': 1.1076, 'grad_norm': 1.5762661695480347, 'learning_rate': 1.674913604937828e-05, 'epoch': 0.29}
{'loss': 1.1829, 'grad_norm': 1.6129353046417236, 'learning_rate': 1.6744537018340662e-05, 'epoch': 0.29}
Step 1490: Updated gated ratio to 0.9426 (progress: 57.4%)
Step 1490: Updated gated ratio to 0.9426 (progress: 57.4%)
Step 1490: Updated gated ratio to 0.9426 (progress: 57.4%)
Step 1490: Updated gated ratio to 0.9426 (progress: 57.4%)
Step 1490: Updated gated ratio to 0.9426 (progress: 57.4%)
Step 1490: Updated gated ratio to 0.9426 (progress: 57.4%)
Step 1490: Updated gated ratio to 0.9426 (progress: 57.4%)
Step 1490: Updated gated ratio to 0.9426 (progress: 57.4%)
Step 1490: Updated gated ratio to 0.9426 (progress: 57.4%)
Step 1490: Updated gated ratio to 0.9426 (progress: 57.4%)
Step 1490: Updated gated ratio to 0.9426 (progress: 57.4%)
Step 1490: Updated gated ratio to 0.9426 (progress: 57.4%)
Step 1490: Updated gated ratio to 0.9426 (progress: 57.4%)
Step 1490: Updated gated ratio to 0.9426 (progress: 57.4%)
Step 1490: Updated gated ratio to 0.9426 (progress: 57.4%)
Step 1490: Updated gated ratio to 0.9426 (progress: 57.4%)
{'loss': 1.1816, 'grad_norm': 1.6965476274490356, 'learning_rate': 1.6739935368841555e-05, 'epoch': 0.29}
{'loss': 1.1116, 'grad_norm': 1.336531162261963, 'learning_rate': 1.6735331102667475e-05, 'epoch': 0.29}
{'loss': 1.0577, 'grad_norm': 1.5716439485549927, 'learning_rate': 1.6730724221605955e-05, 'epoch': 0.29}
{'loss': 1.1611, 'grad_norm': 1.819326400756836, 'learning_rate': 1.6726114727445547e-05, 'epoch': 0.29}
{'loss': 1.0951, 'grad_norm': 1.6261013746261597, 'learning_rate': 1.6721502621975813e-05, 'epoch': 0.29}
{'loss': 1.2118, 'grad_norm': 1.6380258798599243, 'learning_rate': 1.6716887906987332e-05, 'epoch': 0.29}
Step 1496: Updated gated ratio to 0.9424 (progress: 57.6%)
Step 1496: Updated gated ratio to 0.9424 (progress: 57.6%)
Step 1496: Updated gated ratio to 0.9424 (progress: 57.6%)
Step 1496: Updated gated ratio to 0.9424 (progress: 57.6%)
Step 1496: Updated gated ratio to 0.9424 (progress: 57.6%)
Step 1496: Updated gated ratio to 0.9424 (progress: 57.6%)
Step 1496: Updated gated ratio to 0.9424 (progress: 57.6%)
Step 1496: Updated gated ratio to 0.9424 (progress: 57.6%)
Step 1496: Updated gated ratio to 0.9424 (progress: 57.6%)
Step 1496: Updated gated ratio to 0.9424 (progress: 57.6%)
Step 1496: Updated gated ratio to 0.9424 (progress: 57.6%)
Step 1496: Updated gated ratio to 0.9424 (progress: 57.6%)
Step 1496: Updated gated ratio to 0.9424 (progress: 57.6%)
Step 1496: Updated gated ratio to 0.9424 (progress: 57.6%)
Step 1496: Updated gated ratio to 0.9424 (progress: 57.6%)
Step 1496: Updated gated ratio to 0.9424 (progress: 57.6%)
{'loss': 1.1246, 'grad_norm': 1.638217568397522, 'learning_rate': 1.6712270584271703e-05, 'epoch': 0.29}
{'loss': 1.0603, 'grad_norm': 1.550815463066101, 'learning_rate': 1.670765065562152e-05, 'epoch': 0.29}
{'loss': 1.0493, 'grad_norm': 1.4624335765838623, 'learning_rate': 1.67030281228304e-05, 'epoch': 0.29}
{'loss': 1.1645, 'grad_norm': 1.6293549537658691, 'learning_rate': 1.6698402987692968e-05, 'epoch': 0.29}
{'loss': 1.1289, 'grad_norm': 1.5611032247543335, 'learning_rate': 1.6693775252004866e-05, 'epoch': 0.29}
Step 1501: Updated gated ratio to 0.9422 (progress: 57.8%)
Step 1501: Updated gated ratio to 0.9422 (progress: 57.8%)
Step 1501: Updated gated ratio to 0.9422 (progress: 57.8%)
Step 1501: Updated gated ratio to 0.9422 (progress: 57.8%)
Step 1501: Updated gated ratio to 0.9422 (progress: 57.8%)
Step 1501: Updated gated ratio to 0.9422 (progress: 57.8%)
Step 1501: Updated gated ratio to 0.9422 (progress: 57.8%)Step 1501: Updated gated ratio to 0.9422 (progress: 57.8%)

Step 1501: Updated gated ratio to 0.9422 (progress: 57.8%)
Step 1501: Updated gated ratio to 0.9422 (progress: 57.8%)
Step 1501: Updated gated ratio to 0.9422 (progress: 57.8%)
Step 1501: Updated gated ratio to 0.9422 (progress: 57.8%)
Step 1501: Updated gated ratio to 0.9422 (progress: 57.8%)
Step 1501: Updated gated ratio to 0.9422 (progress: 57.8%)
Step 1501: Updated gated ratio to 0.9422 (progress: 57.8%)
Step 1501: Updated gated ratio to 0.9422 (progress: 57.8%)
{'loss': 1.0587, 'grad_norm': 1.6914366483688354, 'learning_rate': 1.668914491756274e-05, 'epoch': 0.29}
{'loss': 1.1499, 'grad_norm': 1.8136460781097412, 'learning_rate': 1.668451198616424e-05, 'epoch': 0.29}
{'loss': 1.0632, 'grad_norm': 1.52586030960083, 'learning_rate': 1.6679876459608033e-05, 'epoch': 0.29}
{'loss': 1.1031, 'grad_norm': 1.721086859703064, 'learning_rate': 1.667523833969379e-05, 'epoch': 0.29}
{'loss': 1.1247, 'grad_norm': 1.8934063911437988, 'learning_rate': 1.667059762822219e-05, 'epoch': 0.29}
Step 1506: Updated gated ratio to 0.9420 (progress: 58.0%)
Step 1506: Updated gated ratio to 0.9420 (progress: 58.0%)
Step 1506: Updated gated ratio to 0.9420 (progress: 58.0%)
Step 1506: Updated gated ratio to 0.9420 (progress: 58.0%)
Step 1506: Updated gated ratio to 0.9420 (progress: 58.0%)Step 1506: Updated gated ratio to 0.9420 (progress: 58.0%)
Step 1506: Updated gated ratio to 0.9420 (progress: 58.0%)
Step 1506: Updated gated ratio to 0.9420 (progress: 58.0%)Step 1506: Updated gated ratio to 0.9420 (progress: 58.0%)

Step 1506: Updated gated ratio to 0.9420 (progress: 58.0%)
Step 1506: Updated gated ratio to 0.9420 (progress: 58.0%)

Step 1506: Updated gated ratio to 0.9420 (progress: 58.0%)
Step 1506: Updated gated ratio to 0.9420 (progress: 58.0%)
Step 1506: Updated gated ratio to 0.9420 (progress: 58.0%)
Step 1506: Updated gated ratio to 0.9420 (progress: 58.0%)
Step 1506: Updated gated ratio to 0.9420 (progress: 58.0%)
{'loss': 1.102, 'grad_norm': 1.6324896812438965, 'learning_rate': 1.666595432699491e-05, 'epoch': 0.29}
{'loss': 1.1168, 'grad_norm': 1.6286786794662476, 'learning_rate': 1.6661308437814652e-05, 'epoch': 0.29}
{'loss': 1.1769, 'grad_norm': 1.6386477947235107, 'learning_rate': 1.6656659962485097e-05, 'epoch': 0.29}
{'loss': 1.1382, 'grad_norm': 1.67727792263031, 'learning_rate': 1.6652008902810952e-05, 'epoch': 0.29}
{'loss': 1.1328, 'grad_norm': 1.286148190498352, 'learning_rate': 1.6647355260597915e-05, 'epoch': 0.29}
Step 1511: Updated gated ratio to 0.9418 (progress: 58.2%)
Step 1511: Updated gated ratio to 0.9418 (progress: 58.2%)
Step 1511: Updated gated ratio to 0.9418 (progress: 58.2%)
Step 1511: Updated gated ratio to 0.9418 (progress: 58.2%)
Step 1511: Updated gated ratio to 0.9418 (progress: 58.2%)
Step 1511: Updated gated ratio to 0.9418 (progress: 58.2%)
Step 1511: Updated gated ratio to 0.9418 (progress: 58.2%)
Step 1511: Updated gated ratio to 0.9418 (progress: 58.2%)
Step 1511: Updated gated ratio to 0.9418 (progress: 58.2%)
Step 1511: Updated gated ratio to 0.9418 (progress: 58.2%)
Step 1511: Updated gated ratio to 0.9418 (progress: 58.2%)
Step 1511: Updated gated ratio to 0.9418 (progress: 58.2%)
Step 1511: Updated gated ratio to 0.9418 (progress: 58.2%)
Step 1511: Updated gated ratio to 0.9418 (progress: 58.2%)
Step 1511: Updated gated ratio to 0.9418 (progress: 58.2%)
Step 1511: Updated gated ratio to 0.9418 (progress: 58.2%)
{'loss': 1.0938, 'grad_norm': 1.5958943367004395, 'learning_rate': 1.664269903765269e-05, 'epoch': 0.29}
{'loss': 1.0738, 'grad_norm': 1.652815818786621, 'learning_rate': 1.6638040235782983e-05, 'epoch': 0.29}
{'loss': 1.1114, 'grad_norm': 1.7053650617599487, 'learning_rate': 1.6633378856797505e-05, 'epoch': 0.29}
{'loss': 1.2009, 'grad_norm': 1.6909866333007812, 'learning_rate': 1.662871490250596e-05, 'epoch': 0.29}
{'loss': 1.0869, 'grad_norm': 1.5628117322921753, 'learning_rate': 1.662404837471905e-05, 'epoch': 0.29}
Step 1516: Updated gated ratio to 0.9416 (progress: 58.4%)
Step 1516: Updated gated ratio to 0.9416 (progress: 58.4%)
Step 1516: Updated gated ratio to 0.9416 (progress: 58.4%)
Step 1516: Updated gated ratio to 0.9416 (progress: 58.4%)
Step 1516: Updated gated ratio to 0.9416 (progress: 58.4%)
Step 1516: Updated gated ratio to 0.9416 (progress: 58.4%)Step 1516: Updated gated ratio to 0.9416 (progress: 58.4%)

Step 1516: Updated gated ratio to 0.9416 (progress: 58.4%)
Step 1516: Updated gated ratio to 0.9416 (progress: 58.4%)
Step 1516: Updated gated ratio to 0.9416 (progress: 58.4%)
Step 1516: Updated gated ratio to 0.9416 (progress: 58.4%)
Step 1516: Updated gated ratio to 0.9416 (progress: 58.4%)
Step 1516: Updated gated ratio to 0.9416 (progress: 58.4%)
Step 1516: Updated gated ratio to 0.9416 (progress: 58.4%)
Step 1516: Updated gated ratio to 0.9416 (progress: 58.4%)
Step 1516: Updated gated ratio to 0.9416 (progress: 58.4%)
{'loss': 1.133, 'grad_norm': 1.4882196187973022, 'learning_rate': 1.66193792752485e-05, 'epoch': 0.29}
{'loss': 1.1297, 'grad_norm': 1.4294462203979492, 'learning_rate': 1.6614707605906995e-05, 'epoch': 0.29}
{'loss': 1.189, 'grad_norm': 1.2566230297088623, 'learning_rate': 1.661003336850825e-05, 'epoch': 0.29}
{'loss': 1.0556, 'grad_norm': 1.5676261186599731, 'learning_rate': 1.660535656486696e-05, 'epoch': 0.29}
{'loss': 1.1438, 'grad_norm': 1.6057088375091553, 'learning_rate': 1.660067719679882e-05, 'epoch': 0.29}
{'loss': 1.21, 'grad_norm': 1.3031888008117676, 'learning_rate': 1.6595995266120528e-05, 'epoch': 0.29}
Step 1522: Updated gated ratio to 0.9414 (progress: 58.6%)
Step 1522: Updated gated ratio to 0.9414 (progress: 58.6%)
Step 1522: Updated gated ratio to 0.9414 (progress: 58.6%)
Step 1522: Updated gated ratio to 0.9414 (progress: 58.6%)Step 1522: Updated gated ratio to 0.9414 (progress: 58.6%)

Step 1522: Updated gated ratio to 0.9414 (progress: 58.6%)
Step 1522: Updated gated ratio to 0.9414 (progress: 58.6%)
Step 1522: Updated gated ratio to 0.9414 (progress: 58.6%)
Step 1522: Updated gated ratio to 0.9414 (progress: 58.6%)
Step 1522: Updated gated ratio to 0.9414 (progress: 58.6%)
Step 1522: Updated gated ratio to 0.9414 (progress: 58.6%)
Step 1522: Updated gated ratio to 0.9414 (progress: 58.6%)
Step 1522: Updated gated ratio to 0.9414 (progress: 58.6%)
Step 1522: Updated gated ratio to 0.9414 (progress: 58.6%)
Step 1522: Updated gated ratio to 0.9414 (progress: 58.6%)
Step 1522: Updated gated ratio to 0.9414 (progress: 58.6%)
Error with cannot identify image file '/scratch3/li309/data/llava_data/train_data/llava_image_tune/ocr_vqa/images/451232577.jpg'
{'loss': 1.1365, 'grad_norm': 1.5272798538208008, 'learning_rate': 1.6591310774649766e-05, 'epoch': 0.29}
{'loss': 1.1622, 'grad_norm': 1.5609498023986816, 'learning_rate': 1.6586623724205216e-05, 'epoch': 0.29}
{'loss': 1.1298, 'grad_norm': 1.5495638847351074, 'learning_rate': 1.6581934116606554e-05, 'epoch': 0.29}
{'loss': 1.0408, 'grad_norm': 1.491702914237976, 'learning_rate': 1.657724195367444e-05, 'epoch': 0.29}
{'loss': 1.1723, 'grad_norm': 1.6492140293121338, 'learning_rate': 1.657254723723054e-05, 'epoch': 0.29}
Step 1527: Updated gated ratio to 0.9412 (progress: 58.8%)
Step 1527: Updated gated ratio to 0.9412 (progress: 58.8%)
Step 1527: Updated gated ratio to 0.9412 (progress: 58.8%)
Step 1527: Updated gated ratio to 0.9412 (progress: 58.8%)
Step 1527: Updated gated ratio to 0.9412 (progress: 58.8%)
Step 1527: Updated gated ratio to 0.9412 (progress: 58.8%)
Step 1527: Updated gated ratio to 0.9412 (progress: 58.8%)
Step 1527: Updated gated ratio to 0.9412 (progress: 58.8%)
Step 1527: Updated gated ratio to 0.9412 (progress: 58.8%)
Step 1527: Updated gated ratio to 0.9412 (progress: 58.8%)
Step 1527: Updated gated ratio to 0.9412 (progress: 58.8%)
Step 1527: Updated gated ratio to 0.9412 (progress: 58.8%)
Step 1527: Updated gated ratio to 0.9412 (progress: 58.8%)
Step 1527: Updated gated ratio to 0.9412 (progress: 58.8%)
Step 1527: Updated gated ratio to 0.9412 (progress: 58.8%)
Step 1527: Updated gated ratio to 0.9412 (progress: 58.8%)
{'loss': 1.0491, 'grad_norm': 1.6757028102874756, 'learning_rate': 1.6567849969097505e-05, 'epoch': 0.29}
{'loss': 1.1479, 'grad_norm': 1.7518962621688843, 'learning_rate': 1.6563150151098973e-05, 'epoch': 0.29}
{'loss': 1.0645, 'grad_norm': 1.676262378692627, 'learning_rate': 1.6558447785059577e-05, 'epoch': 0.29}
{'loss': 1.204, 'grad_norm': 1.7390607595443726, 'learning_rate': 1.655374287280494e-05, 'epoch': 0.29}
{'loss': 1.1285, 'grad_norm': 1.6003365516662598, 'learning_rate': 1.6549035416161662e-05, 'epoch': 0.29}
Step 1532: Updated gated ratio to 0.9410 (progress: 59.0%)
Step 1532: Updated gated ratio to 0.9410 (progress: 59.0%)
Step 1532: Updated gated ratio to 0.9410 (progress: 59.0%)
Step 1532: Updated gated ratio to 0.9410 (progress: 59.0%)
Step 1532: Updated gated ratio to 0.9410 (progress: 59.0%)
Step 1532: Updated gated ratio to 0.9410 (progress: 59.0%)
Step 1532: Updated gated ratio to 0.9410 (progress: 59.0%)
Step 1532: Updated gated ratio to 0.9410 (progress: 59.0%)
Step 1532: Updated gated ratio to 0.9410 (progress: 59.0%)
Step 1532: Updated gated ratio to 0.9410 (progress: 59.0%)
Step 1532: Updated gated ratio to 0.9410 (progress: 59.0%)
Step 1532: Updated gated ratio to 0.9410 (progress: 59.0%)
Step 1532: Updated gated ratio to 0.9410 (progress: 59.0%)
Step 1532: Updated gated ratio to 0.9410 (progress: 59.0%)
Step 1532: Updated gated ratio to 0.9410 (progress: 59.0%)
Step 1532: Updated gated ratio to 0.9410 (progress: 59.0%)
{'loss': 1.0903, 'grad_norm': 1.702821135520935, 'learning_rate': 1.654432541695735e-05, 'epoch': 0.29}
{'loss': 1.1813, 'grad_norm': 1.6143074035644531, 'learning_rate': 1.653961287702058e-05, 'epoch': 0.3}
{'loss': 1.1729, 'grad_norm': 1.7089136838912964, 'learning_rate': 1.653489779818093e-05, 'epoch': 0.3}
{'loss': 1.0465, 'grad_norm': 1.7315583229064941, 'learning_rate': 1.6530180182268946e-05, 'epoch': 0.3}
{'loss': 1.1312, 'grad_norm': 1.5300061702728271, 'learning_rate': 1.652546003111618e-05, 'epoch': 0.3}
Step 1537: Updated gated ratio to 0.9408 (progress: 59.2%)
Step 1537: Updated gated ratio to 0.9408 (progress: 59.2%)
Step 1537: Updated gated ratio to 0.9408 (progress: 59.2%)
Step 1537: Updated gated ratio to 0.9408 (progress: 59.2%)
Step 1537: Updated gated ratio to 0.9408 (progress: 59.2%)
Step 1537: Updated gated ratio to 0.9408 (progress: 59.2%)
Step 1537: Updated gated ratio to 0.9408 (progress: 59.2%)
Step 1537: Updated gated ratio to 0.9408 (progress: 59.2%)
Step 1537: Updated gated ratio to 0.9408 (progress: 59.2%)
Step 1537: Updated gated ratio to 0.9408 (progress: 59.2%)
Step 1537: Updated gated ratio to 0.9408 (progress: 59.2%)
Step 1537: Updated gated ratio to 0.9408 (progress: 59.2%)
Step 1537: Updated gated ratio to 0.9408 (progress: 59.2%)
Step 1537: Updated gated ratio to 0.9408 (progress: 59.2%)
Step 1537: Updated gated ratio to 0.9408 (progress: 59.2%)Step 1537: Updated gated ratio to 0.9408 (progress: 59.2%)

{'loss': 1.1122, 'grad_norm': 1.6584199666976929, 'learning_rate': 1.652073734655515e-05, 'epoch': 0.3}
{'loss': 1.0004, 'grad_norm': 1.5532381534576416, 'learning_rate': 1.6516012130419366e-05, 'epoch': 0.3}
{'loss': 1.1387, 'grad_norm': 1.5459083318710327, 'learning_rate': 1.6511284384543317e-05, 'epoch': 0.3}
{'loss': 1.131, 'grad_norm': 1.8352735042572021, 'learning_rate': 1.6506554110762483e-05, 'epoch': 0.3}
{'loss': 1.0592, 'grad_norm': 1.6192899942398071, 'learning_rate': 1.650182131091332e-05, 'epoch': 0.3}
Step 1542: Updated gated ratio to 0.9406 (progress: 59.4%)
Step 1542: Updated gated ratio to 0.9406 (progress: 59.4%)
Step 1542: Updated gated ratio to 0.9406 (progress: 59.4%)
Step 1542: Updated gated ratio to 0.9406 (progress: 59.4%)
Step 1542: Updated gated ratio to 0.9406 (progress: 59.4%)
Step 1542: Updated gated ratio to 0.9406 (progress: 59.4%)
Step 1542: Updated gated ratio to 0.9406 (progress: 59.4%)
Step 1542: Updated gated ratio to 0.9406 (progress: 59.4%)
Step 1542: Updated gated ratio to 0.9406 (progress: 59.4%)
Step 1542: Updated gated ratio to 0.9406 (progress: 59.4%)
Step 1542: Updated gated ratio to 0.9406 (progress: 59.4%)
Step 1542: Updated gated ratio to 0.9406 (progress: 59.4%)
Step 1542: Updated gated ratio to 0.9406 (progress: 59.4%)
Step 1542: Updated gated ratio to 0.9406 (progress: 59.4%)
Step 1542: Updated gated ratio to 0.9406 (progress: 59.4%)
Step 1542: Updated gated ratio to 0.9406 (progress: 59.4%)
{'loss': 1.1101, 'grad_norm': 1.6486034393310547, 'learning_rate': 1.6497085986833252e-05, 'epoch': 0.3}
{'loss': 1.0957, 'grad_norm': 1.6183172464370728, 'learning_rate': 1.6492348140360704e-05, 'epoch': 0.3}
{'loss': 1.0876, 'grad_norm': 1.5769321918487549, 'learning_rate': 1.6487607773335074e-05, 'epoch': 0.3}
{'loss': 1.0376, 'grad_norm': 1.7159740924835205, 'learning_rate': 1.648286488759673e-05, 'epoch': 0.3}
{'loss': 1.1189, 'grad_norm': 1.3456965684890747, 'learning_rate': 1.6478119484987026e-05, 'epoch': 0.3}
{'loss': 1.1344, 'grad_norm': 1.6810860633850098, 'learning_rate': 1.6473371567348287e-05, 'epoch': 0.3}
Step 1548: Updated gated ratio to 0.9404 (progress: 59.6%)
Step 1548: Updated gated ratio to 0.9404 (progress: 59.6%)
Step 1548: Updated gated ratio to 0.9404 (progress: 59.6%)
Step 1548: Updated gated ratio to 0.9404 (progress: 59.6%)
Step 1548: Updated gated ratio to 0.9404 (progress: 59.6%)
Step 1548: Updated gated ratio to 0.9404 (progress: 59.6%)
Step 1548: Updated gated ratio to 0.9404 (progress: 59.6%)
Step 1548: Updated gated ratio to 0.9404 (progress: 59.6%)
Step 1548: Updated gated ratio to 0.9404 (progress: 59.6%)
Step 1548: Updated gated ratio to 0.9404 (progress: 59.6%)
Step 1548: Updated gated ratio to 0.9404 (progress: 59.6%)
Step 1548: Updated gated ratio to 0.9404 (progress: 59.6%)
Step 1548: Updated gated ratio to 0.9404 (progress: 59.6%)
Step 1548: Updated gated ratio to 0.9404 (progress: 59.6%)
Step 1548: Updated gated ratio to 0.9404 (progress: 59.6%)
Step 1548: Updated gated ratio to 0.9404 (progress: 59.6%)
{'loss': 1.0961, 'grad_norm': 1.6820461750030518, 'learning_rate': 1.6468621136523823e-05, 'epoch': 0.3}
{'loss': 1.0576, 'grad_norm': 1.6373984813690186, 'learning_rate': 1.646386819435791e-05, 'epoch': 0.3}
{'loss': 1.1568, 'grad_norm': 1.2886414527893066, 'learning_rate': 1.6459112742695807e-05, 'epoch': 0.3}
{'loss': 1.1138, 'grad_norm': 1.5754722356796265, 'learning_rate': 1.6454354783383748e-05, 'epoch': 0.3}
{'loss': 1.1493, 'grad_norm': 1.6654640436172485, 'learning_rate': 1.644959431826893e-05, 'epoch': 0.3}
Step 1553: Updated gated ratio to 0.9402 (progress: 59.8%)
Step 1553: Updated gated ratio to 0.9402 (progress: 59.8%)
Step 1553: Updated gated ratio to 0.9402 (progress: 59.8%)
Step 1553: Updated gated ratio to 0.9402 (progress: 59.8%)
Step 1553: Updated gated ratio to 0.9402 (progress: 59.8%)Step 1553: Updated gated ratio to 0.9402 (progress: 59.8%)

Step 1553: Updated gated ratio to 0.9402 (progress: 59.8%)
Step 1553: Updated gated ratio to 0.9402 (progress: 59.8%)
Step 1553: Updated gated ratio to 0.9402 (progress: 59.8%)
Step 1553: Updated gated ratio to 0.9402 (progress: 59.8%)
Step 1553: Updated gated ratio to 0.9402 (progress: 59.8%)
Step 1553: Updated gated ratio to 0.9402 (progress: 59.8%)
Step 1553: Updated gated ratio to 0.9402 (progress: 59.8%)
Step 1553: Updated gated ratio to 0.9402 (progress: 59.8%)
Step 1553: Updated gated ratio to 0.9402 (progress: 59.8%)
Step 1553: Updated gated ratio to 0.9402 (progress: 59.8%)
{'loss': 1.0419, 'grad_norm': 1.6807620525360107, 'learning_rate': 1.6444831349199528e-05, 'epoch': 0.3}
{'loss': 1.0566, 'grad_norm': 1.576813817024231, 'learning_rate': 1.6440065878024697e-05, 'epoch': 0.3}
{'loss': 1.0879, 'grad_norm': 1.6731390953063965, 'learning_rate': 1.6435297906594553e-05, 'epoch': 0.3}
{'loss': 1.0835, 'grad_norm': 1.872865080833435, 'learning_rate': 1.643052743676019e-05, 'epoch': 0.3}
{'loss': 1.1862, 'grad_norm': 1.330075740814209, 'learning_rate': 1.6425754470373667e-05, 'epoch': 0.3}
Step 1558: Updated gated ratio to 0.9400 (progress: 60.0%)
Step 1558: Updated gated ratio to 0.9400 (progress: 60.0%)
Step 1558: Updated gated ratio to 0.9400 (progress: 60.0%)
Step 1558: Updated gated ratio to 0.9400 (progress: 60.0%)
Step 1558: Updated gated ratio to 0.9400 (progress: 60.0%)
Step 1558: Updated gated ratio to 0.9400 (progress: 60.0%)Step 1558: Updated gated ratio to 0.9400 (progress: 60.0%)
Step 1558: Updated gated ratio to 0.9400 (progress: 60.0%)
Step 1558: Updated gated ratio to 0.9400 (progress: 60.0%)
Step 1558: Updated gated ratio to 0.9400 (progress: 60.0%)

Step 1558: Updated gated ratio to 0.9400 (progress: 60.0%)
Step 1558: Updated gated ratio to 0.9400 (progress: 60.0%)
Step 1558: Updated gated ratio to 0.9400 (progress: 60.0%)
Step 1558: Updated gated ratio to 0.9400 (progress: 60.0%)
Step 1558: Updated gated ratio to 0.9400 (progress: 60.0%)
Step 1558: Updated gated ratio to 0.9400 (progress: 60.0%)
{'loss': 1.2898, 'grad_norm': 1.1995596885681152, 'learning_rate': 1.642097900928801e-05, 'epoch': 0.3}
{'loss': 1.139, 'grad_norm': 1.5830187797546387, 'learning_rate': 1.6416201055357225e-05, 'epoch': 0.3}
{'loss': 1.1933, 'grad_norm': 1.3727720975875854, 'learning_rate': 1.641142061043627e-05, 'epoch': 0.3}
{'loss': 1.0909, 'grad_norm': 1.634475827217102, 'learning_rate': 1.640663767638108e-05, 'epoch': 0.3}
{'loss': 1.1961, 'grad_norm': 1.3104768991470337, 'learning_rate': 1.6401852255048564e-05, 'epoch': 0.3}
Step 1563: Updated gated ratio to 0.9398 (progress: 60.2%)
Step 1563: Updated gated ratio to 0.9398 (progress: 60.2%)
Step 1563: Updated gated ratio to 0.9398 (progress: 60.2%)
Step 1563: Updated gated ratio to 0.9398 (progress: 60.2%)
Step 1563: Updated gated ratio to 0.9398 (progress: 60.2%)
Step 1563: Updated gated ratio to 0.9398 (progress: 60.2%)
Step 1563: Updated gated ratio to 0.9398 (progress: 60.2%)
Step 1563: Updated gated ratio to 0.9398 (progress: 60.2%)Step 1563: Updated gated ratio to 0.9398 (progress: 60.2%)

Step 1563: Updated gated ratio to 0.9398 (progress: 60.2%)
Step 1563: Updated gated ratio to 0.9398 (progress: 60.2%)
Step 1563: Updated gated ratio to 0.9398 (progress: 60.2%)
Step 1563: Updated gated ratio to 0.9398 (progress: 60.2%)
Step 1563: Updated gated ratio to 0.9398 (progress: 60.2%)
Step 1563: Updated gated ratio to 0.9398 (progress: 60.2%)
Step 1563: Updated gated ratio to 0.9398 (progress: 60.2%)
{'loss': 1.2235, 'grad_norm': 1.3782038688659668, 'learning_rate': 1.6397064348296578e-05, 'epoch': 0.3}
{'loss': 1.092, 'grad_norm': 1.6499176025390625, 'learning_rate': 1.6392273957983955e-05, 'epoch': 0.3}
{'loss': 1.1779, 'grad_norm': 1.712589979171753, 'learning_rate': 1.638748108597049e-05, 'epoch': 0.3}
{'loss': 1.1477, 'grad_norm': 1.5964269638061523, 'learning_rate': 1.6382685734116934e-05, 'epoch': 0.3}
{'loss': 1.1161, 'grad_norm': 1.4345858097076416, 'learning_rate': 1.6377887904285018e-05, 'epoch': 0.3}
Step 1568: Updated gated ratio to 0.9396 (progress: 60.4%)
Step 1568: Updated gated ratio to 0.9396 (progress: 60.4%)
Step 1568: Updated gated ratio to 0.9396 (progress: 60.4%)
Step 1568: Updated gated ratio to 0.9396 (progress: 60.4%)
Step 1568: Updated gated ratio to 0.9396 (progress: 60.4%)
Step 1568: Updated gated ratio to 0.9396 (progress: 60.4%)
Step 1568: Updated gated ratio to 0.9396 (progress: 60.4%)
Step 1568: Updated gated ratio to 0.9396 (progress: 60.4%)
Step 1568: Updated gated ratio to 0.9396 (progress: 60.4%)
Step 1568: Updated gated ratio to 0.9396 (progress: 60.4%)
Step 1568: Updated gated ratio to 0.9396 (progress: 60.4%)
Step 1568: Updated gated ratio to 0.9396 (progress: 60.4%)
Step 1568: Updated gated ratio to 0.9396 (progress: 60.4%)Step 1568: Updated gated ratio to 0.9396 (progress: 60.4%)

Step 1568: Updated gated ratio to 0.9396 (progress: 60.4%)
Step 1568: Updated gated ratio to 0.9396 (progress: 60.4%)
{'loss': 1.1154, 'grad_norm': 1.5914018154144287, 'learning_rate': 1.637308759833742e-05, 'epoch': 0.3}
{'loss': 1.0936, 'grad_norm': 1.283736228942871, 'learning_rate': 1.6368284818137787e-05, 'epoch': 0.3}
{'loss': 1.2002, 'grad_norm': 1.6390382051467896, 'learning_rate': 1.636347956555072e-05, 'epoch': 0.3}
{'loss': 1.1824, 'grad_norm': 1.7257049083709717, 'learning_rate': 1.635867184244178e-05, 'epoch': 0.3}
{'loss': 1.0618, 'grad_norm': 1.6313358545303345, 'learning_rate': 1.63538616506775e-05, 'epoch': 0.3}
{'loss': 1.1264, 'grad_norm': 1.7057236433029175, 'learning_rate': 1.6349048992125358e-05, 'epoch': 0.3}
Step 1574: Updated gated ratio to 0.9394 (progress: 60.6%)
Step 1574: Updated gated ratio to 0.9394 (progress: 60.6%)
Step 1574: Updated gated ratio to 0.9394 (progress: 60.6%)
Step 1574: Updated gated ratio to 0.9394 (progress: 60.6%)Step 1574: Updated gated ratio to 0.9394 (progress: 60.6%)
Step 1574: Updated gated ratio to 0.9394 (progress: 60.6%)
Step 1574: Updated gated ratio to 0.9394 (progress: 60.6%)

Step 1574: Updated gated ratio to 0.9394 (progress: 60.6%)
Step 1574: Updated gated ratio to 0.9394 (progress: 60.6%)
Step 1574: Updated gated ratio to 0.9394 (progress: 60.6%)
Step 1574: Updated gated ratio to 0.9394 (progress: 60.6%)
Step 1574: Updated gated ratio to 0.9394 (progress: 60.6%)
Step 1574: Updated gated ratio to 0.9394 (progress: 60.6%)
Step 1574: Updated gated ratio to 0.9394 (progress: 60.6%)
Step 1574: Updated gated ratio to 0.9394 (progress: 60.6%)
Step 1574: Updated gated ratio to 0.9394 (progress: 60.6%)
{'loss': 1.1811, 'grad_norm': 1.6370106935501099, 'learning_rate': 1.634423386865379e-05, 'epoch': 0.3}
{'loss': 1.1669, 'grad_norm': 1.63937246799469, 'learning_rate': 1.6339416282132196e-05, 'epoch': 0.3}
{'loss': 1.1245, 'grad_norm': 1.3166189193725586, 'learning_rate': 1.633459623443093e-05, 'epoch': 0.3}
{'loss': 1.0849, 'grad_norm': 1.495008945465088, 'learning_rate': 1.6329773727421297e-05, 'epoch': 0.3}
{'loss': 1.1287, 'grad_norm': 1.6995116472244263, 'learning_rate': 1.6324948762975567e-05, 'epoch': 0.3}
Step 1579: Updated gated ratio to 0.9392 (progress: 60.8%)
Step 1579: Updated gated ratio to 0.9392 (progress: 60.8%)
Step 1579: Updated gated ratio to 0.9392 (progress: 60.8%)
Step 1579: Updated gated ratio to 0.9392 (progress: 60.8%)
Step 1579: Updated gated ratio to 0.9392 (progress: 60.8%)
Step 1579: Updated gated ratio to 0.9392 (progress: 60.8%)
Step 1579: Updated gated ratio to 0.9392 (progress: 60.8%)
Step 1579: Updated gated ratio to 0.9392 (progress: 60.8%)
Step 1579: Updated gated ratio to 0.9392 (progress: 60.8%)
Step 1579: Updated gated ratio to 0.9392 (progress: 60.8%)
Step 1579: Updated gated ratio to 0.9392 (progress: 60.8%)
Step 1579: Updated gated ratio to 0.9392 (progress: 60.8%)
Step 1579: Updated gated ratio to 0.9392 (progress: 60.8%)Step 1579: Updated gated ratio to 0.9392 (progress: 60.8%)

Step 1579: Updated gated ratio to 0.9392 (progress: 60.8%)
Step 1579: Updated gated ratio to 0.9392 (progress: 60.8%)
{'loss': 1.0928, 'grad_norm': 1.697471261024475, 'learning_rate': 1.632012134296695e-05, 'epoch': 0.3}
{'loss': 1.0866, 'grad_norm': 1.4137749671936035, 'learning_rate': 1.6315291469269617e-05, 'epoch': 0.3}
{'loss': 1.0369, 'grad_norm': 1.7675386667251587, 'learning_rate': 1.63104591437587e-05, 'epoch': 0.3}
{'loss': 1.1264, 'grad_norm': 1.5454928874969482, 'learning_rate': 1.6305624368310265e-05, 'epoch': 0.3}
{'loss': 1.1918, 'grad_norm': 1.6631264686584473, 'learning_rate': 1.630078714480134e-05, 'epoch': 0.3}
Step 1584: Updated gated ratio to 0.9390 (progress: 61.0%)
Step 1584: Updated gated ratio to 0.9390 (progress: 61.0%)
Step 1584: Updated gated ratio to 0.9390 (progress: 61.0%)
Step 1584: Updated gated ratio to 0.9390 (progress: 61.0%)
Step 1584: Updated gated ratio to 0.9390 (progress: 61.0%)
Step 1584: Updated gated ratio to 0.9390 (progress: 61.0%)
Step 1584: Updated gated ratio to 0.9390 (progress: 61.0%)
Step 1584: Updated gated ratio to 0.9390 (progress: 61.0%)Step 1584: Updated gated ratio to 0.9390 (progress: 61.0%)

Step 1584: Updated gated ratio to 0.9390 (progress: 61.0%)
Step 1584: Updated gated ratio to 0.9390 (progress: 61.0%)
Step 1584: Updated gated ratio to 0.9390 (progress: 61.0%)
Step 1584: Updated gated ratio to 0.9390 (progress: 61.0%)
Step 1584: Updated gated ratio to 0.9390 (progress: 61.0%)
Step 1584: Updated gated ratio to 0.9390 (progress: 61.0%)
Step 1584: Updated gated ratio to 0.9390 (progress: 61.0%)
{'loss': 1.0775, 'grad_norm': 1.65833580493927, 'learning_rate': 1.6295947475109904e-05, 'epoch': 0.3}
{'loss': 1.1269, 'grad_norm': 1.5171688795089722, 'learning_rate': 1.629110536111488e-05, 'epoch': 0.31}
{'loss': 1.1839, 'grad_norm': 1.5323123931884766, 'learning_rate': 1.628626080469615e-05, 'epoch': 0.31}
{'loss': 1.1056, 'grad_norm': 1.6826331615447998, 'learning_rate': 1.628141380773453e-05, 'epoch': 0.31}
{'loss': 1.2442, 'grad_norm': 1.7880486249923706, 'learning_rate': 1.6276564372111797e-05, 'epoch': 0.31}
Step 1589: Updated gated ratio to 0.9388 (progress: 61.2%)
Step 1589: Updated gated ratio to 0.9388 (progress: 61.2%)
Step 1589: Updated gated ratio to 0.9388 (progress: 61.2%)
Step 1589: Updated gated ratio to 0.9388 (progress: 61.2%)
Step 1589: Updated gated ratio to 0.9388 (progress: 61.2%)
Step 1589: Updated gated ratio to 0.9388 (progress: 61.2%)
Step 1589: Updated gated ratio to 0.9388 (progress: 61.2%)
Step 1589: Updated gated ratio to 0.9388 (progress: 61.2%)
Step 1589: Updated gated ratio to 0.9388 (progress: 61.2%)
Step 1589: Updated gated ratio to 0.9388 (progress: 61.2%)
Step 1589: Updated gated ratio to 0.9388 (progress: 61.2%)
Step 1589: Updated gated ratio to 0.9388 (progress: 61.2%)
Step 1589: Updated gated ratio to 0.9388 (progress: 61.2%)
Step 1589: Updated gated ratio to 0.9388 (progress: 61.2%)
Step 1589: Updated gated ratio to 0.9388 (progress: 61.2%)
Step 1589: Updated gated ratio to 0.9388 (progress: 61.2%)
{'loss': 1.0678, 'grad_norm': 1.5532127618789673, 'learning_rate': 1.6271712499710663e-05, 'epoch': 0.31}
{'loss': 1.0344, 'grad_norm': 1.5170526504516602, 'learning_rate': 1.62668581924148e-05, 'epoch': 0.31}
{'loss': 1.0856, 'grad_norm': 1.615958333015442, 'learning_rate': 1.6262001452108807e-05, 'epoch': 0.31}
{'loss': 1.1077, 'grad_norm': 1.5427733659744263, 'learning_rate': 1.6257142280678247e-05, 'epoch': 0.31}
{'loss': 1.1673, 'grad_norm': 1.666466474533081, 'learning_rate': 1.6252280680009613e-05, 'epoch': 0.31}
Step 1594: Updated gated ratio to 0.9386 (progress: 61.4%)
Step 1594: Updated gated ratio to 0.9386 (progress: 61.4%)
Step 1594: Updated gated ratio to 0.9386 (progress: 61.4%)
Step 1594: Updated gated ratio to 0.9386 (progress: 61.4%)
Step 1594: Updated gated ratio to 0.9386 (progress: 61.4%)
Step 1594: Updated gated ratio to 0.9386 (progress: 61.4%)
Step 1594: Updated gated ratio to 0.9386 (progress: 61.4%)
Step 1594: Updated gated ratio to 0.9386 (progress: 61.4%)
Step 1594: Updated gated ratio to 0.9386 (progress: 61.4%)
Step 1594: Updated gated ratio to 0.9386 (progress: 61.4%)
Step 1594: Updated gated ratio to 0.9386 (progress: 61.4%)
Step 1594: Updated gated ratio to 0.9386 (progress: 61.4%)
Step 1594: Updated gated ratio to 0.9386 (progress: 61.4%)
Step 1594: Updated gated ratio to 0.9386 (progress: 61.4%)
Step 1594: Updated gated ratio to 0.9386 (progress: 61.4%)
Step 1594: Updated gated ratio to 0.9386 (progress: 61.4%)
{'loss': 1.1682, 'grad_norm': 1.5171438455581665, 'learning_rate': 1.6247416651990343e-05, 'epoch': 0.31}
{'loss': 1.1716, 'grad_norm': 1.6272766590118408, 'learning_rate': 1.624255019850883e-05, 'epoch': 0.31}
{'loss': 1.0647, 'grad_norm': 1.593764305114746, 'learning_rate': 1.6237681321454387e-05, 'epoch': 0.31}
{'loss': 1.1075, 'grad_norm': 1.605985164642334, 'learning_rate': 1.623281002271729e-05, 'epoch': 0.31}
{'loss': 1.2272, 'grad_norm': 1.5864201784133911, 'learning_rate': 1.6227936304188738e-05, 'epoch': 0.31}
{'loss': 1.1526, 'grad_norm': 1.6103010177612305, 'learning_rate': 1.622306016776088e-05, 'epoch': 0.31}
Step 1600: Updated gated ratio to 0.9384 (progress: 61.6%)
Step 1600: Updated gated ratio to 0.9384 (progress: 61.6%)
Step 1600: Updated gated ratio to 0.9384 (progress: 61.6%)
Step 1600: Updated gated ratio to 0.9384 (progress: 61.6%)
Step 1600: Updated gated ratio to 0.9384 (progress: 61.6%)
Step 1600: Updated gated ratio to 0.9384 (progress: 61.6%)Step 1600: Updated gated ratio to 0.9384 (progress: 61.6%)

Step 1600: Updated gated ratio to 0.9384 (progress: 61.6%)
Step 1600: Updated gated ratio to 0.9384 (progress: 61.6%)
Step 1600: Updated gated ratio to 0.9384 (progress: 61.6%)
Step 1600: Updated gated ratio to 0.9384 (progress: 61.6%)Step 1600: Updated gated ratio to 0.9384 (progress: 61.6%)

Step 1600: Updated gated ratio to 0.9384 (progress: 61.6%)
Step 1600: Updated gated ratio to 0.9384 (progress: 61.6%)
Step 1600: Updated gated ratio to 0.9384 (progress: 61.6%)
Step 1600: Updated gated ratio to 0.9384 (progress: 61.6%)
{'loss': 1.1156, 'grad_norm': 1.6107958555221558, 'learning_rate': 1.6218181615326795e-05, 'epoch': 0.31}
{'loss': 1.0285, 'grad_norm': 1.510198950767517, 'learning_rate': 1.6213300648780515e-05, 'epoch': 0.31}
{'loss': 1.0547, 'grad_norm': 1.6276652812957764, 'learning_rate': 1.620841727001699e-05, 'epoch': 0.31}
{'loss': 1.0908, 'grad_norm': 1.4421207904815674, 'learning_rate': 1.6203531480932114e-05, 'epoch': 0.31}
{'loss': 1.1045, 'grad_norm': 1.6340618133544922, 'learning_rate': 1.619864328342273e-05, 'epoch': 0.31}
Step 1605: Updated gated ratio to 0.9382 (progress: 61.8%)
Step 1605: Updated gated ratio to 0.9382 (progress: 61.8%)
Step 1605: Updated gated ratio to 0.9382 (progress: 61.8%)
Step 1605: Updated gated ratio to 0.9382 (progress: 61.8%)
Step 1605: Updated gated ratio to 0.9382 (progress: 61.8%)
Step 1605: Updated gated ratio to 0.9382 (progress: 61.8%)
Step 1605: Updated gated ratio to 0.9382 (progress: 61.8%)
Step 1605: Updated gated ratio to 0.9382 (progress: 61.8%)
Step 1605: Updated gated ratio to 0.9382 (progress: 61.8%)
Step 1605: Updated gated ratio to 0.9382 (progress: 61.8%)
Step 1605: Updated gated ratio to 0.9382 (progress: 61.8%)
Step 1605: Updated gated ratio to 0.9382 (progress: 61.8%)
Step 1605: Updated gated ratio to 0.9382 (progress: 61.8%)
Step 1605: Updated gated ratio to 0.9382 (progress: 61.8%)
Step 1605: Updated gated ratio to 0.9382 (progress: 61.8%)
Step 1605: Updated gated ratio to 0.9382 (progress: 61.8%)
{'loss': 1.0486, 'grad_norm': 1.6068830490112305, 'learning_rate': 1.6193752679386593e-05, 'epoch': 0.31}
{'loss': 1.0672, 'grad_norm': 1.5806541442871094, 'learning_rate': 1.6188859670722414e-05, 'epoch': 0.31}
{'loss': 1.0647, 'grad_norm': 1.5006202459335327, 'learning_rate': 1.6183964259329817e-05, 'epoch': 0.31}
{'loss': 1.0564, 'grad_norm': 1.6131454706192017, 'learning_rate': 1.6179066447109376e-05, 'epoch': 0.31}
{'loss': 1.1269, 'grad_norm': 1.6451979875564575, 'learning_rate': 1.6174166235962588e-05, 'epoch': 0.31}
Step 1610: Updated gated ratio to 0.9380 (progress: 62.0%)
Step 1610: Updated gated ratio to 0.9380 (progress: 62.0%)
Step 1610: Updated gated ratio to 0.9380 (progress: 62.0%)
Step 1610: Updated gated ratio to 0.9380 (progress: 62.0%)
Step 1610: Updated gated ratio to 0.9380 (progress: 62.0%)
Step 1610: Updated gated ratio to 0.9380 (progress: 62.0%)
Step 1610: Updated gated ratio to 0.9380 (progress: 62.0%)
Step 1610: Updated gated ratio to 0.9380 (progress: 62.0%)
Step 1610: Updated gated ratio to 0.9380 (progress: 62.0%)
Step 1610: Updated gated ratio to 0.9380 (progress: 62.0%)
Step 1610: Updated gated ratio to 0.9380 (progress: 62.0%)Step 1610: Updated gated ratio to 0.9380 (progress: 62.0%)
Step 1610: Updated gated ratio to 0.9380 (progress: 62.0%)
Step 1610: Updated gated ratio to 0.9380 (progress: 62.0%)

Step 1610: Updated gated ratio to 0.9380 (progress: 62.0%)
Step 1610: Updated gated ratio to 0.9380 (progress: 62.0%)
{'loss': 1.1935, 'grad_norm': 1.4547243118286133, 'learning_rate': 1.6169263627791886e-05, 'epoch': 0.31}
{'loss': 1.1954, 'grad_norm': 1.6750502586364746, 'learning_rate': 1.616435862450063e-05, 'epoch': 0.31}
{'loss': 1.133, 'grad_norm': 1.6191624402999878, 'learning_rate': 1.615945122799311e-05, 'epoch': 0.31}
{'loss': 1.2544, 'grad_norm': 1.1262017488479614, 'learning_rate': 1.6154541440174547e-05, 'epoch': 0.31}
{'loss': 1.1075, 'grad_norm': 1.360520601272583, 'learning_rate': 1.614962926295109e-05, 'epoch': 0.31}
Step 1615: Updated gated ratio to 0.9378 (progress: 62.2%)
Step 1615: Updated gated ratio to 0.9378 (progress: 62.2%)
Step 1615: Updated gated ratio to 0.9378 (progress: 62.2%)
Step 1615: Updated gated ratio to 0.9378 (progress: 62.2%)
Step 1615: Updated gated ratio to 0.9378 (progress: 62.2%)
Step 1615: Updated gated ratio to 0.9378 (progress: 62.2%)
Step 1615: Updated gated ratio to 0.9378 (progress: 62.2%)
Step 1615: Updated gated ratio to 0.9378 (progress: 62.2%)
Step 1615: Updated gated ratio to 0.9378 (progress: 62.2%)
Step 1615: Updated gated ratio to 0.9378 (progress: 62.2%)
Step 1615: Updated gated ratio to 0.9378 (progress: 62.2%)
Step 1615: Updated gated ratio to 0.9378 (progress: 62.2%)
Step 1615: Updated gated ratio to 0.9378 (progress: 62.2%)
Step 1615: Updated gated ratio to 0.9378 (progress: 62.2%)
Step 1615: Updated gated ratio to 0.9378 (progress: 62.2%)
Step 1615: Updated gated ratio to 0.9378 (progress: 62.2%)
{'loss': 1.0341, 'grad_norm': 1.5015456676483154, 'learning_rate': 1.6144714698229814e-05, 'epoch': 0.31}
{'loss': 1.1085, 'grad_norm': 1.5256803035736084, 'learning_rate': 1.6139797747918725e-05, 'epoch': 0.31}
{'loss': 1.0539, 'grad_norm': 1.4487698078155518, 'learning_rate': 1.613487841392675e-05, 'epoch': 0.31}
{'loss': 1.057, 'grad_norm': 1.547451138496399, 'learning_rate': 1.612995669816375e-05, 'epoch': 0.31}
{'loss': 1.0019, 'grad_norm': 1.4813337326049805, 'learning_rate': 1.6125032602540492e-05, 'epoch': 0.31}
Step 1620: Updated gated ratio to 0.9376 (progress: 62.4%)
Step 1620: Updated gated ratio to 0.9376 (progress: 62.4%)
Step 1620: Updated gated ratio to 0.9376 (progress: 62.4%)
Step 1620: Updated gated ratio to 0.9376 (progress: 62.4%)
Step 1620: Updated gated ratio to 0.9376 (progress: 62.4%)
Step 1620: Updated gated ratio to 0.9376 (progress: 62.4%)
Step 1620: Updated gated ratio to 0.9376 (progress: 62.4%)
Step 1620: Updated gated ratio to 0.9376 (progress: 62.4%)
Step 1620: Updated gated ratio to 0.9376 (progress: 62.4%)
Step 1620: Updated gated ratio to 0.9376 (progress: 62.4%)
Step 1620: Updated gated ratio to 0.9376 (progress: 62.4%)
Step 1620: Updated gated ratio to 0.9376 (progress: 62.4%)
Step 1620: Updated gated ratio to 0.9376 (progress: 62.4%)
Step 1620: Updated gated ratio to 0.9376 (progress: 62.4%)
Step 1620: Updated gated ratio to 0.9376 (progress: 62.4%)
Step 1620: Updated gated ratio to 0.9376 (progress: 62.4%)
{'loss': 1.057, 'grad_norm': 1.5206431150436401, 'learning_rate': 1.6120106128968686e-05, 'epoch': 0.31}
{'loss': 1.0728, 'grad_norm': 1.585666537284851, 'learning_rate': 1.6115177279360965e-05, 'epoch': 0.31}
{'loss': 1.0937, 'grad_norm': 1.6845147609710693, 'learning_rate': 1.611024605563087e-05, 'epoch': 0.31}
{'loss': 1.087, 'grad_norm': 1.60853910446167, 'learning_rate': 1.610531245969287e-05, 'epoch': 0.31}
{'loss': 1.2083, 'grad_norm': 1.6833093166351318, 'learning_rate': 1.6100376493462368e-05, 'epoch': 0.31}
{'loss': 1.058, 'grad_norm': 1.4775868654251099, 'learning_rate': 1.6095438158855668e-05, 'epoch': 0.31}
Step 1626: Updated gated ratio to 0.9374 (progress: 62.6%)
Step 1626: Updated gated ratio to 0.9374 (progress: 62.6%)
Step 1626: Updated gated ratio to 0.9374 (progress: 62.6%)
Step 1626: Updated gated ratio to 0.9374 (progress: 62.6%)
Step 1626: Updated gated ratio to 0.9374 (progress: 62.6%)
Step 1626: Updated gated ratio to 0.9374 (progress: 62.6%)
Step 1626: Updated gated ratio to 0.9374 (progress: 62.6%)
Step 1626: Updated gated ratio to 0.9374 (progress: 62.6%)
Step 1626: Updated gated ratio to 0.9374 (progress: 62.6%)
Step 1626: Updated gated ratio to 0.9374 (progress: 62.6%)
Step 1626: Updated gated ratio to 0.9374 (progress: 62.6%)
Step 1626: Updated gated ratio to 0.9374 (progress: 62.6%)
Step 1626: Updated gated ratio to 0.9374 (progress: 62.6%)
Step 1626: Updated gated ratio to 0.9374 (progress: 62.6%)
Step 1626: Updated gated ratio to 0.9374 (progress: 62.6%)
Step 1626: Updated gated ratio to 0.9374 (progress: 62.6%)
{'loss': 1.0658, 'grad_norm': 1.7191578149795532, 'learning_rate': 1.609049745779e-05, 'epoch': 0.31}
{'loss': 1.0109, 'grad_norm': 1.6705313920974731, 'learning_rate': 1.6085554392183517e-05, 'epoch': 0.31}
{'loss': 1.0874, 'grad_norm': 1.5988447666168213, 'learning_rate': 1.608060896395529e-05, 'epoch': 0.31}
{'loss': 1.2001, 'grad_norm': 1.7159802913665771, 'learning_rate': 1.60756611750253e-05, 'epoch': 0.31}
{'loss': 1.1623, 'grad_norm': 1.7167959213256836, 'learning_rate': 1.6070711027314446e-05, 'epoch': 0.31}
Step 1631: Updated gated ratio to 0.9372 (progress: 62.8%)
Step 1631: Updated gated ratio to 0.9372 (progress: 62.8%)
Step 1631: Updated gated ratio to 0.9372 (progress: 62.8%)
Step 1631: Updated gated ratio to 0.9372 (progress: 62.8%)
Step 1631: Updated gated ratio to 0.9372 (progress: 62.8%)
Step 1631: Updated gated ratio to 0.9372 (progress: 62.8%)
Step 1631: Updated gated ratio to 0.9372 (progress: 62.8%)
Step 1631: Updated gated ratio to 0.9372 (progress: 62.8%)
Step 1631: Updated gated ratio to 0.9372 (progress: 62.8%)
Step 1631: Updated gated ratio to 0.9372 (progress: 62.8%)
Step 1631: Updated gated ratio to 0.9372 (progress: 62.8%)Step 1631: Updated gated ratio to 0.9372 (progress: 62.8%)

Step 1631: Updated gated ratio to 0.9372 (progress: 62.8%)
Step 1631: Updated gated ratio to 0.9372 (progress: 62.8%)
Step 1631: Updated gated ratio to 0.9372 (progress: 62.8%)
Step 1631: Updated gated ratio to 0.9372 (progress: 62.8%)
{'loss': 1.1522, 'grad_norm': 1.5337440967559814, 'learning_rate': 1.606575852274456e-05, 'epoch': 0.31}
{'loss': 1.1633, 'grad_norm': 1.5362977981567383, 'learning_rate': 1.6060803663238357e-05, 'epoch': 0.31}
{'loss': 1.1473, 'grad_norm': 1.635096549987793, 'learning_rate': 1.6055846450719498e-05, 'epoch': 0.31}
{'loss': 1.0542, 'grad_norm': 1.5628005266189575, 'learning_rate': 1.6050886887112535e-05, 'epoch': 0.31}
{'loss': 1.1184, 'grad_norm': 1.5614521503448486, 'learning_rate': 1.6045924974342945e-05, 'epoch': 0.31}
Step 1636: Updated gated ratio to 0.9370 (progress: 63.0%)
Step 1636: Updated gated ratio to 0.9370 (progress: 63.0%)
Step 1636: Updated gated ratio to 0.9370 (progress: 63.0%)
Step 1636: Updated gated ratio to 0.9370 (progress: 63.0%)
Step 1636: Updated gated ratio to 0.9370 (progress: 63.0%)
Step 1636: Updated gated ratio to 0.9370 (progress: 63.0%)
Step 1636: Updated gated ratio to 0.9370 (progress: 63.0%)
Step 1636: Updated gated ratio to 0.9370 (progress: 63.0%)
Step 1636: Updated gated ratio to 0.9370 (progress: 63.0%)
Step 1636: Updated gated ratio to 0.9370 (progress: 63.0%)
Step 1636: Updated gated ratio to 0.9370 (progress: 63.0%)
Step 1636: Updated gated ratio to 0.9370 (progress: 63.0%)Step 1636: Updated gated ratio to 0.9370 (progress: 63.0%)

Step 1636: Updated gated ratio to 0.9370 (progress: 63.0%)
Step 1636: Updated gated ratio to 0.9370 (progress: 63.0%)
Step 1636: Updated gated ratio to 0.9370 (progress: 63.0%)
{'loss': 1.024, 'grad_norm': 1.5389257669448853, 'learning_rate': 1.604096071433711e-05, 'epoch': 0.31}
{'loss': 1.1143, 'grad_norm': 1.6493016481399536, 'learning_rate': 1.6035994109022333e-05, 'epoch': 0.32}
{'loss': 1.0337, 'grad_norm': 1.4083514213562012, 'learning_rate': 1.6031025160326814e-05, 'epoch': 0.32}
Step 1639: Updated gated ratio to 0.9369 (progress: 63.1%)
Step 1639: Updated gated ratio to 0.9369 (progress: 63.1%)
Step 1639: Updated gated ratio to 0.9369 (progress: 63.1%)
Step 1639: Updated gated ratio to 0.9369 (progress: 63.1%)
Step 1639: Updated gated ratio to 0.9369 (progress: 63.1%)
Step 1639: Updated gated ratio to 0.9369 (progress: 63.1%)
Step 1639: Updated gated ratio to 0.9369 (progress: 63.1%)
Step 1639: Updated gated ratio to 0.9369 (progress: 63.1%)
Step 1639: Updated gated ratio to 0.9369 (progress: 63.1%)
Step 1639: Updated gated ratio to 0.9369 (progress: 63.1%)
Step 1639: Updated gated ratio to 0.9369 (progress: 63.1%)
Step 1639: Updated gated ratio to 0.9369 (progress: 63.1%)
Step 1639: Updated gated ratio to 0.9369 (progress: 63.1%)
Step 1639: Updated gated ratio to 0.9369 (progress: 63.1%)
Step 1639: Updated gated ratio to 0.9369 (progress: 63.1%)
Step 1639: Updated gated ratio to 0.9369 (progress: 63.1%)
{'loss': 1.1182, 'grad_norm': 1.371033787727356, 'learning_rate': 1.6026053870179678e-05, 'epoch': 0.32}
{'loss': 1.1787, 'grad_norm': 1.4994722604751587, 'learning_rate': 1.6021080240510943e-05, 'epoch': 0.32}
{'loss': 1.089, 'grad_norm': 1.5778489112854004, 'learning_rate': 1.601610427325155e-05, 'epoch': 0.32}
{'loss': 1.1566, 'grad_norm': 1.340406894683838, 'learning_rate': 1.6011125970333333e-05, 'epoch': 0.32}
{'loss': 1.0014, 'grad_norm': 1.47183096408844, 'learning_rate': 1.600614533368905e-05, 'epoch': 0.32}
Step 1644: Updated gated ratio to 0.9367 (progress: 63.3%)
Step 1644: Updated gated ratio to 0.9367 (progress: 63.3%)
Step 1644: Updated gated ratio to 0.9367 (progress: 63.3%)
Step 1644: Updated gated ratio to 0.9367 (progress: 63.3%)
Step 1644: Updated gated ratio to 0.9367 (progress: 63.3%)
Step 1644: Updated gated ratio to 0.9367 (progress: 63.3%)
Step 1644: Updated gated ratio to 0.9367 (progress: 63.3%)Step 1644: Updated gated ratio to 0.9367 (progress: 63.3%)

Step 1644: Updated gated ratio to 0.9367 (progress: 63.3%)
Step 1644: Updated gated ratio to 0.9367 (progress: 63.3%)
Step 1644: Updated gated ratio to 0.9367 (progress: 63.3%)
Step 1644: Updated gated ratio to 0.9367 (progress: 63.3%)
Step 1644: Updated gated ratio to 0.9367 (progress: 63.3%)
Step 1644: Updated gated ratio to 0.9367 (progress: 63.3%)
Step 1644: Updated gated ratio to 0.9367 (progress: 63.3%)
Step 1644: Updated gated ratio to 0.9367 (progress: 63.3%)
{'loss': 1.1778, 'grad_norm': 1.2568678855895996, 'learning_rate': 1.6001162365252348e-05, 'epoch': 0.32}
{'loss': 1.051, 'grad_norm': 1.554358720779419, 'learning_rate': 1.5996177066957787e-05, 'epoch': 0.32}
{'loss': 1.1236, 'grad_norm': 1.7262730598449707, 'learning_rate': 1.5991189440740838e-05, 'epoch': 0.32}
{'loss': 1.1028, 'grad_norm': 1.5324232578277588, 'learning_rate': 1.5986199488537867e-05, 'epoch': 0.32}
{'loss': 1.1132, 'grad_norm': 1.661495327949524, 'learning_rate': 1.598120721228614e-05, 'epoch': 0.32}
Step 1649: Updated gated ratio to 0.9365 (progress: 63.5%)
Step 1649: Updated gated ratio to 0.9365 (progress: 63.5%)
Step 1649: Updated gated ratio to 0.9365 (progress: 63.5%)
Step 1649: Updated gated ratio to 0.9365 (progress: 63.5%)
Step 1649: Updated gated ratio to 0.9365 (progress: 63.5%)
Step 1649: Updated gated ratio to 0.9365 (progress: 63.5%)Step 1649: Updated gated ratio to 0.9365 (progress: 63.5%)

Step 1649: Updated gated ratio to 0.9365 (progress: 63.5%)
Step 1649: Updated gated ratio to 0.9365 (progress: 63.5%)
Step 1649: Updated gated ratio to 0.9365 (progress: 63.5%)
Step 1649: Updated gated ratio to 0.9365 (progress: 63.5%)
Step 1649: Updated gated ratio to 0.9365 (progress: 63.5%)
Step 1649: Updated gated ratio to 0.9365 (progress: 63.5%)
Step 1649: Updated gated ratio to 0.9365 (progress: 63.5%)
Step 1649: Updated gated ratio to 0.9365 (progress: 63.5%)
Step 1649: Updated gated ratio to 0.9365 (progress: 63.5%)
{'loss': 1.1829, 'grad_norm': 1.5646679401397705, 'learning_rate': 1.5976212613923836e-05, 'epoch': 0.32}
{'loss': 1.0755, 'grad_norm': 1.5962417125701904, 'learning_rate': 1.5971215695390026e-05, 'epoch': 0.32}
{'loss': 1.1329, 'grad_norm': 1.6365083456039429, 'learning_rate': 1.5966216458624692e-05, 'epoch': 0.32}
{'loss': 1.0258, 'grad_norm': 1.5856026411056519, 'learning_rate': 1.5961214905568705e-05, 'epoch': 0.32}
{'loss': 0.9775, 'grad_norm': 1.640980839729309, 'learning_rate': 1.595621103816384e-05, 'epoch': 0.32}
Step 1654: Updated gated ratio to 0.9363 (progress: 63.7%)
Step 1654: Updated gated ratio to 0.9363 (progress: 63.7%)
Step 1654: Updated gated ratio to 0.9363 (progress: 63.7%)
Step 1654: Updated gated ratio to 0.9363 (progress: 63.7%)Step 1654: Updated gated ratio to 0.9363 (progress: 63.7%)

Step 1654: Updated gated ratio to 0.9363 (progress: 63.7%)
Step 1654: Updated gated ratio to 0.9363 (progress: 63.7%)
Step 1654: Updated gated ratio to 0.9363 (progress: 63.7%)Step 1654: Updated gated ratio to 0.9363 (progress: 63.7%)

Step 1654: Updated gated ratio to 0.9363 (progress: 63.7%)
Step 1654: Updated gated ratio to 0.9363 (progress: 63.7%)
Step 1654: Updated gated ratio to 0.9363 (progress: 63.7%)
Step 1654: Updated gated ratio to 0.9363 (progress: 63.7%)
Step 1654: Updated gated ratio to 0.9363 (progress: 63.7%)
Step 1654: Updated gated ratio to 0.9363 (progress: 63.7%)
Step 1654: Updated gated ratio to 0.9363 (progress: 63.7%)
{'loss': 1.0778, 'grad_norm': 1.5973169803619385, 'learning_rate': 1.5951204858352772e-05, 'epoch': 0.32}
{'loss': 1.0961, 'grad_norm': 1.6949044466018677, 'learning_rate': 1.594619636807907e-05, 'epoch': 0.32}
{'loss': 1.13, 'grad_norm': 1.723618745803833, 'learning_rate': 1.5941185569287206e-05, 'epoch': 0.32}
{'loss': 1.1693, 'grad_norm': 1.4679341316223145, 'learning_rate': 1.5936172463922542e-05, 'epoch': 0.32}
{'loss': 1.1186, 'grad_norm': 1.614182472229004, 'learning_rate': 1.593115705393134e-05, 'epoch': 0.32}
Step 1659: Updated gated ratio to 0.9361 (progress: 63.9%)
Step 1659: Updated gated ratio to 0.9361 (progress: 63.9%)
Step 1659: Updated gated ratio to 0.9361 (progress: 63.9%)
Step 1659: Updated gated ratio to 0.9361 (progress: 63.9%)
Step 1659: Updated gated ratio to 0.9361 (progress: 63.9%)
Step 1659: Updated gated ratio to 0.9361 (progress: 63.9%)
Step 1659: Updated gated ratio to 0.9361 (progress: 63.9%)
Step 1659: Updated gated ratio to 0.9361 (progress: 63.9%)
Step 1659: Updated gated ratio to 0.9361 (progress: 63.9%)
Step 1659: Updated gated ratio to 0.9361 (progress: 63.9%)
Step 1659: Updated gated ratio to 0.9361 (progress: 63.9%)
Step 1659: Updated gated ratio to 0.9361 (progress: 63.9%)
Step 1659: Updated gated ratio to 0.9361 (progress: 63.9%)
Step 1659: Updated gated ratio to 0.9361 (progress: 63.9%)
Step 1659: Updated gated ratio to 0.9361 (progress: 63.9%)
Step 1659: Updated gated ratio to 0.9361 (progress: 63.9%)
{'loss': 1.1183, 'grad_norm': 1.624201774597168, 'learning_rate': 1.5926139341260755e-05, 'epoch': 0.32}
Error with image file is truncated
{'loss': 1.1479, 'grad_norm': 1.6457124948501587, 'learning_rate': 1.5921119327858835e-05, 'epoch': 0.32}
{'loss': 1.0381, 'grad_norm': 1.5536950826644897, 'learning_rate': 1.5916097015674518e-05, 'epoch': 0.32}
{'loss': 1.1952, 'grad_norm': 1.6643294095993042, 'learning_rate': 1.5911072406657646e-05, 'epoch': 0.32}
{'loss': 1.1551, 'grad_norm': 1.6637343168258667, 'learning_rate': 1.5906045502758943e-05, 'epoch': 0.32}
{'loss': 1.1443, 'grad_norm': 1.4417632818222046, 'learning_rate': 1.590101630593002e-05, 'epoch': 0.32}
Step 1665: Updated gated ratio to 0.9359 (progress: 64.1%)
Step 1665: Updated gated ratio to 0.9359 (progress: 64.1%)
Step 1665: Updated gated ratio to 0.9359 (progress: 64.1%)
Step 1665: Updated gated ratio to 0.9359 (progress: 64.1%)
Step 1665: Updated gated ratio to 0.9359 (progress: 64.1%)
Step 1665: Updated gated ratio to 0.9359 (progress: 64.1%)
Step 1665: Updated gated ratio to 0.9359 (progress: 64.1%)
Step 1665: Updated gated ratio to 0.9359 (progress: 64.1%)
Step 1665: Updated gated ratio to 0.9359 (progress: 64.1%)
Step 1665: Updated gated ratio to 0.9359 (progress: 64.1%)
Step 1665: Updated gated ratio to 0.9359 (progress: 64.1%)
Step 1665: Updated gated ratio to 0.9359 (progress: 64.1%)
Step 1665: Updated gated ratio to 0.9359 (progress: 64.1%)
Step 1665: Updated gated ratio to 0.9359 (progress: 64.1%)
Step 1665: Updated gated ratio to 0.9359 (progress: 64.1%)
Step 1665: Updated gated ratio to 0.9359 (progress: 64.1%)
{'loss': 1.1255, 'grad_norm': 1.462643027305603, 'learning_rate': 1.5895984818123392e-05, 'epoch': 0.32}
{'loss': 1.0981, 'grad_norm': 1.6346931457519531, 'learning_rate': 1.5890951041292453e-05, 'epoch': 0.32}
{'loss': 1.0744, 'grad_norm': 1.5663678646087646, 'learning_rate': 1.588591497739149e-05, 'epoch': 0.32}
{'loss': 1.032, 'grad_norm': 1.6715315580368042, 'learning_rate': 1.5880876628375668e-05, 'epoch': 0.32}
{'loss': 1.0959, 'grad_norm': 1.66432523727417, 'learning_rate': 1.587583599620106e-05, 'epoch': 0.32}
Step 1670: Updated gated ratio to 0.9357 (progress: 64.3%)
Step 1670: Updated gated ratio to 0.9357 (progress: 64.3%)
Step 1670: Updated gated ratio to 0.9357 (progress: 64.3%)Step 1670: Updated gated ratio to 0.9357 (progress: 64.3%)
Step 1670: Updated gated ratio to 0.9357 (progress: 64.3%)
Step 1670: Updated gated ratio to 0.9357 (progress: 64.3%)
Step 1670: Updated gated ratio to 0.9357 (progress: 64.3%)
Step 1670: Updated gated ratio to 0.9357 (progress: 64.3%)
Step 1670: Updated gated ratio to 0.9357 (progress: 64.3%)
Step 1670: Updated gated ratio to 0.9357 (progress: 64.3%)
Step 1670: Updated gated ratio to 0.9357 (progress: 64.3%)
Step 1670: Updated gated ratio to 0.9357 (progress: 64.3%)

Step 1670: Updated gated ratio to 0.9357 (progress: 64.3%)
Step 1670: Updated gated ratio to 0.9357 (progress: 64.3%)
Step 1670: Updated gated ratio to 0.9357 (progress: 64.3%)
Step 1670: Updated gated ratio to 0.9357 (progress: 64.3%)
{'loss': 1.1241, 'grad_norm': 1.7031761407852173, 'learning_rate': 1.5870793082824604e-05, 'epoch': 0.32}
{'loss': 1.1477, 'grad_norm': 1.683335542678833, 'learning_rate': 1.5865747890204138e-05, 'epoch': 0.32}
{'loss': 1.1366, 'grad_norm': 1.7366962432861328, 'learning_rate': 1.5860700420298377e-05, 'epoch': 0.32}
{'loss': 1.116, 'grad_norm': 1.4557448625564575, 'learning_rate': 1.5855650675066924e-05, 'epoch': 0.32}
{'loss': 1.1283, 'grad_norm': 1.585374355316162, 'learning_rate': 1.5850598656470265e-05, 'epoch': 0.32}
Step 1675: Updated gated ratio to 0.9355 (progress: 64.5%)
Step 1675: Updated gated ratio to 0.9355 (progress: 64.5%)
Step 1675: Updated gated ratio to 0.9355 (progress: 64.5%)
Step 1675: Updated gated ratio to 0.9355 (progress: 64.5%)
Step 1675: Updated gated ratio to 0.9355 (progress: 64.5%)
Step 1675: Updated gated ratio to 0.9355 (progress: 64.5%)
Step 1675: Updated gated ratio to 0.9355 (progress: 64.5%)
Step 1675: Updated gated ratio to 0.9355 (progress: 64.5%)
Step 1675: Updated gated ratio to 0.9355 (progress: 64.5%)
Step 1675: Updated gated ratio to 0.9355 (progress: 64.5%)
Step 1675: Updated gated ratio to 0.9355 (progress: 64.5%)
Step 1675: Updated gated ratio to 0.9355 (progress: 64.5%)
Step 1675: Updated gated ratio to 0.9355 (progress: 64.5%)
Step 1675: Updated gated ratio to 0.9355 (progress: 64.5%)
Step 1675: Updated gated ratio to 0.9355 (progress: 64.5%)
Step 1675: Updated gated ratio to 0.9355 (progress: 64.5%)
{'loss': 1.1007, 'grad_norm': 1.4763847589492798, 'learning_rate': 1.584554436646976e-05, 'epoch': 0.32}
{'loss': 1.1436, 'grad_norm': 1.5123428106307983, 'learning_rate': 1.5840487807027665e-05, 'epoch': 0.32}
{'loss': 1.2275, 'grad_norm': 1.4129735231399536, 'learning_rate': 1.5835428980107113e-05, 'epoch': 0.32}
{'loss': 1.0179, 'grad_norm': 1.6335645914077759, 'learning_rate': 1.583036788767211e-05, 'epoch': 0.32}
{'loss': 1.1322, 'grad_norm': 1.6820398569107056, 'learning_rate': 1.5825304531687548e-05, 'epoch': 0.32}
Step 1680: Updated gated ratio to 0.9353 (progress: 64.7%)
Step 1680: Updated gated ratio to 0.9353 (progress: 64.7%)
Step 1680: Updated gated ratio to 0.9353 (progress: 64.7%)
Step 1680: Updated gated ratio to 0.9353 (progress: 64.7%)
Step 1680: Updated gated ratio to 0.9353 (progress: 64.7%)
Step 1680: Updated gated ratio to 0.9353 (progress: 64.7%)
Step 1680: Updated gated ratio to 0.9353 (progress: 64.7%)
Step 1680: Updated gated ratio to 0.9353 (progress: 64.7%)
Step 1680: Updated gated ratio to 0.9353 (progress: 64.7%)
Step 1680: Updated gated ratio to 0.9353 (progress: 64.7%)
Step 1680: Updated gated ratio to 0.9353 (progress: 64.7%)
Step 1680: Updated gated ratio to 0.9353 (progress: 64.7%)
Step 1680: Updated gated ratio to 0.9353 (progress: 64.7%)
Step 1680: Updated gated ratio to 0.9353 (progress: 64.7%)
Step 1680: Updated gated ratio to 0.9353 (progress: 64.7%)
Step 1680: Updated gated ratio to 0.9353 (progress: 64.7%)
{'loss': 1.1402, 'grad_norm': 1.4293287992477417, 'learning_rate': 1.5820238914119195e-05, 'epoch': 0.32}
{'loss': 1.1217, 'grad_norm': 1.6596068143844604, 'learning_rate': 1.5815171036933697e-05, 'epoch': 0.32}
{'loss': 1.1492, 'grad_norm': 1.5897679328918457, 'learning_rate': 1.5810100902098582e-05, 'epoch': 0.32}
{'loss': 1.0639, 'grad_norm': 1.292888879776001, 'learning_rate': 1.580502851158225e-05, 'epoch': 0.32}
{'loss': 1.2042, 'grad_norm': 1.5894876718521118, 'learning_rate': 1.5799953867353975e-05, 'epoch': 0.32}
Step 1685: Updated gated ratio to 0.9351 (progress: 64.9%)
Step 1685: Updated gated ratio to 0.9351 (progress: 64.9%)
Step 1685: Updated gated ratio to 0.9351 (progress: 64.9%)
Step 1685: Updated gated ratio to 0.9351 (progress: 64.9%)
Step 1685: Updated gated ratio to 0.9351 (progress: 64.9%)
Step 1685: Updated gated ratio to 0.9351 (progress: 64.9%)
Step 1685: Updated gated ratio to 0.9351 (progress: 64.9%)
Step 1685: Updated gated ratio to 0.9351 (progress: 64.9%)
Step 1685: Updated gated ratio to 0.9351 (progress: 64.9%)
Step 1685: Updated gated ratio to 0.9351 (progress: 64.9%)
Step 1685: Updated gated ratio to 0.9351 (progress: 64.9%)
Step 1685: Updated gated ratio to 0.9351 (progress: 64.9%)
Step 1685: Updated gated ratio to 0.9351 (progress: 64.9%)
Step 1685: Updated gated ratio to 0.9351 (progress: 64.9%)
Step 1685: Updated gated ratio to 0.9351 (progress: 64.9%)
Step 1685: Updated gated ratio to 0.9351 (progress: 64.9%)
{'loss': 1.1242, 'grad_norm': 1.5827118158340454, 'learning_rate': 1.579487697138391e-05, 'epoch': 0.32}
{'loss': 1.146, 'grad_norm': 1.460978627204895, 'learning_rate': 1.5789797825643086e-05, 'epoch': 0.32}
{'loss': 1.1755, 'grad_norm': 1.2803938388824463, 'learning_rate': 1.5784716432103394e-05, 'epoch': 0.32}
{'loss': 1.0472, 'grad_norm': 1.6340503692626953, 'learning_rate': 1.5779632792737608e-05, 'epoch': 0.32}
{'loss': 1.1062, 'grad_norm': 1.5179163217544556, 'learning_rate': 1.5774546909519376e-05, 'epoch': 0.33}
Step 1690: Updated gated ratio to 0.9349 (progress: 65.1%)
Step 1690: Updated gated ratio to 0.9349 (progress: 65.1%)
Step 1690: Updated gated ratio to 0.9349 (progress: 65.1%)
Step 1690: Updated gated ratio to 0.9349 (progress: 65.1%)
Step 1690: Updated gated ratio to 0.9349 (progress: 65.1%)
Step 1690: Updated gated ratio to 0.9349 (progress: 65.1%)
Step 1690: Updated gated ratio to 0.9349 (progress: 65.1%)
Step 1690: Updated gated ratio to 0.9349 (progress: 65.1%)
Step 1690: Updated gated ratio to 0.9349 (progress: 65.1%)
Step 1690: Updated gated ratio to 0.9349 (progress: 65.1%)
Step 1690: Updated gated ratio to 0.9349 (progress: 65.1%)
Step 1690: Updated gated ratio to 0.9349 (progress: 65.1%)
Step 1690: Updated gated ratio to 0.9349 (progress: 65.1%)
Step 1690: Updated gated ratio to 0.9349 (progress: 65.1%)
Step 1690: Updated gated ratio to 0.9349 (progress: 65.1%)
Step 1690: Updated gated ratio to 0.9349 (progress: 65.1%)
{'loss': 1.2399, 'grad_norm': 1.8275697231292725, 'learning_rate': 1.5769458784423206e-05, 'epoch': 0.33}
{'loss': 1.1478, 'grad_norm': 1.679465889930725, 'learning_rate': 1.5764368419424488e-05, 'epoch': 0.33}
{'loss': 1.0342, 'grad_norm': 1.820467233657837, 'learning_rate': 1.575927581649948e-05, 'epoch': 0.33}
{'loss': 1.2503, 'grad_norm': 1.5566877126693726, 'learning_rate': 1.5754180977625303e-05, 'epoch': 0.33}
{'loss': 1.0926, 'grad_norm': 1.6696734428405762, 'learning_rate': 1.574908390477995e-05, 'epoch': 0.33}
{'loss': 1.1264, 'grad_norm': 1.4839409589767456, 'learning_rate': 1.5743984599942273e-05, 'epoch': 0.33}
Step 1696: Updated gated ratio to 0.9347 (progress: 65.3%)
Step 1696: Updated gated ratio to 0.9347 (progress: 65.3%)
Step 1696: Updated gated ratio to 0.9347 (progress: 65.3%)
Step 1696: Updated gated ratio to 0.9347 (progress: 65.3%)
Step 1696: Updated gated ratio to 0.9347 (progress: 65.3%)
Step 1696: Updated gated ratio to 0.9347 (progress: 65.3%)
Step 1696: Updated gated ratio to 0.9347 (progress: 65.3%)
Step 1696: Updated gated ratio to 0.9347 (progress: 65.3%)
Step 1696: Updated gated ratio to 0.9347 (progress: 65.3%)
Step 1696: Updated gated ratio to 0.9347 (progress: 65.3%)
Step 1696: Updated gated ratio to 0.9347 (progress: 65.3%)
Step 1696: Updated gated ratio to 0.9347 (progress: 65.3%)
Step 1696: Updated gated ratio to 0.9347 (progress: 65.3%)
Step 1696: Updated gated ratio to 0.9347 (progress: 65.3%)
Step 1696: Updated gated ratio to 0.9347 (progress: 65.3%)
Step 1696: Updated gated ratio to 0.9347 (progress: 65.3%)
{'loss': 1.1513, 'grad_norm': 1.5751841068267822, 'learning_rate': 1.5738883065092005e-05, 'epoch': 0.33}
{'loss': 1.0495, 'grad_norm': 1.5243839025497437, 'learning_rate': 1.5733779302209735e-05, 'epoch': 0.33}
{'loss': 1.0385, 'grad_norm': 1.5604803562164307, 'learning_rate': 1.572867331327692e-05, 'epoch': 0.33}
{'loss': 1.0802, 'grad_norm': 1.5685858726501465, 'learning_rate': 1.5723565100275884e-05, 'epoch': 0.33}
{'loss': 1.0641, 'grad_norm': 1.4750646352767944, 'learning_rate': 1.5718454665189806e-05, 'epoch': 0.33}
Step 1701: Updated gated ratio to 0.9345 (progress: 65.5%)
Step 1701: Updated gated ratio to 0.9345 (progress: 65.5%)
Step 1701: Updated gated ratio to 0.9345 (progress: 65.5%)
Step 1701: Updated gated ratio to 0.9345 (progress: 65.5%)
Step 1701: Updated gated ratio to 0.9345 (progress: 65.5%)
Step 1701: Updated gated ratio to 0.9345 (progress: 65.5%)
Step 1701: Updated gated ratio to 0.9345 (progress: 65.5%)
Step 1701: Updated gated ratio to 0.9345 (progress: 65.5%)
Step 1701: Updated gated ratio to 0.9345 (progress: 65.5%)
Step 1701: Updated gated ratio to 0.9345 (progress: 65.5%)
Step 1701: Updated gated ratio to 0.9345 (progress: 65.5%)
Step 1701: Updated gated ratio to 0.9345 (progress: 65.5%)
Step 1701: Updated gated ratio to 0.9345 (progress: 65.5%)
Step 1701: Updated gated ratio to 0.9345 (progress: 65.5%)Step 1701: Updated gated ratio to 0.9345 (progress: 65.5%)

Step 1701: Updated gated ratio to 0.9345 (progress: 65.5%)
{'loss': 1.1269, 'grad_norm': 1.659183144569397, 'learning_rate': 1.5713342010002733e-05, 'epoch': 0.33}
{'loss': 1.0696, 'grad_norm': 1.6528427600860596, 'learning_rate': 1.5708227136699578e-05, 'epoch': 0.33}
{'loss': 1.0198, 'grad_norm': 1.6278220415115356, 'learning_rate': 1.5703110047266105e-05, 'epoch': 0.33}
{'loss': 1.2127, 'grad_norm': 1.592626690864563, 'learning_rate': 1.569799074368895e-05, 'epoch': 0.33}
{'loss': 1.1713, 'grad_norm': 1.7113901376724243, 'learning_rate': 1.5692869227955603e-05, 'epoch': 0.33}
Step 1706: Updated gated ratio to 0.9343 (progress: 65.7%)
Step 1706: Updated gated ratio to 0.9343 (progress: 65.7%)Step 1706: Updated gated ratio to 0.9343 (progress: 65.7%)
Step 1706: Updated gated ratio to 0.9343 (progress: 65.7%)
Step 1706: Updated gated ratio to 0.9343 (progress: 65.7%)Step 1706: Updated gated ratio to 0.9343 (progress: 65.7%)
Step 1706: Updated gated ratio to 0.9343 (progress: 65.7%)

Step 1706: Updated gated ratio to 0.9343 (progress: 65.7%)
Step 1706: Updated gated ratio to 0.9343 (progress: 65.7%)
Step 1706: Updated gated ratio to 0.9343 (progress: 65.7%)
Step 1706: Updated gated ratio to 0.9343 (progress: 65.7%)

Step 1706: Updated gated ratio to 0.9343 (progress: 65.7%)
Step 1706: Updated gated ratio to 0.9343 (progress: 65.7%)
Step 1706: Updated gated ratio to 0.9343 (progress: 65.7%)
Step 1706: Updated gated ratio to 0.9343 (progress: 65.7%)
Step 1706: Updated gated ratio to 0.9343 (progress: 65.7%)
{'loss': 1.1589, 'grad_norm': 1.4587234258651733, 'learning_rate': 1.5687745502054407e-05, 'epoch': 0.33}
{'loss': 1.0615, 'grad_norm': 1.6973856687545776, 'learning_rate': 1.5682619567974575e-05, 'epoch': 0.33}
{'loss': 0.9819, 'grad_norm': 1.6092463731765747, 'learning_rate': 1.567749142770617e-05, 'epoch': 0.33}
{'loss': 1.0775, 'grad_norm': 1.5516974925994873, 'learning_rate': 1.5672361083240106e-05, 'epoch': 0.33}
{'loss': 1.1152, 'grad_norm': 1.5938736200332642, 'learning_rate': 1.5667228536568167e-05, 'epoch': 0.33}
Step 1711: Updated gated ratio to 0.9341 (progress: 65.9%)
Step 1711: Updated gated ratio to 0.9341 (progress: 65.9%)
Step 1711: Updated gated ratio to 0.9341 (progress: 65.9%)
Step 1711: Updated gated ratio to 0.9341 (progress: 65.9%)
Step 1711: Updated gated ratio to 0.9341 (progress: 65.9%)Step 1711: Updated gated ratio to 0.9341 (progress: 65.9%)

Step 1711: Updated gated ratio to 0.9341 (progress: 65.9%)
Step 1711: Updated gated ratio to 0.9341 (progress: 65.9%)
Step 1711: Updated gated ratio to 0.9341 (progress: 65.9%)
Step 1711: Updated gated ratio to 0.9341 (progress: 65.9%)
Step 1711: Updated gated ratio to 0.9341 (progress: 65.9%)
Step 1711: Updated gated ratio to 0.9341 (progress: 65.9%)
Step 1711: Updated gated ratio to 0.9341 (progress: 65.9%)
Step 1711: Updated gated ratio to 0.9341 (progress: 65.9%)
Step 1711: Updated gated ratio to 0.9341 (progress: 65.9%)
Step 1711: Updated gated ratio to 0.9341 (progress: 65.9%)
{'loss': 0.9998, 'grad_norm': 1.537298321723938, 'learning_rate': 1.566209378968298e-05, 'epoch': 0.33}
{'loss': 1.2216, 'grad_norm': 1.7174701690673828, 'learning_rate': 1.565695684457803e-05, 'epoch': 0.33}
{'loss': 1.1385, 'grad_norm': 1.530504822731018, 'learning_rate': 1.5651817703247666e-05, 'epoch': 0.33}
{'loss': 1.1991, 'grad_norm': 1.6129519939422607, 'learning_rate': 1.5646676367687067e-05, 'epoch': 0.33}
{'loss': 1.1558, 'grad_norm': 1.6381181478500366, 'learning_rate': 1.564153283989228e-05, 'epoch': 0.33}
Step 1716: Updated gated ratio to 0.9339 (progress: 66.1%)
Step 1716: Updated gated ratio to 0.9339 (progress: 66.1%)Step 1716: Updated gated ratio to 0.9339 (progress: 66.1%)
Step 1716: Updated gated ratio to 0.9339 (progress: 66.1%)
Step 1716: Updated gated ratio to 0.9339 (progress: 66.1%)
Step 1716: Updated gated ratio to 0.9339 (progress: 66.1%)
Step 1716: Updated gated ratio to 0.9339 (progress: 66.1%)
Step 1716: Updated gated ratio to 0.9339 (progress: 66.1%)
Step 1716: Updated gated ratio to 0.9339 (progress: 66.1%)

Step 1716: Updated gated ratio to 0.9339 (progress: 66.1%)
Step 1716: Updated gated ratio to 0.9339 (progress: 66.1%)
Step 1716: Updated gated ratio to 0.9339 (progress: 66.1%)
Step 1716: Updated gated ratio to 0.9339 (progress: 66.1%)
Step 1716: Updated gated ratio to 0.9339 (progress: 66.1%)
Step 1716: Updated gated ratio to 0.9339 (progress: 66.1%)
Step 1716: Updated gated ratio to 0.9339 (progress: 66.1%)
{'loss': 1.2072, 'grad_norm': 1.7070252895355225, 'learning_rate': 1.5636387121860207e-05, 'epoch': 0.33}
{'loss': 1.1587, 'grad_norm': 1.6273906230926514, 'learning_rate': 1.5631239215588578e-05, 'epoch': 0.33}
{'loss': 1.1019, 'grad_norm': 1.6848691701889038, 'learning_rate': 1.5626089123076004e-05, 'epoch': 0.33}
{'loss': 1.0443, 'grad_norm': 1.7303006649017334, 'learning_rate': 1.5620936846321917e-05, 'epoch': 0.33}
{'loss': 1.0796, 'grad_norm': 1.549038052558899, 'learning_rate': 1.561578238732661e-05, 'epoch': 0.33}
{'loss': 1.2174, 'grad_norm': 1.4323065280914307, 'learning_rate': 1.561062574809123e-05, 'epoch': 0.33}
Step 1722: Updated gated ratio to 0.9337 (progress: 66.3%)
Step 1722: Updated gated ratio to 0.9337 (progress: 66.3%)
Step 1722: Updated gated ratio to 0.9337 (progress: 66.3%)
Step 1722: Updated gated ratio to 0.9337 (progress: 66.3%)Step 1722: Updated gated ratio to 0.9337 (progress: 66.3%)
Step 1722: Updated gated ratio to 0.9337 (progress: 66.3%)
Step 1722: Updated gated ratio to 0.9337 (progress: 66.3%)
Step 1722: Updated gated ratio to 0.9337 (progress: 66.3%)
Step 1722: Updated gated ratio to 0.9337 (progress: 66.3%)
Step 1722: Updated gated ratio to 0.9337 (progress: 66.3%)

Step 1722: Updated gated ratio to 0.9337 (progress: 66.3%)
Step 1722: Updated gated ratio to 0.9337 (progress: 66.3%)
Step 1722: Updated gated ratio to 0.9337 (progress: 66.3%)
Step 1722: Updated gated ratio to 0.9337 (progress: 66.3%)
Step 1722: Updated gated ratio to 0.9337 (progress: 66.3%)
Step 1722: Updated gated ratio to 0.9337 (progress: 66.3%)
{'loss': 1.0885, 'grad_norm': 1.6429158449172974, 'learning_rate': 1.5605466930617747e-05, 'epoch': 0.33}
{'loss': 1.1084, 'grad_norm': 1.5518534183502197, 'learning_rate': 1.5600305936909005e-05, 'epoch': 0.33}
{'loss': 1.1933, 'grad_norm': 1.3729902505874634, 'learning_rate': 1.559514276896867e-05, 'epoch': 0.33}
{'loss': 1.1407, 'grad_norm': 1.8467934131622314, 'learning_rate': 1.558997742880127e-05, 'epoch': 0.33}
{'loss': 1.0944, 'grad_norm': 1.615939974784851, 'learning_rate': 1.5584809918412158e-05, 'epoch': 0.33}
Step 1727: Updated gated ratio to 0.9335 (progress: 66.5%)
Step 1727: Updated gated ratio to 0.9335 (progress: 66.5%)
Step 1727: Updated gated ratio to 0.9335 (progress: 66.5%)
Step 1727: Updated gated ratio to 0.9335 (progress: 66.5%)
Step 1727: Updated gated ratio to 0.9335 (progress: 66.5%)
Step 1727: Updated gated ratio to 0.9335 (progress: 66.5%)
Step 1727: Updated gated ratio to 0.9335 (progress: 66.5%)
Step 1727: Updated gated ratio to 0.9335 (progress: 66.5%)
Step 1727: Updated gated ratio to 0.9335 (progress: 66.5%)
Step 1727: Updated gated ratio to 0.9335 (progress: 66.5%)
Step 1727: Updated gated ratio to 0.9335 (progress: 66.5%)
Step 1727: Updated gated ratio to 0.9335 (progress: 66.5%)
Step 1727: Updated gated ratio to 0.9335 (progress: 66.5%)
Step 1727: Updated gated ratio to 0.9335 (progress: 66.5%)
Step 1727: Updated gated ratio to 0.9335 (progress: 66.5%)
Step 1727: Updated gated ratio to 0.9335 (progress: 66.5%)
{'loss': 1.0401, 'grad_norm': 1.4946544170379639, 'learning_rate': 1.557964023980755e-05, 'epoch': 0.33}
{'loss': 1.1918, 'grad_norm': 1.6454501152038574, 'learning_rate': 1.5574468394994486e-05, 'epoch': 0.33}
{'loss': 1.096, 'grad_norm': 1.6110305786132812, 'learning_rate': 1.5569294385980856e-05, 'epoch': 0.33}
{'loss': 1.1121, 'grad_norm': 1.6481621265411377, 'learning_rate': 1.556411821477539e-05, 'epoch': 0.33}
{'loss': 1.2239, 'grad_norm': 1.7665330171585083, 'learning_rate': 1.5558939883387657e-05, 'epoch': 0.33}
Step 1732: Updated gated ratio to 0.9333 (progress: 66.7%)
Step 1732: Updated gated ratio to 0.9333 (progress: 66.7%)
Step 1732: Updated gated ratio to 0.9333 (progress: 66.7%)
Step 1732: Updated gated ratio to 0.9333 (progress: 66.7%)
Step 1732: Updated gated ratio to 0.9333 (progress: 66.7%)
Step 1732: Updated gated ratio to 0.9333 (progress: 66.7%)
Step 1732: Updated gated ratio to 0.9333 (progress: 66.7%)
Step 1732: Updated gated ratio to 0.9333 (progress: 66.7%)
Step 1732: Updated gated ratio to 0.9333 (progress: 66.7%)
Step 1732: Updated gated ratio to 0.9333 (progress: 66.7%)
Step 1732: Updated gated ratio to 0.9333 (progress: 66.7%)
Step 1732: Updated gated ratio to 0.9333 (progress: 66.7%)
Step 1732: Updated gated ratio to 0.9333 (progress: 66.7%)
Step 1732: Updated gated ratio to 0.9333 (progress: 66.7%)
Step 1732: Updated gated ratio to 0.9333 (progress: 66.7%)
Step 1732: Updated gated ratio to 0.9333 (progress: 66.7%)
{'loss': 1.1653, 'grad_norm': 1.61863112449646, 'learning_rate': 1.5553759393828058e-05, 'epoch': 0.33}
{'loss': 1.132, 'grad_norm': 1.6942058801651, 'learning_rate': 1.554857674810784e-05, 'epoch': 0.33}
{'loss': 1.0778, 'grad_norm': 1.5365149974822998, 'learning_rate': 1.554339194823909e-05, 'epoch': 0.33}
{'loss': 1.1276, 'grad_norm': 1.6730984449386597, 'learning_rate': 1.553820499623472e-05, 'epoch': 0.33}
{'loss': 1.206, 'grad_norm': 1.73122239112854, 'learning_rate': 1.553301589410848e-05, 'epoch': 0.33}
Step 1737: Updated gated ratio to 0.9331 (progress: 66.9%)
Step 1737: Updated gated ratio to 0.9331 (progress: 66.9%)
Step 1737: Updated gated ratio to 0.9331 (progress: 66.9%)
Step 1737: Updated gated ratio to 0.9331 (progress: 66.9%)
Step 1737: Updated gated ratio to 0.9331 (progress: 66.9%)
Step 1737: Updated gated ratio to 0.9331 (progress: 66.9%)
Step 1737: Updated gated ratio to 0.9331 (progress: 66.9%)
Step 1737: Updated gated ratio to 0.9331 (progress: 66.9%)
Step 1737: Updated gated ratio to 0.9331 (progress: 66.9%)
Step 1737: Updated gated ratio to 0.9331 (progress: 66.9%)
Step 1737: Updated gated ratio to 0.9331 (progress: 66.9%)
Step 1737: Updated gated ratio to 0.9331 (progress: 66.9%)
Step 1737: Updated gated ratio to 0.9331 (progress: 66.9%)
Step 1737: Updated gated ratio to 0.9331 (progress: 66.9%)
Step 1737: Updated gated ratio to 0.9331 (progress: 66.9%)
Step 1737: Updated gated ratio to 0.9331 (progress: 66.9%)
{'loss': 1.1422, 'grad_norm': 1.5577588081359863, 'learning_rate': 1.5527824643874968e-05, 'epoch': 0.33}
{'loss': 1.0917, 'grad_norm': 1.7178031206130981, 'learning_rate': 1.5522631247549598e-05, 'epoch': 0.33}
{'loss': 1.1079, 'grad_norm': 1.6335383653640747, 'learning_rate': 1.5517435707148628e-05, 'epoch': 0.33}
{'loss': 1.1533, 'grad_norm': 1.678741216659546, 'learning_rate': 1.5512238024689144e-05, 'epoch': 0.33}
{'loss': 1.0202, 'grad_norm': 1.5564008951187134, 'learning_rate': 1.550703820218907e-05, 'epoch': 0.34}
Step 1742: Updated gated ratio to 0.9329 (progress: 67.1%)
Step 1742: Updated gated ratio to 0.9329 (progress: 67.1%)
Step 1742: Updated gated ratio to 0.9329 (progress: 67.1%)
Step 1742: Updated gated ratio to 0.9329 (progress: 67.1%)
Step 1742: Updated gated ratio to 0.9329 (progress: 67.1%)
Step 1742: Updated gated ratio to 0.9329 (progress: 67.1%)
Step 1742: Updated gated ratio to 0.9329 (progress: 67.1%)
Step 1742: Updated gated ratio to 0.9329 (progress: 67.1%)
Step 1742: Updated gated ratio to 0.9329 (progress: 67.1%)
Step 1742: Updated gated ratio to 0.9329 (progress: 67.1%)
Step 1742: Updated gated ratio to 0.9329 (progress: 67.1%)
Step 1742: Updated gated ratio to 0.9329 (progress: 67.1%)
Step 1742: Updated gated ratio to 0.9329 (progress: 67.1%)
Step 1742: Updated gated ratio to 0.9329 (progress: 67.1%)
Step 1742: Updated gated ratio to 0.9329 (progress: 67.1%)
Step 1742: Updated gated ratio to 0.9329 (progress: 67.1%)
{'loss': 1.1892, 'grad_norm': 1.611046552658081, 'learning_rate': 1.550183624166715e-05, 'epoch': 0.34}
{'loss': 1.2231, 'grad_norm': 1.9069699048995972, 'learning_rate': 1.549663214514297e-05, 'epoch': 0.34}
{'loss': 1.2082, 'grad_norm': 1.5885487794876099, 'learning_rate': 1.5491425914636934e-05, 'epoch': 0.34}
{'loss': 1.1112, 'grad_norm': 1.7038449048995972, 'learning_rate': 1.5486217552170283e-05, 'epoch': 0.34}
{'loss': 1.1219, 'grad_norm': 1.6348016262054443, 'learning_rate': 1.548100705976508e-05, 'epoch': 0.34}
{'loss': 1.1473, 'grad_norm': 1.6902859210968018, 'learning_rate': 1.5475794439444226e-05, 'epoch': 0.34}
Step 1748: Updated gated ratio to 0.9327 (progress: 67.3%)
Step 1748: Updated gated ratio to 0.9327 (progress: 67.3%)
Step 1748: Updated gated ratio to 0.9327 (progress: 67.3%)
Step 1748: Updated gated ratio to 0.9327 (progress: 67.3%)
Step 1748: Updated gated ratio to 0.9327 (progress: 67.3%)
Step 1748: Updated gated ratio to 0.9327 (progress: 67.3%)
Step 1748: Updated gated ratio to 0.9327 (progress: 67.3%)
Step 1748: Updated gated ratio to 0.9327 (progress: 67.3%)
Step 1748: Updated gated ratio to 0.9327 (progress: 67.3%)
Step 1748: Updated gated ratio to 0.9327 (progress: 67.3%)Step 1748: Updated gated ratio to 0.9327 (progress: 67.3%)

Step 1748: Updated gated ratio to 0.9327 (progress: 67.3%)
Step 1748: Updated gated ratio to 0.9327 (progress: 67.3%)
Step 1748: Updated gated ratio to 0.9327 (progress: 67.3%)
Step 1748: Updated gated ratio to 0.9327 (progress: 67.3%)
Step 1748: Updated gated ratio to 0.9327 (progress: 67.3%)
{'loss': 1.0928, 'grad_norm': 1.561528205871582, 'learning_rate': 1.5470579693231432e-05, 'epoch': 0.34}
{'loss': 1.2469, 'grad_norm': 1.579465627670288, 'learning_rate': 1.5465362823151245e-05, 'epoch': 0.34}
{'loss': 1.1468, 'grad_norm': 1.6703424453735352, 'learning_rate': 1.5460143831229026e-05, 'epoch': 0.34}
{'loss': 1.1202, 'grad_norm': 1.7208994626998901, 'learning_rate': 1.545492271949098e-05, 'epoch': 0.34}
{'loss': 1.1858, 'grad_norm': 1.4489390850067139, 'learning_rate': 1.544969948996411e-05, 'epoch': 0.34}
Step 1753: Updated gated ratio to 0.9325 (progress: 67.5%)
Step 1753: Updated gated ratio to 0.9325 (progress: 67.5%)
Step 1753: Updated gated ratio to 0.9325 (progress: 67.5%)
Step 1753: Updated gated ratio to 0.9325 (progress: 67.5%)
Step 1753: Updated gated ratio to 0.9325 (progress: 67.5%)
Step 1753: Updated gated ratio to 0.9325 (progress: 67.5%)
Step 1753: Updated gated ratio to 0.9325 (progress: 67.5%)
Step 1753: Updated gated ratio to 0.9325 (progress: 67.5%)
Step 1753: Updated gated ratio to 0.9325 (progress: 67.5%)
Step 1753: Updated gated ratio to 0.9325 (progress: 67.5%)
Step 1753: Updated gated ratio to 0.9325 (progress: 67.5%)
Step 1753: Updated gated ratio to 0.9325 (progress: 67.5%)
Step 1753: Updated gated ratio to 0.9325 (progress: 67.5%)
Step 1753: Updated gated ratio to 0.9325 (progress: 67.5%)
Step 1753: Updated gated ratio to 0.9325 (progress: 67.5%)
Step 1753: Updated gated ratio to 0.9325 (progress: 67.5%)
{'loss': 1.1032, 'grad_norm': 1.7345285415649414, 'learning_rate': 1.544447414467626e-05, 'epoch': 0.34}
{'loss': 1.1759, 'grad_norm': 1.6532313823699951, 'learning_rate': 1.5439246685656093e-05, 'epoch': 0.34}
{'loss': 1.0947, 'grad_norm': 1.5083417892456055, 'learning_rate': 1.5434017114933082e-05, 'epoch': 0.34}
{'loss': 1.1099, 'grad_norm': 1.5029605627059937, 'learning_rate': 1.5428785434537527e-05, 'epoch': 0.34}
{'loss': 1.1565, 'grad_norm': 1.6220697164535522, 'learning_rate': 1.542355164650055e-05, 'epoch': 0.34}
Step 1758: Updated gated ratio to 0.9323 (progress: 67.7%)
Step 1758: Updated gated ratio to 0.9323 (progress: 67.7%)
Step 1758: Updated gated ratio to 0.9323 (progress: 67.7%)
Step 1758: Updated gated ratio to 0.9323 (progress: 67.7%)
Step 1758: Updated gated ratio to 0.9323 (progress: 67.7%)
Step 1758: Updated gated ratio to 0.9323 (progress: 67.7%)
Step 1758: Updated gated ratio to 0.9323 (progress: 67.7%)
Step 1758: Updated gated ratio to 0.9323 (progress: 67.7%)
Step 1758: Updated gated ratio to 0.9323 (progress: 67.7%)
Step 1758: Updated gated ratio to 0.9323 (progress: 67.7%)
Step 1758: Updated gated ratio to 0.9323 (progress: 67.7%)
Step 1758: Updated gated ratio to 0.9323 (progress: 67.7%)
Step 1758: Updated gated ratio to 0.9323 (progress: 67.7%)
Step 1758: Updated gated ratio to 0.9323 (progress: 67.7%)
Step 1758: Updated gated ratio to 0.9323 (progress: 67.7%)
Step 1758: Updated gated ratio to 0.9323 (progress: 67.7%)
{'loss': 1.1823, 'grad_norm': 1.5166614055633545, 'learning_rate': 1.541831575285408e-05, 'epoch': 0.34}
{'loss': 1.0937, 'grad_norm': 1.652579665184021, 'learning_rate': 1.541307775563088e-05, 'epoch': 0.34}
{'loss': 1.2408, 'grad_norm': 1.4457658529281616, 'learning_rate': 1.540783765686452e-05, 'epoch': 0.34}
{'loss': 1.1192, 'grad_norm': 1.5029313564300537, 'learning_rate': 1.540259545858938e-05, 'epoch': 0.34}
{'loss': 1.2011, 'grad_norm': 1.6888463497161865, 'learning_rate': 1.539735116284067e-05, 'epoch': 0.34}
Step 1763: Updated gated ratio to 0.9321 (progress: 67.9%)
Step 1763: Updated gated ratio to 0.9321 (progress: 67.9%)
Step 1763: Updated gated ratio to 0.9321 (progress: 67.9%)
Step 1763: Updated gated ratio to 0.9321 (progress: 67.9%)
Step 1763: Updated gated ratio to 0.9321 (progress: 67.9%)
Step 1763: Updated gated ratio to 0.9321 (progress: 67.9%)
Step 1763: Updated gated ratio to 0.9321 (progress: 67.9%)
Step 1763: Updated gated ratio to 0.9321 (progress: 67.9%)
Step 1763: Updated gated ratio to 0.9321 (progress: 67.9%)
Step 1763: Updated gated ratio to 0.9321 (progress: 67.9%)
Step 1763: Updated gated ratio to 0.9321 (progress: 67.9%)
Step 1763: Updated gated ratio to 0.9321 (progress: 67.9%)
Step 1763: Updated gated ratio to 0.9321 (progress: 67.9%)
Step 1763: Updated gated ratio to 0.9321 (progress: 67.9%)
Step 1763: Updated gated ratio to 0.9321 (progress: 67.9%)
Step 1763: Updated gated ratio to 0.9321 (progress: 67.9%)
{'loss': 1.1305, 'grad_norm': 1.590553641319275, 'learning_rate': 1.53921047716544e-05, 'epoch': 0.34}
{'loss': 1.1078, 'grad_norm': 1.4713820219039917, 'learning_rate': 1.53868562870674e-05, 'epoch': 0.34}
{'loss': 1.1079, 'grad_norm': 1.533510684967041, 'learning_rate': 1.5381605711117318e-05, 'epoch': 0.34}
{'loss': 1.1787, 'grad_norm': 1.6546376943588257, 'learning_rate': 1.5376353045842604e-05, 'epoch': 0.34}
{'loss': 1.2212, 'grad_norm': 1.5503448247909546, 'learning_rate': 1.5371098293282526e-05, 'epoch': 0.34}
Step 1768: Updated gated ratio to 0.9319 (progress: 68.1%)
Step 1768: Updated gated ratio to 0.9319 (progress: 68.1%)
Step 1768: Updated gated ratio to 0.9319 (progress: 68.1%)
Step 1768: Updated gated ratio to 0.9319 (progress: 68.1%)
Step 1768: Updated gated ratio to 0.9319 (progress: 68.1%)
Step 1768: Updated gated ratio to 0.9319 (progress: 68.1%)
Step 1768: Updated gated ratio to 0.9319 (progress: 68.1%)
Step 1768: Updated gated ratio to 0.9319 (progress: 68.1%)
Step 1768: Updated gated ratio to 0.9319 (progress: 68.1%)
Step 1768: Updated gated ratio to 0.9319 (progress: 68.1%)
Step 1768: Updated gated ratio to 0.9319 (progress: 68.1%)
Step 1768: Updated gated ratio to 0.9319 (progress: 68.1%)
Step 1768: Updated gated ratio to 0.9319 (progress: 68.1%)
Step 1768: Updated gated ratio to 0.9319 (progress: 68.1%)
Step 1768: Updated gated ratio to 0.9319 (progress: 68.1%)
Step 1768: Updated gated ratio to 0.9319 (progress: 68.1%)
{'loss': 1.129, 'grad_norm': 1.5384869575500488, 'learning_rate': 1.5365841455477158e-05, 'epoch': 0.34}
{'loss': 1.0761, 'grad_norm': 1.5835996866226196, 'learning_rate': 1.5360582534467382e-05, 'epoch': 0.34}
{'loss': 1.0771, 'grad_norm': 1.4867656230926514, 'learning_rate': 1.5355321532294897e-05, 'epoch': 0.34}
{'loss': 1.1388, 'grad_norm': 1.574316143989563, 'learning_rate': 1.5350058451002204e-05, 'epoch': 0.34}
{'loss': 1.1145, 'grad_norm': 1.521560788154602, 'learning_rate': 1.5344793292632614e-05, 'epoch': 0.34}
{'loss': 1.0688, 'grad_norm': 1.5278600454330444, 'learning_rate': 1.533952605923024e-05, 'epoch': 0.34}
Step 1774: Updated gated ratio to 0.9317 (progress: 68.3%)
Step 1774: Updated gated ratio to 0.9317 (progress: 68.3%)
Step 1774: Updated gated ratio to 0.9317 (progress: 68.3%)
Step 1774: Updated gated ratio to 0.9317 (progress: 68.3%)
Step 1774: Updated gated ratio to 0.9317 (progress: 68.3%)
Step 1774: Updated gated ratio to 0.9317 (progress: 68.3%)
Step 1774: Updated gated ratio to 0.9317 (progress: 68.3%)
Step 1774: Updated gated ratio to 0.9317 (progress: 68.3%)
Step 1774: Updated gated ratio to 0.9317 (progress: 68.3%)
Step 1774: Updated gated ratio to 0.9317 (progress: 68.3%)
Step 1774: Updated gated ratio to 0.9317 (progress: 68.3%)
Step 1774: Updated gated ratio to 0.9317 (progress: 68.3%)
Step 1774: Updated gated ratio to 0.9317 (progress: 68.3%)
Step 1774: Updated gated ratio to 0.9317 (progress: 68.3%)
Step 1774: Updated gated ratio to 0.9317 (progress: 68.3%)
Step 1774: Updated gated ratio to 0.9317 (progress: 68.3%)
{'loss': 1.1363, 'grad_norm': 1.5776312351226807, 'learning_rate': 1.5334256752840007e-05, 'epoch': 0.34}
{'loss': 1.1305, 'grad_norm': 1.65932035446167, 'learning_rate': 1.532898537550764e-05, 'epoch': 0.34}
{'loss': 1.1822, 'grad_norm': 1.6554452180862427, 'learning_rate': 1.532371192927966e-05, 'epoch': 0.34}
{'loss': 1.0232, 'grad_norm': 1.6467639207839966, 'learning_rate': 1.5318436416203412e-05, 'epoch': 0.34}
{'loss': 1.042, 'grad_norm': 1.7412810325622559, 'learning_rate': 1.531315883832703e-05, 'epoch': 0.34}
Step 1779: Updated gated ratio to 0.9315 (progress: 68.5%)
Step 1779: Updated gated ratio to 0.9315 (progress: 68.5%)
Step 1779: Updated gated ratio to 0.9315 (progress: 68.5%)
Step 1779: Updated gated ratio to 0.9315 (progress: 68.5%)
Step 1779: Updated gated ratio to 0.9315 (progress: 68.5%)
Step 1779: Updated gated ratio to 0.9315 (progress: 68.5%)Step 1779: Updated gated ratio to 0.9315 (progress: 68.5%)
Step 1779: Updated gated ratio to 0.9315 (progress: 68.5%)
Step 1779: Updated gated ratio to 0.9315 (progress: 68.5%)
Step 1779: Updated gated ratio to 0.9315 (progress: 68.5%)
Step 1779: Updated gated ratio to 0.9315 (progress: 68.5%)

Step 1779: Updated gated ratio to 0.9315 (progress: 68.5%)
Step 1779: Updated gated ratio to 0.9315 (progress: 68.5%)
Step 1779: Updated gated ratio to 0.9315 (progress: 68.5%)
Step 1779: Updated gated ratio to 0.9315 (progress: 68.5%)
Step 1779: Updated gated ratio to 0.9315 (progress: 68.5%)
{'loss': 1.1356, 'grad_norm': 1.5374490022659302, 'learning_rate': 1.530787919769945e-05, 'epoch': 0.34}
{'loss': 1.1353, 'grad_norm': 1.8166816234588623, 'learning_rate': 1.5302597496370408e-05, 'epoch': 0.34}
{'loss': 1.169, 'grad_norm': 1.6471436023712158, 'learning_rate': 1.5297313736390447e-05, 'epoch': 0.34}
{'loss': 1.0649, 'grad_norm': 1.6264240741729736, 'learning_rate': 1.5292027919810898e-05, 'epoch': 0.34}
{'loss': 1.1448, 'grad_norm': 1.5524691343307495, 'learning_rate': 1.52867400486839e-05, 'epoch': 0.34}
Step 1784: Updated gated ratio to 0.9313 (progress: 68.7%)
Step 1784: Updated gated ratio to 0.9313 (progress: 68.7%)
Step 1784: Updated gated ratio to 0.9313 (progress: 68.7%)
Step 1784: Updated gated ratio to 0.9313 (progress: 68.7%)
Step 1784: Updated gated ratio to 0.9313 (progress: 68.7%)
Step 1784: Updated gated ratio to 0.9313 (progress: 68.7%)
Step 1784: Updated gated ratio to 0.9313 (progress: 68.7%)
Step 1784: Updated gated ratio to 0.9313 (progress: 68.7%)
Step 1784: Updated gated ratio to 0.9313 (progress: 68.7%)
Step 1784: Updated gated ratio to 0.9313 (progress: 68.7%)
Step 1784: Updated gated ratio to 0.9313 (progress: 68.7%)
Step 1784: Updated gated ratio to 0.9313 (progress: 68.7%)
Step 1784: Updated gated ratio to 0.9313 (progress: 68.7%)
Step 1784: Updated gated ratio to 0.9313 (progress: 68.7%)
Step 1784: Updated gated ratio to 0.9313 (progress: 68.7%)
Step 1784: Updated gated ratio to 0.9313 (progress: 68.7%)
{'loss': 1.0923, 'grad_norm': 1.7072279453277588, 'learning_rate': 1.528145012506239e-05, 'epoch': 0.34}
{'loss': 1.0945, 'grad_norm': 1.6027425527572632, 'learning_rate': 1.5276158151000096e-05, 'epoch': 0.34}
{'loss': 1.0714, 'grad_norm': 1.4791926145553589, 'learning_rate': 1.5270864128551542e-05, 'epoch': 0.34}
{'loss': 1.1004, 'grad_norm': 1.7231743335723877, 'learning_rate': 1.5265568059772053e-05, 'epoch': 0.34}
{'loss': 1.1825, 'grad_norm': 1.6480534076690674, 'learning_rate': 1.5260269946717746e-05, 'epoch': 0.34}
Step 1789: Updated gated ratio to 0.9311 (progress: 68.9%)
Step 1789: Updated gated ratio to 0.9311 (progress: 68.9%)Step 1789: Updated gated ratio to 0.9311 (progress: 68.9%)
Step 1789: Updated gated ratio to 0.9311 (progress: 68.9%)
Step 1789: Updated gated ratio to 0.9311 (progress: 68.9%)
Step 1789: Updated gated ratio to 0.9311 (progress: 68.9%)
Step 1789: Updated gated ratio to 0.9311 (progress: 68.9%)
Step 1789: Updated gated ratio to 0.9311 (progress: 68.9%)Step 1789: Updated gated ratio to 0.9311 (progress: 68.9%)

Step 1789: Updated gated ratio to 0.9311 (progress: 68.9%)
Step 1789: Updated gated ratio to 0.9311 (progress: 68.9%)

Step 1789: Updated gated ratio to 0.9311 (progress: 68.9%)
Step 1789: Updated gated ratio to 0.9311 (progress: 68.9%)
Step 1789: Updated gated ratio to 0.9311 (progress: 68.9%)
Step 1789: Updated gated ratio to 0.9311 (progress: 68.9%)
Step 1789: Updated gated ratio to 0.9311 (progress: 68.9%)
{'loss': 1.0659, 'grad_norm': 1.5667587518692017, 'learning_rate': 1.5254969791445526e-05, 'epoch': 0.34}
{'loss': 1.1376, 'grad_norm': 1.6810117959976196, 'learning_rate': 1.5249667596013102e-05, 'epoch': 0.34}
{'loss': 1.0925, 'grad_norm': 1.59880793094635, 'learning_rate': 1.5244363362478967e-05, 'epoch': 0.34}
{'loss': 1.2096, 'grad_norm': 1.7316492795944214, 'learning_rate': 1.5239057092902404e-05, 'epoch': 0.34}
{'loss': 1.2386, 'grad_norm': 1.4068679809570312, 'learning_rate': 1.523374878934349e-05, 'epoch': 0.35}
Step 1794: Updated gated ratio to 0.9309 (progress: 69.1%)
Step 1794: Updated gated ratio to 0.9309 (progress: 69.1%)
Step 1794: Updated gated ratio to 0.9309 (progress: 69.1%)
Step 1794: Updated gated ratio to 0.9309 (progress: 69.1%)
Step 1794: Updated gated ratio to 0.9309 (progress: 69.1%)
Step 1794: Updated gated ratio to 0.9309 (progress: 69.1%)Step 1794: Updated gated ratio to 0.9309 (progress: 69.1%)
Step 1794: Updated gated ratio to 0.9309 (progress: 69.1%)
Step 1794: Updated gated ratio to 0.9309 (progress: 69.1%)
Step 1794: Updated gated ratio to 0.9309 (progress: 69.1%)
Step 1794: Updated gated ratio to 0.9309 (progress: 69.1%)

Step 1794: Updated gated ratio to 0.9309 (progress: 69.1%)
Step 1794: Updated gated ratio to 0.9309 (progress: 69.1%)
Step 1794: Updated gated ratio to 0.9309 (progress: 69.1%)
Step 1794: Updated gated ratio to 0.9309 (progress: 69.1%)
Step 1794: Updated gated ratio to 0.9309 (progress: 69.1%)
{'loss': 1.1482, 'grad_norm': 1.6564995050430298, 'learning_rate': 1.5228438453863095e-05, 'epoch': 0.35}
{'loss': 1.143, 'grad_norm': 1.6687740087509155, 'learning_rate': 1.522312608852287e-05, 'epoch': 0.35}
{'loss': 1.1133, 'grad_norm': 1.642844319343567, 'learning_rate': 1.5217811695385263e-05, 'epoch': 0.35}
{'loss': 1.1266, 'grad_norm': 1.5521302223205566, 'learning_rate': 1.52124952765135e-05, 'epoch': 0.35}
{'loss': 1.1031, 'grad_norm': 1.6653244495391846, 'learning_rate': 1.5207176833971598e-05, 'epoch': 0.35}
{'loss': 1.1622, 'grad_norm': 1.5109360218048096, 'learning_rate': 1.520185636982436e-05, 'epoch': 0.35}
Step 1800: Updated gated ratio to 0.9307 (progress: 69.3%)
Step 1800: Updated gated ratio to 0.9307 (progress: 69.3%)
Step 1800: Updated gated ratio to 0.9307 (progress: 69.3%)
Step 1800: Updated gated ratio to 0.9307 (progress: 69.3%)
Step 1800: Updated gated ratio to 0.9307 (progress: 69.3%)
Step 1800: Updated gated ratio to 0.9307 (progress: 69.3%)
Step 1800: Updated gated ratio to 0.9307 (progress: 69.3%)
Step 1800: Updated gated ratio to 0.9307 (progress: 69.3%)
Step 1800: Updated gated ratio to 0.9307 (progress: 69.3%)
Step 1800: Updated gated ratio to 0.9307 (progress: 69.3%)
Step 1800: Updated gated ratio to 0.9307 (progress: 69.3%)
Step 1800: Updated gated ratio to 0.9307 (progress: 69.3%)
Step 1800: Updated gated ratio to 0.9307 (progress: 69.3%)
Step 1800: Updated gated ratio to 0.9307 (progress: 69.3%)
Step 1800: Updated gated ratio to 0.9307 (progress: 69.3%)
Step 1800: Updated gated ratio to 0.9307 (progress: 69.3%)
{'loss': 1.1911, 'grad_norm': 1.2672091722488403, 'learning_rate': 1.5196533886137376e-05, 'epoch': 0.35}
{'loss': 1.1457, 'grad_norm': 1.4914566278457642, 'learning_rate': 1.5191209384977014e-05, 'epoch': 0.35}
{'loss': 1.2227, 'grad_norm': 1.282250165939331, 'learning_rate': 1.5185882868410431e-05, 'epoch': 0.35}
{'loss': 1.2224, 'grad_norm': 1.2343264818191528, 'learning_rate': 1.5180554338505564e-05, 'epoch': 0.35}
{'loss': 1.0544, 'grad_norm': 1.4993592500686646, 'learning_rate': 1.517522379733113e-05, 'epoch': 0.35}
Step 1805: Updated gated ratio to 0.9305 (progress: 69.5%)
Step 1805: Updated gated ratio to 0.9305 (progress: 69.5%)
Step 1805: Updated gated ratio to 0.9305 (progress: 69.5%)
Step 1805: Updated gated ratio to 0.9305 (progress: 69.5%)
Step 1805: Updated gated ratio to 0.9305 (progress: 69.5%)
Step 1805: Updated gated ratio to 0.9305 (progress: 69.5%)
Step 1805: Updated gated ratio to 0.9305 (progress: 69.5%)
Step 1805: Updated gated ratio to 0.9305 (progress: 69.5%)
Step 1805: Updated gated ratio to 0.9305 (progress: 69.5%)
Step 1805: Updated gated ratio to 0.9305 (progress: 69.5%)
Step 1805: Updated gated ratio to 0.9305 (progress: 69.5%)
Step 1805: Updated gated ratio to 0.9305 (progress: 69.5%)
Step 1805: Updated gated ratio to 0.9305 (progress: 69.5%)
Step 1805: Updated gated ratio to 0.9305 (progress: 69.5%)
Step 1805: Updated gated ratio to 0.9305 (progress: 69.5%)
Step 1805: Updated gated ratio to 0.9305 (progress: 69.5%)
{'loss': 1.1156, 'grad_norm': 1.717529535293579, 'learning_rate': 1.5169891246956629e-05, 'epoch': 0.35}
{'loss': 1.1245, 'grad_norm': 1.5719759464263916, 'learning_rate': 1.5164556689452346e-05, 'epoch': 0.35}
{'loss': 1.1727, 'grad_norm': 1.3313357830047607, 'learning_rate': 1.5159220126889329e-05, 'epoch': 0.35}
{'loss': 1.1271, 'grad_norm': 1.5668535232543945, 'learning_rate': 1.5153881561339426e-05, 'epoch': 0.35}
Error with image file is truncated (10 bytes not processed)
{'loss': 1.0159, 'grad_norm': 1.5001293420791626, 'learning_rate': 1.5148540994875242e-05, 'epoch': 0.35}
Step 1810: Updated gated ratio to 0.9303 (progress: 69.7%)
Step 1810: Updated gated ratio to 0.9303 (progress: 69.7%)
Step 1810: Updated gated ratio to 0.9303 (progress: 69.7%)
Step 1810: Updated gated ratio to 0.9303 (progress: 69.7%)
Step 1810: Updated gated ratio to 0.9303 (progress: 69.7%)
Step 1810: Updated gated ratio to 0.9303 (progress: 69.7%)
Step 1810: Updated gated ratio to 0.9303 (progress: 69.7%)
Step 1810: Updated gated ratio to 0.9303 (progress: 69.7%)
Step 1810: Updated gated ratio to 0.9303 (progress: 69.7%)
Step 1810: Updated gated ratio to 0.9303 (progress: 69.7%)
Step 1810: Updated gated ratio to 0.9303 (progress: 69.7%)
Step 1810: Updated gated ratio to 0.9303 (progress: 69.7%)
Step 1810: Updated gated ratio to 0.9303 (progress: 69.7%)
Step 1810: Updated gated ratio to 0.9303 (progress: 69.7%)
Step 1810: Updated gated ratio to 0.9303 (progress: 69.7%)
Step 1810: Updated gated ratio to 0.9303 (progress: 69.7%)
{'loss': 1.1165, 'grad_norm': 1.685960292816162, 'learning_rate': 1.5143198429570181e-05, 'epoch': 0.35}
{'loss': 1.1555, 'grad_norm': 1.693906307220459, 'learning_rate': 1.5137853867498403e-05, 'epoch': 0.35}
{'loss': 1.1464, 'grad_norm': 1.605488657951355, 'learning_rate': 1.5132507310734847e-05, 'epoch': 0.35}
{'loss': 1.1313, 'grad_norm': 1.6300228834152222, 'learning_rate': 1.5127158761355241e-05, 'epoch': 0.35}
{'loss': 1.1297, 'grad_norm': 1.7075167894363403, 'learning_rate': 1.512180822143607e-05, 'epoch': 0.35}
Step 1815: Updated gated ratio to 0.9301 (progress: 69.9%)
Step 1815: Updated gated ratio to 0.9301 (progress: 69.9%)
Step 1815: Updated gated ratio to 0.9301 (progress: 69.9%)
Step 1815: Updated gated ratio to 0.9301 (progress: 69.9%)
Step 1815: Updated gated ratio to 0.9301 (progress: 69.9%)
Step 1815: Updated gated ratio to 0.9301 (progress: 69.9%)
Step 1815: Updated gated ratio to 0.9301 (progress: 69.9%)
Step 1815: Updated gated ratio to 0.9301 (progress: 69.9%)
Step 1815: Updated gated ratio to 0.9301 (progress: 69.9%)
Step 1815: Updated gated ratio to 0.9301 (progress: 69.9%)
Step 1815: Updated gated ratio to 0.9301 (progress: 69.9%)
Step 1815: Updated gated ratio to 0.9301 (progress: 69.9%)
Step 1815: Updated gated ratio to 0.9301 (progress: 69.9%)
Step 1815: Updated gated ratio to 0.9301 (progress: 69.9%)
Step 1815: Updated gated ratio to 0.9301 (progress: 69.9%)
Step 1815: Updated gated ratio to 0.9301 (progress: 69.9%)
{'loss': 1.1054, 'grad_norm': 1.7195299863815308, 'learning_rate': 1.5116455693054594e-05, 'epoch': 0.35}
{'loss': 1.1615, 'grad_norm': 1.5367820262908936, 'learning_rate': 1.5111101178288858e-05, 'epoch': 0.35}
{'loss': 1.0197, 'grad_norm': 1.513080358505249, 'learning_rate': 1.510574467921766e-05, 'epoch': 0.35}
{'loss': 1.0691, 'grad_norm': 1.7235838174819946, 'learning_rate': 1.5100386197920585e-05, 'epoch': 0.35}
{'loss': 1.1457, 'grad_norm': 1.6849372386932373, 'learning_rate': 1.5095025736477977e-05, 'epoch': 0.35}
Step 1820: Updated gated ratio to 0.9299 (progress: 70.1%)
Step 1820: Updated gated ratio to 0.9299 (progress: 70.1%)
Step 1820: Updated gated ratio to 0.9299 (progress: 70.1%)
Step 1820: Updated gated ratio to 0.9299 (progress: 70.1%)
Step 1820: Updated gated ratio to 0.9299 (progress: 70.1%)
Step 1820: Updated gated ratio to 0.9299 (progress: 70.1%)
Step 1820: Updated gated ratio to 0.9299 (progress: 70.1%)
Step 1820: Updated gated ratio to 0.9299 (progress: 70.1%)
Step 1820: Updated gated ratio to 0.9299 (progress: 70.1%)
Step 1820: Updated gated ratio to 0.9299 (progress: 70.1%)
Step 1820: Updated gated ratio to 0.9299 (progress: 70.1%)Step 1820: Updated gated ratio to 0.9299 (progress: 70.1%)

Step 1820: Updated gated ratio to 0.9299 (progress: 70.1%)
Step 1820: Updated gated ratio to 0.9299 (progress: 70.1%)
Step 1820: Updated gated ratio to 0.9299 (progress: 70.1%)
Step 1820: Updated gated ratio to 0.9299 (progress: 70.1%)
{'loss': 1.0774, 'grad_norm': 1.624504566192627, 'learning_rate': 1.5089663296970952e-05, 'epoch': 0.35}
{'loss': 1.1542, 'grad_norm': 1.6578788757324219, 'learning_rate': 1.5084298881481388e-05, 'epoch': 0.35}
{'loss': 1.0525, 'grad_norm': 1.675337314605713, 'learning_rate': 1.5078932492091942e-05, 'epoch': 0.35}
{'loss': 1.1847, 'grad_norm': 1.5897258520126343, 'learning_rate': 1.5073564130886032e-05, 'epoch': 0.35}
{'loss': 1.1582, 'grad_norm': 1.579451322555542, 'learning_rate': 1.506819379994784e-05, 'epoch': 0.35}
{'loss': 1.1715, 'grad_norm': 1.74446439743042, 'learning_rate': 1.5062821501362308e-05, 'epoch': 0.35}
Step 1826: Updated gated ratio to 0.9297 (progress: 70.3%)
Step 1826: Updated gated ratio to 0.9297 (progress: 70.3%)
Step 1826: Updated gated ratio to 0.9297 (progress: 70.3%)
Step 1826: Updated gated ratio to 0.9297 (progress: 70.3%)
Step 1826: Updated gated ratio to 0.9297 (progress: 70.3%)
Step 1826: Updated gated ratio to 0.9297 (progress: 70.3%)
Step 1826: Updated gated ratio to 0.9297 (progress: 70.3%)
Step 1826: Updated gated ratio to 0.9297 (progress: 70.3%)
Step 1826: Updated gated ratio to 0.9297 (progress: 70.3%)
Step 1826: Updated gated ratio to 0.9297 (progress: 70.3%)
Step 1826: Updated gated ratio to 0.9297 (progress: 70.3%)
Step 1826: Updated gated ratio to 0.9297 (progress: 70.3%)
Step 1826: Updated gated ratio to 0.9297 (progress: 70.3%)
Step 1826: Updated gated ratio to 0.9297 (progress: 70.3%)
Step 1826: Updated gated ratio to 0.9297 (progress: 70.3%)
Step 1826: Updated gated ratio to 0.9297 (progress: 70.3%)
{'loss': 1.0751, 'grad_norm': 1.5525656938552856, 'learning_rate': 1.5057447237215152e-05, 'epoch': 0.35}
{'loss': 1.0848, 'grad_norm': 1.6543828248977661, 'learning_rate': 1.5052071009592846e-05, 'epoch': 0.35}
{'loss': 1.0595, 'grad_norm': 1.6372064352035522, 'learning_rate': 1.5046692820582625e-05, 'epoch': 0.35}
{'loss': 1.0847, 'grad_norm': 1.5233815908432007, 'learning_rate': 1.504131267227249e-05, 'epoch': 0.35}
{'loss': 1.0991, 'grad_norm': 1.5705804824829102, 'learning_rate': 1.5035930566751198e-05, 'epoch': 0.35}
Step 1831: Updated gated ratio to 0.9295 (progress: 70.5%)
Step 1831: Updated gated ratio to 0.9295 (progress: 70.5%)
Step 1831: Updated gated ratio to 0.9295 (progress: 70.5%)Step 1831: Updated gated ratio to 0.9295 (progress: 70.5%)
Step 1831: Updated gated ratio to 0.9295 (progress: 70.5%)
Step 1831: Updated gated ratio to 0.9295 (progress: 70.5%)
Step 1831: Updated gated ratio to 0.9295 (progress: 70.5%)
Step 1831: Updated gated ratio to 0.9295 (progress: 70.5%)
Step 1831: Updated gated ratio to 0.9295 (progress: 70.5%)

Step 1831: Updated gated ratio to 0.9295 (progress: 70.5%)
Step 1831: Updated gated ratio to 0.9295 (progress: 70.5%)
Step 1831: Updated gated ratio to 0.9295 (progress: 70.5%)
Step 1831: Updated gated ratio to 0.9295 (progress: 70.5%)
Step 1831: Updated gated ratio to 0.9295 (progress: 70.5%)
Step 1831: Updated gated ratio to 0.9295 (progress: 70.5%)
Step 1831: Updated gated ratio to 0.9295 (progress: 70.5%)
{'loss': 1.0657, 'grad_norm': 1.6957201957702637, 'learning_rate': 1.5030546506108268e-05, 'epoch': 0.35}
{'loss': 1.1409, 'grad_norm': 1.610089898109436, 'learning_rate': 1.5025160492433976e-05, 'epoch': 0.35}
{'loss': 1.1966, 'grad_norm': 1.693780541419983, 'learning_rate': 1.501977252781936e-05, 'epoch': 0.35}
{'loss': 1.1588, 'grad_norm': 1.6574207544326782, 'learning_rate': 1.5014382614356213e-05, 'epoch': 0.35}
{'loss': 1.0063, 'grad_norm': 1.6027929782867432, 'learning_rate': 1.5008990754137088e-05, 'epoch': 0.35}
Step 1836: Updated gated ratio to 0.9293 (progress: 70.7%)
Step 1836: Updated gated ratio to 0.9293 (progress: 70.7%)
Step 1836: Updated gated ratio to 0.9293 (progress: 70.7%)
Step 1836: Updated gated ratio to 0.9293 (progress: 70.7%)
Step 1836: Updated gated ratio to 0.9293 (progress: 70.7%)
Step 1836: Updated gated ratio to 0.9293 (progress: 70.7%)
Step 1836: Updated gated ratio to 0.9293 (progress: 70.7%)
Step 1836: Updated gated ratio to 0.9293 (progress: 70.7%)
Step 1836: Updated gated ratio to 0.9293 (progress: 70.7%)
Step 1836: Updated gated ratio to 0.9293 (progress: 70.7%)
Step 1836: Updated gated ratio to 0.9293 (progress: 70.7%)
Step 1836: Updated gated ratio to 0.9293 (progress: 70.7%)
Step 1836: Updated gated ratio to 0.9293 (progress: 70.7%)
Step 1836: Updated gated ratio to 0.9293 (progress: 70.7%)
Step 1836: Updated gated ratio to 0.9293 (progress: 70.7%)
Step 1836: Updated gated ratio to 0.9293 (progress: 70.7%)
{'loss': 1.1096, 'grad_norm': 1.643770694732666, 'learning_rate': 1.5003596949255284e-05, 'epoch': 0.35}
{'loss': 1.1427, 'grad_norm': 1.6637169122695923, 'learning_rate': 1.4998201201804867e-05, 'epoch': 0.35}
{'loss': 1.1774, 'grad_norm': 1.347161054611206, 'learning_rate': 1.499280351388065e-05, 'epoch': 0.35}
{'loss': 1.1442, 'grad_norm': 1.6212645769119263, 'learning_rate': 1.49874038875782e-05, 'epoch': 0.35}
{'loss': 1.1941, 'grad_norm': 1.325887680053711, 'learning_rate': 1.498200232499384e-05, 'epoch': 0.35}
Step 1841: Updated gated ratio to 0.9291 (progress: 70.9%)Step 1841: Updated gated ratio to 0.9291 (progress: 70.9%)

Step 1841: Updated gated ratio to 0.9291 (progress: 70.9%)Step 1841: Updated gated ratio to 0.9291 (progress: 70.9%)

Step 1841: Updated gated ratio to 0.9291 (progress: 70.9%)
Step 1841: Updated gated ratio to 0.9291 (progress: 70.9%)
Step 1841: Updated gated ratio to 0.9291 (progress: 70.9%)
Step 1841: Updated gated ratio to 0.9291 (progress: 70.9%)
Step 1841: Updated gated ratio to 0.9291 (progress: 70.9%)
Step 1841: Updated gated ratio to 0.9291 (progress: 70.9%)
Step 1841: Updated gated ratio to 0.9291 (progress: 70.9%)
Step 1841: Updated gated ratio to 0.9291 (progress: 70.9%)
Step 1841: Updated gated ratio to 0.9291 (progress: 70.9%)
Step 1841: Updated gated ratio to 0.9291 (progress: 70.9%)
Step 1841: Updated gated ratio to 0.9291 (progress: 70.9%)
Step 1841: Updated gated ratio to 0.9291 (progress: 70.9%)
{'loss': 1.2233, 'grad_norm': 1.3296016454696655, 'learning_rate': 1.4976598828224643e-05, 'epoch': 0.35}
{'loss': 1.1217, 'grad_norm': 1.5015144348144531, 'learning_rate': 1.497119339936843e-05, 'epoch': 0.35}
{'loss': 1.2913, 'grad_norm': 1.5618836879730225, 'learning_rate': 1.4965786040523779e-05, 'epoch': 0.35}
{'loss': 1.1374, 'grad_norm': 1.514085054397583, 'learning_rate': 1.496037675379001e-05, 'epoch': 0.35}
{'loss': 1.1285, 'grad_norm': 1.4717159271240234, 'learning_rate': 1.4954965541267192e-05, 'epoch': 0.36}
Step 1846: Updated gated ratio to 0.9289 (progress: 71.1%)
Step 1846: Updated gated ratio to 0.9289 (progress: 71.1%)
Step 1846: Updated gated ratio to 0.9289 (progress: 71.1%)
Step 1846: Updated gated ratio to 0.9289 (progress: 71.1%)
Step 1846: Updated gated ratio to 0.9289 (progress: 71.1%)
Step 1846: Updated gated ratio to 0.9289 (progress: 71.1%)
Step 1846: Updated gated ratio to 0.9289 (progress: 71.1%)
Step 1846: Updated gated ratio to 0.9289 (progress: 71.1%)
Step 1846: Updated gated ratio to 0.9289 (progress: 71.1%)
Step 1846: Updated gated ratio to 0.9289 (progress: 71.1%)
Step 1846: Updated gated ratio to 0.9289 (progress: 71.1%)
Step 1846: Updated gated ratio to 0.9289 (progress: 71.1%)
Step 1846: Updated gated ratio to 0.9289 (progress: 71.1%)Step 1846: Updated gated ratio to 0.9289 (progress: 71.1%)

Step 1846: Updated gated ratio to 0.9289 (progress: 71.1%)
Step 1846: Updated gated ratio to 0.9289 (progress: 71.1%)
{'loss': 1.0693, 'grad_norm': 1.5371323823928833, 'learning_rate': 1.494955240505615e-05, 'epoch': 0.36}
{'loss': 1.2324, 'grad_norm': 1.6229802370071411, 'learning_rate': 1.494413734725844e-05, 'epoch': 0.36}
{'loss': 1.1559, 'grad_norm': 1.7956055402755737, 'learning_rate': 1.4938720369976385e-05, 'epoch': 0.36}
{'loss': 1.1262, 'grad_norm': 1.5909442901611328, 'learning_rate': 1.4933301475313036e-05, 'epoch': 0.36}
{'loss': 1.163, 'grad_norm': 1.5644491910934448, 'learning_rate': 1.4927880665372197e-05, 'epoch': 0.36}
{'loss': 1.1161, 'grad_norm': 1.6077938079833984, 'learning_rate': 1.4922457942258411e-05, 'epoch': 0.36}
Step 1852: Updated gated ratio to 0.9287 (progress: 71.3%)
Step 1852: Updated gated ratio to 0.9287 (progress: 71.3%)
Step 1852: Updated gated ratio to 0.9287 (progress: 71.3%)
Step 1852: Updated gated ratio to 0.9287 (progress: 71.3%)
Step 1852: Updated gated ratio to 0.9287 (progress: 71.3%)
Step 1852: Updated gated ratio to 0.9287 (progress: 71.3%)Step 1852: Updated gated ratio to 0.9287 (progress: 71.3%)
Step 1852: Updated gated ratio to 0.9287 (progress: 71.3%)
Step 1852: Updated gated ratio to 0.9287 (progress: 71.3%)
Step 1852: Updated gated ratio to 0.9287 (progress: 71.3%)
Step 1852: Updated gated ratio to 0.9287 (progress: 71.3%)
Step 1852: Updated gated ratio to 0.9287 (progress: 71.3%)
Step 1852: Updated gated ratio to 0.9287 (progress: 71.3%)

Step 1852: Updated gated ratio to 0.9287 (progress: 71.3%)
Step 1852: Updated gated ratio to 0.9287 (progress: 71.3%)
Step 1852: Updated gated ratio to 0.9287 (progress: 71.3%)
{'loss': 1.1637, 'grad_norm': 1.3480688333511353, 'learning_rate': 1.4917033308076967e-05, 'epoch': 0.36}
{'loss': 1.0702, 'grad_norm': 1.6007236242294312, 'learning_rate': 1.4911606764933892e-05, 'epoch': 0.36}
{'loss': 1.1154, 'grad_norm': 1.4732887744903564, 'learning_rate': 1.490617831493596e-05, 'epoch': 0.36}
{'loss': 1.0961, 'grad_norm': 1.578687310218811, 'learning_rate': 1.4900747960190682e-05, 'epoch': 0.36}
{'loss': 1.134, 'grad_norm': 1.5845896005630493, 'learning_rate': 1.489531570280631e-05, 'epoch': 0.36}
Step 1857: Updated gated ratio to 0.9285 (progress: 71.5%)
Step 1857: Updated gated ratio to 0.9285 (progress: 71.5%)
Step 1857: Updated gated ratio to 0.9285 (progress: 71.5%)
Step 1857: Updated gated ratio to 0.9285 (progress: 71.5%)
Step 1857: Updated gated ratio to 0.9285 (progress: 71.5%)
Step 1857: Updated gated ratio to 0.9285 (progress: 71.5%)
Step 1857: Updated gated ratio to 0.9285 (progress: 71.5%)
Step 1857: Updated gated ratio to 0.9285 (progress: 71.5%)
Step 1857: Updated gated ratio to 0.9285 (progress: 71.5%)
Step 1857: Updated gated ratio to 0.9285 (progress: 71.5%)
Step 1857: Updated gated ratio to 0.9285 (progress: 71.5%)
Step 1857: Updated gated ratio to 0.9285 (progress: 71.5%)
Step 1857: Updated gated ratio to 0.9285 (progress: 71.5%)
Step 1857: Updated gated ratio to 0.9285 (progress: 71.5%)
Step 1857: Updated gated ratio to 0.9285 (progress: 71.5%)
Step 1857: Updated gated ratio to 0.9285 (progress: 71.5%)
{'loss': 1.1203, 'grad_norm': 1.4887936115264893, 'learning_rate': 1.488988154489183e-05, 'epoch': 0.36}
{'loss': 1.0119, 'grad_norm': 1.5603948831558228, 'learning_rate': 1.4884445488556972e-05, 'epoch': 0.36}
{'loss': 1.0877, 'grad_norm': 1.4696288108825684, 'learning_rate': 1.4879007535912198e-05, 'epoch': 0.36}
Error with image file is truncated (21 bytes not processed)
{'loss': 0.9573, 'grad_norm': 1.495103120803833, 'learning_rate': 1.4873567689068708e-05, 'epoch': 0.36}
{'loss': 1.19, 'grad_norm': 1.323773741722107, 'learning_rate': 1.4868125950138442e-05, 'epoch': 0.36}
Step 1862: Updated gated ratio to 0.9283 (progress: 71.7%)
Step 1862: Updated gated ratio to 0.9283 (progress: 71.7%)
Step 1862: Updated gated ratio to 0.9283 (progress: 71.7%)
Step 1862: Updated gated ratio to 0.9283 (progress: 71.7%)
Step 1862: Updated gated ratio to 0.9283 (progress: 71.7%)
Step 1862: Updated gated ratio to 0.9283 (progress: 71.7%)
Step 1862: Updated gated ratio to 0.9283 (progress: 71.7%)
Step 1862: Updated gated ratio to 0.9283 (progress: 71.7%)
Step 1862: Updated gated ratio to 0.9283 (progress: 71.7%)
Step 1862: Updated gated ratio to 0.9283 (progress: 71.7%)
Step 1862: Updated gated ratio to 0.9283 (progress: 71.7%)
Step 1862: Updated gated ratio to 0.9283 (progress: 71.7%)
Step 1862: Updated gated ratio to 0.9283 (progress: 71.7%)
Step 1862: Updated gated ratio to 0.9283 (progress: 71.7%)
Step 1862: Updated gated ratio to 0.9283 (progress: 71.7%)
Step 1862: Updated gated ratio to 0.9283 (progress: 71.7%)
{'loss': 1.1186, 'grad_norm': 1.7604516744613647, 'learning_rate': 1.4862682321234064e-05, 'epoch': 0.36}
{'loss': 1.1189, 'grad_norm': 1.5588816404342651, 'learning_rate': 1.4857236804468983e-05, 'epoch': 0.36}
{'loss': 1.1913, 'grad_norm': 1.649705410003662, 'learning_rate': 1.4851789401957338e-05, 'epoch': 0.36}
{'loss': 1.1998, 'grad_norm': 1.6144112348556519, 'learning_rate': 1.4846340115813993e-05, 'epoch': 0.36}
{'loss': 1.2974, 'grad_norm': 1.1306887865066528, 'learning_rate': 1.484088894815455e-05, 'epoch': 0.36}
Step 1867: Updated gated ratio to 0.9281 (progress: 71.9%)
Step 1867: Updated gated ratio to 0.9281 (progress: 71.9%)
Step 1867: Updated gated ratio to 0.9281 (progress: 71.9%)
Step 1867: Updated gated ratio to 0.9281 (progress: 71.9%)
Step 1867: Updated gated ratio to 0.9281 (progress: 71.9%)
Step 1867: Updated gated ratio to 0.9281 (progress: 71.9%)
Step 1867: Updated gated ratio to 0.9281 (progress: 71.9%)
Step 1867: Updated gated ratio to 0.9281 (progress: 71.9%)
Step 1867: Updated gated ratio to 0.9281 (progress: 71.9%)
Step 1867: Updated gated ratio to 0.9281 (progress: 71.9%)
Step 1867: Updated gated ratio to 0.9281 (progress: 71.9%)
Step 1867: Updated gated ratio to 0.9281 (progress: 71.9%)
Step 1867: Updated gated ratio to 0.9281 (progress: 71.9%)
Step 1867: Updated gated ratio to 0.9281 (progress: 71.9%)
Step 1867: Updated gated ratio to 0.9281 (progress: 71.9%)
Step 1867: Updated gated ratio to 0.9281 (progress: 71.9%)
{'loss': 1.1693, 'grad_norm': 1.5692951679229736, 'learning_rate': 1.4835435901095341e-05, 'epoch': 0.36}
{'loss': 1.1418, 'grad_norm': 1.2300617694854736, 'learning_rate': 1.4829980976753426e-05, 'epoch': 0.36}
{'loss': 1.2016, 'grad_norm': 1.6028566360473633, 'learning_rate': 1.4824524177246597e-05, 'epoch': 0.36}
{'loss': 1.1182, 'grad_norm': 1.69167160987854, 'learning_rate': 1.4819065504693365e-05, 'epoch': 0.36}
{'loss': 1.0987, 'grad_norm': 1.5017669200897217, 'learning_rate': 1.4813604961212984e-05, 'epoch': 0.36}
Step 1872: Updated gated ratio to 0.9279 (progress: 72.1%)
Step 1872: Updated gated ratio to 0.9279 (progress: 72.1%)
Step 1872: Updated gated ratio to 0.9279 (progress: 72.1%)
Step 1872: Updated gated ratio to 0.9279 (progress: 72.1%)
Step 1872: Updated gated ratio to 0.9279 (progress: 72.1%)
Step 1872: Updated gated ratio to 0.9279 (progress: 72.1%)
Step 1872: Updated gated ratio to 0.9279 (progress: 72.1%)
Step 1872: Updated gated ratio to 0.9279 (progress: 72.1%)
Step 1872: Updated gated ratio to 0.9279 (progress: 72.1%)
Step 1872: Updated gated ratio to 0.9279 (progress: 72.1%)
Step 1872: Updated gated ratio to 0.9279 (progress: 72.1%)
Step 1872: Updated gated ratio to 0.9279 (progress: 72.1%)
Step 1872: Updated gated ratio to 0.9279 (progress: 72.1%)
Step 1872: Updated gated ratio to 0.9279 (progress: 72.1%)
Step 1872: Updated gated ratio to 0.9279 (progress: 72.1%)
Step 1872: Updated gated ratio to 0.9279 (progress: 72.1%)
{'loss': 0.982, 'grad_norm': 1.517244577407837, 'learning_rate': 1.4808142548925417e-05, 'epoch': 0.36}
{'loss': 1.1258, 'grad_norm': 1.5160752534866333, 'learning_rate': 1.4802678269951365e-05, 'epoch': 0.36}
{'loss': 1.1094, 'grad_norm': 1.5611076354980469, 'learning_rate': 1.4797212126412243e-05, 'epoch': 0.36}
{'loss': 1.0802, 'grad_norm': 1.6218785047531128, 'learning_rate': 1.4791744120430202e-05, 'epoch': 0.36}
{'loss': 1.086, 'grad_norm': 1.5916602611541748, 'learning_rate': 1.4786274254128112e-05, 'epoch': 0.36}
{'loss': 1.0509, 'grad_norm': 1.541778564453125, 'learning_rate': 1.4780802529629559e-05, 'epoch': 0.36}
Step 1878: Updated gated ratio to 0.9277 (progress: 72.3%)
Step 1878: Updated gated ratio to 0.9277 (progress: 72.3%)
Step 1878: Updated gated ratio to 0.9277 (progress: 72.3%)Step 1878: Updated gated ratio to 0.9277 (progress: 72.3%)

Step 1878: Updated gated ratio to 0.9277 (progress: 72.3%)
Step 1878: Updated gated ratio to 0.9277 (progress: 72.3%)
Step 1878: Updated gated ratio to 0.9277 (progress: 72.3%)
Step 1878: Updated gated ratio to 0.9277 (progress: 72.3%)
Step 1878: Updated gated ratio to 0.9277 (progress: 72.3%)
Step 1878: Updated gated ratio to 0.9277 (progress: 72.3%)
Step 1878: Updated gated ratio to 0.9277 (progress: 72.3%)
Step 1878: Updated gated ratio to 0.9277 (progress: 72.3%)
Step 1878: Updated gated ratio to 0.9277 (progress: 72.3%)
Step 1878: Updated gated ratio to 0.9277 (progress: 72.3%)
Step 1878: Updated gated ratio to 0.9277 (progress: 72.3%)
Step 1878: Updated gated ratio to 0.9277 (progress: 72.3%)
{'loss': 1.2097, 'grad_norm': 1.317409634590149, 'learning_rate': 1.4775328949058856e-05, 'epoch': 0.36}
{'loss': 1.1487, 'grad_norm': 1.6490718126296997, 'learning_rate': 1.4769853514541037e-05, 'epoch': 0.36}
{'loss': 1.0879, 'grad_norm': 1.704834222793579, 'learning_rate': 1.4764376228201848e-05, 'epoch': 0.36}
{'loss': 1.1135, 'grad_norm': 1.5560402870178223, 'learning_rate': 1.475889709216777e-05, 'epoch': 0.36}
{'loss': 1.0517, 'grad_norm': 1.6699318885803223, 'learning_rate': 1.4753416108565985e-05, 'epoch': 0.36}
Step 1883: Updated gated ratio to 0.9275 (progress: 72.5%)
Step 1883: Updated gated ratio to 0.9275 (progress: 72.5%)
Step 1883: Updated gated ratio to 0.9275 (progress: 72.5%)
Step 1883: Updated gated ratio to 0.9275 (progress: 72.5%)
Step 1883: Updated gated ratio to 0.9275 (progress: 72.5%)
Step 1883: Updated gated ratio to 0.9275 (progress: 72.5%)
Step 1883: Updated gated ratio to 0.9275 (progress: 72.5%)
Step 1883: Updated gated ratio to 0.9275 (progress: 72.5%)
Step 1883: Updated gated ratio to 0.9275 (progress: 72.5%)
Step 1883: Updated gated ratio to 0.9275 (progress: 72.5%)
Step 1883: Updated gated ratio to 0.9275 (progress: 72.5%)
Step 1883: Updated gated ratio to 0.9275 (progress: 72.5%)
Step 1883: Updated gated ratio to 0.9275 (progress: 72.5%)
Step 1883: Updated gated ratio to 0.9275 (progress: 72.5%)
Step 1883: Updated gated ratio to 0.9275 (progress: 72.5%)
Step 1883: Updated gated ratio to 0.9275 (progress: 72.5%)
{'loss': 1.1594, 'grad_norm': 1.320344090461731, 'learning_rate': 1.47479332795244e-05, 'epoch': 0.36}
{'loss': 1.1793, 'grad_norm': 1.781453251838684, 'learning_rate': 1.4742448607171644e-05, 'epoch': 0.36}
{'loss': 1.0325, 'grad_norm': 1.6926462650299072, 'learning_rate': 1.473696209363705e-05, 'epoch': 0.36}
{'loss': 1.1678, 'grad_norm': 1.5538005828857422, 'learning_rate': 1.4731473741050673e-05, 'epoch': 0.36}
{'loss': 1.1422, 'grad_norm': 1.4808878898620605, 'learning_rate': 1.4725983551543279e-05, 'epoch': 0.36}
Step 1888: Updated gated ratio to 0.9273 (progress: 72.7%)
Step 1888: Updated gated ratio to 0.9273 (progress: 72.7%)
Step 1888: Updated gated ratio to 0.9273 (progress: 72.7%)
Step 1888: Updated gated ratio to 0.9273 (progress: 72.7%)
Step 1888: Updated gated ratio to 0.9273 (progress: 72.7%)
Step 1888: Updated gated ratio to 0.9273 (progress: 72.7%)
Step 1888: Updated gated ratio to 0.9273 (progress: 72.7%)
Step 1888: Updated gated ratio to 0.9273 (progress: 72.7%)
Step 1888: Updated gated ratio to 0.9273 (progress: 72.7%)
Step 1888: Updated gated ratio to 0.9273 (progress: 72.7%)
Step 1888: Updated gated ratio to 0.9273 (progress: 72.7%)
Step 1888: Updated gated ratio to 0.9273 (progress: 72.7%)
Step 1888: Updated gated ratio to 0.9273 (progress: 72.7%)
Step 1888: Updated gated ratio to 0.9273 (progress: 72.7%)
Step 1888: Updated gated ratio to 0.9273 (progress: 72.7%)
Step 1888: Updated gated ratio to 0.9273 (progress: 72.7%)
{'loss': 1.067, 'grad_norm': 1.6408299207687378, 'learning_rate': 1.472049152724635e-05, 'epoch': 0.36}
{'loss': 1.1024, 'grad_norm': 1.5514272451400757, 'learning_rate': 1.471499767029208e-05, 'epoch': 0.36}
{'loss': 1.0824, 'grad_norm': 1.5756235122680664, 'learning_rate': 1.470950198281337e-05, 'epoch': 0.36}
{'loss': 1.0503, 'grad_norm': 1.5562082529067993, 'learning_rate': 1.470400446694384e-05, 'epoch': 0.36}
{'loss': 1.2482, 'grad_norm': 1.4641838073730469, 'learning_rate': 1.4698505124817811e-05, 'epoch': 0.36}
Step 1893: Updated gated ratio to 0.9271 (progress: 72.9%)
Step 1893: Updated gated ratio to 0.9271 (progress: 72.9%)
Step 1893: Updated gated ratio to 0.9271 (progress: 72.9%)
Step 1893: Updated gated ratio to 0.9271 (progress: 72.9%)
Step 1893: Updated gated ratio to 0.9271 (progress: 72.9%)
Step 1893: Updated gated ratio to 0.9271 (progress: 72.9%)
Step 1893: Updated gated ratio to 0.9271 (progress: 72.9%)
Step 1893: Updated gated ratio to 0.9271 (progress: 72.9%)
Step 1893: Updated gated ratio to 0.9271 (progress: 72.9%)
Step 1893: Updated gated ratio to 0.9271 (progress: 72.9%)
Step 1893: Updated gated ratio to 0.9271 (progress: 72.9%)
Step 1893: Updated gated ratio to 0.9271 (progress: 72.9%)
Step 1893: Updated gated ratio to 0.9271 (progress: 72.9%)
Step 1893: Updated gated ratio to 0.9271 (progress: 72.9%)
Step 1893: Updated gated ratio to 0.9271 (progress: 72.9%)
Step 1893: Updated gated ratio to 0.9271 (progress: 72.9%)
{'loss': 1.072, 'grad_norm': 1.5474592447280884, 'learning_rate': 1.4693003958570318e-05, 'epoch': 0.36}
{'loss': 1.1556, 'grad_norm': 1.588714838027954, 'learning_rate': 1.4687500970337103e-05, 'epoch': 0.36}
{'loss': 1.1469, 'grad_norm': 1.6007949113845825, 'learning_rate': 1.4681996162254618e-05, 'epoch': 0.36}
{'loss': 1.1112, 'grad_norm': 1.6329336166381836, 'learning_rate': 1.4676489536460015e-05, 'epoch': 0.36}
{'loss': 1.1139, 'grad_norm': 1.627602219581604, 'learning_rate': 1.467098109509116e-05, 'epoch': 0.37}
Step 1898: Updated gated ratio to 0.9269 (progress: 73.1%)
Step 1898: Updated gated ratio to 0.9269 (progress: 73.1%)
Step 1898: Updated gated ratio to 0.9269 (progress: 73.1%)
Step 1898: Updated gated ratio to 0.9269 (progress: 73.1%)
Step 1898: Updated gated ratio to 0.9269 (progress: 73.1%)
Step 1898: Updated gated ratio to 0.9269 (progress: 73.1%)
Step 1898: Updated gated ratio to 0.9269 (progress: 73.1%)
Step 1898: Updated gated ratio to 0.9269 (progress: 73.1%)
Step 1898: Updated gated ratio to 0.9269 (progress: 73.1%)
Step 1898: Updated gated ratio to 0.9269 (progress: 73.1%)
Step 1898: Updated gated ratio to 0.9269 (progress: 73.1%)
Step 1898: Updated gated ratio to 0.9269 (progress: 73.1%)
Step 1898: Updated gated ratio to 0.9269 (progress: 73.1%)
Step 1898: Updated gated ratio to 0.9269 (progress: 73.1%)
Step 1898: Updated gated ratio to 0.9269 (progress: 73.1%)
Step 1898: Updated gated ratio to 0.9269 (progress: 73.1%)
{'loss': 1.1307, 'grad_norm': 1.5989950895309448, 'learning_rate': 1.4665470840286614e-05, 'epoch': 0.37}
{'loss': 1.0268, 'grad_norm': 1.4402906894683838, 'learning_rate': 1.4659958774185654e-05, 'epoch': 0.37}
{'loss': 1.1001, 'grad_norm': 1.5807358026504517, 'learning_rate': 1.4654444898928249e-05, 'epoch': 0.37}
{'loss': 1.0937, 'grad_norm': 1.613333821296692, 'learning_rate': 1.4648929216655077e-05, 'epoch': 0.37}
{'loss': 1.1096, 'grad_norm': 1.5503236055374146, 'learning_rate': 1.4643411729507517e-05, 'epoch': 0.37}
{'loss': 1.1091, 'grad_norm': 1.6237921714782715, 'learning_rate': 1.4637892439627644e-05, 'epoch': 0.37}
Step 1904: Updated gated ratio to 0.9267 (progress: 73.3%)
Step 1904: Updated gated ratio to 0.9267 (progress: 73.3%)
Step 1904: Updated gated ratio to 0.9267 (progress: 73.3%)
Step 1904: Updated gated ratio to 0.9267 (progress: 73.3%)Step 1904: Updated gated ratio to 0.9267 (progress: 73.3%)

Step 1904: Updated gated ratio to 0.9267 (progress: 73.3%)
Step 1904: Updated gated ratio to 0.9267 (progress: 73.3%)
Step 1904: Updated gated ratio to 0.9267 (progress: 73.3%)
Step 1904: Updated gated ratio to 0.9267 (progress: 73.3%)
Step 1904: Updated gated ratio to 0.9267 (progress: 73.3%)
Step 1904: Updated gated ratio to 0.9267 (progress: 73.3%)
Step 1904: Updated gated ratio to 0.9267 (progress: 73.3%)
Step 1904: Updated gated ratio to 0.9267 (progress: 73.3%)
Step 1904: Updated gated ratio to 0.9267 (progress: 73.3%)
Step 1904: Updated gated ratio to 0.9267 (progress: 73.3%)
Step 1904: Updated gated ratio to 0.9267 (progress: 73.3%)
{'loss': 1.1318, 'grad_norm': 1.6002159118652344, 'learning_rate': 1.4632371349158241e-05, 'epoch': 0.37}
{'loss': 1.1558, 'grad_norm': 1.6138867139816284, 'learning_rate': 1.4626848460242782e-05, 'epoch': 0.37}
{'loss': 1.2003, 'grad_norm': 1.3804552555084229, 'learning_rate': 1.4621323775025444e-05, 'epoch': 0.37}
{'loss': 1.1004, 'grad_norm': 1.6121999025344849, 'learning_rate': 1.4615797295651099e-05, 'epoch': 0.37}
{'loss': 1.0627, 'grad_norm': 1.6177856922149658, 'learning_rate': 1.4610269024265317e-05, 'epoch': 0.37}
Step 1909: Updated gated ratio to 0.9265 (progress: 73.5%)
Step 1909: Updated gated ratio to 0.9265 (progress: 73.5%)
Step 1909: Updated gated ratio to 0.9265 (progress: 73.5%)
Step 1909: Updated gated ratio to 0.9265 (progress: 73.5%)
Step 1909: Updated gated ratio to 0.9265 (progress: 73.5%)
Step 1909: Updated gated ratio to 0.9265 (progress: 73.5%)
Step 1909: Updated gated ratio to 0.9265 (progress: 73.5%)
Step 1909: Updated gated ratio to 0.9265 (progress: 73.5%)
Step 1909: Updated gated ratio to 0.9265 (progress: 73.5%)
Step 1909: Updated gated ratio to 0.9265 (progress: 73.5%)
Step 1909: Updated gated ratio to 0.9265 (progress: 73.5%)
Step 1909: Updated gated ratio to 0.9265 (progress: 73.5%)
Step 1909: Updated gated ratio to 0.9265 (progress: 73.5%)
Step 1909: Updated gated ratio to 0.9265 (progress: 73.5%)
Step 1909: Updated gated ratio to 0.9265 (progress: 73.5%)
Step 1909: Updated gated ratio to 0.9265 (progress: 73.5%)
{'loss': 1.1449, 'grad_norm': 1.4490562677383423, 'learning_rate': 1.4604738963014365e-05, 'epoch': 0.37}
{'loss': 1.0951, 'grad_norm': 1.5135513544082642, 'learning_rate': 1.4599207114045202e-05, 'epoch': 0.37}
{'loss': 1.0945, 'grad_norm': 1.577905297279358, 'learning_rate': 1.4593673479505482e-05, 'epoch': 0.37}
{'loss': 1.1227, 'grad_norm': 1.778196096420288, 'learning_rate': 1.4588138061543551e-05, 'epoch': 0.37}
{'loss': 1.0356, 'grad_norm': 1.640446662902832, 'learning_rate': 1.458260086230845e-05, 'epoch': 0.37}
Step 1914: Updated gated ratio to 0.9263 (progress: 73.7%)
Step 1914: Updated gated ratio to 0.9263 (progress: 73.7%)
Step 1914: Updated gated ratio to 0.9263 (progress: 73.7%)
Step 1914: Updated gated ratio to 0.9263 (progress: 73.7%)
Step 1914: Updated gated ratio to 0.9263 (progress: 73.7%)
Step 1914: Updated gated ratio to 0.9263 (progress: 73.7%)
Step 1914: Updated gated ratio to 0.9263 (progress: 73.7%)
Step 1914: Updated gated ratio to 0.9263 (progress: 73.7%)
Step 1914: Updated gated ratio to 0.9263 (progress: 73.7%)Step 1914: Updated gated ratio to 0.9263 (progress: 73.7%)

Step 1914: Updated gated ratio to 0.9263 (progress: 73.7%)
Step 1914: Updated gated ratio to 0.9263 (progress: 73.7%)
Step 1914: Updated gated ratio to 0.9263 (progress: 73.7%)
Step 1914: Updated gated ratio to 0.9263 (progress: 73.7%)
Step 1914: Updated gated ratio to 0.9263 (progress: 73.7%)
Step 1914: Updated gated ratio to 0.9263 (progress: 73.7%)
{'loss': 1.1458, 'grad_norm': 1.6388659477233887, 'learning_rate': 1.4577061883949912e-05, 'epoch': 0.37}
{'loss': 1.0494, 'grad_norm': 1.5834425687789917, 'learning_rate': 1.4571521128618358e-05, 'epoch': 0.37}
{'loss': 1.21, 'grad_norm': 1.3815363645553589, 'learning_rate': 1.4565978598464895e-05, 'epoch': 0.37}
{'loss': 1.2012, 'grad_norm': 1.4015741348266602, 'learning_rate': 1.4560434295641338e-05, 'epoch': 0.37}
{'loss': 1.1124, 'grad_norm': 1.5332821607589722, 'learning_rate': 1.455488822230016e-05, 'epoch': 0.37}
Step 1919: Updated gated ratio to 0.9261 (progress: 73.9%)
Step 1919: Updated gated ratio to 0.9261 (progress: 73.9%)Step 1919: Updated gated ratio to 0.9261 (progress: 73.9%)

Step 1919: Updated gated ratio to 0.9261 (progress: 73.9%)
Step 1919: Updated gated ratio to 0.9261 (progress: 73.9%)
Step 1919: Updated gated ratio to 0.9261 (progress: 73.9%)
Step 1919: Updated gated ratio to 0.9261 (progress: 73.9%)Step 1919: Updated gated ratio to 0.9261 (progress: 73.9%)
Step 1919: Updated gated ratio to 0.9261 (progress: 73.9%)
Step 1919: Updated gated ratio to 0.9261 (progress: 73.9%)
Step 1919: Updated gated ratio to 0.9261 (progress: 73.9%)

Step 1919: Updated gated ratio to 0.9261 (progress: 73.9%)
Step 1919: Updated gated ratio to 0.9261 (progress: 73.9%)
Step 1919: Updated gated ratio to 0.9261 (progress: 73.9%)
Step 1919: Updated gated ratio to 0.9261 (progress: 73.9%)
Step 1919: Updated gated ratio to 0.9261 (progress: 73.9%)
{'loss': 1.0437, 'grad_norm': 1.6534879207611084, 'learning_rate': 1.4549340380594545e-05, 'epoch': 0.37}
{'loss': 1.0933, 'grad_norm': 1.540368676185608, 'learning_rate': 1.454379077267836e-05, 'epoch': 0.37}
{'loss': 1.1013, 'grad_norm': 1.534371018409729, 'learning_rate': 1.4538239400706147e-05, 'epoch': 0.37}
{'loss': 0.9345, 'grad_norm': 1.5492135286331177, 'learning_rate': 1.4532686266833143e-05, 'epoch': 0.37}
{'loss': 1.0871, 'grad_norm': 1.5454084873199463, 'learning_rate': 1.4527131373215265e-05, 'epoch': 0.37}
Step 1924: Updated gated ratio to 0.9259 (progress: 74.1%)
Step 1924: Updated gated ratio to 0.9259 (progress: 74.1%)
Step 1924: Updated gated ratio to 0.9259 (progress: 74.1%)
Step 1924: Updated gated ratio to 0.9259 (progress: 74.1%)Step 1924: Updated gated ratio to 0.9259 (progress: 74.1%)
Step 1924: Updated gated ratio to 0.9259 (progress: 74.1%)Step 1924: Updated gated ratio to 0.9259 (progress: 74.1%)

Step 1924: Updated gated ratio to 0.9259 (progress: 74.1%)
Step 1924: Updated gated ratio to 0.9259 (progress: 74.1%)
Step 1924: Updated gated ratio to 0.9259 (progress: 74.1%)

Step 1924: Updated gated ratio to 0.9259 (progress: 74.1%)
Step 1924: Updated gated ratio to 0.9259 (progress: 74.1%)
Step 1924: Updated gated ratio to 0.9259 (progress: 74.1%)
Step 1924: Updated gated ratio to 0.9259 (progress: 74.1%)
Step 1924: Updated gated ratio to 0.9259 (progress: 74.1%)
Step 1924: Updated gated ratio to 0.9259 (progress: 74.1%)
{'loss': 1.069, 'grad_norm': 1.4516420364379883, 'learning_rate': 1.4521574722009115e-05, 'epoch': 0.37}
{'loss': 1.1195, 'grad_norm': 1.6204627752304077, 'learning_rate': 1.4516016315371974e-05, 'epoch': 0.37}
{'loss': 1.1859, 'grad_norm': 1.576696753501892, 'learning_rate': 1.4510456155461807e-05, 'epoch': 0.37}
{'loss': 1.1671, 'grad_norm': 1.5961419343948364, 'learning_rate': 1.4504894244437264e-05, 'epoch': 0.37}
{'loss': 1.0638, 'grad_norm': 1.6273244619369507, 'learning_rate': 1.4499330584457667e-05, 'epoch': 0.37}
{'loss': 1.021, 'grad_norm': 1.6063933372497559, 'learning_rate': 1.4493765177683017e-05, 'epoch': 0.37}
Step 1930: Updated gated ratio to 0.9257 (progress: 74.3%)
Step 1930: Updated gated ratio to 0.9257 (progress: 74.3%)
Step 1930: Updated gated ratio to 0.9257 (progress: 74.3%)
Step 1930: Updated gated ratio to 0.9257 (progress: 74.3%)
Step 1930: Updated gated ratio to 0.9257 (progress: 74.3%)
Step 1930: Updated gated ratio to 0.9257 (progress: 74.3%)
Step 1930: Updated gated ratio to 0.9257 (progress: 74.3%)
Step 1930: Updated gated ratio to 0.9257 (progress: 74.3%)
Step 1930: Updated gated ratio to 0.9257 (progress: 74.3%)
Step 1930: Updated gated ratio to 0.9257 (progress: 74.3%)
Step 1930: Updated gated ratio to 0.9257 (progress: 74.3%)
Step 1930: Updated gated ratio to 0.9257 (progress: 74.3%)
Step 1930: Updated gated ratio to 0.9257 (progress: 74.3%)
Step 1930: Updated gated ratio to 0.9257 (progress: 74.3%)
Step 1930: Updated gated ratio to 0.9257 (progress: 74.3%)
Step 1930: Updated gated ratio to 0.9257 (progress: 74.3%)
{'loss': 1.1446, 'grad_norm': 1.611322045326233, 'learning_rate': 1.4488198026274007e-05, 'epoch': 0.37}
{'loss': 1.0741, 'grad_norm': 1.6065911054611206, 'learning_rate': 1.4482629132391985e-05, 'epoch': 0.37}
{'loss': 1.1261, 'grad_norm': 1.6017241477966309, 'learning_rate': 1.4477058498198993e-05, 'epoch': 0.37}
{'loss': 1.171, 'grad_norm': 1.6669819355010986, 'learning_rate': 1.4471486125857743e-05, 'epoch': 0.37}
{'loss': 1.0924, 'grad_norm': 1.5602017641067505, 'learning_rate': 1.446591201753162e-05, 'epoch': 0.37}
Step 1935: Updated gated ratio to 0.9255 (progress: 74.5%)
Step 1935: Updated gated ratio to 0.9255 (progress: 74.5%)
Step 1935: Updated gated ratio to 0.9255 (progress: 74.5%)
Step 1935: Updated gated ratio to 0.9255 (progress: 74.5%)
Step 1935: Updated gated ratio to 0.9255 (progress: 74.5%)
Step 1935: Updated gated ratio to 0.9255 (progress: 74.5%)
Step 1935: Updated gated ratio to 0.9255 (progress: 74.5%)
Step 1935: Updated gated ratio to 0.9255 (progress: 74.5%)
Step 1935: Updated gated ratio to 0.9255 (progress: 74.5%)
Step 1935: Updated gated ratio to 0.9255 (progress: 74.5%)
Step 1935: Updated gated ratio to 0.9255 (progress: 74.5%)
Step 1935: Updated gated ratio to 0.9255 (progress: 74.5%)
Step 1935: Updated gated ratio to 0.9255 (progress: 74.5%)
Step 1935: Updated gated ratio to 0.9255 (progress: 74.5%)
Step 1935: Updated gated ratio to 0.9255 (progress: 74.5%)
Step 1935: Updated gated ratio to 0.9255 (progress: 74.5%)
{'loss': 1.1035, 'grad_norm': 1.6112295389175415, 'learning_rate': 1.4460336175384688e-05, 'epoch': 0.37}
{'loss': 1.0403, 'grad_norm': 1.5269943475723267, 'learning_rate': 1.4454758601581675e-05, 'epoch': 0.37}
{'loss': 1.078, 'grad_norm': 1.6034990549087524, 'learning_rate': 1.4449179298287999e-05, 'epoch': 0.37}
{'loss': 1.1301, 'grad_norm': 1.6832022666931152, 'learning_rate': 1.4443598267669723e-05, 'epoch': 0.37}
{'loss': 1.2115, 'grad_norm': 1.6505932807922363, 'learning_rate': 1.4438015511893602e-05, 'epoch': 0.37}
Step 1940: Updated gated ratio to 0.9253 (progress: 74.7%)
Step 1940: Updated gated ratio to 0.9253 (progress: 74.7%)
Step 1940: Updated gated ratio to 0.9253 (progress: 74.7%)
Step 1940: Updated gated ratio to 0.9253 (progress: 74.7%)
Step 1940: Updated gated ratio to 0.9253 (progress: 74.7%)
Step 1940: Updated gated ratio to 0.9253 (progress: 74.7%)
Step 1940: Updated gated ratio to 0.9253 (progress: 74.7%)
Step 1940: Updated gated ratio to 0.9253 (progress: 74.7%)
Step 1940: Updated gated ratio to 0.9253 (progress: 74.7%)
Step 1940: Updated gated ratio to 0.9253 (progress: 74.7%)
Step 1940: Updated gated ratio to 0.9253 (progress: 74.7%)
Step 1940: Updated gated ratio to 0.9253 (progress: 74.7%)
Step 1940: Updated gated ratio to 0.9253 (progress: 74.7%)
Step 1940: Updated gated ratio to 0.9253 (progress: 74.7%)
Step 1940: Updated gated ratio to 0.9253 (progress: 74.7%)
Step 1940: Updated gated ratio to 0.9253 (progress: 74.7%)
{'loss': 1.1784, 'grad_norm': 1.5936768054962158, 'learning_rate': 1.4432431033127056e-05, 'epoch': 0.37}
{'loss': 1.2035, 'grad_norm': 1.5932822227478027, 'learning_rate': 1.442684483353817e-05, 'epoch': 0.37}
{'loss': 1.1223, 'grad_norm': 1.6454498767852783, 'learning_rate': 1.4421256915295697e-05, 'epoch': 0.37}
{'loss': 1.0685, 'grad_norm': 1.4473308324813843, 'learning_rate': 1.4415667280569064e-05, 'epoch': 0.37}
{'loss': 1.1586, 'grad_norm': 1.7003692388534546, 'learning_rate': 1.4410075931528356e-05, 'epoch': 0.37}
Step 1945: Updated gated ratio to 0.9251 (progress: 74.9%)
Step 1945: Updated gated ratio to 0.9251 (progress: 74.9%)
Step 1945: Updated gated ratio to 0.9251 (progress: 74.9%)
Step 1945: Updated gated ratio to 0.9251 (progress: 74.9%)
Step 1945: Updated gated ratio to 0.9251 (progress: 74.9%)
Step 1945: Updated gated ratio to 0.9251 (progress: 74.9%)
Step 1945: Updated gated ratio to 0.9251 (progress: 74.9%)
Step 1945: Updated gated ratio to 0.9251 (progress: 74.9%)
Step 1945: Updated gated ratio to 0.9251 (progress: 74.9%)
Step 1945: Updated gated ratio to 0.9251 (progress: 74.9%)
Step 1945: Updated gated ratio to 0.9251 (progress: 74.9%)
Step 1945: Updated gated ratio to 0.9251 (progress: 74.9%)
Step 1945: Updated gated ratio to 0.9251 (progress: 74.9%)
Step 1945: Updated gated ratio to 0.9251 (progress: 74.9%)
Step 1945: Updated gated ratio to 0.9251 (progress: 74.9%)
Step 1945: Updated gated ratio to 0.9251 (progress: 74.9%)
{'loss': 1.1257, 'grad_norm': 1.4801876544952393, 'learning_rate': 1.4404482870344322e-05, 'epoch': 0.37}
{'loss': 1.1572, 'grad_norm': 1.5596860647201538, 'learning_rate': 1.4398888099188396e-05, 'epoch': 0.37}
{'loss': 1.0987, 'grad_norm': 1.524554967880249, 'learning_rate': 1.4393291620232646e-05, 'epoch': 0.37}
{'loss': 1.1492, 'grad_norm': 1.5488371849060059, 'learning_rate': 1.4387693435649826e-05, 'epoch': 0.37}
{'loss': 1.2428, 'grad_norm': 1.4330675601959229, 'learning_rate': 1.4382093547613338e-05, 'epoch': 0.38}
Step 1950: Updated gated ratio to 0.9249 (progress: 75.1%)
Step 1950: Updated gated ratio to 0.9249 (progress: 75.1%)
Step 1950: Updated gated ratio to 0.9249 (progress: 75.1%)
Step 1950: Updated gated ratio to 0.9249 (progress: 75.1%)
Step 1950: Updated gated ratio to 0.9249 (progress: 75.1%)
Step 1950: Updated gated ratio to 0.9249 (progress: 75.1%)
Step 1950: Updated gated ratio to 0.9249 (progress: 75.1%)
Step 1950: Updated gated ratio to 0.9249 (progress: 75.1%)
Step 1950: Updated gated ratio to 0.9249 (progress: 75.1%)
Step 1950: Updated gated ratio to 0.9249 (progress: 75.1%)
Step 1950: Updated gated ratio to 0.9249 (progress: 75.1%)
Step 1950: Updated gated ratio to 0.9249 (progress: 75.1%)
Step 1950: Updated gated ratio to 0.9249 (progress: 75.1%)
Step 1950: Updated gated ratio to 0.9249 (progress: 75.1%)
Step 1950: Updated gated ratio to 0.9249 (progress: 75.1%)
Step 1950: Updated gated ratio to 0.9249 (progress: 75.1%)
{'loss': 1.1039, 'grad_norm': 1.5712625980377197, 'learning_rate': 1.4376491958297263e-05, 'epoch': 0.38}
{'loss': 1.1417, 'grad_norm': 1.6329010725021362, 'learning_rate': 1.4370888669876317e-05, 'epoch': 0.38}
{'loss': 1.1213, 'grad_norm': 1.5749328136444092, 'learning_rate': 1.4365283684525895e-05, 'epoch': 0.38}
Step 1953: Updated gated ratio to 0.9248 (progress: 75.2%)
Step 1953: Updated gated ratio to 0.9248 (progress: 75.2%)
Step 1953: Updated gated ratio to 0.9248 (progress: 75.2%)
Step 1953: Updated gated ratio to 0.9248 (progress: 75.2%)Step 1953: Updated gated ratio to 0.9248 (progress: 75.2%)
Step 1953: Updated gated ratio to 0.9248 (progress: 75.2%)
Step 1953: Updated gated ratio to 0.9248 (progress: 75.2%)
Step 1953: Updated gated ratio to 0.9248 (progress: 75.2%)
Step 1953: Updated gated ratio to 0.9248 (progress: 75.2%)
Step 1953: Updated gated ratio to 0.9248 (progress: 75.2%)
Step 1953: Updated gated ratio to 0.9248 (progress: 75.2%)
Step 1953: Updated gated ratio to 0.9248 (progress: 75.2%)
Step 1953: Updated gated ratio to 0.9248 (progress: 75.2%)

Step 1953: Updated gated ratio to 0.9248 (progress: 75.2%)
Step 1953: Updated gated ratio to 0.9248 (progress: 75.2%)
Step 1953: Updated gated ratio to 0.9248 (progress: 75.2%)
{'loss': 1.0597, 'grad_norm': 1.4803686141967773, 'learning_rate': 1.4359677004422045e-05, 'epoch': 0.38}
{'loss': 1.1541, 'grad_norm': 1.6533290147781372, 'learning_rate': 1.4354068631741476e-05, 'epoch': 0.38}
{'loss': 1.1024, 'grad_norm': 1.442363977432251, 'learning_rate': 1.4348458568661548e-05, 'epoch': 0.38}
{'loss': 1.2516, 'grad_norm': 1.3394584655761719, 'learning_rate': 1.434284681736028e-05, 'epoch': 0.38}
{'loss': 1.0957, 'grad_norm': 1.6404728889465332, 'learning_rate': 1.4337233380016354e-05, 'epoch': 0.38}
Step 1958: Updated gated ratio to 0.9246 (progress: 75.4%)
Step 1958: Updated gated ratio to 0.9246 (progress: 75.4%)
Step 1958: Updated gated ratio to 0.9246 (progress: 75.4%)
Step 1958: Updated gated ratio to 0.9246 (progress: 75.4%)
Step 1958: Updated gated ratio to 0.9246 (progress: 75.4%)Step 1958: Updated gated ratio to 0.9246 (progress: 75.4%)

Step 1958: Updated gated ratio to 0.9246 (progress: 75.4%)
Step 1958: Updated gated ratio to 0.9246 (progress: 75.4%)
Step 1958: Updated gated ratio to 0.9246 (progress: 75.4%)
Step 1958: Updated gated ratio to 0.9246 (progress: 75.4%)
Step 1958: Updated gated ratio to 0.9246 (progress: 75.4%)
Step 1958: Updated gated ratio to 0.9246 (progress: 75.4%)
Step 1958: Updated gated ratio to 0.9246 (progress: 75.4%)
Step 1958: Updated gated ratio to 0.9246 (progress: 75.4%)
Step 1958: Updated gated ratio to 0.9246 (progress: 75.4%)
Step 1958: Updated gated ratio to 0.9246 (progress: 75.4%)
{'loss': 1.116, 'grad_norm': 1.5588206052780151, 'learning_rate': 1.433161825880909e-05, 'epoch': 0.38}
{'loss': 1.1037, 'grad_norm': 1.5727497339248657, 'learning_rate': 1.432600145591848e-05, 'epoch': 0.38}
{'loss': 1.1691, 'grad_norm': 1.7807668447494507, 'learning_rate': 1.4320382973525151e-05, 'epoch': 0.38}
{'loss': 1.1389, 'grad_norm': 1.501102089881897, 'learning_rate': 1.43147628138104e-05, 'epoch': 0.38}
{'loss': 1.1437, 'grad_norm': 1.6478959321975708, 'learning_rate': 1.4309140978956161e-05, 'epoch': 0.38}
Step 1963: Updated gated ratio to 0.9244 (progress: 75.6%)
Step 1963: Updated gated ratio to 0.9244 (progress: 75.6%)
Step 1963: Updated gated ratio to 0.9244 (progress: 75.6%)
Step 1963: Updated gated ratio to 0.9244 (progress: 75.6%)
Step 1963: Updated gated ratio to 0.9244 (progress: 75.6%)Step 1963: Updated gated ratio to 0.9244 (progress: 75.6%)

Step 1963: Updated gated ratio to 0.9244 (progress: 75.6%)
Step 1963: Updated gated ratio to 0.9244 (progress: 75.6%)
Step 1963: Updated gated ratio to 0.9244 (progress: 75.6%)
Step 1963: Updated gated ratio to 0.9244 (progress: 75.6%)
Step 1963: Updated gated ratio to 0.9244 (progress: 75.6%)
Step 1963: Updated gated ratio to 0.9244 (progress: 75.6%)
Step 1963: Updated gated ratio to 0.9244 (progress: 75.6%)
Step 1963: Updated gated ratio to 0.9244 (progress: 75.6%)
Step 1963: Updated gated ratio to 0.9244 (progress: 75.6%)
Step 1963: Updated gated ratio to 0.9244 (progress: 75.6%)
{'loss': 1.0816, 'grad_norm': 1.4806292057037354, 'learning_rate': 1.430351747114503e-05, 'epoch': 0.38}
{'loss': 1.0834, 'grad_norm': 1.5702333450317383, 'learning_rate': 1.429789229256024e-05, 'epoch': 0.38}
{'loss': 1.1397, 'grad_norm': 1.6367969512939453, 'learning_rate': 1.429226544538568e-05, 'epoch': 0.38}
{'loss': 1.1308, 'grad_norm': 1.5768537521362305, 'learning_rate': 1.4286636931805887e-05, 'epoch': 0.38}
{'loss': 1.0807, 'grad_norm': 1.4870635271072388, 'learning_rate': 1.4281006754006045e-05, 'epoch': 0.38}
Step 1968: Updated gated ratio to 0.9242 (progress: 75.8%)
Step 1968: Updated gated ratio to 0.9242 (progress: 75.8%)
Step 1968: Updated gated ratio to 0.9242 (progress: 75.8%)
Step 1968: Updated gated ratio to 0.9242 (progress: 75.8%)
Step 1968: Updated gated ratio to 0.9242 (progress: 75.8%)
Step 1968: Updated gated ratio to 0.9242 (progress: 75.8%)
Step 1968: Updated gated ratio to 0.9242 (progress: 75.8%)
Step 1968: Updated gated ratio to 0.9242 (progress: 75.8%)
Step 1968: Updated gated ratio to 0.9242 (progress: 75.8%)Step 1968: Updated gated ratio to 0.9242 (progress: 75.8%)

Step 1968: Updated gated ratio to 0.9242 (progress: 75.8%)
Step 1968: Updated gated ratio to 0.9242 (progress: 75.8%)Step 1968: Updated gated ratio to 0.9242 (progress: 75.8%)

Step 1968: Updated gated ratio to 0.9242 (progress: 75.8%)
Step 1968: Updated gated ratio to 0.9242 (progress: 75.8%)
Step 1968: Updated gated ratio to 0.9242 (progress: 75.8%)
{'loss': 1.0824, 'grad_norm': 1.4710721969604492, 'learning_rate': 1.427537491417198e-05, 'epoch': 0.38}
{'loss': 1.0733, 'grad_norm': 1.5477937459945679, 'learning_rate': 1.426974141449017e-05, 'epoch': 0.38}
{'loss': 1.1481, 'grad_norm': 1.5584474802017212, 'learning_rate': 1.4264106257147732e-05, 'epoch': 0.38}
{'loss': 1.0488, 'grad_norm': 1.4396504163742065, 'learning_rate': 1.4258469444332423e-05, 'epoch': 0.38}
{'loss': 1.0465, 'grad_norm': 1.5327860116958618, 'learning_rate': 1.4252830978232658e-05, 'epoch': 0.38}
{'loss': 1.1148, 'grad_norm': 1.5715222358703613, 'learning_rate': 1.4247190861037474e-05, 'epoch': 0.38}
Step 1974: Updated gated ratio to 0.9240 (progress: 76.0%)
Step 1974: Updated gated ratio to 0.9240 (progress: 76.0%)
Step 1974: Updated gated ratio to 0.9240 (progress: 76.0%)
Step 1974: Updated gated ratio to 0.9240 (progress: 76.0%)
Step 1974: Updated gated ratio to 0.9240 (progress: 76.0%)Step 1974: Updated gated ratio to 0.9240 (progress: 76.0%)

Step 1974: Updated gated ratio to 0.9240 (progress: 76.0%)Step 1974: Updated gated ratio to 0.9240 (progress: 76.0%)

Step 1974: Updated gated ratio to 0.9240 (progress: 76.0%)
Step 1974: Updated gated ratio to 0.9240 (progress: 76.0%)
Step 1974: Updated gated ratio to 0.9240 (progress: 76.0%)
Step 1974: Updated gated ratio to 0.9240 (progress: 76.0%)
Step 1974: Updated gated ratio to 0.9240 (progress: 76.0%)
Step 1974: Updated gated ratio to 0.9240 (progress: 76.0%)
Step 1974: Updated gated ratio to 0.9240 (progress: 76.0%)
Step 1974: Updated gated ratio to 0.9240 (progress: 76.0%)
{'loss': 1.075, 'grad_norm': 1.5768128633499146, 'learning_rate': 1.4241549094936567e-05, 'epoch': 0.38}
{'loss': 1.1297, 'grad_norm': 1.5355424880981445, 'learning_rate': 1.4235905682120255e-05, 'epoch': 0.38}
{'loss': 1.1511, 'grad_norm': 1.7280449867248535, 'learning_rate': 1.4230260624779512e-05, 'epoch': 0.38}
{'loss': 1.1863, 'grad_norm': 1.691083550453186, 'learning_rate': 1.4224613925105947e-05, 'epoch': 0.38}
{'loss': 1.1301, 'grad_norm': 1.6443135738372803, 'learning_rate': 1.4218965585291792e-05, 'epoch': 0.38}
Step 1979: Updated gated ratio to 0.9238 (progress: 76.2%)
Step 1979: Updated gated ratio to 0.9238 (progress: 76.2%)
Step 1979: Updated gated ratio to 0.9238 (progress: 76.2%)
Step 1979: Updated gated ratio to 0.9238 (progress: 76.2%)
Step 1979: Updated gated ratio to 0.9238 (progress: 76.2%)
Step 1979: Updated gated ratio to 0.9238 (progress: 76.2%)
Step 1979: Updated gated ratio to 0.9238 (progress: 76.2%)
Step 1979: Updated gated ratio to 0.9238 (progress: 76.2%)
Step 1979: Updated gated ratio to 0.9238 (progress: 76.2%)
Step 1979: Updated gated ratio to 0.9238 (progress: 76.2%)
Step 1979: Updated gated ratio to 0.9238 (progress: 76.2%)
Step 1979: Updated gated ratio to 0.9238 (progress: 76.2%)
Step 1979: Updated gated ratio to 0.9238 (progress: 76.2%)
Step 1979: Updated gated ratio to 0.9238 (progress: 76.2%)
Step 1979: Updated gated ratio to 0.9238 (progress: 76.2%)
Step 1979: Updated gated ratio to 0.9238 (progress: 76.2%)
{'loss': 1.2875, 'grad_norm': 1.4281704425811768, 'learning_rate': 1.4213315607529939e-05, 'epoch': 0.38}
{'loss': 1.1281, 'grad_norm': 1.7528011798858643, 'learning_rate': 1.4207663994013896e-05, 'epoch': 0.38}
{'loss': 1.1262, 'grad_norm': 1.6664092540740967, 'learning_rate': 1.4202010746937815e-05, 'epoch': 0.38}
{'loss': 1.133, 'grad_norm': 1.4792951345443726, 'learning_rate': 1.4196355868496485e-05, 'epoch': 0.38}
{'loss': 1.245, 'grad_norm': 1.4588730335235596, 'learning_rate': 1.4190699360885323e-05, 'epoch': 0.38}
Step 1984: Updated gated ratio to 0.9236 (progress: 76.4%)
Step 1984: Updated gated ratio to 0.9236 (progress: 76.4%)
Step 1984: Updated gated ratio to 0.9236 (progress: 76.4%)
Step 1984: Updated gated ratio to 0.9236 (progress: 76.4%)
Step 1984: Updated gated ratio to 0.9236 (progress: 76.4%)
Step 1984: Updated gated ratio to 0.9236 (progress: 76.4%)Step 1984: Updated gated ratio to 0.9236 (progress: 76.4%)

Step 1984: Updated gated ratio to 0.9236 (progress: 76.4%)
Step 1984: Updated gated ratio to 0.9236 (progress: 76.4%)
Step 1984: Updated gated ratio to 0.9236 (progress: 76.4%)
Step 1984: Updated gated ratio to 0.9236 (progress: 76.4%)
Step 1984: Updated gated ratio to 0.9236 (progress: 76.4%)
Step 1984: Updated gated ratio to 0.9236 (progress: 76.4%)
Step 1984: Updated gated ratio to 0.9236 (progress: 76.4%)
Step 1984: Updated gated ratio to 0.9236 (progress: 76.4%)
Step 1984: Updated gated ratio to 0.9236 (progress: 76.4%)
Error with image file is truncated (41 bytes not processed)
{'loss': 1.16, 'grad_norm': 1.401861548423767, 'learning_rate': 1.4185041226300376e-05, 'epoch': 0.38}
{'loss': 1.0088, 'grad_norm': 1.595395565032959, 'learning_rate': 1.4179381466938332e-05, 'epoch': 0.38}
{'loss': 1.1781, 'grad_norm': 1.3620375394821167, 'learning_rate': 1.4173720084996501e-05, 'epoch': 0.38}
{'loss': 1.1054, 'grad_norm': 1.591017723083496, 'learning_rate': 1.4168057082672828e-05, 'epoch': 0.38}
{'loss': 1.0825, 'grad_norm': 1.564121127128601, 'learning_rate': 1.4162392462165884e-05, 'epoch': 0.38}
Step 1989: Updated gated ratio to 0.9234 (progress: 76.6%)
Step 1989: Updated gated ratio to 0.9234 (progress: 76.6%)
Step 1989: Updated gated ratio to 0.9234 (progress: 76.6%)
Step 1989: Updated gated ratio to 0.9234 (progress: 76.6%)
Step 1989: Updated gated ratio to 0.9234 (progress: 76.6%)
Step 1989: Updated gated ratio to 0.9234 (progress: 76.6%)
Step 1989: Updated gated ratio to 0.9234 (progress: 76.6%)
Step 1989: Updated gated ratio to 0.9234 (progress: 76.6%)
Step 1989: Updated gated ratio to 0.9234 (progress: 76.6%)
Step 1989: Updated gated ratio to 0.9234 (progress: 76.6%)
Step 1989: Updated gated ratio to 0.9234 (progress: 76.6%)
Step 1989: Updated gated ratio to 0.9234 (progress: 76.6%)
Step 1989: Updated gated ratio to 0.9234 (progress: 76.6%)
Step 1989: Updated gated ratio to 0.9234 (progress: 76.6%)
Step 1989: Updated gated ratio to 0.9234 (progress: 76.6%)
Step 1989: Updated gated ratio to 0.9234 (progress: 76.6%)
{'loss': 1.1006, 'grad_norm': 1.5357717275619507, 'learning_rate': 1.4156726225674874e-05, 'epoch': 0.38}
{'loss': 1.21, 'grad_norm': 1.6176735162734985, 'learning_rate': 1.415105837539962e-05, 'epoch': 0.38}
{'loss': 1.1018, 'grad_norm': 1.605431079864502, 'learning_rate': 1.414538891354058e-05, 'epoch': 0.38}
{'loss': 1.1416, 'grad_norm': 1.5042107105255127, 'learning_rate': 1.4139717842298835e-05, 'epoch': 0.38}
{'loss': 1.1402, 'grad_norm': 1.5139966011047363, 'learning_rate': 1.4134045163876086e-05, 'epoch': 0.38}
Step 1994: Updated gated ratio to 0.9232 (progress: 76.8%)
Step 1994: Updated gated ratio to 0.9232 (progress: 76.8%)
Step 1994: Updated gated ratio to 0.9232 (progress: 76.8%)
Step 1994: Updated gated ratio to 0.9232 (progress: 76.8%)
Step 1994: Updated gated ratio to 0.9232 (progress: 76.8%)Step 1994: Updated gated ratio to 0.9232 (progress: 76.8%)

Step 1994: Updated gated ratio to 0.9232 (progress: 76.8%)
Step 1994: Updated gated ratio to 0.9232 (progress: 76.8%)
Step 1994: Updated gated ratio to 0.9232 (progress: 76.8%)
Step 1994: Updated gated ratio to 0.9232 (progress: 76.8%)
Step 1994: Updated gated ratio to 0.9232 (progress: 76.8%)
Step 1994: Updated gated ratio to 0.9232 (progress: 76.8%)
Step 1994: Updated gated ratio to 0.9232 (progress: 76.8%)
Step 1994: Updated gated ratio to 0.9232 (progress: 76.8%)
Step 1994: Updated gated ratio to 0.9232 (progress: 76.8%)
Step 1994: Updated gated ratio to 0.9232 (progress: 76.8%)
{'loss': 1.1418, 'grad_norm': 1.7435829639434814, 'learning_rate': 1.4128370880474667e-05, 'epoch': 0.38}
{'loss': 1.0128, 'grad_norm': 1.5062741041183472, 'learning_rate': 1.412269499429753e-05, 'epoch': 0.38}
{'loss': 1.1123, 'grad_norm': 1.6652923822402954, 'learning_rate': 1.4117017507548244e-05, 'epoch': 0.38}
{'loss': 1.2355, 'grad_norm': 1.1790738105773926, 'learning_rate': 1.4111338422431013e-05, 'epoch': 0.38}
{'loss': 1.0694, 'grad_norm': 1.6594125032424927, 'learning_rate': 1.4105657741150648e-05, 'epoch': 0.38}
{'loss': 1.0722, 'grad_norm': 1.527052402496338, 'learning_rate': 1.4099975465912584e-05, 'epoch': 0.38}
Step 2000: Updated gated ratio to 0.9230 (progress: 77.0%)
Step 2000: Updated gated ratio to 0.9230 (progress: 77.0%)
Step 2000: Updated gated ratio to 0.9230 (progress: 77.0%)
Step 2000: Updated gated ratio to 0.9230 (progress: 77.0%)
Step 2000: Updated gated ratio to 0.9230 (progress: 77.0%)
Step 2000: Updated gated ratio to 0.9230 (progress: 77.0%)
Step 2000: Updated gated ratio to 0.9230 (progress: 77.0%)
Step 2000: Updated gated ratio to 0.9230 (progress: 77.0%)
Step 2000: Updated gated ratio to 0.9230 (progress: 77.0%)
Step 2000: Updated gated ratio to 0.9230 (progress: 77.0%)
Step 2000: Updated gated ratio to 0.9230 (progress: 77.0%)
Step 2000: Updated gated ratio to 0.9230 (progress: 77.0%)
Step 2000: Updated gated ratio to 0.9230 (progress: 77.0%)
Step 2000: Updated gated ratio to 0.9230 (progress: 77.0%)
Step 2000: Updated gated ratio to 0.9230 (progress: 77.0%)
Step 2000: Updated gated ratio to 0.9230 (progress: 77.0%)
{'loss': 1.1174, 'grad_norm': 1.5303882360458374, 'learning_rate': 1.4094291598922877e-05, 'epoch': 0.38}
{'loss': 0.9922, 'grad_norm': 1.5855903625488281, 'learning_rate': 1.40886061423882e-05, 'epoch': 0.39}
{'loss': 1.1265, 'grad_norm': 1.511765480041504, 'learning_rate': 1.4082919098515846e-05, 'epoch': 0.39}
{'loss': 1.0947, 'grad_norm': 1.4731181859970093, 'learning_rate': 1.407723046951372e-05, 'epoch': 0.39}
{'loss': 1.2155, 'grad_norm': 1.461488962173462, 'learning_rate': 1.4071540257590341e-05, 'epoch': 0.39}
Step 2005: Updated gated ratio to 0.9228 (progress: 77.2%)
Step 2005: Updated gated ratio to 0.9228 (progress: 77.2%)
Step 2005: Updated gated ratio to 0.9228 (progress: 77.2%)
Step 2005: Updated gated ratio to 0.9228 (progress: 77.2%)
Step 2005: Updated gated ratio to 0.9228 (progress: 77.2%)
Step 2005: Updated gated ratio to 0.9228 (progress: 77.2%)
Step 2005: Updated gated ratio to 0.9228 (progress: 77.2%)
Step 2005: Updated gated ratio to 0.9228 (progress: 77.2%)Step 2005: Updated gated ratio to 0.9228 (progress: 77.2%)
Step 2005: Updated gated ratio to 0.9228 (progress: 77.2%)
Step 2005: Updated gated ratio to 0.9228 (progress: 77.2%)
Step 2005: Updated gated ratio to 0.9228 (progress: 77.2%)
Step 2005: Updated gated ratio to 0.9228 (progress: 77.2%)
Step 2005: Updated gated ratio to 0.9228 (progress: 77.2%)
Step 2005: Updated gated ratio to 0.9228 (progress: 77.2%)

Step 2005: Updated gated ratio to 0.9228 (progress: 77.2%)
{'loss': 1.189, 'grad_norm': 1.3579833507537842, 'learning_rate': 1.4065848464954848e-05, 'epoch': 0.39}
{'loss': 1.1226, 'grad_norm': 1.6134482622146606, 'learning_rate': 1.4060155093816988e-05, 'epoch': 0.39}
{'loss': 1.0766, 'grad_norm': 1.6126494407653809, 'learning_rate': 1.4054460146387124e-05, 'epoch': 0.39}
{'loss': 1.1231, 'grad_norm': 1.5455747842788696, 'learning_rate': 1.4048763624876233e-05, 'epoch': 0.39}
{'loss': 1.0506, 'grad_norm': 1.5644551515579224, 'learning_rate': 1.4043065531495904e-05, 'epoch': 0.39}
Step 2010: Updated gated ratio to 0.9226 (progress: 77.4%)
Step 2010: Updated gated ratio to 0.9226 (progress: 77.4%)
Step 2010: Updated gated ratio to 0.9226 (progress: 77.4%)
Step 2010: Updated gated ratio to 0.9226 (progress: 77.4%)
Step 2010: Updated gated ratio to 0.9226 (progress: 77.4%)Step 2010: Updated gated ratio to 0.9226 (progress: 77.4%)

Step 2010: Updated gated ratio to 0.9226 (progress: 77.4%)
Step 2010: Updated gated ratio to 0.9226 (progress: 77.4%)
Step 2010: Updated gated ratio to 0.9226 (progress: 77.4%)Step 2010: Updated gated ratio to 0.9226 (progress: 77.4%)

Step 2010: Updated gated ratio to 0.9226 (progress: 77.4%)
Step 2010: Updated gated ratio to 0.9226 (progress: 77.4%)
Step 2010: Updated gated ratio to 0.9226 (progress: 77.4%)
Step 2010: Updated gated ratio to 0.9226 (progress: 77.4%)
Step 2010: Updated gated ratio to 0.9226 (progress: 77.4%)
Step 2010: Updated gated ratio to 0.9226 (progress: 77.4%)
{'loss': 1.0912, 'grad_norm': 1.6333509683609009, 'learning_rate': 1.4037365868458325e-05, 'epoch': 0.39}
{'loss': 1.1068, 'grad_norm': 1.5282758474349976, 'learning_rate': 1.4031664637976305e-05, 'epoch': 0.39}
{'loss': 1.077, 'grad_norm': 1.5685056447982788, 'learning_rate': 1.402596184226326e-05, 'epoch': 0.39}
{'loss': 1.1077, 'grad_norm': 1.5803109407424927, 'learning_rate': 1.4020257483533208e-05, 'epoch': 0.39}
{'loss': 1.241, 'grad_norm': 1.4244424104690552, 'learning_rate': 1.401455156400078e-05, 'epoch': 0.39}
Step 2015: Updated gated ratio to 0.9224 (progress: 77.6%)
Step 2015: Updated gated ratio to 0.9224 (progress: 77.6%)
Step 2015: Updated gated ratio to 0.9224 (progress: 77.6%)
Step 2015: Updated gated ratio to 0.9224 (progress: 77.6%)
Step 2015: Updated gated ratio to 0.9224 (progress: 77.6%)
Step 2015: Updated gated ratio to 0.9224 (progress: 77.6%)
Step 2015: Updated gated ratio to 0.9224 (progress: 77.6%)
Step 2015: Updated gated ratio to 0.9224 (progress: 77.6%)
Step 2015: Updated gated ratio to 0.9224 (progress: 77.6%)
Step 2015: Updated gated ratio to 0.9224 (progress: 77.6%)
Step 2015: Updated gated ratio to 0.9224 (progress: 77.6%)
Step 2015: Updated gated ratio to 0.9224 (progress: 77.6%)
Step 2015: Updated gated ratio to 0.9224 (progress: 77.6%)
Step 2015: Updated gated ratio to 0.9224 (progress: 77.6%)
Step 2015: Updated gated ratio to 0.9224 (progress: 77.6%)
Step 2015: Updated gated ratio to 0.9224 (progress: 77.6%)
{'loss': 1.1208, 'grad_norm': 1.6696215867996216, 'learning_rate': 1.400884408588121e-05, 'epoch': 0.39}
{'loss': 1.1248, 'grad_norm': 1.4102048873901367, 'learning_rate': 1.400313505139034e-05, 'epoch': 0.39}
{'loss': 1.0753, 'grad_norm': 1.5238698720932007, 'learning_rate': 1.3997424462744607e-05, 'epoch': 0.39}
{'loss': 1.1077, 'grad_norm': 1.5379194021224976, 'learning_rate': 1.3991712322161065e-05, 'epoch': 0.39}
{'loss': 1.0546, 'grad_norm': 1.6289244890213013, 'learning_rate': 1.3985998631857359e-05, 'epoch': 0.39}
Step 2020: Updated gated ratio to 0.9222 (progress: 77.8%)
Step 2020: Updated gated ratio to 0.9222 (progress: 77.8%)
Step 2020: Updated gated ratio to 0.9222 (progress: 77.8%)
Step 2020: Updated gated ratio to 0.9222 (progress: 77.8%)
Step 2020: Updated gated ratio to 0.9222 (progress: 77.8%)
Step 2020: Updated gated ratio to 0.9222 (progress: 77.8%)
Step 2020: Updated gated ratio to 0.9222 (progress: 77.8%)
Step 2020: Updated gated ratio to 0.9222 (progress: 77.8%)
Step 2020: Updated gated ratio to 0.9222 (progress: 77.8%)
Step 2020: Updated gated ratio to 0.9222 (progress: 77.8%)
Step 2020: Updated gated ratio to 0.9222 (progress: 77.8%)Step 2020: Updated gated ratio to 0.9222 (progress: 77.8%)

Step 2020: Updated gated ratio to 0.9222 (progress: 77.8%)Step 2020: Updated gated ratio to 0.9222 (progress: 77.8%)
Step 2020: Updated gated ratio to 0.9222 (progress: 77.8%)

Step 2020: Updated gated ratio to 0.9222 (progress: 77.8%)
{'loss': 1.0324, 'grad_norm': 1.4883503913879395, 'learning_rate': 1.398028339405174e-05, 'epoch': 0.39}
{'loss': 1.1376, 'grad_norm': 1.5585415363311768, 'learning_rate': 1.3974566610963068e-05, 'epoch': 0.39}
{'loss': 1.1297, 'grad_norm': 1.4914252758026123, 'learning_rate': 1.3968848284810785e-05, 'epoch': 0.39}
{'loss': 1.2008, 'grad_norm': 1.6502175331115723, 'learning_rate': 1.3963128417814951e-05, 'epoch': 0.39}
{'loss': 1.0691, 'grad_norm': 1.5535409450531006, 'learning_rate': 1.3957407012196204e-05, 'epoch': 0.39}
{'loss': 1.2138, 'grad_norm': 1.3830686807632446, 'learning_rate': 1.3951684070175802e-05, 'epoch': 0.39}
Step 2026: Updated gated ratio to 0.9220 (progress: 78.0%)
Step 2026: Updated gated ratio to 0.9220 (progress: 78.0%)
Step 2026: Updated gated ratio to 0.9220 (progress: 78.0%)
Step 2026: Updated gated ratio to 0.9220 (progress: 78.0%)
Step 2026: Updated gated ratio to 0.9220 (progress: 78.0%)
Step 2026: Updated gated ratio to 0.9220 (progress: 78.0%)
Step 2026: Updated gated ratio to 0.9220 (progress: 78.0%)
Step 2026: Updated gated ratio to 0.9220 (progress: 78.0%)
Step 2026: Updated gated ratio to 0.9220 (progress: 78.0%)
Step 2026: Updated gated ratio to 0.9220 (progress: 78.0%)
Step 2026: Updated gated ratio to 0.9220 (progress: 78.0%)
Step 2026: Updated gated ratio to 0.9220 (progress: 78.0%)
Step 2026: Updated gated ratio to 0.9220 (progress: 78.0%)
Step 2026: Updated gated ratio to 0.9220 (progress: 78.0%)
Step 2026: Updated gated ratio to 0.9220 (progress: 78.0%)
Step 2026: Updated gated ratio to 0.9220 (progress: 78.0%)
{'loss': 1.0455, 'grad_norm': 1.522316575050354, 'learning_rate': 1.3945959593975582e-05, 'epoch': 0.39}
{'loss': 1.0684, 'grad_norm': 1.4994769096374512, 'learning_rate': 1.3940233585817984e-05, 'epoch': 0.39}
{'loss': 1.0769, 'grad_norm': 1.6338640451431274, 'learning_rate': 1.3934506047926042e-05, 'epoch': 0.39}
{'loss': 1.0856, 'grad_norm': 1.6929562091827393, 'learning_rate': 1.3928776982523384e-05, 'epoch': 0.39}
{'loss': 1.1819, 'grad_norm': 1.27013099193573, 'learning_rate': 1.3923046391834229e-05, 'epoch': 0.39}
Step 2031: Updated gated ratio to 0.9218 (progress: 78.2%)
Step 2031: Updated gated ratio to 0.9218 (progress: 78.2%)
Step 2031: Updated gated ratio to 0.9218 (progress: 78.2%)
Step 2031: Updated gated ratio to 0.9218 (progress: 78.2%)
Step 2031: Updated gated ratio to 0.9218 (progress: 78.2%)
Step 2031: Updated gated ratio to 0.9218 (progress: 78.2%)
Step 2031: Updated gated ratio to 0.9218 (progress: 78.2%)
Step 2031: Updated gated ratio to 0.9218 (progress: 78.2%)
Step 2031: Updated gated ratio to 0.9218 (progress: 78.2%)
Step 2031: Updated gated ratio to 0.9218 (progress: 78.2%)
Step 2031: Updated gated ratio to 0.9218 (progress: 78.2%)
Step 2031: Updated gated ratio to 0.9218 (progress: 78.2%)
Step 2031: Updated gated ratio to 0.9218 (progress: 78.2%)
Step 2031: Updated gated ratio to 0.9218 (progress: 78.2%)
Step 2031: Updated gated ratio to 0.9218 (progress: 78.2%)
Step 2031: Updated gated ratio to 0.9218 (progress: 78.2%)
{'loss': 1.0893, 'grad_norm': 1.5995219945907593, 'learning_rate': 1.3917314278083391e-05, 'epoch': 0.39}
{'loss': 1.0911, 'grad_norm': 1.2541460990905762, 'learning_rate': 1.3911580643496272e-05, 'epoch': 0.39}
{'loss': 1.1188, 'grad_norm': 1.6700325012207031, 'learning_rate': 1.3905845490298867e-05, 'epoch': 0.39}
{'loss': 1.0774, 'grad_norm': 1.5032048225402832, 'learning_rate': 1.390010882071776e-05, 'epoch': 0.39}
{'loss': 1.0649, 'grad_norm': 1.5505783557891846, 'learning_rate': 1.3894370636980128e-05, 'epoch': 0.39}
Step 2036: Updated gated ratio to 0.9216 (progress: 78.4%)
Step 2036: Updated gated ratio to 0.9216 (progress: 78.4%)Step 2036: Updated gated ratio to 0.9216 (progress: 78.4%)
Step 2036: Updated gated ratio to 0.9216 (progress: 78.4%)
Step 2036: Updated gated ratio to 0.9216 (progress: 78.4%)
Step 2036: Updated gated ratio to 0.9216 (progress: 78.4%)

Step 2036: Updated gated ratio to 0.9216 (progress: 78.4%)
Step 2036: Updated gated ratio to 0.9216 (progress: 78.4%)
Step 2036: Updated gated ratio to 0.9216 (progress: 78.4%)
Step 2036: Updated gated ratio to 0.9216 (progress: 78.4%)
Step 2036: Updated gated ratio to 0.9216 (progress: 78.4%)
Step 2036: Updated gated ratio to 0.9216 (progress: 78.4%)
Step 2036: Updated gated ratio to 0.9216 (progress: 78.4%)
Step 2036: Updated gated ratio to 0.9216 (progress: 78.4%)
Step 2036: Updated gated ratio to 0.9216 (progress: 78.4%)
Step 2036: Updated gated ratio to 0.9216 (progress: 78.4%)
{'loss': 1.1441, 'grad_norm': 1.533730387687683, 'learning_rate': 1.3888630941313728e-05, 'epoch': 0.39}
{'loss': 1.1555, 'grad_norm': 1.6878184080123901, 'learning_rate': 1.3882889735946901e-05, 'epoch': 0.39}
{'loss': 1.1601, 'grad_norm': 1.5952647924423218, 'learning_rate': 1.3877147023108592e-05, 'epoch': 0.39}
{'loss': 1.1237, 'grad_norm': 1.6763771772384644, 'learning_rate': 1.3871402805028314e-05, 'epoch': 0.39}
{'loss': 1.1084, 'grad_norm': 1.2952766418457031, 'learning_rate': 1.3865657083936167e-05, 'epoch': 0.39}
Step 2041: Updated gated ratio to 0.9214 (progress: 78.6%)
Step 2041: Updated gated ratio to 0.9214 (progress: 78.6%)
Step 2041: Updated gated ratio to 0.9214 (progress: 78.6%)
Step 2041: Updated gated ratio to 0.9214 (progress: 78.6%)
Step 2041: Updated gated ratio to 0.9214 (progress: 78.6%)
Step 2041: Updated gated ratio to 0.9214 (progress: 78.6%)
Step 2041: Updated gated ratio to 0.9214 (progress: 78.6%)
Step 2041: Updated gated ratio to 0.9214 (progress: 78.6%)
Step 2041: Updated gated ratio to 0.9214 (progress: 78.6%)
Step 2041: Updated gated ratio to 0.9214 (progress: 78.6%)
Step 2041: Updated gated ratio to 0.9214 (progress: 78.6%)
Step 2041: Updated gated ratio to 0.9214 (progress: 78.6%)
Step 2041: Updated gated ratio to 0.9214 (progress: 78.6%)
Step 2041: Updated gated ratio to 0.9214 (progress: 78.6%)
Step 2041: Updated gated ratio to 0.9214 (progress: 78.6%)
Step 2041: Updated gated ratio to 0.9214 (progress: 78.6%)
{'loss': 1.1583, 'grad_norm': 1.7127304077148438, 'learning_rate': 1.3859909862062844e-05, 'epoch': 0.39}
{'loss': 1.1679, 'grad_norm': 1.333717703819275, 'learning_rate': 1.385416114163961e-05, 'epoch': 0.39}
{'loss': 1.2134, 'grad_norm': 1.4803643226623535, 'learning_rate': 1.3848410924898321e-05, 'epoch': 0.39}
{'loss': 1.0975, 'grad_norm': 1.5801079273223877, 'learning_rate': 1.3842659214071406e-05, 'epoch': 0.39}
{'loss': 1.2195, 'grad_norm': 1.6368441581726074, 'learning_rate': 1.3836906011391878e-05, 'epoch': 0.39}
Step 2046: Updated gated ratio to 0.9212 (progress: 78.8%)
Step 2046: Updated gated ratio to 0.9212 (progress: 78.8%)
Step 2046: Updated gated ratio to 0.9212 (progress: 78.8%)
Step 2046: Updated gated ratio to 0.9212 (progress: 78.8%)Step 2046: Updated gated ratio to 0.9212 (progress: 78.8%)
Step 2046: Updated gated ratio to 0.9212 (progress: 78.8%)
Step 2046: Updated gated ratio to 0.9212 (progress: 78.8%)
Step 2046: Updated gated ratio to 0.9212 (progress: 78.8%)

Step 2046: Updated gated ratio to 0.9212 (progress: 78.8%)
Step 2046: Updated gated ratio to 0.9212 (progress: 78.8%)
Step 2046: Updated gated ratio to 0.9212 (progress: 78.8%)
Step 2046: Updated gated ratio to 0.9212 (progress: 78.8%)
Step 2046: Updated gated ratio to 0.9212 (progress: 78.8%)
Step 2046: Updated gated ratio to 0.9212 (progress: 78.8%)
Step 2046: Updated gated ratio to 0.9212 (progress: 78.8%)
Step 2046: Updated gated ratio to 0.9212 (progress: 78.8%)
{'loss': 1.1392, 'grad_norm': 1.6360284090042114, 'learning_rate': 1.3831151319093323e-05, 'epoch': 0.39}
{'loss': 1.0683, 'grad_norm': 1.5841748714447021, 'learning_rate': 1.382539513940992e-05, 'epoch': 0.39}
{'loss': 1.0522, 'grad_norm': 1.4797292947769165, 'learning_rate': 1.3819637474576411e-05, 'epoch': 0.39}
{'loss': 1.1451, 'grad_norm': 1.5427876710891724, 'learning_rate': 1.381387832682812e-05, 'epoch': 0.39}
{'loss': 1.1198, 'grad_norm': 1.4972790479660034, 'learning_rate': 1.380811769840095e-05, 'epoch': 0.39}
{'loss': 1.1126, 'grad_norm': 1.5344411134719849, 'learning_rate': 1.3802355591531366e-05, 'epoch': 0.39}
Step 2052: Updated gated ratio to 0.9210 (progress: 79.0%)
Step 2052: Updated gated ratio to 0.9210 (progress: 79.0%)
Step 2052: Updated gated ratio to 0.9210 (progress: 79.0%)
Step 2052: Updated gated ratio to 0.9210 (progress: 79.0%)
Step 2052: Updated gated ratio to 0.9210 (progress: 79.0%)
Step 2052: Updated gated ratio to 0.9210 (progress: 79.0%)
Step 2052: Updated gated ratio to 0.9210 (progress: 79.0%)
Step 2052: Updated gated ratio to 0.9210 (progress: 79.0%)Step 2052: Updated gated ratio to 0.9210 (progress: 79.0%)

Step 2052: Updated gated ratio to 0.9210 (progress: 79.0%)Step 2052: Updated gated ratio to 0.9210 (progress: 79.0%)
Step 2052: Updated gated ratio to 0.9210 (progress: 79.0%)
Step 2052: Updated gated ratio to 0.9210 (progress: 79.0%)
Step 2052: Updated gated ratio to 0.9210 (progress: 79.0%)
Step 2052: Updated gated ratio to 0.9210 (progress: 79.0%)

Step 2052: Updated gated ratio to 0.9210 (progress: 79.0%)
{'loss': 1.1324, 'grad_norm': 1.5594043731689453, 'learning_rate': 1.3796592008456427e-05, 'epoch': 0.39}
{'loss': 1.2382, 'grad_norm': 1.4425898790359497, 'learning_rate': 1.3790826951413747e-05, 'epoch': 0.4}
{'loss': 1.0434, 'grad_norm': 1.5982714891433716, 'learning_rate': 1.3785060422641526e-05, 'epoch': 0.4}
{'loss': 1.1496, 'grad_norm': 1.5286911725997925, 'learning_rate': 1.3779292424378521e-05, 'epoch': 0.4}
{'loss': 1.1026, 'grad_norm': 1.677398443222046, 'learning_rate': 1.3773522958864076e-05, 'epoch': 0.4}
Step 2057: Updated gated ratio to 0.9208 (progress: 79.2%)
Step 2057: Updated gated ratio to 0.9208 (progress: 79.2%)Step 2057: Updated gated ratio to 0.9208 (progress: 79.2%)

Step 2057: Updated gated ratio to 0.9208 (progress: 79.2%)
Step 2057: Updated gated ratio to 0.9208 (progress: 79.2%)
Step 2057: Updated gated ratio to 0.9208 (progress: 79.2%)
Step 2057: Updated gated ratio to 0.9208 (progress: 79.2%)
Step 2057: Updated gated ratio to 0.9208 (progress: 79.2%)
Step 2057: Updated gated ratio to 0.9208 (progress: 79.2%)
Step 2057: Updated gated ratio to 0.9208 (progress: 79.2%)
Step 2057: Updated gated ratio to 0.9208 (progress: 79.2%)
Step 2057: Updated gated ratio to 0.9208 (progress: 79.2%)
Step 2057: Updated gated ratio to 0.9208 (progress: 79.2%)
Step 2057: Updated gated ratio to 0.9208 (progress: 79.2%)
Step 2057: Updated gated ratio to 0.9208 (progress: 79.2%)
Step 2057: Updated gated ratio to 0.9208 (progress: 79.2%)
{'loss': 1.1667, 'grad_norm': 1.6261900663375854, 'learning_rate': 1.3767752028338091e-05, 'epoch': 0.4}
{'loss': 1.1873, 'grad_norm': 1.705730676651001, 'learning_rate': 1.376197963504104e-05, 'epoch': 0.4}
{'loss': 1.1589, 'grad_norm': 1.5690337419509888, 'learning_rate': 1.3756205781213965e-05, 'epoch': 0.4}
{'loss': 1.1232, 'grad_norm': 1.7778818607330322, 'learning_rate': 1.375043046909848e-05, 'epoch': 0.4}
{'loss': 1.1599, 'grad_norm': 1.5514698028564453, 'learning_rate': 1.3744653700936752e-05, 'epoch': 0.4}
Step 2062: Updated gated ratio to 0.9206 (progress: 79.4%)
Step 2062: Updated gated ratio to 0.9206 (progress: 79.4%)
Step 2062: Updated gated ratio to 0.9206 (progress: 79.4%)
Step 2062: Updated gated ratio to 0.9206 (progress: 79.4%)
Step 2062: Updated gated ratio to 0.9206 (progress: 79.4%)Step 2062: Updated gated ratio to 0.9206 (progress: 79.4%)

Step 2062: Updated gated ratio to 0.9206 (progress: 79.4%)
Step 2062: Updated gated ratio to 0.9206 (progress: 79.4%)
Step 2062: Updated gated ratio to 0.9206 (progress: 79.4%)
Step 2062: Updated gated ratio to 0.9206 (progress: 79.4%)
Step 2062: Updated gated ratio to 0.9206 (progress: 79.4%)
Step 2062: Updated gated ratio to 0.9206 (progress: 79.4%)
Step 2062: Updated gated ratio to 0.9206 (progress: 79.4%)
Step 2062: Updated gated ratio to 0.9206 (progress: 79.4%)
Step 2062: Updated gated ratio to 0.9206 (progress: 79.4%)
Step 2062: Updated gated ratio to 0.9206 (progress: 79.4%)
{'loss': 1.1107, 'grad_norm': 1.3322325944900513, 'learning_rate': 1.3738875478971526e-05, 'epoch': 0.4}
{'loss': 1.187, 'grad_norm': 1.6587351560592651, 'learning_rate': 1.3733095805446107e-05, 'epoch': 0.4}
{'loss': 1.0802, 'grad_norm': 1.6524169445037842, 'learning_rate': 1.372731468260436e-05, 'epoch': 0.4}
{'loss': 1.1523, 'grad_norm': 1.5516911745071411, 'learning_rate': 1.372153211269072e-05, 'epoch': 0.4}
{'loss': 1.1332, 'grad_norm': 1.630979299545288, 'learning_rate': 1.3715748097950176e-05, 'epoch': 0.4}
Step 2067: Updated gated ratio to 0.9204 (progress: 79.6%)
Step 2067: Updated gated ratio to 0.9204 (progress: 79.6%)
Step 2067: Updated gated ratio to 0.9204 (progress: 79.6%)
Step 2067: Updated gated ratio to 0.9204 (progress: 79.6%)
Step 2067: Updated gated ratio to 0.9204 (progress: 79.6%)Step 2067: Updated gated ratio to 0.9204 (progress: 79.6%)
Step 2067: Updated gated ratio to 0.9204 (progress: 79.6%)
Step 2067: Updated gated ratio to 0.9204 (progress: 79.6%)
Step 2067: Updated gated ratio to 0.9204 (progress: 79.6%)
Step 2067: Updated gated ratio to 0.9204 (progress: 79.6%)
Step 2067: Updated gated ratio to 0.9204 (progress: 79.6%)

Step 2067: Updated gated ratio to 0.9204 (progress: 79.6%)
Step 2067: Updated gated ratio to 0.9204 (progress: 79.6%)
Step 2067: Updated gated ratio to 0.9204 (progress: 79.6%)
Step 2067: Updated gated ratio to 0.9204 (progress: 79.6%)
Step 2067: Updated gated ratio to 0.9204 (progress: 79.6%)
{'loss': 1.1482, 'grad_norm': 1.6076821088790894, 'learning_rate': 1.3709962640628284e-05, 'epoch': 0.4}
{'loss': 1.1737, 'grad_norm': 1.6426867246627808, 'learning_rate': 1.3704175742971158e-05, 'epoch': 0.4}
{'loss': 1.0839, 'grad_norm': 1.5269817113876343, 'learning_rate': 1.369838740722547e-05, 'epoch': 0.4}
{'loss': 1.1245, 'grad_norm': 1.5796197652816772, 'learning_rate': 1.3692597635638452e-05, 'epoch': 0.4}
{'loss': 1.097, 'grad_norm': 1.5697426795959473, 'learning_rate': 1.368680643045789e-05, 'epoch': 0.4}
Step 2072: Updated gated ratio to 0.9202 (progress: 79.8%)
Step 2072: Updated gated ratio to 0.9202 (progress: 79.8%)
Step 2072: Updated gated ratio to 0.9202 (progress: 79.8%)
Step 2072: Updated gated ratio to 0.9202 (progress: 79.8%)
Step 2072: Updated gated ratio to 0.9202 (progress: 79.8%)
Step 2072: Updated gated ratio to 0.9202 (progress: 79.8%)
Step 2072: Updated gated ratio to 0.9202 (progress: 79.8%)
Step 2072: Updated gated ratio to 0.9202 (progress: 79.8%)
Step 2072: Updated gated ratio to 0.9202 (progress: 79.8%)
Step 2072: Updated gated ratio to 0.9202 (progress: 79.8%)
Step 2072: Updated gated ratio to 0.9202 (progress: 79.8%)
Step 2072: Updated gated ratio to 0.9202 (progress: 79.8%)
Step 2072: Updated gated ratio to 0.9202 (progress: 79.8%)
Step 2072: Updated gated ratio to 0.9202 (progress: 79.8%)
Step 2072: Updated gated ratio to 0.9202 (progress: 79.8%)
Step 2072: Updated gated ratio to 0.9202 (progress: 79.8%)
{'loss': 1.15, 'grad_norm': 1.4960756301879883, 'learning_rate': 1.3681013793932132e-05, 'epoch': 0.4}
{'loss': 1.1525, 'grad_norm': 1.597756266593933, 'learning_rate': 1.3675219728310076e-05, 'epoch': 0.4}
WARNING: tokenization mismatch: 0 vs. 1275. (ignored)
{'loss': 1.1335, 'grad_norm': 1.4975191354751587, 'learning_rate': 1.3669424235841185e-05, 'epoch': 0.4}
{'loss': 1.2016, 'grad_norm': 1.314793586730957, 'learning_rate': 1.3663627318775459e-05, 'epoch': 0.4}
{'loss': 1.0728, 'grad_norm': 1.6338467597961426, 'learning_rate': 1.3657828979363468e-05, 'epoch': 0.4}
{'loss': 1.1834, 'grad_norm': 1.5345455408096313, 'learning_rate': 1.3652029219856324e-05, 'epoch': 0.4}
Step 2078: Updated gated ratio to 0.9200 (progress: 80.0%)
Step 2078: Updated gated ratio to 0.9200 (progress: 80.0%)Step 2078: Updated gated ratio to 0.9200 (progress: 80.0%)
Step 2078: Updated gated ratio to 0.9200 (progress: 80.0%)
Step 2078: Updated gated ratio to 0.9200 (progress: 80.0%)
Step 2078: Updated gated ratio to 0.9200 (progress: 80.0%)Step 2078: Updated gated ratio to 0.9200 (progress: 80.0%)

Step 2078: Updated gated ratio to 0.9200 (progress: 80.0%)

Step 2078: Updated gated ratio to 0.9200 (progress: 80.0%)
Step 2078: Updated gated ratio to 0.9200 (progress: 80.0%)
Step 2078: Updated gated ratio to 0.9200 (progress: 80.0%)
Step 2078: Updated gated ratio to 0.9200 (progress: 80.0%)
Step 2078: Updated gated ratio to 0.9200 (progress: 80.0%)
Step 2078: Updated gated ratio to 0.9200 (progress: 80.0%)
Step 2078: Updated gated ratio to 0.9200 (progress: 80.0%)
Step 2078: Updated gated ratio to 0.9200 (progress: 80.0%)
{'loss': 1.1375, 'grad_norm': 1.7351354360580444, 'learning_rate': 1.3646228042505694e-05, 'epoch': 0.4}
{'loss': 1.1695, 'grad_norm': 1.5401113033294678, 'learning_rate': 1.3640425449563793e-05, 'epoch': 0.4}
{'loss': 1.1993, 'grad_norm': 1.2852152585983276, 'learning_rate': 1.3634621443283389e-05, 'epoch': 0.4}
{'loss': 1.1134, 'grad_norm': 1.5337753295898438, 'learning_rate': 1.36288160259178e-05, 'epoch': 0.4}
{'loss': 1.1694, 'grad_norm': 1.6336076259613037, 'learning_rate': 1.3623009199720882e-05, 'epoch': 0.4}
Step 2083: Updated gated ratio to 0.9198 (progress: 80.2%)
Step 2083: Updated gated ratio to 0.9198 (progress: 80.2%)
Step 2083: Updated gated ratio to 0.9198 (progress: 80.2%)
Step 2083: Updated gated ratio to 0.9198 (progress: 80.2%)
Step 2083: Updated gated ratio to 0.9198 (progress: 80.2%)
Step 2083: Updated gated ratio to 0.9198 (progress: 80.2%)
Step 2083: Updated gated ratio to 0.9198 (progress: 80.2%)
Step 2083: Updated gated ratio to 0.9198 (progress: 80.2%)
Step 2083: Updated gated ratio to 0.9198 (progress: 80.2%)
Step 2083: Updated gated ratio to 0.9198 (progress: 80.2%)
Step 2083: Updated gated ratio to 0.9198 (progress: 80.2%)
Step 2083: Updated gated ratio to 0.9198 (progress: 80.2%)
Step 2083: Updated gated ratio to 0.9198 (progress: 80.2%)
Step 2083: Updated gated ratio to 0.9198 (progress: 80.2%)
Step 2083: Updated gated ratio to 0.9198 (progress: 80.2%)
Step 2083: Updated gated ratio to 0.9198 (progress: 80.2%)
{'loss': 1.1187, 'grad_norm': 1.628677487373352, 'learning_rate': 1.3617200966947053e-05, 'epoch': 0.4}
{'loss': 1.1675, 'grad_norm': 1.4816734790802002, 'learning_rate': 1.3611391329851262e-05, 'epoch': 0.4}
{'loss': 1.1182, 'grad_norm': 1.5336644649505615, 'learning_rate': 1.3605580290689013e-05, 'epoch': 0.4}
{'loss': 1.1706, 'grad_norm': 1.6491988897323608, 'learning_rate': 1.3599767851716353e-05, 'epoch': 0.4}
{'loss': 1.1004, 'grad_norm': 1.5420528650283813, 'learning_rate': 1.3593954015189867e-05, 'epoch': 0.4}
Step 2088: Updated gated ratio to 0.9196 (progress: 80.4%)
Step 2088: Updated gated ratio to 0.9196 (progress: 80.4%)
Step 2088: Updated gated ratio to 0.9196 (progress: 80.4%)
Step 2088: Updated gated ratio to 0.9196 (progress: 80.4%)
Step 2088: Updated gated ratio to 0.9196 (progress: 80.4%)
Step 2088: Updated gated ratio to 0.9196 (progress: 80.4%)
Step 2088: Updated gated ratio to 0.9196 (progress: 80.4%)
Step 2088: Updated gated ratio to 0.9196 (progress: 80.4%)
Step 2088: Updated gated ratio to 0.9196 (progress: 80.4%)
Step 2088: Updated gated ratio to 0.9196 (progress: 80.4%)
Step 2088: Updated gated ratio to 0.9196 (progress: 80.4%)
Step 2088: Updated gated ratio to 0.9196 (progress: 80.4%)
Step 2088: Updated gated ratio to 0.9196 (progress: 80.4%)
Step 2088: Updated gated ratio to 0.9196 (progress: 80.4%)
Step 2088: Updated gated ratio to 0.9196 (progress: 80.4%)
Step 2088: Updated gated ratio to 0.9196 (progress: 80.4%)
{'loss': 1.1764, 'grad_norm': 1.5212924480438232, 'learning_rate': 1.3588138783366692e-05, 'epoch': 0.4}
{'loss': 1.1639, 'grad_norm': 1.6618678569793701, 'learning_rate': 1.3582322158504495e-05, 'epoch': 0.4}
{'loss': 1.0793, 'grad_norm': 1.6155364513397217, 'learning_rate': 1.3576504142861496e-05, 'epoch': 0.4}
{'loss': 1.0357, 'grad_norm': 1.526909351348877, 'learning_rate': 1.3570684738696444e-05, 'epoch': 0.4}
{'loss': 1.2368, 'grad_norm': 1.3233249187469482, 'learning_rate': 1.3564863948268631e-05, 'epoch': 0.4}
Step 2093: Updated gated ratio to 0.9194 (progress: 80.6%)
Step 2093: Updated gated ratio to 0.9194 (progress: 80.6%)
Step 2093: Updated gated ratio to 0.9194 (progress: 80.6%)
Step 2093: Updated gated ratio to 0.9194 (progress: 80.6%)
Step 2093: Updated gated ratio to 0.9194 (progress: 80.6%)
Step 2093: Updated gated ratio to 0.9194 (progress: 80.6%)
Step 2093: Updated gated ratio to 0.9194 (progress: 80.6%)
Step 2093: Updated gated ratio to 0.9194 (progress: 80.6%)
Step 2093: Updated gated ratio to 0.9194 (progress: 80.6%)
Step 2093: Updated gated ratio to 0.9194 (progress: 80.6%)
Step 2093: Updated gated ratio to 0.9194 (progress: 80.6%)
Step 2093: Updated gated ratio to 0.9194 (progress: 80.6%)
Step 2093: Updated gated ratio to 0.9194 (progress: 80.6%)
Step 2093: Updated gated ratio to 0.9194 (progress: 80.6%)
Step 2093: Updated gated ratio to 0.9194 (progress: 80.6%)
Step 2093: Updated gated ratio to 0.9194 (progress: 80.6%)
{'loss': 1.2999, 'grad_norm': 1.3144195079803467, 'learning_rate': 1.3559041773837898e-05, 'epoch': 0.4}
{'loss': 1.1495, 'grad_norm': 1.6118277311325073, 'learning_rate': 1.3553218217664603e-05, 'epoch': 0.4}
{'loss': 1.0968, 'grad_norm': 1.5287752151489258, 'learning_rate': 1.3547393282009656e-05, 'epoch': 0.4}
{'loss': 1.1673, 'grad_norm': 1.703743815422058, 'learning_rate': 1.3541566969134496e-05, 'epoch': 0.4}
Step 2098: Updated gated ratio to 0.9192 (progress: 80.8%)
Step 2098: Updated gated ratio to 0.9192 (progress: 80.8%)
Step 2098: Updated gated ratio to 0.9192 (progress: 80.8%)
Step 2098: Updated gated ratio to 0.9192 (progress: 80.8%)
Step 2098: Updated gated ratio to 0.9192 (progress: 80.8%)
Step 2098: Updated gated ratio to 0.9192 (progress: 80.8%)
Step 2098: Updated gated ratio to 0.9192 (progress: 80.8%)
Step 2098: Updated gated ratio to 0.9192 (progress: 80.8%)
Step 2098: Updated gated ratio to 0.9192 (progress: 80.8%)
Step 2098: Updated gated ratio to 0.9192 (progress: 80.8%)
Step 2098: Updated gated ratio to 0.9192 (progress: 80.8%)
Step 2098: Updated gated ratio to 0.9192 (progress: 80.8%)
Step 2098: Updated gated ratio to 0.9192 (progress: 80.8%)
Step 2098: Updated gated ratio to 0.9192 (progress: 80.8%)
Step 2098: Updated gated ratio to 0.9192 (progress: 80.8%)
{'loss': 1.1266, 'grad_norm': 1.5506187677383423, 'learning_rate': 1.3535739281301102e-05, 'epoch': 0.4}
Step 2098: Updated gated ratio to 0.9192 (progress: 80.8%)
{'loss': 1.1229, 'grad_norm': 1.6485077142715454, 'learning_rate': 1.3529910220771975e-05, 'epoch': 0.4}
{'loss': 1.1634, 'grad_norm': 1.5833910703659058, 'learning_rate': 1.3524079789810163e-05, 'epoch': 0.4}
{'loss': 1.1313, 'grad_norm': 1.7923966646194458, 'learning_rate': 1.3518247990679241e-05, 'epoch': 0.4}
{'loss': 1.1047, 'grad_norm': 1.5704772472381592, 'learning_rate': 1.3512414825643312e-05, 'epoch': 0.4}
{'loss': 1.066, 'grad_norm': 1.5514787435531616, 'learning_rate': 1.3506580296967011e-05, 'epoch': 0.4}
{'loss': 1.1362, 'grad_norm': 1.419967770576477, 'learning_rate': 1.3500744406915505e-05, 'epoch': 0.4}
Step 2104: Updated gated ratio to 0.9190 (progress: 81.0%)
Step 2104: Updated gated ratio to 0.9190 (progress: 81.0%)
Step 2104: Updated gated ratio to 0.9190 (progress: 81.0%)
Step 2104: Updated gated ratio to 0.9190 (progress: 81.0%)
Step 2104: Updated gated ratio to 0.9190 (progress: 81.0%)
Step 2104: Updated gated ratio to 0.9190 (progress: 81.0%)
Step 2104: Updated gated ratio to 0.9190 (progress: 81.0%)
Step 2104: Updated gated ratio to 0.9190 (progress: 81.0%)
Step 2104: Updated gated ratio to 0.9190 (progress: 81.0%)
Step 2104: Updated gated ratio to 0.9190 (progress: 81.0%)
Step 2104: Updated gated ratio to 0.9190 (progress: 81.0%)
Step 2104: Updated gated ratio to 0.9190 (progress: 81.0%)
Step 2104: Updated gated ratio to 0.9190 (progress: 81.0%)
Step 2104: Updated gated ratio to 0.9190 (progress: 81.0%)
Step 2104: Updated gated ratio to 0.9190 (progress: 81.0%)
Step 2104: Updated gated ratio to 0.9190 (progress: 81.0%)
{'loss': 1.0221, 'grad_norm': 1.4149047136306763, 'learning_rate': 1.3494907157754485e-05, 'epoch': 0.4}
{'loss': 1.0906, 'grad_norm': 1.4973331689834595, 'learning_rate': 1.348906855175017e-05, 'epoch': 0.41}
{'loss': 1.168, 'grad_norm': 1.5363531112670898, 'learning_rate': 1.3483228591169315e-05, 'epoch': 0.41}
{'loss': 1.2209, 'grad_norm': 1.965071439743042, 'learning_rate': 1.347738727827919e-05, 'epoch': 0.41}
{'loss': 1.1354, 'grad_norm': 1.6549651622772217, 'learning_rate': 1.3471544615347591e-05, 'epoch': 0.41}
Step 2109: Updated gated ratio to 0.9188 (progress: 81.2%)
Step 2109: Updated gated ratio to 0.9188 (progress: 81.2%)
Step 2109: Updated gated ratio to 0.9188 (progress: 81.2%)
Step 2109: Updated gated ratio to 0.9188 (progress: 81.2%)
Step 2109: Updated gated ratio to 0.9188 (progress: 81.2%)
Step 2109: Updated gated ratio to 0.9188 (progress: 81.2%)
Step 2109: Updated gated ratio to 0.9188 (progress: 81.2%)
Step 2109: Updated gated ratio to 0.9188 (progress: 81.2%)
Step 2109: Updated gated ratio to 0.9188 (progress: 81.2%)
Step 2109: Updated gated ratio to 0.9188 (progress: 81.2%)
Step 2109: Updated gated ratio to 0.9188 (progress: 81.2%)
Step 2109: Updated gated ratio to 0.9188 (progress: 81.2%)Step 2109: Updated gated ratio to 0.9188 (progress: 81.2%)

Step 2109: Updated gated ratio to 0.9188 (progress: 81.2%)
Step 2109: Updated gated ratio to 0.9188 (progress: 81.2%)
Step 2109: Updated gated ratio to 0.9188 (progress: 81.2%)
{'loss': 1.0843, 'grad_norm': 1.5085991621017456, 'learning_rate': 1.3465700604642847e-05, 'epoch': 0.41}
{'loss': 1.134, 'grad_norm': 1.6208192110061646, 'learning_rate': 1.34598552484338e-05, 'epoch': 0.41}
{'loss': 1.1647, 'grad_norm': 1.5476906299591064, 'learning_rate': 1.3454008548989816e-05, 'epoch': 0.41}
{'loss': 1.0975, 'grad_norm': 1.5458941459655762, 'learning_rate': 1.3448160508580789e-05, 'epoch': 0.41}
{'loss': 1.1168, 'grad_norm': 1.6976267099380493, 'learning_rate': 1.3442311129477133e-05, 'epoch': 0.41}
Step 2114: Updated gated ratio to 0.9186 (progress: 81.4%)
Step 2114: Updated gated ratio to 0.9186 (progress: 81.4%)
Step 2114: Updated gated ratio to 0.9186 (progress: 81.4%)
Step 2114: Updated gated ratio to 0.9186 (progress: 81.4%)
Step 2114: Updated gated ratio to 0.9186 (progress: 81.4%)
Step 2114: Updated gated ratio to 0.9186 (progress: 81.4%)
Step 2114: Updated gated ratio to 0.9186 (progress: 81.4%)
Step 2114: Updated gated ratio to 0.9186 (progress: 81.4%)
Step 2114: Updated gated ratio to 0.9186 (progress: 81.4%)
Step 2114: Updated gated ratio to 0.9186 (progress: 81.4%)Step 2114: Updated gated ratio to 0.9186 (progress: 81.4%)

Step 2114: Updated gated ratio to 0.9186 (progress: 81.4%)
Step 2114: Updated gated ratio to 0.9186 (progress: 81.4%)
Step 2114: Updated gated ratio to 0.9186 (progress: 81.4%)
Step 2114: Updated gated ratio to 0.9186 (progress: 81.4%)
Step 2114: Updated gated ratio to 0.9186 (progress: 81.4%)
{'loss': 1.0841, 'grad_norm': 1.5576393604278564, 'learning_rate': 1.343646041394977e-05, 'epoch': 0.41}
{'loss': 1.2238, 'grad_norm': 1.7558954954147339, 'learning_rate': 1.3430608364270156e-05, 'epoch': 0.41}
{'loss': 1.1342, 'grad_norm': 1.6102403402328491, 'learning_rate': 1.3424754982710256e-05, 'epoch': 0.41}
{'loss': 1.0824, 'grad_norm': 1.6448251008987427, 'learning_rate': 1.3418900271542552e-05, 'epoch': 0.41}
{'loss': 1.1783, 'grad_norm': 1.653756022453308, 'learning_rate': 1.3413044233040045e-05, 'epoch': 0.41}
Step 2119: Updated gated ratio to 0.9184 (progress: 81.6%)
Step 2119: Updated gated ratio to 0.9184 (progress: 81.6%)
Step 2119: Updated gated ratio to 0.9184 (progress: 81.6%)
Step 2119: Updated gated ratio to 0.9184 (progress: 81.6%)
Step 2119: Updated gated ratio to 0.9184 (progress: 81.6%)Step 2119: Updated gated ratio to 0.9184 (progress: 81.6%)

Step 2119: Updated gated ratio to 0.9184 (progress: 81.6%)
Step 2119: Updated gated ratio to 0.9184 (progress: 81.6%)
Step 2119: Updated gated ratio to 0.9184 (progress: 81.6%)
Step 2119: Updated gated ratio to 0.9184 (progress: 81.6%)
Step 2119: Updated gated ratio to 0.9184 (progress: 81.6%)
Step 2119: Updated gated ratio to 0.9184 (progress: 81.6%)
Step 2119: Updated gated ratio to 0.9184 (progress: 81.6%)
Step 2119: Updated gated ratio to 0.9184 (progress: 81.6%)
Step 2119: Updated gated ratio to 0.9184 (progress: 81.6%)
Step 2119: Updated gated ratio to 0.9184 (progress: 81.6%)
{'loss': 1.1386, 'grad_norm': 1.7314140796661377, 'learning_rate': 1.3407186869476253e-05, 'epoch': 0.41}
{'loss': 1.0742, 'grad_norm': 1.556406021118164, 'learning_rate': 1.3401328183125208e-05, 'epoch': 0.41}
{'loss': 1.1086, 'grad_norm': 1.4547250270843506, 'learning_rate': 1.339546817626145e-05, 'epoch': 0.41}
{'loss': 0.9884, 'grad_norm': 1.5183649063110352, 'learning_rate': 1.3389606851160037e-05, 'epoch': 0.41}
{'loss': 1.2107, 'grad_norm': 1.5500590801239014, 'learning_rate': 1.3383744210096537e-05, 'epoch': 0.41}
Step 2124: Updated gated ratio to 0.9182 (progress: 81.8%)
Step 2124: Updated gated ratio to 0.9182 (progress: 81.8%)
Step 2124: Updated gated ratio to 0.9182 (progress: 81.8%)
Step 2124: Updated gated ratio to 0.9182 (progress: 81.8%)
Step 2124: Updated gated ratio to 0.9182 (progress: 81.8%)
Step 2124: Updated gated ratio to 0.9182 (progress: 81.8%)
Step 2124: Updated gated ratio to 0.9182 (progress: 81.8%)
Step 2124: Updated gated ratio to 0.9182 (progress: 81.8%)
Step 2124: Updated gated ratio to 0.9182 (progress: 81.8%)
Step 2124: Updated gated ratio to 0.9182 (progress: 81.8%)
Step 2124: Updated gated ratio to 0.9182 (progress: 81.8%)
Step 2124: Updated gated ratio to 0.9182 (progress: 81.8%)
Step 2124: Updated gated ratio to 0.9182 (progress: 81.8%)
Step 2124: Updated gated ratio to 0.9182 (progress: 81.8%)
Step 2124: Updated gated ratio to 0.9182 (progress: 81.8%)
Step 2124: Updated gated ratio to 0.9182 (progress: 81.8%)
{'loss': 1.1324, 'grad_norm': 1.5040313005447388, 'learning_rate': 1.3377880255347026e-05, 'epoch': 0.41}
{'loss': 1.1, 'grad_norm': 1.5239348411560059, 'learning_rate': 1.3372014989188098e-05, 'epoch': 0.41}
{'loss': 1.063, 'grad_norm': 1.5473792552947998, 'learning_rate': 1.3366148413896851e-05, 'epoch': 0.41}
{'loss': 1.1379, 'grad_norm': 1.5561600923538208, 'learning_rate': 1.3360280531750886e-05, 'epoch': 0.41}
{'loss': 1.1553, 'grad_norm': 1.5023819208145142, 'learning_rate': 1.3354411345028324e-05, 'epoch': 0.41}
{'loss': 1.0371, 'grad_norm': 1.5686097145080566, 'learning_rate': 1.3348540856007782e-05, 'epoch': 0.41}
Step 2130: Updated gated ratio to 0.9180 (progress: 82.0%)
Step 2130: Updated gated ratio to 0.9180 (progress: 82.0%)
Step 2130: Updated gated ratio to 0.9180 (progress: 82.0%)
Step 2130: Updated gated ratio to 0.9180 (progress: 82.0%)Step 2130: Updated gated ratio to 0.9180 (progress: 82.0%)
Step 2130: Updated gated ratio to 0.9180 (progress: 82.0%)
Step 2130: Updated gated ratio to 0.9180 (progress: 82.0%)
Step 2130: Updated gated ratio to 0.9180 (progress: 82.0%)

Step 2130: Updated gated ratio to 0.9180 (progress: 82.0%)
Step 2130: Updated gated ratio to 0.9180 (progress: 82.0%)Step 2130: Updated gated ratio to 0.9180 (progress: 82.0%)

Step 2130: Updated gated ratio to 0.9180 (progress: 82.0%)
Step 2130: Updated gated ratio to 0.9180 (progress: 82.0%)
Step 2130: Updated gated ratio to 0.9180 (progress: 82.0%)
Step 2130: Updated gated ratio to 0.9180 (progress: 82.0%)
Step 2130: Updated gated ratio to 0.9180 (progress: 82.0%)
{'loss': 1.0002, 'grad_norm': 1.562547206878662, 'learning_rate': 1.3342669066968385e-05, 'epoch': 0.41}
{'loss': 1.1923, 'grad_norm': 1.7720441818237305, 'learning_rate': 1.3336795980189763e-05, 'epoch': 0.41}
{'loss': 1.1281, 'grad_norm': 1.7739956378936768, 'learning_rate': 1.3330921597952056e-05, 'epoch': 0.41}
{'loss': 1.0647, 'grad_norm': 1.7064868211746216, 'learning_rate': 1.3325045922535896e-05, 'epoch': 0.41}
{'loss': 1.1025, 'grad_norm': 1.6290206909179688, 'learning_rate': 1.3319168956222423e-05, 'epoch': 0.41}
Step 2135: Updated gated ratio to 0.9178 (progress: 82.2%)
Step 2135: Updated gated ratio to 0.9178 (progress: 82.2%)
Step 2135: Updated gated ratio to 0.9178 (progress: 82.2%)
Step 2135: Updated gated ratio to 0.9178 (progress: 82.2%)
Step 2135: Updated gated ratio to 0.9178 (progress: 82.2%)
Step 2135: Updated gated ratio to 0.9178 (progress: 82.2%)
Step 2135: Updated gated ratio to 0.9178 (progress: 82.2%)
Step 2135: Updated gated ratio to 0.9178 (progress: 82.2%)
Step 2135: Updated gated ratio to 0.9178 (progress: 82.2%)
Step 2135: Updated gated ratio to 0.9178 (progress: 82.2%)
Step 2135: Updated gated ratio to 0.9178 (progress: 82.2%)
Step 2135: Updated gated ratio to 0.9178 (progress: 82.2%)
Step 2135: Updated gated ratio to 0.9178 (progress: 82.2%)
Step 2135: Updated gated ratio to 0.9178 (progress: 82.2%)
Step 2135: Updated gated ratio to 0.9178 (progress: 82.2%)
Step 2135: Updated gated ratio to 0.9178 (progress: 82.2%)
{'loss': 1.1488, 'grad_norm': 1.6169096231460571, 'learning_rate': 1.331329070129328e-05, 'epoch': 0.41}
{'loss': 1.1204, 'grad_norm': 1.7811659574508667, 'learning_rate': 1.3307411160030608e-05, 'epoch': 0.41}
{'loss': 1.1032, 'grad_norm': 1.492529034614563, 'learning_rate': 1.3301530334717046e-05, 'epoch': 0.41}
{'loss': 1.1139, 'grad_norm': 1.671897292137146, 'learning_rate': 1.3295648227635729e-05, 'epoch': 0.41}
{'loss': 1.1727, 'grad_norm': 1.5819082260131836, 'learning_rate': 1.32897648410703e-05, 'epoch': 0.41}
Step 2140: Updated gated ratio to 0.9176 (progress: 82.4%)
Step 2140: Updated gated ratio to 0.9176 (progress: 82.4%)
Step 2140: Updated gated ratio to 0.9176 (progress: 82.4%)
Step 2140: Updated gated ratio to 0.9176 (progress: 82.4%)
Step 2140: Updated gated ratio to 0.9176 (progress: 82.4%)
Step 2140: Updated gated ratio to 0.9176 (progress: 82.4%)
Step 2140: Updated gated ratio to 0.9176 (progress: 82.4%)
Step 2140: Updated gated ratio to 0.9176 (progress: 82.4%)
Step 2140: Updated gated ratio to 0.9176 (progress: 82.4%)
Step 2140: Updated gated ratio to 0.9176 (progress: 82.4%)
Step 2140: Updated gated ratio to 0.9176 (progress: 82.4%)
Step 2140: Updated gated ratio to 0.9176 (progress: 82.4%)
Step 2140: Updated gated ratio to 0.9176 (progress: 82.4%)
Step 2140: Updated gated ratio to 0.9176 (progress: 82.4%)
Step 2140: Updated gated ratio to 0.9176 (progress: 82.4%)
Step 2140: Updated gated ratio to 0.9176 (progress: 82.4%)
{'loss': 1.1007, 'grad_norm': 1.4089194536209106, 'learning_rate': 1.328388017730489e-05, 'epoch': 0.41}
{'loss': 1.046, 'grad_norm': 1.5310492515563965, 'learning_rate': 1.327799423862413e-05, 'epoch': 0.41}
{'loss': 1.1095, 'grad_norm': 1.6585355997085571, 'learning_rate': 1.3272107027313142e-05, 'epoch': 0.41}
{'loss': 1.23, 'grad_norm': 1.520344853401184, 'learning_rate': 1.3266218545657541e-05, 'epoch': 0.41}
{'loss': 1.0872, 'grad_norm': 1.597455620765686, 'learning_rate': 1.326032879594344e-05, 'epoch': 0.41}
Step 2145: Updated gated ratio to 0.9174 (progress: 82.6%)
Step 2145: Updated gated ratio to 0.9174 (progress: 82.6%)
Step 2145: Updated gated ratio to 0.9174 (progress: 82.6%)
Step 2145: Updated gated ratio to 0.9174 (progress: 82.6%)
Step 2145: Updated gated ratio to 0.9174 (progress: 82.6%)
Step 2145: Updated gated ratio to 0.9174 (progress: 82.6%)
Step 2145: Updated gated ratio to 0.9174 (progress: 82.6%)
Step 2145: Updated gated ratio to 0.9174 (progress: 82.6%)
Step 2145: Updated gated ratio to 0.9174 (progress: 82.6%)
Step 2145: Updated gated ratio to 0.9174 (progress: 82.6%)
Step 2145: Updated gated ratio to 0.9174 (progress: 82.6%)Step 2145: Updated gated ratio to 0.9174 (progress: 82.6%)

Step 2145: Updated gated ratio to 0.9174 (progress: 82.6%)
Step 2145: Updated gated ratio to 0.9174 (progress: 82.6%)Step 2145: Updated gated ratio to 0.9174 (progress: 82.6%)

Step 2145: Updated gated ratio to 0.9174 (progress: 82.6%)
{'loss': 1.0659, 'grad_norm': 1.563415288925171, 'learning_rate': 1.3254437780457448e-05, 'epoch': 0.41}
{'loss': 1.1266, 'grad_norm': 1.5759176015853882, 'learning_rate': 1.3248545501486654e-05, 'epoch': 0.41}
{'loss': 1.084, 'grad_norm': 1.5432807207107544, 'learning_rate': 1.3242651961318646e-05, 'epoch': 0.41}
{'loss': 1.1423, 'grad_norm': 1.6267939805984497, 'learning_rate': 1.32367571622415e-05, 'epoch': 0.41}
{'loss': 1.0409, 'grad_norm': 1.52743399143219, 'learning_rate': 1.3230861106543777e-05, 'epoch': 0.41}
Step 2150: Updated gated ratio to 0.9172 (progress: 82.8%)
Step 2150: Updated gated ratio to 0.9172 (progress: 82.8%)
Step 2150: Updated gated ratio to 0.9172 (progress: 82.8%)
Step 2150: Updated gated ratio to 0.9172 (progress: 82.8%)
Step 2150: Updated gated ratio to 0.9172 (progress: 82.8%)
Step 2150: Updated gated ratio to 0.9172 (progress: 82.8%)
Step 2150: Updated gated ratio to 0.9172 (progress: 82.8%)
Step 2150: Updated gated ratio to 0.9172 (progress: 82.8%)
Step 2150: Updated gated ratio to 0.9172 (progress: 82.8%)
Step 2150: Updated gated ratio to 0.9172 (progress: 82.8%)
Step 2150: Updated gated ratio to 0.9172 (progress: 82.8%)
Step 2150: Updated gated ratio to 0.9172 (progress: 82.8%)
Step 2150: Updated gated ratio to 0.9172 (progress: 82.8%)
Step 2150: Updated gated ratio to 0.9172 (progress: 82.8%)
Step 2150: Updated gated ratio to 0.9172 (progress: 82.8%)
Step 2150: Updated gated ratio to 0.9172 (progress: 82.8%)
{'loss': 1.1073, 'grad_norm': 1.5684863328933716, 'learning_rate': 1.3224963796514532e-05, 'epoch': 0.41}
{'loss': 1.0931, 'grad_norm': 1.5777181386947632, 'learning_rate': 1.32190652344433e-05, 'epoch': 0.41}
{'loss': 1.2592, 'grad_norm': 1.433828592300415, 'learning_rate': 1.3213165422620111e-05, 'epoch': 0.41}
{'loss': 1.0594, 'grad_norm': 1.4734712839126587, 'learning_rate': 1.3207264363335472e-05, 'epoch': 0.41}
{'loss': 1.0539, 'grad_norm': 1.5746508836746216, 'learning_rate': 1.3201362058880375e-05, 'epoch': 0.41}
{'loss': 1.0961, 'grad_norm': 1.4816153049468994, 'learning_rate': 1.3195458511546307e-05, 'epoch': 0.41}
Step 2156: Updated gated ratio to 0.9170 (progress: 83.0%)
Step 2156: Updated gated ratio to 0.9170 (progress: 83.0%)Step 2156: Updated gated ratio to 0.9170 (progress: 83.0%)
Step 2156: Updated gated ratio to 0.9170 (progress: 83.0%)
Step 2156: Updated gated ratio to 0.9170 (progress: 83.0%)
Step 2156: Updated gated ratio to 0.9170 (progress: 83.0%)
Step 2156: Updated gated ratio to 0.9170 (progress: 83.0%)
Step 2156: Updated gated ratio to 0.9170 (progress: 83.0%)

Step 2156: Updated gated ratio to 0.9170 (progress: 83.0%)
Step 2156: Updated gated ratio to 0.9170 (progress: 83.0%)
Step 2156: Updated gated ratio to 0.9170 (progress: 83.0%)
Step 2156: Updated gated ratio to 0.9170 (progress: 83.0%)
Step 2156: Updated gated ratio to 0.9170 (progress: 83.0%)
Step 2156: Updated gated ratio to 0.9170 (progress: 83.0%)
Step 2156: Updated gated ratio to 0.9170 (progress: 83.0%)
Step 2156: Updated gated ratio to 0.9170 (progress: 83.0%)
{'loss': 1.1317, 'grad_norm': 1.5930933952331543, 'learning_rate': 1.3189553723625217e-05, 'epoch': 0.41}
{'loss': 1.0448, 'grad_norm': 1.5197970867156982, 'learning_rate': 1.318364769740955e-05, 'epoch': 0.42}
{'loss': 1.1071, 'grad_norm': 1.6495925188064575, 'learning_rate': 1.3177740435192235e-05, 'epoch': 0.42}
{'loss': 1.2708, 'grad_norm': 1.6872961521148682, 'learning_rate': 1.3171831939266668e-05, 'epoch': 0.42}
{'loss': 1.1593, 'grad_norm': 1.5130935907363892, 'learning_rate': 1.3165922211926734e-05, 'epoch': 0.42}
Step 2161: Updated gated ratio to 0.9168 (progress: 83.2%)
Step 2161: Updated gated ratio to 0.9168 (progress: 83.2%)
Step 2161: Updated gated ratio to 0.9168 (progress: 83.2%)
Step 2161: Updated gated ratio to 0.9168 (progress: 83.2%)
Step 2161: Updated gated ratio to 0.9168 (progress: 83.2%)
Step 2161: Updated gated ratio to 0.9168 (progress: 83.2%)
Step 2161: Updated gated ratio to 0.9168 (progress: 83.2%)
Step 2161: Updated gated ratio to 0.9168 (progress: 83.2%)
Step 2161: Updated gated ratio to 0.9168 (progress: 83.2%)
Step 2161: Updated gated ratio to 0.9168 (progress: 83.2%)
Step 2161: Updated gated ratio to 0.9168 (progress: 83.2%)
Step 2161: Updated gated ratio to 0.9168 (progress: 83.2%)
Step 2161: Updated gated ratio to 0.9168 (progress: 83.2%)
Step 2161: Updated gated ratio to 0.9168 (progress: 83.2%)
Step 2161: Updated gated ratio to 0.9168 (progress: 83.2%)
Step 2161: Updated gated ratio to 0.9168 (progress: 83.2%)
{'loss': 1.1182, 'grad_norm': 1.3310868740081787, 'learning_rate': 1.3160011255466791e-05, 'epoch': 0.42}
{'loss': 1.0284, 'grad_norm': 1.592234492301941, 'learning_rate': 1.3154099072181677e-05, 'epoch': 0.42}
{'loss': 1.1215, 'grad_norm': 1.6666673421859741, 'learning_rate': 1.3148185664366704e-05, 'epoch': 0.42}
{'loss': 1.1428, 'grad_norm': 1.6541266441345215, 'learning_rate': 1.314227103431766e-05, 'epoch': 0.42}
{'loss': 1.0267, 'grad_norm': 1.4856452941894531, 'learning_rate': 1.3136355184330809e-05, 'epoch': 0.42}
Step 2166: Updated gated ratio to 0.9166 (progress: 83.4%)
Step 2166: Updated gated ratio to 0.9166 (progress: 83.4%)
Step 2166: Updated gated ratio to 0.9166 (progress: 83.4%)
Step 2166: Updated gated ratio to 0.9166 (progress: 83.4%)Step 2166: Updated gated ratio to 0.9166 (progress: 83.4%)

Step 2166: Updated gated ratio to 0.9166 (progress: 83.4%)
Step 2166: Updated gated ratio to 0.9166 (progress: 83.4%)
Step 2166: Updated gated ratio to 0.9166 (progress: 83.4%)
Step 2166: Updated gated ratio to 0.9166 (progress: 83.4%)
Step 2166: Updated gated ratio to 0.9166 (progress: 83.4%)
Step 2166: Updated gated ratio to 0.9166 (progress: 83.4%)
Step 2166: Updated gated ratio to 0.9166 (progress: 83.4%)
Step 2166: Updated gated ratio to 0.9166 (progress: 83.4%)
Step 2166: Updated gated ratio to 0.9166 (progress: 83.4%)
Step 2166: Updated gated ratio to 0.9166 (progress: 83.4%)
Step 2166: Updated gated ratio to 0.9166 (progress: 83.4%)
{'loss': 1.0972, 'grad_norm': 1.5601361989974976, 'learning_rate': 1.3130438116702888e-05, 'epoch': 0.42}
{'loss': 1.1118, 'grad_norm': 1.625024437904358, 'learning_rate': 1.3124519833731106e-05, 'epoch': 0.42}
{'loss': 1.1036, 'grad_norm': 1.627404808998108, 'learning_rate': 1.3118600337713146e-05, 'epoch': 0.42}
{'loss': 1.1711, 'grad_norm': 1.6505781412124634, 'learning_rate': 1.3112679630947156e-05, 'epoch': 0.42}
{'loss': 1.16, 'grad_norm': 1.7203091382980347, 'learning_rate': 1.310675771573176e-05, 'epoch': 0.42}
Step 2171: Updated gated ratio to 0.9164 (progress: 83.6%)
Step 2171: Updated gated ratio to 0.9164 (progress: 83.6%)
Step 2171: Updated gated ratio to 0.9164 (progress: 83.6%)
Step 2171: Updated gated ratio to 0.9164 (progress: 83.6%)
Step 2171: Updated gated ratio to 0.9164 (progress: 83.6%)
Step 2171: Updated gated ratio to 0.9164 (progress: 83.6%)
Step 2171: Updated gated ratio to 0.9164 (progress: 83.6%)
Step 2171: Updated gated ratio to 0.9164 (progress: 83.6%)
Step 2171: Updated gated ratio to 0.9164 (progress: 83.6%)
Step 2171: Updated gated ratio to 0.9164 (progress: 83.6%)
Step 2171: Updated gated ratio to 0.9164 (progress: 83.6%)
Step 2171: Updated gated ratio to 0.9164 (progress: 83.6%)
Step 2171: Updated gated ratio to 0.9164 (progress: 83.6%)
Step 2171: Updated gated ratio to 0.9164 (progress: 83.6%)
Step 2171: Updated gated ratio to 0.9164 (progress: 83.6%)
Step 2171: Updated gated ratio to 0.9164 (progress: 83.6%)
{'loss': 1.1096, 'grad_norm': 1.5185068845748901, 'learning_rate': 1.310083459436605e-05, 'epoch': 0.42}
{'loss': 1.2602, 'grad_norm': 1.280728816986084, 'learning_rate': 1.3094910269149587e-05, 'epoch': 0.42}
WARNING: tokenization mismatch: 80 vs. 81. (ignored)
{'loss': 0.9991, 'grad_norm': 1.760093331336975, 'learning_rate': 1.3088984742382395e-05, 'epoch': 0.42}
{'loss': 1.1007, 'grad_norm': 1.7252953052520752, 'learning_rate': 1.3083058016364972e-05, 'epoch': 0.42}
{'loss': 1.0329, 'grad_norm': 1.5570188760757446, 'learning_rate': 1.3077130093398274e-05, 'epoch': 0.42}
Step 2176: Updated gated ratio to 0.9162 (progress: 83.8%)
Step 2176: Updated gated ratio to 0.9162 (progress: 83.8%)
Step 2176: Updated gated ratio to 0.9162 (progress: 83.8%)
Step 2176: Updated gated ratio to 0.9162 (progress: 83.8%)
Step 2176: Updated gated ratio to 0.9162 (progress: 83.8%)
Step 2176: Updated gated ratio to 0.9162 (progress: 83.8%)Step 2176: Updated gated ratio to 0.9162 (progress: 83.8%)

Step 2176: Updated gated ratio to 0.9162 (progress: 83.8%)
Step 2176: Updated gated ratio to 0.9162 (progress: 83.8%)
Step 2176: Updated gated ratio to 0.9162 (progress: 83.8%)
Step 2176: Updated gated ratio to 0.9162 (progress: 83.8%)
Step 2176: Updated gated ratio to 0.9162 (progress: 83.8%)
Step 2176: Updated gated ratio to 0.9162 (progress: 83.8%)
Step 2176: Updated gated ratio to 0.9162 (progress: 83.8%)
Step 2176: Updated gated ratio to 0.9162 (progress: 83.8%)
Step 2176: Updated gated ratio to 0.9162 (progress: 83.8%)
{'loss': 1.0084, 'grad_norm': 1.4459303617477417, 'learning_rate': 1.3071200975783725e-05, 'epoch': 0.42}
{'loss': 1.2853, 'grad_norm': 1.4447954893112183, 'learning_rate': 1.3065270665823206e-05, 'epoch': 0.42}
{'loss': 1.1128, 'grad_norm': 1.5583511590957642, 'learning_rate': 1.3059339165819082e-05, 'epoch': 0.42}
{'loss': 1.1534, 'grad_norm': 1.6069377660751343, 'learning_rate': 1.3053406478074155e-05, 'epoch': 0.42}
{'loss': 1.1319, 'grad_norm': 1.5078818798065186, 'learning_rate': 1.3047472604891701e-05, 'epoch': 0.42}
{'loss': 1.0935, 'grad_norm': 1.6293833255767822, 'learning_rate': 1.3041537548575455e-05, 'epoch': 0.42}
Step 2182: Updated gated ratio to 0.9160 (progress: 84.0%)
Step 2182: Updated gated ratio to 0.9160 (progress: 84.0%)
Step 2182: Updated gated ratio to 0.9160 (progress: 84.0%)
Step 2182: Updated gated ratio to 0.9160 (progress: 84.0%)
Step 2182: Updated gated ratio to 0.9160 (progress: 84.0%)
Step 2182: Updated gated ratio to 0.9160 (progress: 84.0%)
Step 2182: Updated gated ratio to 0.9160 (progress: 84.0%)
Step 2182: Updated gated ratio to 0.9160 (progress: 84.0%)
Step 2182: Updated gated ratio to 0.9160 (progress: 84.0%)
Step 2182: Updated gated ratio to 0.9160 (progress: 84.0%)
Step 2182: Updated gated ratio to 0.9160 (progress: 84.0%)
Step 2182: Updated gated ratio to 0.9160 (progress: 84.0%)
Step 2182: Updated gated ratio to 0.9160 (progress: 84.0%)
Step 2182: Updated gated ratio to 0.9160 (progress: 84.0%)
Step 2182: Updated gated ratio to 0.9160 (progress: 84.0%)
Step 2182: Updated gated ratio to 0.9160 (progress: 84.0%)
{'loss': 1.2173, 'grad_norm': 1.649754285812378, 'learning_rate': 1.303560131142961e-05, 'epoch': 0.42}
{'loss': 1.093, 'grad_norm': 1.5357269048690796, 'learning_rate': 1.3029663895758814e-05, 'epoch': 0.42}
{'loss': 1.1901, 'grad_norm': 1.5742197036743164, 'learning_rate': 1.3023725303868183e-05, 'epoch': 0.42}
{'loss': 1.0998, 'grad_norm': 1.5088812112808228, 'learning_rate': 1.3017785538063277e-05, 'epoch': 0.42}
{'loss': 1.1295, 'grad_norm': 1.7354710102081299, 'learning_rate': 1.3011844600650121e-05, 'epoch': 0.42}
Step 2187: Updated gated ratio to 0.9158 (progress: 84.2%)
Step 2187: Updated gated ratio to 0.9158 (progress: 84.2%)
Step 2187: Updated gated ratio to 0.9158 (progress: 84.2%)
Step 2187: Updated gated ratio to 0.9158 (progress: 84.2%)
Step 2187: Updated gated ratio to 0.9158 (progress: 84.2%)
Step 2187: Updated gated ratio to 0.9158 (progress: 84.2%)
Step 2187: Updated gated ratio to 0.9158 (progress: 84.2%)
Step 2187: Updated gated ratio to 0.9158 (progress: 84.2%)
Step 2187: Updated gated ratio to 0.9158 (progress: 84.2%)
Step 2187: Updated gated ratio to 0.9158 (progress: 84.2%)
Step 2187: Updated gated ratio to 0.9158 (progress: 84.2%)
Step 2187: Updated gated ratio to 0.9158 (progress: 84.2%)Step 2187: Updated gated ratio to 0.9158 (progress: 84.2%)

Step 2187: Updated gated ratio to 0.9158 (progress: 84.2%)
Step 2187: Updated gated ratio to 0.9158 (progress: 84.2%)
Step 2187: Updated gated ratio to 0.9158 (progress: 84.2%)
{'loss': 1.1825, 'grad_norm': 1.6325656175613403, 'learning_rate': 1.300590249393519e-05, 'epoch': 0.42}
{'loss': 1.0277, 'grad_norm': 1.559241771697998, 'learning_rate': 1.2999959220225416e-05, 'epoch': 0.42}
{'loss': 1.0837, 'grad_norm': 1.5331226587295532, 'learning_rate': 1.299401478182818e-05, 'epoch': 0.42}
{'loss': 1.0731, 'grad_norm': 1.6416113376617432, 'learning_rate': 1.2988069181051314e-05, 'epoch': 0.42}
{'loss': 1.1434, 'grad_norm': 1.6038199663162231, 'learning_rate': 1.2982122420203114e-05, 'epoch': 0.42}
Step 2192: Updated gated ratio to 0.9156 (progress: 84.4%)
Step 2192: Updated gated ratio to 0.9156 (progress: 84.4%)
Step 2192: Updated gated ratio to 0.9156 (progress: 84.4%)
Step 2192: Updated gated ratio to 0.9156 (progress: 84.4%)
Step 2192: Updated gated ratio to 0.9156 (progress: 84.4%)Step 2192: Updated gated ratio to 0.9156 (progress: 84.4%)
Step 2192: Updated gated ratio to 0.9156 (progress: 84.4%)Step 2192: Updated gated ratio to 0.9156 (progress: 84.4%)


Step 2192: Updated gated ratio to 0.9156 (progress: 84.4%)
Step 2192: Updated gated ratio to 0.9156 (progress: 84.4%)
Step 2192: Updated gated ratio to 0.9156 (progress: 84.4%)
Step 2192: Updated gated ratio to 0.9156 (progress: 84.4%)
Step 2192: Updated gated ratio to 0.9156 (progress: 84.4%)
Step 2192: Updated gated ratio to 0.9156 (progress: 84.4%)
Step 2192: Updated gated ratio to 0.9156 (progress: 84.4%)
Step 2192: Updated gated ratio to 0.9156 (progress: 84.4%)
{'loss': 1.2412, 'grad_norm': 1.4048659801483154, 'learning_rate': 1.2976174501592313e-05, 'epoch': 0.42}
{'loss': 1.1634, 'grad_norm': 1.274363398551941, 'learning_rate': 1.2970225427528098e-05, 'epoch': 0.42}
{'loss': 1.1396, 'grad_norm': 1.6446596384048462, 'learning_rate': 1.2964275200320104e-05, 'epoch': 0.42}
{'loss': 1.0971, 'grad_norm': 1.569021224975586, 'learning_rate': 1.2958323822278413e-05, 'epoch': 0.42}
{'loss': 1.2172, 'grad_norm': 1.9991391897201538, 'learning_rate': 1.2952371295713558e-05, 'epoch': 0.42}
Step 2197: Updated gated ratio to 0.9154 (progress: 84.6%)
Step 2197: Updated gated ratio to 0.9154 (progress: 84.6%)
Step 2197: Updated gated ratio to 0.9154 (progress: 84.6%)
Step 2197: Updated gated ratio to 0.9154 (progress: 84.6%)
Step 2197: Updated gated ratio to 0.9154 (progress: 84.6%)
Step 2197: Updated gated ratio to 0.9154 (progress: 84.6%)
Step 2197: Updated gated ratio to 0.9154 (progress: 84.6%)
Step 2197: Updated gated ratio to 0.9154 (progress: 84.6%)
Step 2197: Updated gated ratio to 0.9154 (progress: 84.6%)
Step 2197: Updated gated ratio to 0.9154 (progress: 84.6%)
Step 2197: Updated gated ratio to 0.9154 (progress: 84.6%)
Step 2197: Updated gated ratio to 0.9154 (progress: 84.6%)
Step 2197: Updated gated ratio to 0.9154 (progress: 84.6%)
Step 2197: Updated gated ratio to 0.9154 (progress: 84.6%)
Step 2197: Updated gated ratio to 0.9154 (progress: 84.6%)
Step 2197: Updated gated ratio to 0.9154 (progress: 84.6%)
{'loss': 1.0781, 'grad_norm': 1.55522620677948, 'learning_rate': 1.2946417622936512e-05, 'epoch': 0.42}
{'loss': 1.1449, 'grad_norm': 1.7277355194091797, 'learning_rate': 1.2940462806258696e-05, 'epoch': 0.42}
{'loss': 1.0119, 'grad_norm': 1.5081491470336914, 'learning_rate': 1.2934506847991976e-05, 'epoch': 0.42}
{'loss': 0.9224, 'grad_norm': 1.6211200952529907, 'learning_rate': 1.2928549750448661e-05, 'epoch': 0.42}
{'loss': 1.1037, 'grad_norm': 1.4923365116119385, 'learning_rate': 1.2922591515941498e-05, 'epoch': 0.42}
Step 2202: Updated gated ratio to 0.9152 (progress: 84.8%)
Step 2202: Updated gated ratio to 0.9152 (progress: 84.8%)
Step 2202: Updated gated ratio to 0.9152 (progress: 84.8%)
Step 2202: Updated gated ratio to 0.9152 (progress: 84.8%)
Step 2202: Updated gated ratio to 0.9152 (progress: 84.8%)
Step 2202: Updated gated ratio to 0.9152 (progress: 84.8%)
Step 2202: Updated gated ratio to 0.9152 (progress: 84.8%)
Step 2202: Updated gated ratio to 0.9152 (progress: 84.8%)
Step 2202: Updated gated ratio to 0.9152 (progress: 84.8%)
Step 2202: Updated gated ratio to 0.9152 (progress: 84.8%)
Step 2202: Updated gated ratio to 0.9152 (progress: 84.8%)
Step 2202: Updated gated ratio to 0.9152 (progress: 84.8%)
Step 2202: Updated gated ratio to 0.9152 (progress: 84.8%)
Step 2202: Updated gated ratio to 0.9152 (progress: 84.8%)
Step 2202: Updated gated ratio to 0.9152 (progress: 84.8%)
Step 2202: Updated gated ratio to 0.9152 (progress: 84.8%)
{'loss': 1.0211, 'grad_norm': 1.4172965288162231, 'learning_rate': 1.2916632146783683e-05, 'epoch': 0.42}
{'loss': 1.0962, 'grad_norm': 1.5218530893325806, 'learning_rate': 1.2910671645288841e-05, 'epoch': 0.42}
{'loss': 1.0547, 'grad_norm': 1.4652719497680664, 'learning_rate': 1.2904710013771054e-05, 'epoch': 0.42}
{'loss': 1.0989, 'grad_norm': 1.5728938579559326, 'learning_rate': 1.2898747254544826e-05, 'epoch': 0.42}
{'loss': 1.1714, 'grad_norm': 1.5280364751815796, 'learning_rate': 1.2892783369925105e-05, 'epoch': 0.42}
{'loss': 1.2012, 'grad_norm': 1.3087247610092163, 'learning_rate': 1.2886818362227283e-05, 'epoch': 0.42}
Step 2208: Updated gated ratio to 0.9150 (progress: 85.0%)
Step 2208: Updated gated ratio to 0.9150 (progress: 85.0%)
Step 2208: Updated gated ratio to 0.9150 (progress: 85.0%)
Step 2208: Updated gated ratio to 0.9150 (progress: 85.0%)
Step 2208: Updated gated ratio to 0.9150 (progress: 85.0%)
Step 2208: Updated gated ratio to 0.9150 (progress: 85.0%)
Step 2208: Updated gated ratio to 0.9150 (progress: 85.0%)
Step 2208: Updated gated ratio to 0.9150 (progress: 85.0%)
Step 2208: Updated gated ratio to 0.9150 (progress: 85.0%)
Step 2208: Updated gated ratio to 0.9150 (progress: 85.0%)
Step 2208: Updated gated ratio to 0.9150 (progress: 85.0%)
Step 2208: Updated gated ratio to 0.9150 (progress: 85.0%)
Step 2208: Updated gated ratio to 0.9150 (progress: 85.0%)
Step 2208: Updated gated ratio to 0.9150 (progress: 85.0%)
Step 2208: Updated gated ratio to 0.9150 (progress: 85.0%)
Step 2208: Updated gated ratio to 0.9150 (progress: 85.0%)
{'loss': 1.052, 'grad_norm': 1.5856444835662842, 'learning_rate': 1.2880852233767174e-05, 'epoch': 0.42}
{'loss': 1.0768, 'grad_norm': 1.604753017425537, 'learning_rate': 1.2874884986861038e-05, 'epoch': 0.43}
{'loss': 1.2116, 'grad_norm': 1.676680088043213, 'learning_rate': 1.2868916623825561e-05, 'epoch': 0.43}
{'loss': 1.1706, 'grad_norm': 1.720359206199646, 'learning_rate': 1.2862947146977876e-05, 'epoch': 0.43}
{'loss': 1.0762, 'grad_norm': 1.5660817623138428, 'learning_rate': 1.2856976558635532e-05, 'epoch': 0.43}
Step 2213: Updated gated ratio to 0.9148 (progress: 85.2%)
Step 2213: Updated gated ratio to 0.9148 (progress: 85.2%)
Step 2213: Updated gated ratio to 0.9148 (progress: 85.2%)
Step 2213: Updated gated ratio to 0.9148 (progress: 85.2%)
Step 2213: Updated gated ratio to 0.9148 (progress: 85.2%)
Step 2213: Updated gated ratio to 0.9148 (progress: 85.2%)
Step 2213: Updated gated ratio to 0.9148 (progress: 85.2%)
Step 2213: Updated gated ratio to 0.9148 (progress: 85.2%)
Step 2213: Updated gated ratio to 0.9148 (progress: 85.2%)
Step 2213: Updated gated ratio to 0.9148 (progress: 85.2%)
Step 2213: Updated gated ratio to 0.9148 (progress: 85.2%)
Step 2213: Updated gated ratio to 0.9148 (progress: 85.2%)
Step 2213: Updated gated ratio to 0.9148 (progress: 85.2%)
Step 2213: Updated gated ratio to 0.9148 (progress: 85.2%)
Step 2213: Updated gated ratio to 0.9148 (progress: 85.2%)
Step 2213: Updated gated ratio to 0.9148 (progress: 85.2%)
{'loss': 1.1175, 'grad_norm': 1.476656198501587, 'learning_rate': 1.2851004861116519e-05, 'epoch': 0.43}
{'loss': 1.1598, 'grad_norm': 1.6373265981674194, 'learning_rate': 1.2845032056739257e-05, 'epoch': 0.43}
{'loss': 1.1487, 'grad_norm': 1.646134376525879, 'learning_rate': 1.2839058147822595e-05, 'epoch': 0.43}
{'loss': 1.0443, 'grad_norm': 1.6706947088241577, 'learning_rate': 1.2833083136685803e-05, 'epoch': 0.43}
{'loss': 1.2124, 'grad_norm': 1.5236189365386963, 'learning_rate': 1.2827107025648595e-05, 'epoch': 0.43}
Step 2218: Updated gated ratio to 0.9146 (progress: 85.4%)
Step 2218: Updated gated ratio to 0.9146 (progress: 85.4%)
Step 2218: Updated gated ratio to 0.9146 (progress: 85.4%)
Step 2218: Updated gated ratio to 0.9146 (progress: 85.4%)
Step 2218: Updated gated ratio to 0.9146 (progress: 85.4%)
Step 2218: Updated gated ratio to 0.9146 (progress: 85.4%)
Step 2218: Updated gated ratio to 0.9146 (progress: 85.4%)
Step 2218: Updated gated ratio to 0.9146 (progress: 85.4%)
Step 2218: Updated gated ratio to 0.9146 (progress: 85.4%)
Step 2218: Updated gated ratio to 0.9146 (progress: 85.4%)
Step 2218: Updated gated ratio to 0.9146 (progress: 85.4%)
Step 2218: Updated gated ratio to 0.9146 (progress: 85.4%)
Step 2218: Updated gated ratio to 0.9146 (progress: 85.4%)
Step 2218: Updated gated ratio to 0.9146 (progress: 85.4%)
Step 2218: Updated gated ratio to 0.9146 (progress: 85.4%)
Step 2218: Updated gated ratio to 0.9146 (progress: 85.4%)
{'loss': 1.2068, 'grad_norm': 1.6389161348342896, 'learning_rate': 1.2821129817031099e-05, 'epoch': 0.43}
{'loss': 1.1791, 'grad_norm': 1.4928032159805298, 'learning_rate': 1.2815151513153874e-05, 'epoch': 0.43}
{'loss': 1.1271, 'grad_norm': 1.681251049041748, 'learning_rate': 1.2809172116337903e-05, 'epoch': 0.43}
{'loss': 1.157, 'grad_norm': 1.5670433044433594, 'learning_rate': 1.2803191628904594e-05, 'epoch': 0.43}
{'loss': 1.0511, 'grad_norm': 1.5259383916854858, 'learning_rate': 1.2797210053175779e-05, 'epoch': 0.43}
Step 2223: Updated gated ratio to 0.9144 (progress: 85.6%)
Step 2223: Updated gated ratio to 0.9144 (progress: 85.6%)
Step 2223: Updated gated ratio to 0.9144 (progress: 85.6%)
Step 2223: Updated gated ratio to 0.9144 (progress: 85.6%)
Step 2223: Updated gated ratio to 0.9144 (progress: 85.6%)
Step 2223: Updated gated ratio to 0.9144 (progress: 85.6%)
Step 2223: Updated gated ratio to 0.9144 (progress: 85.6%)
Step 2223: Updated gated ratio to 0.9144 (progress: 85.6%)
Step 2223: Updated gated ratio to 0.9144 (progress: 85.6%)
Step 2223: Updated gated ratio to 0.9144 (progress: 85.6%)
Step 2223: Updated gated ratio to 0.9144 (progress: 85.6%)
Step 2223: Updated gated ratio to 0.9144 (progress: 85.6%)
Step 2223: Updated gated ratio to 0.9144 (progress: 85.6%)
Step 2223: Updated gated ratio to 0.9144 (progress: 85.6%)
Step 2223: Updated gated ratio to 0.9144 (progress: 85.6%)
Step 2223: Updated gated ratio to 0.9144 (progress: 85.6%)
{'loss': 1.113, 'grad_norm': 1.6783826351165771, 'learning_rate': 1.2791227391473706e-05, 'epoch': 0.43}
{'loss': 1.0858, 'grad_norm': 1.5821870565414429, 'learning_rate': 1.2785243646121059e-05, 'epoch': 0.43}
{'loss': 1.064, 'grad_norm': 1.5956451892852783, 'learning_rate': 1.277925881944093e-05, 'epoch': 0.43}
{'loss': 1.1916, 'grad_norm': 1.2496198415756226, 'learning_rate': 1.2773272913756833e-05, 'epoch': 0.43}
{'loss': 1.1437, 'grad_norm': 1.6246895790100098, 'learning_rate': 1.2767285931392705e-05, 'epoch': 0.43}
Step 2228: Updated gated ratio to 0.9142 (progress: 85.8%)
Step 2228: Updated gated ratio to 0.9142 (progress: 85.8%)
Step 2228: Updated gated ratio to 0.9142 (progress: 85.8%)
Step 2228: Updated gated ratio to 0.9142 (progress: 85.8%)
Step 2228: Updated gated ratio to 0.9142 (progress: 85.8%)
Step 2228: Updated gated ratio to 0.9142 (progress: 85.8%)
Step 2228: Updated gated ratio to 0.9142 (progress: 85.8%)
Step 2228: Updated gated ratio to 0.9142 (progress: 85.8%)
Step 2228: Updated gated ratio to 0.9142 (progress: 85.8%)
Step 2228: Updated gated ratio to 0.9142 (progress: 85.8%)
Step 2228: Updated gated ratio to 0.9142 (progress: 85.8%)
Step 2228: Updated gated ratio to 0.9142 (progress: 85.8%)
Step 2228: Updated gated ratio to 0.9142 (progress: 85.8%)
Step 2228: Updated gated ratio to 0.9142 (progress: 85.8%)
Step 2228: Updated gated ratio to 0.9142 (progress: 85.8%)
Step 2228: Updated gated ratio to 0.9142 (progress: 85.8%)
{'loss': 1.2082, 'grad_norm': 1.3723825216293335, 'learning_rate': 1.27612978746729e-05, 'epoch': 0.43}
{'loss': 1.1783, 'grad_norm': 1.6277812719345093, 'learning_rate': 1.2755308745922182e-05, 'epoch': 0.43}
{'loss': 1.1245, 'grad_norm': 1.675611138343811, 'learning_rate': 1.2749318547465742e-05, 'epoch': 0.43}
{'loss': 0.9988, 'grad_norm': 1.5144984722137451, 'learning_rate': 1.2743327281629181e-05, 'epoch': 0.43}
{'loss': 1.1583, 'grad_norm': 1.7437989711761475, 'learning_rate': 1.2737334950738512e-05, 'epoch': 0.43}
Step 2233: Updated gated ratio to 0.9140 (progress: 86.0%)
Step 2233: Updated gated ratio to 0.9140 (progress: 86.0%)
Step 2233: Updated gated ratio to 0.9140 (progress: 86.0%)
Step 2233: Updated gated ratio to 0.9140 (progress: 86.0%)
Step 2233: Updated gated ratio to 0.9140 (progress: 86.0%)
Step 2233: Updated gated ratio to 0.9140 (progress: 86.0%)
Step 2233: Updated gated ratio to 0.9140 (progress: 86.0%)
Step 2233: Updated gated ratio to 0.9140 (progress: 86.0%)
Step 2233: Updated gated ratio to 0.9140 (progress: 86.0%)
Step 2233: Updated gated ratio to 0.9140 (progress: 86.0%)
Step 2233: Updated gated ratio to 0.9140 (progress: 86.0%)Step 2233: Updated gated ratio to 0.9140 (progress: 86.0%)
Step 2233: Updated gated ratio to 0.9140 (progress: 86.0%)
Step 2233: Updated gated ratio to 0.9140 (progress: 86.0%)
Step 2233: Updated gated ratio to 0.9140 (progress: 86.0%)

Step 2233: Updated gated ratio to 0.9140 (progress: 86.0%)
{'loss': 1.1195, 'grad_norm': 1.5159404277801514, 'learning_rate': 1.273134155712017e-05, 'epoch': 0.43}
{'loss': 1.1966, 'grad_norm': 1.3106623888015747, 'learning_rate': 1.272534710310099e-05, 'epoch': 0.43}
{'loss': 1.12, 'grad_norm': 1.592650055885315, 'learning_rate': 1.2719351591008228e-05, 'epoch': 0.43}
{'loss': 1.138, 'grad_norm': 1.648676872253418, 'learning_rate': 1.2713355023169547e-05, 'epoch': 0.43}
{'loss': 1.0926, 'grad_norm': 1.5272148847579956, 'learning_rate': 1.2707357401913022e-05, 'epoch': 0.43}
{'loss': 1.1184, 'grad_norm': 1.5866920948028564, 'learning_rate': 1.270135872956714e-05, 'epoch': 0.43}
Step 2239: Updated gated ratio to 0.9138 (progress: 86.2%)
Step 2239: Updated gated ratio to 0.9138 (progress: 86.2%)
Step 2239: Updated gated ratio to 0.9138 (progress: 86.2%)
Step 2239: Updated gated ratio to 0.9138 (progress: 86.2%)
Step 2239: Updated gated ratio to 0.9138 (progress: 86.2%)
Step 2239: Updated gated ratio to 0.9138 (progress: 86.2%)
Step 2239: Updated gated ratio to 0.9138 (progress: 86.2%)Step 2239: Updated gated ratio to 0.9138 (progress: 86.2%)

Step 2239: Updated gated ratio to 0.9138 (progress: 86.2%)
Step 2239: Updated gated ratio to 0.9138 (progress: 86.2%)
Step 2239: Updated gated ratio to 0.9138 (progress: 86.2%)
Step 2239: Updated gated ratio to 0.9138 (progress: 86.2%)
Step 2239: Updated gated ratio to 0.9138 (progress: 86.2%)
Step 2239: Updated gated ratio to 0.9138 (progress: 86.2%)
Step 2239: Updated gated ratio to 0.9138 (progress: 86.2%)
Step 2239: Updated gated ratio to 0.9138 (progress: 86.2%)
{'loss': 1.2768, 'grad_norm': 1.722832202911377, 'learning_rate': 1.2695359008460785e-05, 'epoch': 0.43}
{'loss': 1.105, 'grad_norm': 1.6489918231964111, 'learning_rate': 1.2689358240923264e-05, 'epoch': 0.43}
{'loss': 1.1369, 'grad_norm': 1.602952241897583, 'learning_rate': 1.2683356429284273e-05, 'epoch': 0.43}
{'loss': 1.2097, 'grad_norm': 1.6503806114196777, 'learning_rate': 1.2677353575873926e-05, 'epoch': 0.43}
{'loss': 1.2552, 'grad_norm': 1.455953598022461, 'learning_rate': 1.2671349683022736e-05, 'epoch': 0.43}
Step 2244: Updated gated ratio to 0.9136 (progress: 86.4%)
Step 2244: Updated gated ratio to 0.9136 (progress: 86.4%)
Step 2244: Updated gated ratio to 0.9136 (progress: 86.4%)
Step 2244: Updated gated ratio to 0.9136 (progress: 86.4%)Step 2244: Updated gated ratio to 0.9136 (progress: 86.4%)

Step 2244: Updated gated ratio to 0.9136 (progress: 86.4%)
Step 2244: Updated gated ratio to 0.9136 (progress: 86.4%)
Step 2244: Updated gated ratio to 0.9136 (progress: 86.4%)
Step 2244: Updated gated ratio to 0.9136 (progress: 86.4%)
Step 2244: Updated gated ratio to 0.9136 (progress: 86.4%)Step 2244: Updated gated ratio to 0.9136 (progress: 86.4%)

Step 2244: Updated gated ratio to 0.9136 (progress: 86.4%)
Step 2244: Updated gated ratio to 0.9136 (progress: 86.4%)
Step 2244: Updated gated ratio to 0.9136 (progress: 86.4%)
Step 2244: Updated gated ratio to 0.9136 (progress: 86.4%)
Step 2244: Updated gated ratio to 0.9136 (progress: 86.4%)
{'loss': 1.0605, 'grad_norm': 1.6139554977416992, 'learning_rate': 1.2665344753061622e-05, 'epoch': 0.43}
{'loss': 1.1468, 'grad_norm': 1.631230115890503, 'learning_rate': 1.2659338788321904e-05, 'epoch': 0.43}
{'loss': 1.1398, 'grad_norm': 1.5614991188049316, 'learning_rate': 1.2653331791135308e-05, 'epoch': 0.43}
{'loss': 1.0928, 'grad_norm': 1.4685163497924805, 'learning_rate': 1.2647323763833952e-05, 'epoch': 0.43}
{'loss': 1.1857, 'grad_norm': 1.623766541481018, 'learning_rate': 1.264131470875036e-05, 'epoch': 0.43}
Step 2249: Updated gated ratio to 0.9134 (progress: 86.6%)
Step 2249: Updated gated ratio to 0.9134 (progress: 86.6%)
Step 2249: Updated gated ratio to 0.9134 (progress: 86.6%)
Step 2249: Updated gated ratio to 0.9134 (progress: 86.6%)
Step 2249: Updated gated ratio to 0.9134 (progress: 86.6%)
Step 2249: Updated gated ratio to 0.9134 (progress: 86.6%)
Step 2249: Updated gated ratio to 0.9134 (progress: 86.6%)
Step 2249: Updated gated ratio to 0.9134 (progress: 86.6%)
Step 2249: Updated gated ratio to 0.9134 (progress: 86.6%)
Step 2249: Updated gated ratio to 0.9134 (progress: 86.6%)
Step 2249: Updated gated ratio to 0.9134 (progress: 86.6%)
Step 2249: Updated gated ratio to 0.9134 (progress: 86.6%)
Step 2249: Updated gated ratio to 0.9134 (progress: 86.6%)
Step 2249: Updated gated ratio to 0.9134 (progress: 86.6%)
Step 2249: Updated gated ratio to 0.9134 (progress: 86.6%)
Step 2249: Updated gated ratio to 0.9134 (progress: 86.6%)
{'loss': 1.1578, 'grad_norm': 1.5956342220306396, 'learning_rate': 1.2635304628217452e-05, 'epoch': 0.43}
{'loss': 1.0516, 'grad_norm': 1.548423409461975, 'learning_rate': 1.2629293524568555e-05, 'epoch': 0.43}
{'loss': 1.0701, 'grad_norm': 1.5073506832122803, 'learning_rate': 1.2623281400137383e-05, 'epoch': 0.43}
{'loss': 1.2137, 'grad_norm': 1.2930837869644165, 'learning_rate': 1.2617268257258051e-05, 'epoch': 0.43}
{'loss': 1.1584, 'grad_norm': 1.6271647214889526, 'learning_rate': 1.2611254098265063e-05, 'epoch': 0.43}
Step 2254: Updated gated ratio to 0.9132 (progress: 86.8%)
Step 2254: Updated gated ratio to 0.9132 (progress: 86.8%)
Step 2254: Updated gated ratio to 0.9132 (progress: 86.8%)
Step 2254: Updated gated ratio to 0.9132 (progress: 86.8%)
Step 2254: Updated gated ratio to 0.9132 (progress: 86.8%)
Step 2254: Updated gated ratio to 0.9132 (progress: 86.8%)
Step 2254: Updated gated ratio to 0.9132 (progress: 86.8%)
Step 2254: Updated gated ratio to 0.9132 (progress: 86.8%)
Step 2254: Updated gated ratio to 0.9132 (progress: 86.8%)
Step 2254: Updated gated ratio to 0.9132 (progress: 86.8%)
Step 2254: Updated gated ratio to 0.9132 (progress: 86.8%)
Step 2254: Updated gated ratio to 0.9132 (progress: 86.8%)
Step 2254: Updated gated ratio to 0.9132 (progress: 86.8%)
Step 2254: Updated gated ratio to 0.9132 (progress: 86.8%)
Step 2254: Updated gated ratio to 0.9132 (progress: 86.8%)
Step 2254: Updated gated ratio to 0.9132 (progress: 86.8%)
{'loss': 1.087, 'grad_norm': 1.5842646360397339, 'learning_rate': 1.2605238925493326e-05, 'epoch': 0.43}
{'loss': 1.1243, 'grad_norm': 1.6830204725265503, 'learning_rate': 1.2599222741278136e-05, 'epoch': 0.43}
{'loss': 1.0084, 'grad_norm': 1.475326418876648, 'learning_rate': 1.2593205547955185e-05, 'epoch': 0.43}
{'loss': 1.2138, 'grad_norm': 1.324803352355957, 'learning_rate': 1.2587187347860554e-05, 'epoch': 0.43}
{'loss': 1.2365, 'grad_norm': 1.2967830896377563, 'learning_rate': 1.2581168143330716e-05, 'epoch': 0.43}
Step 2259: Updated gated ratio to 0.9130 (progress: 87.0%)
Step 2259: Updated gated ratio to 0.9130 (progress: 87.0%)
Step 2259: Updated gated ratio to 0.9130 (progress: 87.0%)
Step 2259: Updated gated ratio to 0.9130 (progress: 87.0%)
Step 2259: Updated gated ratio to 0.9130 (progress: 87.0%)
Step 2259: Updated gated ratio to 0.9130 (progress: 87.0%)
Step 2259: Updated gated ratio to 0.9130 (progress: 87.0%)
Step 2259: Updated gated ratio to 0.9130 (progress: 87.0%)
Step 2259: Updated gated ratio to 0.9130 (progress: 87.0%)
Step 2259: Updated gated ratio to 0.9130 (progress: 87.0%)
Step 2259: Updated gated ratio to 0.9130 (progress: 87.0%)
Step 2259: Updated gated ratio to 0.9130 (progress: 87.0%)
Step 2259: Updated gated ratio to 0.9130 (progress: 87.0%)
Step 2259: Updated gated ratio to 0.9130 (progress: 87.0%)
Step 2259: Updated gated ratio to 0.9130 (progress: 87.0%)
Step 2259: Updated gated ratio to 0.9130 (progress: 87.0%)
{'loss': 1.0922, 'grad_norm': 1.6341758966445923, 'learning_rate': 1.2575147936702531e-05, 'epoch': 0.43}
{'loss': 1.2847, 'grad_norm': 1.256313443183899, 'learning_rate': 1.2569126730313255e-05, 'epoch': 0.43}
{'loss': 1.1174, 'grad_norm': 1.481496810913086, 'learning_rate': 1.2563104526500523e-05, 'epoch': 0.44}
{'loss': 1.0665, 'grad_norm': 1.5225794315338135, 'learning_rate': 1.2557081327602361e-05, 'epoch': 0.44}
{'loss': 1.0332, 'grad_norm': 1.424452543258667, 'learning_rate': 1.2551057135957187e-05, 'epoch': 0.44}
{'loss': 1.2072, 'grad_norm': 1.5748584270477295, 'learning_rate': 1.2545031953903796e-05, 'epoch': 0.44}
Step 2265: Updated gated ratio to 0.9128 (progress: 87.2%)
Step 2265: Updated gated ratio to 0.9128 (progress: 87.2%)
Step 2265: Updated gated ratio to 0.9128 (progress: 87.2%)
Step 2265: Updated gated ratio to 0.9128 (progress: 87.2%)
Step 2265: Updated gated ratio to 0.9128 (progress: 87.2%)
Step 2265: Updated gated ratio to 0.9128 (progress: 87.2%)
Step 2265: Updated gated ratio to 0.9128 (progress: 87.2%)
Step 2265: Updated gated ratio to 0.9128 (progress: 87.2%)
Step 2265: Updated gated ratio to 0.9128 (progress: 87.2%)
Step 2265: Updated gated ratio to 0.9128 (progress: 87.2%)
Step 2265: Updated gated ratio to 0.9128 (progress: 87.2%)
Step 2265: Updated gated ratio to 0.9128 (progress: 87.2%)
Step 2265: Updated gated ratio to 0.9128 (progress: 87.2%)
Step 2265: Updated gated ratio to 0.9128 (progress: 87.2%)
Step 2265: Updated gated ratio to 0.9128 (progress: 87.2%)
Step 2265: Updated gated ratio to 0.9128 (progress: 87.2%)
{'loss': 1.245, 'grad_norm': 1.6573035717010498, 'learning_rate': 1.2539005783781374e-05, 'epoch': 0.44}
{'loss': 1.1784, 'grad_norm': 1.6475200653076172, 'learning_rate': 1.2532978627929486e-05, 'epoch': 0.44}
{'loss': 1.0756, 'grad_norm': 1.595116138458252, 'learning_rate': 1.2526950488688083e-05, 'epoch': 0.44}
{'loss': 1.1571, 'grad_norm': 1.555788516998291, 'learning_rate': 1.2520921368397492e-05, 'epoch': 0.44}
{'loss': 1.0867, 'grad_norm': 1.5446979999542236, 'learning_rate': 1.2514891269398429e-05, 'epoch': 0.44}
Step 2270: Updated gated ratio to 0.9126 (progress: 87.4%)
Step 2270: Updated gated ratio to 0.9126 (progress: 87.4%)
Step 2270: Updated gated ratio to 0.9126 (progress: 87.4%)
Step 2270: Updated gated ratio to 0.9126 (progress: 87.4%)
Step 2270: Updated gated ratio to 0.9126 (progress: 87.4%)
Step 2270: Updated gated ratio to 0.9126 (progress: 87.4%)
Step 2270: Updated gated ratio to 0.9126 (progress: 87.4%)Step 2270: Updated gated ratio to 0.9126 (progress: 87.4%)

Step 2270: Updated gated ratio to 0.9126 (progress: 87.4%)
Step 2270: Updated gated ratio to 0.9126 (progress: 87.4%)
Step 2270: Updated gated ratio to 0.9126 (progress: 87.4%)
Step 2270: Updated gated ratio to 0.9126 (progress: 87.4%)
Step 2270: Updated gated ratio to 0.9126 (progress: 87.4%)
Step 2270: Updated gated ratio to 0.9126 (progress: 87.4%)
Step 2270: Updated gated ratio to 0.9126 (progress: 87.4%)
Step 2270: Updated gated ratio to 0.9126 (progress: 87.4%)
{'loss': 1.0538, 'grad_norm': 1.5657094717025757, 'learning_rate': 1.2508860194031986e-05, 'epoch': 0.44}
Error with image file is truncated (152 bytes not processed)
{'loss': 1.2107, 'grad_norm': 1.6607528924942017, 'learning_rate': 1.2502828144639629e-05, 'epoch': 0.44}
{'loss': 1.1851, 'grad_norm': 1.7710468769073486, 'learning_rate': 1.2496795123563218e-05, 'epoch': 0.44}
{'loss': 1.0578, 'grad_norm': 1.4665296077728271, 'learning_rate': 1.249076113314497e-05, 'epoch': 0.44}
{'loss': 1.0981, 'grad_norm': 1.510832667350769, 'learning_rate': 1.248472617572749e-05, 'epoch': 0.44}
Step 2275: Updated gated ratio to 0.9124 (progress: 87.6%)
Step 2275: Updated gated ratio to 0.9124 (progress: 87.6%)Step 2275: Updated gated ratio to 0.9124 (progress: 87.6%)

Step 2275: Updated gated ratio to 0.9124 (progress: 87.6%)
Step 2275: Updated gated ratio to 0.9124 (progress: 87.6%)
Step 2275: Updated gated ratio to 0.9124 (progress: 87.6%)Step 2275: Updated gated ratio to 0.9124 (progress: 87.6%)

Step 2275: Updated gated ratio to 0.9124 (progress: 87.6%)
Step 2275: Updated gated ratio to 0.9124 (progress: 87.6%)
Step 2275: Updated gated ratio to 0.9124 (progress: 87.6%)
Step 2275: Updated gated ratio to 0.9124 (progress: 87.6%)
Step 2275: Updated gated ratio to 0.9124 (progress: 87.6%)
Step 2275: Updated gated ratio to 0.9124 (progress: 87.6%)
Step 2275: Updated gated ratio to 0.9124 (progress: 87.6%)
Step 2275: Updated gated ratio to 0.9124 (progress: 87.6%)
Step 2275: Updated gated ratio to 0.9124 (progress: 87.6%)
{'loss': 1.1426, 'grad_norm': 1.7299413681030273, 'learning_rate': 1.2478690253653756e-05, 'epoch': 0.44}
{'loss': 1.1641, 'grad_norm': 1.663618564605713, 'learning_rate': 1.2472653369267122e-05, 'epoch': 0.44}
{'loss': 1.1071, 'grad_norm': 1.6496087312698364, 'learning_rate': 1.2466615524911316e-05, 'epoch': 0.44}
{'loss': 1.071, 'grad_norm': 1.5711331367492676, 'learning_rate': 1.2460576722930432e-05, 'epoch': 0.44}
{'loss': 1.0943, 'grad_norm': 1.7202640771865845, 'learning_rate': 1.2454536965668949e-05, 'epoch': 0.44}
Step 2280: Updated gated ratio to 0.9122 (progress: 87.8%)
Step 2280: Updated gated ratio to 0.9122 (progress: 87.8%)
Step 2280: Updated gated ratio to 0.9122 (progress: 87.8%)
Step 2280: Updated gated ratio to 0.9122 (progress: 87.8%)
Step 2280: Updated gated ratio to 0.9122 (progress: 87.8%)
Step 2280: Updated gated ratio to 0.9122 (progress: 87.8%)
Step 2280: Updated gated ratio to 0.9122 (progress: 87.8%)
Step 2280: Updated gated ratio to 0.9122 (progress: 87.8%)
Step 2280: Updated gated ratio to 0.9122 (progress: 87.8%)
Step 2280: Updated gated ratio to 0.9122 (progress: 87.8%)
Step 2280: Updated gated ratio to 0.9122 (progress: 87.8%)
Step 2280: Updated gated ratio to 0.9122 (progress: 87.8%)
Step 2280: Updated gated ratio to 0.9122 (progress: 87.8%)
Step 2280: Updated gated ratio to 0.9122 (progress: 87.8%)
Step 2280: Updated gated ratio to 0.9122 (progress: 87.8%)
Step 2280: Updated gated ratio to 0.9122 (progress: 87.8%)
{'loss': 1.1682, 'grad_norm': 1.581684947013855, 'learning_rate': 1.24484962554717e-05, 'epoch': 0.44}
{'loss': 1.1284, 'grad_norm': 1.4443752765655518, 'learning_rate': 1.24424545946839e-05, 'epoch': 0.44}
{'loss': 1.0922, 'grad_norm': 1.5185662508010864, 'learning_rate': 1.2436411985651131e-05, 'epoch': 0.44}
{'loss': 1.1811, 'grad_norm': 1.5977357625961304, 'learning_rate': 1.2430368430719342e-05, 'epoch': 0.44}
{'loss': 1.0735, 'grad_norm': 1.2139753103256226, 'learning_rate': 1.242432393223485e-05, 'epoch': 0.44}
Step 2285: Updated gated ratio to 0.9120 (progress: 88.0%)
Step 2285: Updated gated ratio to 0.9120 (progress: 88.0%)
Step 2285: Updated gated ratio to 0.9120 (progress: 88.0%)
Step 2285: Updated gated ratio to 0.9120 (progress: 88.0%)
Step 2285: Updated gated ratio to 0.9120 (progress: 88.0%)
Step 2285: Updated gated ratio to 0.9120 (progress: 88.0%)
Step 2285: Updated gated ratio to 0.9120 (progress: 88.0%)
Step 2285: Updated gated ratio to 0.9120 (progress: 88.0%)
Step 2285: Updated gated ratio to 0.9120 (progress: 88.0%)
Step 2285: Updated gated ratio to 0.9120 (progress: 88.0%)
Step 2285: Updated gated ratio to 0.9120 (progress: 88.0%)
Step 2285: Updated gated ratio to 0.9120 (progress: 88.0%)
Step 2285: Updated gated ratio to 0.9120 (progress: 88.0%)
Step 2285: Updated gated ratio to 0.9120 (progress: 88.0%)
Step 2285: Updated gated ratio to 0.9120 (progress: 88.0%)
Step 2285: Updated gated ratio to 0.9120 (progress: 88.0%)
{'loss': 1.1919, 'grad_norm': 1.597192406654358, 'learning_rate': 1.2418278492544328e-05, 'epoch': 0.44}
{'loss': 1.092, 'grad_norm': 1.4939548969268799, 'learning_rate': 1.2412232113994841e-05, 'epoch': 0.44}
{'loss': 1.1868, 'grad_norm': 1.5927441120147705, 'learning_rate': 1.2406184798933786e-05, 'epoch': 0.44}
{'loss': 1.1947, 'grad_norm': 1.3551652431488037, 'learning_rate': 1.2400136549708945e-05, 'epoch': 0.44}
{'loss': 1.1516, 'grad_norm': 1.5565961599349976, 'learning_rate': 1.239408736866846e-05, 'epoch': 0.44}
{'loss': 1.122, 'grad_norm': 1.7018910646438599, 'learning_rate': 1.2388037258160823e-05, 'epoch': 0.44}
Step 2291: Updated gated ratio to 0.9118 (progress: 88.2%)
Step 2291: Updated gated ratio to 0.9118 (progress: 88.2%)
Step 2291: Updated gated ratio to 0.9118 (progress: 88.2%)
Step 2291: Updated gated ratio to 0.9118 (progress: 88.2%)
Step 2291: Updated gated ratio to 0.9118 (progress: 88.2%)
Step 2291: Updated gated ratio to 0.9118 (progress: 88.2%)
Step 2291: Updated gated ratio to 0.9118 (progress: 88.2%)
Step 2291: Updated gated ratio to 0.9118 (progress: 88.2%)
Step 2291: Updated gated ratio to 0.9118 (progress: 88.2%)
Step 2291: Updated gated ratio to 0.9118 (progress: 88.2%)
Step 2291: Updated gated ratio to 0.9118 (progress: 88.2%)
Step 2291: Updated gated ratio to 0.9118 (progress: 88.2%)
Step 2291: Updated gated ratio to 0.9118 (progress: 88.2%)
Step 2291: Updated gated ratio to 0.9118 (progress: 88.2%)
Step 2291: Updated gated ratio to 0.9118 (progress: 88.2%)
Step 2291: Updated gated ratio to 0.9118 (progress: 88.2%)
{'loss': 1.1084, 'grad_norm': 1.5996475219726562, 'learning_rate': 1.23819862205349e-05, 'epoch': 0.44}
{'loss': 1.0759, 'grad_norm': 1.6742950677871704, 'learning_rate': 1.2375934258139917e-05, 'epoch': 0.44}
Step 2293: Updated gated ratio to 0.9117 (progress: 88.3%)
Step 2293: Updated gated ratio to 0.9117 (progress: 88.3%)
Step 2293: Updated gated ratio to 0.9117 (progress: 88.3%)
Step 2293: Updated gated ratio to 0.9117 (progress: 88.3%)
Step 2293: Updated gated ratio to 0.9117 (progress: 88.3%)
Step 2293: Updated gated ratio to 0.9117 (progress: 88.3%)
Step 2293: Updated gated ratio to 0.9117 (progress: 88.3%)
Step 2293: Updated gated ratio to 0.9117 (progress: 88.3%)
Step 2293: Updated gated ratio to 0.9117 (progress: 88.3%)
Step 2293: Updated gated ratio to 0.9117 (progress: 88.3%)Step 2293: Updated gated ratio to 0.9117 (progress: 88.3%)

Step 2293: Updated gated ratio to 0.9117 (progress: 88.3%)
Step 2293: Updated gated ratio to 0.9117 (progress: 88.3%)
Step 2293: Updated gated ratio to 0.9117 (progress: 88.3%)
Step 2293: Updated gated ratio to 0.9117 (progress: 88.3%)
Step 2293: Updated gated ratio to 0.9117 (progress: 88.3%)
{'loss': 1.3106, 'grad_norm': 1.399033546447754, 'learning_rate': 1.2369881373325448e-05, 'epoch': 0.44}
{'loss': 1.1203, 'grad_norm': 1.5407114028930664, 'learning_rate': 1.236382756844143e-05, 'epoch': 0.44}
{'loss': 1.2328, 'grad_norm': 1.588837742805481, 'learning_rate': 1.2357772845838159e-05, 'epoch': 0.44}
{'loss': 1.1085, 'grad_norm': 1.5473544597625732, 'learning_rate': 1.2351717207866292e-05, 'epoch': 0.44}
{'loss': 1.0423, 'grad_norm': 1.495418667793274, 'learning_rate': 1.2345660656876832e-05, 'epoch': 0.44}
Step 2298: Updated gated ratio to 0.9115 (progress: 88.5%)
Step 2298: Updated gated ratio to 0.9115 (progress: 88.5%)
Step 2298: Updated gated ratio to 0.9115 (progress: 88.5%)
Step 2298: Updated gated ratio to 0.9115 (progress: 88.5%)
Step 2298: Updated gated ratio to 0.9115 (progress: 88.5%)
Step 2298: Updated gated ratio to 0.9115 (progress: 88.5%)
Step 2298: Updated gated ratio to 0.9115 (progress: 88.5%)Step 2298: Updated gated ratio to 0.9115 (progress: 88.5%)

Step 2298: Updated gated ratio to 0.9115 (progress: 88.5%)
Step 2298: Updated gated ratio to 0.9115 (progress: 88.5%)
Step 2298: Updated gated ratio to 0.9115 (progress: 88.5%)
Step 2298: Updated gated ratio to 0.9115 (progress: 88.5%)
Step 2298: Updated gated ratio to 0.9115 (progress: 88.5%)
Step 2298: Updated gated ratio to 0.9115 (progress: 88.5%)
Step 2298: Updated gated ratio to 0.9115 (progress: 88.5%)
Step 2298: Updated gated ratio to 0.9115 (progress: 88.5%)
{'loss': 1.1217, 'grad_norm': 1.5392745733261108, 'learning_rate': 1.233960319522114e-05, 'epoch': 0.44}
{'loss': 1.1236, 'grad_norm': 1.6623083353042603, 'learning_rate': 1.2333544825250938e-05, 'epoch': 0.44}
{'loss': 1.2077, 'grad_norm': 1.815493106842041, 'learning_rate': 1.2327485549318285e-05, 'epoch': 0.44}
{'loss': 1.2217, 'grad_norm': 1.3906537294387817, 'learning_rate': 1.2321425369775601e-05, 'epoch': 0.44}
{'loss': 1.0488, 'grad_norm': 1.5115467309951782, 'learning_rate': 1.2315364288975665e-05, 'epoch': 0.44}
{'loss': 1.1422, 'grad_norm': 1.509310245513916, 'learning_rate': 1.2309302309271587e-05, 'epoch': 0.44}
Step 2304: Updated gated ratio to 0.9113 (progress: 88.7%)
Step 2304: Updated gated ratio to 0.9113 (progress: 88.7%)
Step 2304: Updated gated ratio to 0.9113 (progress: 88.7%)
Step 2304: Updated gated ratio to 0.9113 (progress: 88.7%)Step 2304: Updated gated ratio to 0.9113 (progress: 88.7%)

Step 2304: Updated gated ratio to 0.9113 (progress: 88.7%)
Step 2304: Updated gated ratio to 0.9113 (progress: 88.7%)Step 2304: Updated gated ratio to 0.9113 (progress: 88.7%)

Step 2304: Updated gated ratio to 0.9113 (progress: 88.7%)
Step 2304: Updated gated ratio to 0.9113 (progress: 88.7%)
Step 2304: Updated gated ratio to 0.9113 (progress: 88.7%)
Step 2304: Updated gated ratio to 0.9113 (progress: 88.7%)
Step 2304: Updated gated ratio to 0.9113 (progress: 88.7%)
Step 2304: Updated gated ratio to 0.9113 (progress: 88.7%)
Step 2304: Updated gated ratio to 0.9113 (progress: 88.7%)
Step 2304: Updated gated ratio to 0.9113 (progress: 88.7%)
{'loss': 1.2461, 'grad_norm': 1.5506588220596313, 'learning_rate': 1.2303239433016842e-05, 'epoch': 0.44}
{'loss': 1.047, 'grad_norm': 1.5646369457244873, 'learning_rate': 1.2297175662565248e-05, 'epoch': 0.44}
{'loss': 1.152, 'grad_norm': 1.5155832767486572, 'learning_rate': 1.229111100027097e-05, 'epoch': 0.44}
{'loss': 1.1207, 'grad_norm': 1.5835102796554565, 'learning_rate': 1.228504544848851e-05, 'epoch': 0.44}
{'loss': 1.1157, 'grad_norm': 1.65647292137146, 'learning_rate': 1.2278979009572736e-05, 'epoch': 0.44}
Step 2309: Updated gated ratio to 0.9111 (progress: 88.9%)
Step 2309: Updated gated ratio to 0.9111 (progress: 88.9%)
Step 2309: Updated gated ratio to 0.9111 (progress: 88.9%)
Step 2309: Updated gated ratio to 0.9111 (progress: 88.9%)
Step 2309: Updated gated ratio to 0.9111 (progress: 88.9%)
Step 2309: Updated gated ratio to 0.9111 (progress: 88.9%)
Step 2309: Updated gated ratio to 0.9111 (progress: 88.9%)
Step 2309: Updated gated ratio to 0.9111 (progress: 88.9%)
Step 2309: Updated gated ratio to 0.9111 (progress: 88.9%)
Step 2309: Updated gated ratio to 0.9111 (progress: 88.9%)
Step 2309: Updated gated ratio to 0.9111 (progress: 88.9%)
Step 2309: Updated gated ratio to 0.9111 (progress: 88.9%)
Step 2309: Updated gated ratio to 0.9111 (progress: 88.9%)
Step 2309: Updated gated ratio to 0.9111 (progress: 88.9%)
Step 2309: Updated gated ratio to 0.9111 (progress: 88.9%)
Step 2309: Updated gated ratio to 0.9111 (progress: 88.9%)
{'loss': 1.1952, 'grad_norm': 1.7251768112182617, 'learning_rate': 1.2272911685878841e-05, 'epoch': 0.44}
{'loss': 1.0903, 'grad_norm': 1.5831115245819092, 'learning_rate': 1.2266843479762372e-05, 'epoch': 0.44}
{'loss': 1.1582, 'grad_norm': 1.6483832597732544, 'learning_rate': 1.2260774393579209e-05, 'epoch': 0.44}
{'loss': 1.2346, 'grad_norm': 2.0066604614257812, 'learning_rate': 1.2254704429685593e-05, 'epoch': 0.44}
{'loss': 0.9795, 'grad_norm': 1.4814658164978027, 'learning_rate': 1.2248633590438084e-05, 'epoch': 0.45}
Step 2314: Updated gated ratio to 0.9109 (progress: 89.1%)
Step 2314: Updated gated ratio to 0.9109 (progress: 89.1%)
Step 2314: Updated gated ratio to 0.9109 (progress: 89.1%)
Step 2314: Updated gated ratio to 0.9109 (progress: 89.1%)
Step 2314: Updated gated ratio to 0.9109 (progress: 89.1%)
Step 2314: Updated gated ratio to 0.9109 (progress: 89.1%)
Step 2314: Updated gated ratio to 0.9109 (progress: 89.1%)
Step 2314: Updated gated ratio to 0.9109 (progress: 89.1%)
Step 2314: Updated gated ratio to 0.9109 (progress: 89.1%)
Step 2314: Updated gated ratio to 0.9109 (progress: 89.1%)
Step 2314: Updated gated ratio to 0.9109 (progress: 89.1%)
Step 2314: Updated gated ratio to 0.9109 (progress: 89.1%)
Step 2314: Updated gated ratio to 0.9109 (progress: 89.1%)
Step 2314: Updated gated ratio to 0.9109 (progress: 89.1%)
Step 2314: Updated gated ratio to 0.9109 (progress: 89.1%)
Step 2314: Updated gated ratio to 0.9109 (progress: 89.1%)
{'loss': 1.2233, 'grad_norm': 1.6663694381713867, 'learning_rate': 1.2242561878193589e-05, 'epoch': 0.45}
{'loss': 1.1465, 'grad_norm': 1.6484384536743164, 'learning_rate': 1.2236489295309362e-05, 'epoch': 0.45}
{'loss': 1.0557, 'grad_norm': 1.4757665395736694, 'learning_rate': 1.2230415844142984e-05, 'epoch': 0.45}
{'loss': 1.0897, 'grad_norm': 1.6478490829467773, 'learning_rate': 1.2224341527052378e-05, 'epoch': 0.45}
{'loss': 1.1276, 'grad_norm': 1.5152384042739868, 'learning_rate': 1.2218266346395811e-05, 'epoch': 0.45}
Step 2319: Updated gated ratio to 0.9107 (progress: 89.3%)
Step 2319: Updated gated ratio to 0.9107 (progress: 89.3%)
Step 2319: Updated gated ratio to 0.9107 (progress: 89.3%)
Step 2319: Updated gated ratio to 0.9107 (progress: 89.3%)
Step 2319: Updated gated ratio to 0.9107 (progress: 89.3%)
Step 2319: Updated gated ratio to 0.9107 (progress: 89.3%)
Step 2319: Updated gated ratio to 0.9107 (progress: 89.3%)
Step 2319: Updated gated ratio to 0.9107 (progress: 89.3%)
Step 2319: Updated gated ratio to 0.9107 (progress: 89.3%)
Step 2319: Updated gated ratio to 0.9107 (progress: 89.3%)
Step 2319: Updated gated ratio to 0.9107 (progress: 89.3%)
Step 2319: Updated gated ratio to 0.9107 (progress: 89.3%)
Step 2319: Updated gated ratio to 0.9107 (progress: 89.3%)
Step 2319: Updated gated ratio to 0.9107 (progress: 89.3%)
Step 2319: Updated gated ratio to 0.9107 (progress: 89.3%)
Step 2319: Updated gated ratio to 0.9107 (progress: 89.3%)
{'loss': 1.1241, 'grad_norm': 1.5995278358459473, 'learning_rate': 1.221219030453187e-05, 'epoch': 0.45}
{'loss': 1.2701, 'grad_norm': 1.760161280632019, 'learning_rate': 1.220611340381948e-05, 'epoch': 0.45}
{'loss': 1.1882, 'grad_norm': 1.3844562768936157, 'learning_rate': 1.2200035646617912e-05, 'epoch': 0.45}
{'loss': 1.0385, 'grad_norm': 1.6408709287643433, 'learning_rate': 1.2193957035286757e-05, 'epoch': 0.45}
{'loss': 1.1176, 'grad_norm': 1.6601399183273315, 'learning_rate': 1.2187877572185937e-05, 'epoch': 0.45}
Step 2324: Updated gated ratio to 0.9105 (progress: 89.5%)
Step 2324: Updated gated ratio to 0.9105 (progress: 89.5%)
Step 2324: Updated gated ratio to 0.9105 (progress: 89.5%)
Step 2324: Updated gated ratio to 0.9105 (progress: 89.5%)
Step 2324: Updated gated ratio to 0.9105 (progress: 89.5%)
Step 2324: Updated gated ratio to 0.9105 (progress: 89.5%)
Step 2324: Updated gated ratio to 0.9105 (progress: 89.5%)
Step 2324: Updated gated ratio to 0.9105 (progress: 89.5%)
Step 2324: Updated gated ratio to 0.9105 (progress: 89.5%)
Step 2324: Updated gated ratio to 0.9105 (progress: 89.5%)
Step 2324: Updated gated ratio to 0.9105 (progress: 89.5%)
Step 2324: Updated gated ratio to 0.9105 (progress: 89.5%)
Step 2324: Updated gated ratio to 0.9105 (progress: 89.5%)
Step 2324: Updated gated ratio to 0.9105 (progress: 89.5%)
Step 2324: Updated gated ratio to 0.9105 (progress: 89.5%)
Step 2324: Updated gated ratio to 0.9105 (progress: 89.5%)
{'loss': 1.1789, 'grad_norm': 1.3850539922714233, 'learning_rate': 1.2181797259675713e-05, 'epoch': 0.45}
{'loss': 1.1121, 'grad_norm': 1.5058212280273438, 'learning_rate': 1.2175716100116677e-05, 'epoch': 0.45}
{'loss': 1.1255, 'grad_norm': 1.5837665796279907, 'learning_rate': 1.2169634095869736e-05, 'epoch': 0.45}
{'loss': 1.1604, 'grad_norm': 1.5260330438613892, 'learning_rate': 1.2163551249296132e-05, 'epoch': 0.45}
{'loss': 1.1575, 'grad_norm': 1.6847209930419922, 'learning_rate': 1.2157467562757443e-05, 'epoch': 0.45}
{'loss': 1.1306, 'grad_norm': 1.7025938034057617, 'learning_rate': 1.2151383038615563e-05, 'epoch': 0.45}
Step 2330: Updated gated ratio to 0.9103 (progress: 89.7%)
Step 2330: Updated gated ratio to 0.9103 (progress: 89.7%)Step 2330: Updated gated ratio to 0.9103 (progress: 89.7%)
Step 2330: Updated gated ratio to 0.9103 (progress: 89.7%)
Step 2330: Updated gated ratio to 0.9103 (progress: 89.7%)
Step 2330: Updated gated ratio to 0.9103 (progress: 89.7%)
Step 2330: Updated gated ratio to 0.9103 (progress: 89.7%)

Step 2330: Updated gated ratio to 0.9103 (progress: 89.7%)
Step 2330: Updated gated ratio to 0.9103 (progress: 89.7%)
Step 2330: Updated gated ratio to 0.9103 (progress: 89.7%)
Step 2330: Updated gated ratio to 0.9103 (progress: 89.7%)
Step 2330: Updated gated ratio to 0.9103 (progress: 89.7%)
Step 2330: Updated gated ratio to 0.9103 (progress: 89.7%)
Step 2330: Updated gated ratio to 0.9103 (progress: 89.7%)
Step 2330: Updated gated ratio to 0.9103 (progress: 89.7%)
Step 2330: Updated gated ratio to 0.9103 (progress: 89.7%)
{'loss': 1.1142, 'grad_norm': 1.4973554611206055, 'learning_rate': 1.214529767923271e-05, 'epoch': 0.45}
{'loss': 1.1236, 'grad_norm': 1.5382519960403442, 'learning_rate': 1.2139211486971436e-05, 'epoch': 0.45}
{'loss': 1.0928, 'grad_norm': 1.495363473892212, 'learning_rate': 1.213312446419461e-05, 'epoch': 0.45}
{'loss': 1.1423, 'grad_norm': 1.5768816471099854, 'learning_rate': 1.2127036613265418e-05, 'epoch': 0.45}
{'loss': 1.1142, 'grad_norm': 1.606221318244934, 'learning_rate': 1.2120947936547375e-05, 'epoch': 0.45}
Step 2335: Updated gated ratio to 0.9101 (progress: 89.9%)
Step 2335: Updated gated ratio to 0.9101 (progress: 89.9%)
Step 2335: Updated gated ratio to 0.9101 (progress: 89.9%)
Step 2335: Updated gated ratio to 0.9101 (progress: 89.9%)
Step 2335: Updated gated ratio to 0.9101 (progress: 89.9%)
Step 2335: Updated gated ratio to 0.9101 (progress: 89.9%)
Step 2335: Updated gated ratio to 0.9101 (progress: 89.9%)
Step 2335: Updated gated ratio to 0.9101 (progress: 89.9%)Step 2335: Updated gated ratio to 0.9101 (progress: 89.9%)

Step 2335: Updated gated ratio to 0.9101 (progress: 89.9%)
Step 2335: Updated gated ratio to 0.9101 (progress: 89.9%)
Step 2335: Updated gated ratio to 0.9101 (progress: 89.9%)
Step 2335: Updated gated ratio to 0.9101 (progress: 89.9%)
Step 2335: Updated gated ratio to 0.9101 (progress: 89.9%)
Step 2335: Updated gated ratio to 0.9101 (progress: 89.9%)
Step 2335: Updated gated ratio to 0.9101 (progress: 89.9%)
{'loss': 1.0316, 'grad_norm': 1.4726136922836304, 'learning_rate': 1.2114858436404322e-05, 'epoch': 0.45}
{'loss': 1.2788, 'grad_norm': 1.5244300365447998, 'learning_rate': 1.2108768115200405e-05, 'epoch': 0.45}
{'loss': 1.0528, 'grad_norm': 1.775288462638855, 'learning_rate': 1.2102676975300095e-05, 'epoch': 0.45}
{'loss': 1.1735, 'grad_norm': 1.6237800121307373, 'learning_rate': 1.209658501906819e-05, 'epoch': 0.45}
{'loss': 1.1985, 'grad_norm': 1.2710968255996704, 'learning_rate': 1.2090492248869795e-05, 'epoch': 0.45}
Step 2340: Updated gated ratio to 0.9099 (progress: 90.1%)
Step 2340: Updated gated ratio to 0.9099 (progress: 90.1%)
Step 2340: Updated gated ratio to 0.9099 (progress: 90.1%)
Step 2340: Updated gated ratio to 0.9099 (progress: 90.1%)
Step 2340: Updated gated ratio to 0.9099 (progress: 90.1%)
Step 2340: Updated gated ratio to 0.9099 (progress: 90.1%)
Step 2340: Updated gated ratio to 0.9099 (progress: 90.1%)
Step 2340: Updated gated ratio to 0.9099 (progress: 90.1%)
Step 2340: Updated gated ratio to 0.9099 (progress: 90.1%)
Step 2340: Updated gated ratio to 0.9099 (progress: 90.1%)
Step 2340: Updated gated ratio to 0.9099 (progress: 90.1%)
Step 2340: Updated gated ratio to 0.9099 (progress: 90.1%)
Step 2340: Updated gated ratio to 0.9099 (progress: 90.1%)
Step 2340: Updated gated ratio to 0.9099 (progress: 90.1%)
Step 2340: Updated gated ratio to 0.9099 (progress: 90.1%)
Step 2340: Updated gated ratio to 0.9099 (progress: 90.1%)
{'loss': 1.1579, 'grad_norm': 1.7151639461517334, 'learning_rate': 1.2084398667070325e-05, 'epoch': 0.45}
{'loss': 1.0778, 'grad_norm': 1.5436742305755615, 'learning_rate': 1.2078304276035527e-05, 'epoch': 0.45}
{'loss': 1.1203, 'grad_norm': 1.5627875328063965, 'learning_rate': 1.2072209078131451e-05, 'epoch': 0.45}
{'loss': 1.0374, 'grad_norm': 1.564760684967041, 'learning_rate': 1.2066113075724461e-05, 'epoch': 0.45}
{'loss': 1.1058, 'grad_norm': 1.5826389789581299, 'learning_rate': 1.206001627118124e-05, 'epoch': 0.45}
Step 2345: Updated gated ratio to 0.9097 (progress: 90.3%)
Step 2345: Updated gated ratio to 0.9097 (progress: 90.3%)
Step 2345: Updated gated ratio to 0.9097 (progress: 90.3%)
Step 2345: Updated gated ratio to 0.9097 (progress: 90.3%)
Step 2345: Updated gated ratio to 0.9097 (progress: 90.3%)
Step 2345: Updated gated ratio to 0.9097 (progress: 90.3%)
Step 2345: Updated gated ratio to 0.9097 (progress: 90.3%)Step 2345: Updated gated ratio to 0.9097 (progress: 90.3%)

Step 2345: Updated gated ratio to 0.9097 (progress: 90.3%)
Step 2345: Updated gated ratio to 0.9097 (progress: 90.3%)
Step 2345: Updated gated ratio to 0.9097 (progress: 90.3%)Step 2345: Updated gated ratio to 0.9097 (progress: 90.3%)

Step 2345: Updated gated ratio to 0.9097 (progress: 90.3%)
Step 2345: Updated gated ratio to 0.9097 (progress: 90.3%)
Step 2345: Updated gated ratio to 0.9097 (progress: 90.3%)
Step 2345: Updated gated ratio to 0.9097 (progress: 90.3%)
{'loss': 1.139, 'grad_norm': 1.7043601274490356, 'learning_rate': 1.2053918666868776e-05, 'epoch': 0.45}
{'loss': 1.1609, 'grad_norm': 1.6308424472808838, 'learning_rate': 1.2047820265154362e-05, 'epoch': 0.45}
{'loss': 1.2275, 'grad_norm': 1.6995110511779785, 'learning_rate': 1.2041721068405614e-05, 'epoch': 0.45}
{'loss': 1.1687, 'grad_norm': 1.7396279573440552, 'learning_rate': 1.203562107899045e-05, 'epoch': 0.45}
{'loss': 1.0998, 'grad_norm': 1.5154011249542236, 'learning_rate': 1.2029520299277095e-05, 'epoch': 0.45}
Step 2350: Updated gated ratio to 0.9095 (progress: 90.5%)
Step 2350: Updated gated ratio to 0.9095 (progress: 90.5%)
Step 2350: Updated gated ratio to 0.9095 (progress: 90.5%)
Step 2350: Updated gated ratio to 0.9095 (progress: 90.5%)
Step 2350: Updated gated ratio to 0.9095 (progress: 90.5%)
Step 2350: Updated gated ratio to 0.9095 (progress: 90.5%)
Step 2350: Updated gated ratio to 0.9095 (progress: 90.5%)
Step 2350: Updated gated ratio to 0.9095 (progress: 90.5%)
Step 2350: Updated gated ratio to 0.9095 (progress: 90.5%)
Step 2350: Updated gated ratio to 0.9095 (progress: 90.5%)
Step 2350: Updated gated ratio to 0.9095 (progress: 90.5%)
Step 2350: Updated gated ratio to 0.9095 (progress: 90.5%)
Step 2350: Updated gated ratio to 0.9095 (progress: 90.5%)
Step 2350: Updated gated ratio to 0.9095 (progress: 90.5%)
Step 2350: Updated gated ratio to 0.9095 (progress: 90.5%)
Step 2350: Updated gated ratio to 0.9095 (progress: 90.5%)
{'loss': 1.0731, 'grad_norm': 1.4679101705551147, 'learning_rate': 1.2023418731634078e-05, 'epoch': 0.45}
{'loss': 1.0643, 'grad_norm': 1.5304368734359741, 'learning_rate': 1.2017316378430244e-05, 'epoch': 0.45}
{'loss': 1.0715, 'grad_norm': 1.5508437156677246, 'learning_rate': 1.2011213242034733e-05, 'epoch': 0.45}
{'loss': 1.1296, 'grad_norm': 1.472637414932251, 'learning_rate': 1.2005109324816992e-05, 'epoch': 0.45}
{'loss': 1.2234, 'grad_norm': 1.6259006261825562, 'learning_rate': 1.1999004629146775e-05, 'epoch': 0.45}
{'loss': 1.2574, 'grad_norm': 1.4359374046325684, 'learning_rate': 1.1992899157394133e-05, 'epoch': 0.45}
Step 2356: Updated gated ratio to 0.9093 (progress: 90.7%)
Step 2356: Updated gated ratio to 0.9093 (progress: 90.7%)
Step 2356: Updated gated ratio to 0.9093 (progress: 90.7%)
Step 2356: Updated gated ratio to 0.9093 (progress: 90.7%)
Step 2356: Updated gated ratio to 0.9093 (progress: 90.7%)
Step 2356: Updated gated ratio to 0.9093 (progress: 90.7%)
Step 2356: Updated gated ratio to 0.9093 (progress: 90.7%)
Step 2356: Updated gated ratio to 0.9093 (progress: 90.7%)
Step 2356: Updated gated ratio to 0.9093 (progress: 90.7%)
Step 2356: Updated gated ratio to 0.9093 (progress: 90.7%)
Step 2356: Updated gated ratio to 0.9093 (progress: 90.7%)
Step 2356: Updated gated ratio to 0.9093 (progress: 90.7%)
Step 2356: Updated gated ratio to 0.9093 (progress: 90.7%)
Step 2356: Updated gated ratio to 0.9093 (progress: 90.7%)
Step 2356: Updated gated ratio to 0.9093 (progress: 90.7%)
Step 2356: Updated gated ratio to 0.9093 (progress: 90.7%)
{'loss': 1.1203, 'grad_norm': 1.7235628366470337, 'learning_rate': 1.1986792911929418e-05, 'epoch': 0.45}
{'loss': 1.1955, 'grad_norm': 1.4109785556793213, 'learning_rate': 1.198068589512329e-05, 'epoch': 0.45}
{'loss': 1.1627, 'grad_norm': 1.6584792137145996, 'learning_rate': 1.1974578109346702e-05, 'epoch': 0.45}
{'loss': 1.1642, 'grad_norm': 1.366355299949646, 'learning_rate': 1.1968469556970905e-05, 'epoch': 0.45}
{'loss': 1.09, 'grad_norm': 1.5512573719024658, 'learning_rate': 1.1962360240367445e-05, 'epoch': 0.45}
Step 2361: Updated gated ratio to 0.9091 (progress: 90.9%)
Step 2361: Updated gated ratio to 0.9091 (progress: 90.9%)
Step 2361: Updated gated ratio to 0.9091 (progress: 90.9%)
Step 2361: Updated gated ratio to 0.9091 (progress: 90.9%)
Step 2361: Updated gated ratio to 0.9091 (progress: 90.9%)
Step 2361: Updated gated ratio to 0.9091 (progress: 90.9%)
Step 2361: Updated gated ratio to 0.9091 (progress: 90.9%)
Step 2361: Updated gated ratio to 0.9091 (progress: 90.9%)
Step 2361: Updated gated ratio to 0.9091 (progress: 90.9%)
Step 2361: Updated gated ratio to 0.9091 (progress: 90.9%)
Step 2361: Updated gated ratio to 0.9091 (progress: 90.9%)
Step 2361: Updated gated ratio to 0.9091 (progress: 90.9%)
Step 2361: Updated gated ratio to 0.9091 (progress: 90.9%)
Step 2361: Updated gated ratio to 0.9091 (progress: 90.9%)
Step 2361: Updated gated ratio to 0.9091 (progress: 90.9%)
Step 2361: Updated gated ratio to 0.9091 (progress: 90.9%)
{'loss': 1.2114, 'grad_norm': 1.7050070762634277, 'learning_rate': 1.1956250161908179e-05, 'epoch': 0.45}
{'loss': 1.1958, 'grad_norm': 1.6444122791290283, 'learning_rate': 1.195013932396524e-05, 'epoch': 0.45}
{'loss': 1.0932, 'grad_norm': 1.5278578996658325, 'learning_rate': 1.1944027728911072e-05, 'epoch': 0.45}
{'loss': 1.2685, 'grad_norm': 1.4188354015350342, 'learning_rate': 1.1937915379118406e-05, 'epoch': 0.45}
{'loss': 1.1369, 'grad_norm': 1.5982599258422852, 'learning_rate': 1.1931802276960265e-05, 'epoch': 0.46}
Step 2366: Updated gated ratio to 0.9089 (progress: 91.1%)
Step 2366: Updated gated ratio to 0.9089 (progress: 91.1%)
Step 2366: Updated gated ratio to 0.9089 (progress: 91.1%)
Step 2366: Updated gated ratio to 0.9089 (progress: 91.1%)
Step 2366: Updated gated ratio to 0.9089 (progress: 91.1%)
Step 2366: Updated gated ratio to 0.9089 (progress: 91.1%)
Step 2366: Updated gated ratio to 0.9089 (progress: 91.1%)
Step 2366: Updated gated ratio to 0.9089 (progress: 91.1%)
Step 2366: Updated gated ratio to 0.9089 (progress: 91.1%)
Step 2366: Updated gated ratio to 0.9089 (progress: 91.1%)
Step 2366: Updated gated ratio to 0.9089 (progress: 91.1%)
Step 2366: Updated gated ratio to 0.9089 (progress: 91.1%)Step 2366: Updated gated ratio to 0.9089 (progress: 91.1%)
Step 2366: Updated gated ratio to 0.9089 (progress: 91.1%)
Step 2366: Updated gated ratio to 0.9089 (progress: 91.1%)

Step 2366: Updated gated ratio to 0.9089 (progress: 91.1%)
{'loss': 1.0661, 'grad_norm': 1.546732783317566, 'learning_rate': 1.1925688424809965e-05, 'epoch': 0.46}
{'loss': 1.1254, 'grad_norm': 1.62168288230896, 'learning_rate': 1.1919573825041115e-05, 'epoch': 0.46}
{'loss': 1.2187, 'grad_norm': 1.6480960845947266, 'learning_rate': 1.1913458480027614e-05, 'epoch': 0.46}
{'loss': 1.1592, 'grad_norm': 1.679918885231018, 'learning_rate': 1.1907342392143646e-05, 'epoch': 0.46}
{'loss': 1.1237, 'grad_norm': 1.5088095664978027, 'learning_rate': 1.1901225563763694e-05, 'epoch': 0.46}
Step 2371: Updated gated ratio to 0.9087 (progress: 91.3%)
Step 2371: Updated gated ratio to 0.9087 (progress: 91.3%)
Step 2371: Updated gated ratio to 0.9087 (progress: 91.3%)
Step 2371: Updated gated ratio to 0.9087 (progress: 91.3%)
Step 2371: Updated gated ratio to 0.9087 (progress: 91.3%)Step 2371: Updated gated ratio to 0.9087 (progress: 91.3%)
Step 2371: Updated gated ratio to 0.9087 (progress: 91.3%)
Step 2371: Updated gated ratio to 0.9087 (progress: 91.3%)
Step 2371: Updated gated ratio to 0.9087 (progress: 91.3%)

Step 2371: Updated gated ratio to 0.9087 (progress: 91.3%)
Step 2371: Updated gated ratio to 0.9087 (progress: 91.3%)
Step 2371: Updated gated ratio to 0.9087 (progress: 91.3%)
Step 2371: Updated gated ratio to 0.9087 (progress: 91.3%)
Step 2371: Updated gated ratio to 0.9087 (progress: 91.3%)
Step 2371: Updated gated ratio to 0.9087 (progress: 91.3%)
Step 2371: Updated gated ratio to 0.9087 (progress: 91.3%)
{'loss': 1.0948, 'grad_norm': 1.46139657497406, 'learning_rate': 1.1895107997262516e-05, 'epoch': 0.46}
{'loss': 1.1103, 'grad_norm': 1.5277214050292969, 'learning_rate': 1.1888989695015166e-05, 'epoch': 0.46}
{'loss': 1.104, 'grad_norm': 1.6609398126602173, 'learning_rate': 1.1882870659396968e-05, 'epoch': 0.46}
{'loss': 1.1543, 'grad_norm': 1.6401686668395996, 'learning_rate': 1.1876750892783558e-05, 'epoch': 0.46}
{'loss': 1.1415, 'grad_norm': 1.7364604473114014, 'learning_rate': 1.1870630397550831e-05, 'epoch': 0.46}
Step 2376: Updated gated ratio to 0.9085 (progress: 91.5%)
Step 2376: Updated gated ratio to 0.9085 (progress: 91.5%)
Step 2376: Updated gated ratio to 0.9085 (progress: 91.5%)
Step 2376: Updated gated ratio to 0.9085 (progress: 91.5%)
Step 2376: Updated gated ratio to 0.9085 (progress: 91.5%)
Step 2376: Updated gated ratio to 0.9085 (progress: 91.5%)
Step 2376: Updated gated ratio to 0.9085 (progress: 91.5%)
Step 2376: Updated gated ratio to 0.9085 (progress: 91.5%)
Step 2376: Updated gated ratio to 0.9085 (progress: 91.5%)
Step 2376: Updated gated ratio to 0.9085 (progress: 91.5%)
Step 2376: Updated gated ratio to 0.9085 (progress: 91.5%)
Step 2376: Updated gated ratio to 0.9085 (progress: 91.5%)
Step 2376: Updated gated ratio to 0.9085 (progress: 91.5%)
Step 2376: Updated gated ratio to 0.9085 (progress: 91.5%)
Step 2376: Updated gated ratio to 0.9085 (progress: 91.5%)
Step 2376: Updated gated ratio to 0.9085 (progress: 91.5%)
{'loss': 1.0211, 'grad_norm': 1.5082509517669678, 'learning_rate': 1.1864509176074974e-05, 'epoch': 0.46}
{'loss': 1.065, 'grad_norm': 1.5542984008789062, 'learning_rate': 1.185838723073246e-05, 'epoch': 0.46}
{'loss': 1.2006, 'grad_norm': 1.4902968406677246, 'learning_rate': 1.1852264563900038e-05, 'epoch': 0.46}
{'loss': 1.1368, 'grad_norm': 1.723727822303772, 'learning_rate': 1.1846141177954733e-05, 'epoch': 0.46}
{'loss': 1.1458, 'grad_norm': 1.6038086414337158, 'learning_rate': 1.1840017075273861e-05, 'epoch': 0.46}
{'loss': 1.0769, 'grad_norm': 1.6302796602249146, 'learning_rate': 1.1833892258235008e-05, 'epoch': 0.46}
Step 2382: Updated gated ratio to 0.9083 (progress: 91.7%)
Step 2382: Updated gated ratio to 0.9083 (progress: 91.7%)
Step 2382: Updated gated ratio to 0.9083 (progress: 91.7%)
Step 2382: Updated gated ratio to 0.9083 (progress: 91.7%)
Step 2382: Updated gated ratio to 0.9083 (progress: 91.7%)
Step 2382: Updated gated ratio to 0.9083 (progress: 91.7%)
Step 2382: Updated gated ratio to 0.9083 (progress: 91.7%)
Step 2382: Updated gated ratio to 0.9083 (progress: 91.7%)
Step 2382: Updated gated ratio to 0.9083 (progress: 91.7%)
Step 2382: Updated gated ratio to 0.9083 (progress: 91.7%)
Step 2382: Updated gated ratio to 0.9083 (progress: 91.7%)
Step 2382: Updated gated ratio to 0.9083 (progress: 91.7%)Step 2382: Updated gated ratio to 0.9083 (progress: 91.7%)

Step 2382: Updated gated ratio to 0.9083 (progress: 91.7%)
Step 2382: Updated gated ratio to 0.9083 (progress: 91.7%)
Step 2382: Updated gated ratio to 0.9083 (progress: 91.7%)
{'loss': 1.182, 'grad_norm': 1.5302205085754395, 'learning_rate': 1.1827766729216035e-05, 'epoch': 0.46}
{'loss': 1.146, 'grad_norm': 1.6294294595718384, 'learning_rate': 1.1821640490595086e-05, 'epoch': 0.46}
{'loss': 1.1069, 'grad_norm': 1.4851195812225342, 'learning_rate': 1.181551354475058e-05, 'epoch': 0.46}
{'loss': 1.2164, 'grad_norm': 1.465412974357605, 'learning_rate': 1.1809385894061206e-05, 'epoch': 0.46}
{'loss': 1.1956, 'grad_norm': 1.667410135269165, 'learning_rate': 1.1803257540905926e-05, 'epoch': 0.46}
Step 2387: Updated gated ratio to 0.9081 (progress: 91.9%)
Step 2387: Updated gated ratio to 0.9081 (progress: 91.9%)
Step 2387: Updated gated ratio to 0.9081 (progress: 91.9%)
Step 2387: Updated gated ratio to 0.9081 (progress: 91.9%)
Step 2387: Updated gated ratio to 0.9081 (progress: 91.9%)
Step 2387: Updated gated ratio to 0.9081 (progress: 91.9%)Step 2387: Updated gated ratio to 0.9081 (progress: 91.9%)

Step 2387: Updated gated ratio to 0.9081 (progress: 91.9%)
Step 2387: Updated gated ratio to 0.9081 (progress: 91.9%)
Step 2387: Updated gated ratio to 0.9081 (progress: 91.9%)
Step 2387: Updated gated ratio to 0.9081 (progress: 91.9%)
Step 2387: Updated gated ratio to 0.9081 (progress: 91.9%)
Step 2387: Updated gated ratio to 0.9081 (progress: 91.9%)
Step 2387: Updated gated ratio to 0.9081 (progress: 91.9%)
Step 2387: Updated gated ratio to 0.9081 (progress: 91.9%)
Step 2387: Updated gated ratio to 0.9081 (progress: 91.9%)
{'loss': 1.2489, 'grad_norm': 1.2706594467163086, 'learning_rate': 1.1797128487663982e-05, 'epoch': 0.46}
{'loss': 1.0603, 'grad_norm': 1.5330564975738525, 'learning_rate': 1.1790998736714882e-05, 'epoch': 0.46}
{'loss': 1.19, 'grad_norm': 1.7172242403030396, 'learning_rate': 1.1784868290438404e-05, 'epoch': 0.46}
{'loss': 1.2607, 'grad_norm': 1.3489162921905518, 'learning_rate': 1.1778737151214606e-05, 'epoch': 0.46}
{'loss': 1.2596, 'grad_norm': 1.693516731262207, 'learning_rate': 1.17726053214238e-05, 'epoch': 0.46}
Step 2392: Updated gated ratio to 0.9079 (progress: 92.1%)
Step 2392: Updated gated ratio to 0.9079 (progress: 92.1%)
Step 2392: Updated gated ratio to 0.9079 (progress: 92.1%)
Step 2392: Updated gated ratio to 0.9079 (progress: 92.1%)
Step 2392: Updated gated ratio to 0.9079 (progress: 92.1%)
Step 2392: Updated gated ratio to 0.9079 (progress: 92.1%)
Step 2392: Updated gated ratio to 0.9079 (progress: 92.1%)
Step 2392: Updated gated ratio to 0.9079 (progress: 92.1%)
Step 2392: Updated gated ratio to 0.9079 (progress: 92.1%)
Step 2392: Updated gated ratio to 0.9079 (progress: 92.1%)
Step 2392: Updated gated ratio to 0.9079 (progress: 92.1%)
Step 2392: Updated gated ratio to 0.9079 (progress: 92.1%)
Step 2392: Updated gated ratio to 0.9079 (progress: 92.1%)
Step 2392: Updated gated ratio to 0.9079 (progress: 92.1%)
Step 2392: Updated gated ratio to 0.9079 (progress: 92.1%)
Step 2392: Updated gated ratio to 0.9079 (progress: 92.1%)
{'loss': 1.203, 'grad_norm': 1.6859861612319946, 'learning_rate': 1.1766472803446577e-05, 'epoch': 0.46}
{'loss': 1.1646, 'grad_norm': 1.6199212074279785, 'learning_rate': 1.1760339599663788e-05, 'epoch': 0.46}
{'loss': 1.1831, 'grad_norm': 1.6446619033813477, 'learning_rate': 1.1754205712456556e-05, 'epoch': 0.46}
{'loss': 1.084, 'grad_norm': 1.671754002571106, 'learning_rate': 1.1748071144206266e-05, 'epoch': 0.46}
{'loss': 1.0682, 'grad_norm': 1.516124963760376, 'learning_rate': 1.1741935897294572e-05, 'epoch': 0.46}
Step 2397: Updated gated ratio to 0.9077 (progress: 92.3%)
Step 2397: Updated gated ratio to 0.9077 (progress: 92.3%)
Step 2397: Updated gated ratio to 0.9077 (progress: 92.3%)
Step 2397: Updated gated ratio to 0.9077 (progress: 92.3%)Step 2397: Updated gated ratio to 0.9077 (progress: 92.3%)

Step 2397: Updated gated ratio to 0.9077 (progress: 92.3%)
Step 2397: Updated gated ratio to 0.9077 (progress: 92.3%)
Step 2397: Updated gated ratio to 0.9077 (progress: 92.3%)
Step 2397: Updated gated ratio to 0.9077 (progress: 92.3%)Step 2397: Updated gated ratio to 0.9077 (progress: 92.3%)

Step 2397: Updated gated ratio to 0.9077 (progress: 92.3%)
Step 2397: Updated gated ratio to 0.9077 (progress: 92.3%)
Step 2397: Updated gated ratio to 0.9077 (progress: 92.3%)
Step 2397: Updated gated ratio to 0.9077 (progress: 92.3%)
Step 2397: Updated gated ratio to 0.9077 (progress: 92.3%)
Step 2397: Updated gated ratio to 0.9077 (progress: 92.3%)
{'loss': 1.0996, 'grad_norm': 1.4720326662063599, 'learning_rate': 1.1735799974103388e-05, 'epoch': 0.46}
{'loss': 1.1268, 'grad_norm': 1.6031079292297363, 'learning_rate': 1.1729663377014888e-05, 'epoch': 0.46}
{'loss': 1.1479, 'grad_norm': 1.550662636756897, 'learning_rate': 1.172352610841151e-05, 'epoch': 0.46}
{'loss': 1.1428, 'grad_norm': 1.5152854919433594, 'learning_rate': 1.1717388170675954e-05, 'epoch': 0.46}
{'loss': 1.106, 'grad_norm': 1.5661430358886719, 'learning_rate': 1.1711249566191179e-05, 'epoch': 0.46}
Step 2402: Updated gated ratio to 0.9075 (progress: 92.5%)
Step 2402: Updated gated ratio to 0.9075 (progress: 92.5%)
Step 2402: Updated gated ratio to 0.9075 (progress: 92.5%)
Step 2402: Updated gated ratio to 0.9075 (progress: 92.5%)
Step 2402: Updated gated ratio to 0.9075 (progress: 92.5%)
Step 2402: Updated gated ratio to 0.9075 (progress: 92.5%)
Step 2402: Updated gated ratio to 0.9075 (progress: 92.5%)
Step 2402: Updated gated ratio to 0.9075 (progress: 92.5%)
Step 2402: Updated gated ratio to 0.9075 (progress: 92.5%)
Step 2402: Updated gated ratio to 0.9075 (progress: 92.5%)
Step 2402: Updated gated ratio to 0.9075 (progress: 92.5%)
Step 2402: Updated gated ratio to 0.9075 (progress: 92.5%)
Step 2402: Updated gated ratio to 0.9075 (progress: 92.5%)
Step 2402: Updated gated ratio to 0.9075 (progress: 92.5%)
Step 2402: Updated gated ratio to 0.9075 (progress: 92.5%)Step 2402: Updated gated ratio to 0.9075 (progress: 92.5%)

{'loss': 1.1747, 'grad_norm': 1.5973386764526367, 'learning_rate': 1.17051102973404e-05, 'epoch': 0.46}
{'loss': 1.0644, 'grad_norm': 1.598347544670105, 'learning_rate': 1.1698970366507096e-05, 'epoch': 0.46}
{'loss': 1.1252, 'grad_norm': 1.5905252695083618, 'learning_rate': 1.1692829776074999e-05, 'epoch': 0.46}
{'loss': 1.161, 'grad_norm': 1.661773920059204, 'learning_rate': 1.1686688528428099e-05, 'epoch': 0.46}
{'loss': 1.103, 'grad_norm': 1.5985922813415527, 'learning_rate': 1.1680546625950635e-05, 'epoch': 0.46}
{'loss': 1.0574, 'grad_norm': 1.5956941843032837, 'learning_rate': 1.167440407102711e-05, 'epoch': 0.46}
Step 2408: Updated gated ratio to 0.9073 (progress: 92.7%)
Step 2408: Updated gated ratio to 0.9073 (progress: 92.7%)Step 2408: Updated gated ratio to 0.9073 (progress: 92.7%)

Step 2408: Updated gated ratio to 0.9073 (progress: 92.7%)
Step 2408: Updated gated ratio to 0.9073 (progress: 92.7%)
Step 2408: Updated gated ratio to 0.9073 (progress: 92.7%)
Step 2408: Updated gated ratio to 0.9073 (progress: 92.7%)
Step 2408: Updated gated ratio to 0.9073 (progress: 92.7%)
Step 2408: Updated gated ratio to 0.9073 (progress: 92.7%)Step 2408: Updated gated ratio to 0.9073 (progress: 92.7%)

Step 2408: Updated gated ratio to 0.9073 (progress: 92.7%)
Step 2408: Updated gated ratio to 0.9073 (progress: 92.7%)
Step 2408: Updated gated ratio to 0.9073 (progress: 92.7%)
Step 2408: Updated gated ratio to 0.9073 (progress: 92.7%)
Step 2408: Updated gated ratio to 0.9073 (progress: 92.7%)
Step 2408: Updated gated ratio to 0.9073 (progress: 92.7%)
{'loss': 1.0491, 'grad_norm': 1.6584656238555908, 'learning_rate': 1.1668260866042271e-05, 'epoch': 0.46}
{'loss': 1.1731, 'grad_norm': 1.6181427240371704, 'learning_rate': 1.1662117013381126e-05, 'epoch': 0.46}
{'loss': 1.1457, 'grad_norm': 1.7279455661773682, 'learning_rate': 1.1655972515428928e-05, 'epoch': 0.46}
{'loss': 1.112, 'grad_norm': 1.5804821252822876, 'learning_rate': 1.1649827374571182e-05, 'epoch': 0.46}
{'loss': 1.1108, 'grad_norm': 1.5992711782455444, 'learning_rate': 1.1643681593193642e-05, 'epoch': 0.46}
Step 2413: Updated gated ratio to 0.9071 (progress: 92.9%)
Step 2413: Updated gated ratio to 0.9071 (progress: 92.9%)
Step 2413: Updated gated ratio to 0.9071 (progress: 92.9%)
Step 2413: Updated gated ratio to 0.9071 (progress: 92.9%)
Step 2413: Updated gated ratio to 0.9071 (progress: 92.9%)
Step 2413: Updated gated ratio to 0.9071 (progress: 92.9%)
Step 2413: Updated gated ratio to 0.9071 (progress: 92.9%)
Step 2413: Updated gated ratio to 0.9071 (progress: 92.9%)
Step 2413: Updated gated ratio to 0.9071 (progress: 92.9%)
Step 2413: Updated gated ratio to 0.9071 (progress: 92.9%)
Step 2413: Updated gated ratio to 0.9071 (progress: 92.9%)
Step 2413: Updated gated ratio to 0.9071 (progress: 92.9%)
Step 2413: Updated gated ratio to 0.9071 (progress: 92.9%)
Step 2413: Updated gated ratio to 0.9071 (progress: 92.9%)
Step 2413: Updated gated ratio to 0.9071 (progress: 92.9%)
Step 2413: Updated gated ratio to 0.9071 (progress: 92.9%)
{'loss': 1.1377, 'grad_norm': 1.584813117980957, 'learning_rate': 1.1637535173682318e-05, 'epoch': 0.46}
{'loss': 1.1378, 'grad_norm': 1.5461686849594116, 'learning_rate': 1.1631388118423457e-05, 'epoch': 0.46}
{'loss': 1.0795, 'grad_norm': 1.6244637966156006, 'learning_rate': 1.1625240429803553e-05, 'epoch': 0.46}
{'loss': 1.2696, 'grad_norm': 1.40323007106781, 'learning_rate': 1.1619092110209361e-05, 'epoch': 0.46}
{'loss': 1.0753, 'grad_norm': 1.5591530799865723, 'learning_rate': 1.1612943162027863e-05, 'epoch': 0.47}
Step 2418: Updated gated ratio to 0.9069 (progress: 93.1%)
Step 2418: Updated gated ratio to 0.9069 (progress: 93.1%)
Step 2418: Updated gated ratio to 0.9069 (progress: 93.1%)
Step 2418: Updated gated ratio to 0.9069 (progress: 93.1%)
Step 2418: Updated gated ratio to 0.9069 (progress: 93.1%)Step 2418: Updated gated ratio to 0.9069 (progress: 93.1%)

Step 2418: Updated gated ratio to 0.9069 (progress: 93.1%)
Step 2418: Updated gated ratio to 0.9069 (progress: 93.1%)
Step 2418: Updated gated ratio to 0.9069 (progress: 93.1%)
Step 2418: Updated gated ratio to 0.9069 (progress: 93.1%)
Step 2418: Updated gated ratio to 0.9069 (progress: 93.1%)
Step 2418: Updated gated ratio to 0.9069 (progress: 93.1%)
Step 2418: Updated gated ratio to 0.9069 (progress: 93.1%)
Step 2418: Updated gated ratio to 0.9069 (progress: 93.1%)
Step 2418: Updated gated ratio to 0.9069 (progress: 93.1%)
Step 2418: Updated gated ratio to 0.9069 (progress: 93.1%)
{'loss': 1.1153, 'grad_norm': 1.6832629442214966, 'learning_rate': 1.1606793587646295e-05, 'epoch': 0.47}
{'loss': 1.1208, 'grad_norm': 1.6084743738174438, 'learning_rate': 1.160064338945213e-05, 'epoch': 0.47}
{'loss': 1.1424, 'grad_norm': 1.6960195302963257, 'learning_rate': 1.1594492569833093e-05, 'epoch': 0.47}
{'loss': 1.1134, 'grad_norm': 1.529671549797058, 'learning_rate': 1.1588341131177137e-05, 'epoch': 0.47}
{'loss': 1.1027, 'grad_norm': 1.5718817710876465, 'learning_rate': 1.1582189075872467e-05, 'epoch': 0.47}
Step 2423: Updated gated ratio to 0.9067 (progress: 93.3%)
Step 2423: Updated gated ratio to 0.9067 (progress: 93.3%)
Step 2423: Updated gated ratio to 0.9067 (progress: 93.3%)Step 2423: Updated gated ratio to 0.9067 (progress: 93.3%)
Step 2423: Updated gated ratio to 0.9067 (progress: 93.3%)
Step 2423: Updated gated ratio to 0.9067 (progress: 93.3%)
Step 2423: Updated gated ratio to 0.9067 (progress: 93.3%)
Step 2423: Updated gated ratio to 0.9067 (progress: 93.3%)Step 2423: Updated gated ratio to 0.9067 (progress: 93.3%)


Step 2423: Updated gated ratio to 0.9067 (progress: 93.3%)
Step 2423: Updated gated ratio to 0.9067 (progress: 93.3%)
Step 2423: Updated gated ratio to 0.9067 (progress: 93.3%)
Step 2423: Updated gated ratio to 0.9067 (progress: 93.3%)
Step 2423: Updated gated ratio to 0.9067 (progress: 93.3%)
Step 2423: Updated gated ratio to 0.9067 (progress: 93.3%)
Step 2423: Updated gated ratio to 0.9067 (progress: 93.3%)
{'loss': 1.0569, 'grad_norm': 1.6999895572662354, 'learning_rate': 1.1576036406307523e-05, 'epoch': 0.47}
{'loss': 1.1198, 'grad_norm': 1.609861969947815, 'learning_rate': 1.156988312487098e-05, 'epoch': 0.47}
{'loss': 1.0923, 'grad_norm': 1.5127084255218506, 'learning_rate': 1.1563729233951757e-05, 'epoch': 0.47}
{'loss': 1.0481, 'grad_norm': 1.4217171669006348, 'learning_rate': 1.1557574735939003e-05, 'epoch': 0.47}
{'loss': 1.123, 'grad_norm': 1.6455003023147583, 'learning_rate': 1.1551419633222107e-05, 'epoch': 0.47}
Step 2428: Updated gated ratio to 0.9065 (progress: 93.5%)
Step 2428: Updated gated ratio to 0.9065 (progress: 93.5%)
Step 2428: Updated gated ratio to 0.9065 (progress: 93.5%)
Step 2428: Updated gated ratio to 0.9065 (progress: 93.5%)
Step 2428: Updated gated ratio to 0.9065 (progress: 93.5%)
Step 2428: Updated gated ratio to 0.9065 (progress: 93.5%)
Step 2428: Updated gated ratio to 0.9065 (progress: 93.5%)
Step 2428: Updated gated ratio to 0.9065 (progress: 93.5%)
Step 2428: Updated gated ratio to 0.9065 (progress: 93.5%)Step 2428: Updated gated ratio to 0.9065 (progress: 93.5%)

Step 2428: Updated gated ratio to 0.9065 (progress: 93.5%)
Step 2428: Updated gated ratio to 0.9065 (progress: 93.5%)
Step 2428: Updated gated ratio to 0.9065 (progress: 93.5%)
Step 2428: Updated gated ratio to 0.9065 (progress: 93.5%)
Step 2428: Updated gated ratio to 0.9065 (progress: 93.5%)
Step 2428: Updated gated ratio to 0.9065 (progress: 93.5%)
{'loss': 1.0997, 'grad_norm': 1.4130970239639282, 'learning_rate': 1.1545263928190692e-05, 'epoch': 0.47}
{'loss': 1.104, 'grad_norm': 1.6322951316833496, 'learning_rate': 1.1539107623234618e-05, 'epoch': 0.47}
{'loss': 1.0159, 'grad_norm': 1.4653290510177612, 'learning_rate': 1.153295072074397e-05, 'epoch': 0.47}
{'loss': 1.0695, 'grad_norm': 1.570122480392456, 'learning_rate': 1.1526793223109072e-05, 'epoch': 0.47}
{'loss': 1.0066, 'grad_norm': 1.5776865482330322, 'learning_rate': 1.1520635132720475e-05, 'epoch': 0.47}
{'loss': 1.1571, 'grad_norm': 1.6211180686950684, 'learning_rate': 1.1514476451968961e-05, 'epoch': 0.47}
Step 2434: Updated gated ratio to 0.9063 (progress: 93.7%)
Step 2434: Updated gated ratio to 0.9063 (progress: 93.7%)
Step 2434: Updated gated ratio to 0.9063 (progress: 93.7%)
Step 2434: Updated gated ratio to 0.9063 (progress: 93.7%)Step 2434: Updated gated ratio to 0.9063 (progress: 93.7%)
Step 2434: Updated gated ratio to 0.9063 (progress: 93.7%)
Step 2434: Updated gated ratio to 0.9063 (progress: 93.7%)
Step 2434: Updated gated ratio to 0.9063 (progress: 93.7%)
Step 2434: Updated gated ratio to 0.9063 (progress: 93.7%)

Step 2434: Updated gated ratio to 0.9063 (progress: 93.7%)
Step 2434: Updated gated ratio to 0.9063 (progress: 93.7%)
Step 2434: Updated gated ratio to 0.9063 (progress: 93.7%)
Step 2434: Updated gated ratio to 0.9063 (progress: 93.7%)Step 2434: Updated gated ratio to 0.9063 (progress: 93.7%)

Step 2434: Updated gated ratio to 0.9063 (progress: 93.7%)
Step 2434: Updated gated ratio to 0.9063 (progress: 93.7%)
{'loss': 1.2095, 'grad_norm': 1.619682788848877, 'learning_rate': 1.1508317183245545e-05, 'epoch': 0.47}
{'loss': 1.1427, 'grad_norm': 1.710354208946228, 'learning_rate': 1.1502157328941466e-05, 'epoch': 0.47}
{'loss': 1.0626, 'grad_norm': 1.4309806823730469, 'learning_rate': 1.149599689144819e-05, 'epoch': 0.47}
{'loss': 1.1389, 'grad_norm': 1.5907927751541138, 'learning_rate': 1.1489835873157414e-05, 'epoch': 0.47}
{'loss': 1.0853, 'grad_norm': 1.6253653764724731, 'learning_rate': 1.1483674276461053e-05, 'epoch': 0.47}
Step 2439: Updated gated ratio to 0.9061 (progress: 93.9%)
Step 2439: Updated gated ratio to 0.9061 (progress: 93.9%)
Step 2439: Updated gated ratio to 0.9061 (progress: 93.9%)
Step 2439: Updated gated ratio to 0.9061 (progress: 93.9%)
Step 2439: Updated gated ratio to 0.9061 (progress: 93.9%)
Step 2439: Updated gated ratio to 0.9061 (progress: 93.9%)
Step 2439: Updated gated ratio to 0.9061 (progress: 93.9%)
Step 2439: Updated gated ratio to 0.9061 (progress: 93.9%)
Step 2439: Updated gated ratio to 0.9061 (progress: 93.9%)
Step 2439: Updated gated ratio to 0.9061 (progress: 93.9%)
Step 2439: Updated gated ratio to 0.9061 (progress: 93.9%)
Step 2439: Updated gated ratio to 0.9061 (progress: 93.9%)
Step 2439: Updated gated ratio to 0.9061 (progress: 93.9%)
Step 2439: Updated gated ratio to 0.9061 (progress: 93.9%)
Step 2439: Updated gated ratio to 0.9061 (progress: 93.9%)
Step 2439: Updated gated ratio to 0.9061 (progress: 93.9%)
{'loss': 1.0504, 'grad_norm': 1.4656641483306885, 'learning_rate': 1.1477512103751254e-05, 'epoch': 0.47}
{'loss': 1.1159, 'grad_norm': 1.511285424232483, 'learning_rate': 1.1471349357420384e-05, 'epoch': 0.47}
{'loss': 1.1713, 'grad_norm': 1.451339602470398, 'learning_rate': 1.1465186039861033e-05, 'epoch': 0.47}
{'loss': 1.1352, 'grad_norm': 1.517758846282959, 'learning_rate': 1.1459022153466016e-05, 'epoch': 0.47}
{'loss': 1.0759, 'grad_norm': 1.6056804656982422, 'learning_rate': 1.1452857700628362e-05, 'epoch': 0.47}
Step 2444: Updated gated ratio to 0.9059 (progress: 94.1%)
Step 2444: Updated gated ratio to 0.9059 (progress: 94.1%)Step 2444: Updated gated ratio to 0.9059 (progress: 94.1%)
Step 2444: Updated gated ratio to 0.9059 (progress: 94.1%)
Step 2444: Updated gated ratio to 0.9059 (progress: 94.1%)
Step 2444: Updated gated ratio to 0.9059 (progress: 94.1%)Step 2444: Updated gated ratio to 0.9059 (progress: 94.1%)

Step 2444: Updated gated ratio to 0.9059 (progress: 94.1%)
Step 2444: Updated gated ratio to 0.9059 (progress: 94.1%)
Step 2444: Updated gated ratio to 0.9059 (progress: 94.1%)Step 2444: Updated gated ratio to 0.9059 (progress: 94.1%)


Step 2444: Updated gated ratio to 0.9059 (progress: 94.1%)
Step 2444: Updated gated ratio to 0.9059 (progress: 94.1%)Step 2444: Updated gated ratio to 0.9059 (progress: 94.1%)

Step 2444: Updated gated ratio to 0.9059 (progress: 94.1%)
Step 2444: Updated gated ratio to 0.9059 (progress: 94.1%)
{'loss': 1.0352, 'grad_norm': 1.5189015865325928, 'learning_rate': 1.1446692683741326e-05, 'epoch': 0.47}
{'loss': 1.0345, 'grad_norm': 1.467164158821106, 'learning_rate': 1.1440527105198377e-05, 'epoch': 0.47}
{'loss': 1.0915, 'grad_norm': 1.6651160717010498, 'learning_rate': 1.143436096739321e-05, 'epoch': 0.47}
{'loss': 1.1292, 'grad_norm': 1.6974194049835205, 'learning_rate': 1.1428194272719729e-05, 'epoch': 0.47}
{'loss': 1.177, 'grad_norm': 1.5575820207595825, 'learning_rate': 1.1422027023572052e-05, 'epoch': 0.47}
Step 2449: Updated gated ratio to 0.9057 (progress: 94.3%)
Step 2449: Updated gated ratio to 0.9057 (progress: 94.3%)
Step 2449: Updated gated ratio to 0.9057 (progress: 94.3%)
Step 2449: Updated gated ratio to 0.9057 (progress: 94.3%)
Step 2449: Updated gated ratio to 0.9057 (progress: 94.3%)
Step 2449: Updated gated ratio to 0.9057 (progress: 94.3%)
Step 2449: Updated gated ratio to 0.9057 (progress: 94.3%)
Step 2449: Updated gated ratio to 0.9057 (progress: 94.3%)
Step 2449: Updated gated ratio to 0.9057 (progress: 94.3%)
Step 2449: Updated gated ratio to 0.9057 (progress: 94.3%)
Step 2449: Updated gated ratio to 0.9057 (progress: 94.3%)Step 2449: Updated gated ratio to 0.9057 (progress: 94.3%)

Step 2449: Updated gated ratio to 0.9057 (progress: 94.3%)
Step 2449: Updated gated ratio to 0.9057 (progress: 94.3%)
Step 2449: Updated gated ratio to 0.9057 (progress: 94.3%)
Step 2449: Updated gated ratio to 0.9057 (progress: 94.3%)
{'loss': 1.1591, 'grad_norm': 1.5717980861663818, 'learning_rate': 1.1415859222344525e-05, 'epoch': 0.47}
{'loss': 1.0905, 'grad_norm': 1.526205062866211, 'learning_rate': 1.14096908714317e-05, 'epoch': 0.47}
{'loss': 1.1955, 'grad_norm': 1.6495895385742188, 'learning_rate': 1.1403521973228342e-05, 'epoch': 0.47}
{'loss': 1.1327, 'grad_norm': 1.6402586698532104, 'learning_rate': 1.1397352530129428e-05, 'epoch': 0.47}
{'loss': 1.0979, 'grad_norm': 1.5524563789367676, 'learning_rate': 1.139118254453015e-05, 'epoch': 0.47}
Step 2454: Updated gated ratio to 0.9055 (progress: 94.5%)
Step 2454: Updated gated ratio to 0.9055 (progress: 94.5%)
Step 2454: Updated gated ratio to 0.9055 (progress: 94.5%)
Step 2454: Updated gated ratio to 0.9055 (progress: 94.5%)
Step 2454: Updated gated ratio to 0.9055 (progress: 94.5%)
Step 2454: Updated gated ratio to 0.9055 (progress: 94.5%)
Step 2454: Updated gated ratio to 0.9055 (progress: 94.5%)
Step 2454: Updated gated ratio to 0.9055 (progress: 94.5%)
Step 2454: Updated gated ratio to 0.9055 (progress: 94.5%)
Step 2454: Updated gated ratio to 0.9055 (progress: 94.5%)
Step 2454: Updated gated ratio to 0.9055 (progress: 94.5%)
Step 2454: Updated gated ratio to 0.9055 (progress: 94.5%)
Step 2454: Updated gated ratio to 0.9055 (progress: 94.5%)
Step 2454: Updated gated ratio to 0.9055 (progress: 94.5%)
Step 2454: Updated gated ratio to 0.9055 (progress: 94.5%)
Step 2454: Updated gated ratio to 0.9055 (progress: 94.5%)
{'loss': 1.1518, 'grad_norm': 1.691253423690796, 'learning_rate': 1.1385012018825907e-05, 'epoch': 0.47}
{'loss': 1.155, 'grad_norm': 1.6143369674682617, 'learning_rate': 1.1378840955412313e-05, 'epoch': 0.47}
{'loss': 1.1351, 'grad_norm': 1.5158147811889648, 'learning_rate': 1.1372669356685185e-05, 'epoch': 0.47}
{'loss': 1.2062, 'grad_norm': 1.259045958518982, 'learning_rate': 1.1366497225040549e-05, 'epoch': 0.47}
{'loss': 0.9929, 'grad_norm': 1.4499586820602417, 'learning_rate': 1.1360324562874643e-05, 'epoch': 0.47}
{'loss': 1.0904, 'grad_norm': 1.500421166419983, 'learning_rate': 1.1354151372583901e-05, 'epoch': 0.47}
Step 2460: Updated gated ratio to 0.9053 (progress: 94.7%)
Step 2460: Updated gated ratio to 0.9053 (progress: 94.7%)
Step 2460: Updated gated ratio to 0.9053 (progress: 94.7%)
Step 2460: Updated gated ratio to 0.9053 (progress: 94.7%)
Step 2460: Updated gated ratio to 0.9053 (progress: 94.7%)
Step 2460: Updated gated ratio to 0.9053 (progress: 94.7%)
Step 2460: Updated gated ratio to 0.9053 (progress: 94.7%)
Step 2460: Updated gated ratio to 0.9053 (progress: 94.7%)
Step 2460: Updated gated ratio to 0.9053 (progress: 94.7%)
Step 2460: Updated gated ratio to 0.9053 (progress: 94.7%)
Step 2460: Updated gated ratio to 0.9053 (progress: 94.7%)
Step 2460: Updated gated ratio to 0.9053 (progress: 94.7%)
Step 2460: Updated gated ratio to 0.9053 (progress: 94.7%)
Step 2460: Updated gated ratio to 0.9053 (progress: 94.7%)
Step 2460: Updated gated ratio to 0.9053 (progress: 94.7%)
Step 2460: Updated gated ratio to 0.9053 (progress: 94.7%)
{'loss': 1.1942, 'grad_norm': 1.5852996110916138, 'learning_rate': 1.1347977656564974e-05, 'epoch': 0.47}
{'loss': 1.0613, 'grad_norm': 1.6251741647720337, 'learning_rate': 1.1341803417214705e-05, 'epoch': 0.47}
{'loss': 1.2764, 'grad_norm': 1.2564899921417236, 'learning_rate': 1.1335628656930153e-05, 'epoch': 0.47}
{'loss': 1.1189, 'grad_norm': 1.4900825023651123, 'learning_rate': 1.132945337810857e-05, 'epoch': 0.47}
{'loss': 1.2406, 'grad_norm': 1.3451627492904663, 'learning_rate': 1.132327758314741e-05, 'epoch': 0.47}
Step 2465: Updated gated ratio to 0.9051 (progress: 94.9%)
Step 2465: Updated gated ratio to 0.9051 (progress: 94.9%)
Step 2465: Updated gated ratio to 0.9051 (progress: 94.9%)
Step 2465: Updated gated ratio to 0.9051 (progress: 94.9%)
Step 2465: Updated gated ratio to 0.9051 (progress: 94.9%)
Step 2465: Updated gated ratio to 0.9051 (progress: 94.9%)
Step 2465: Updated gated ratio to 0.9051 (progress: 94.9%)
Step 2465: Updated gated ratio to 0.9051 (progress: 94.9%)
Step 2465: Updated gated ratio to 0.9051 (progress: 94.9%)
Step 2465: Updated gated ratio to 0.9051 (progress: 94.9%)
Step 2465: Updated gated ratio to 0.9051 (progress: 94.9%)
Step 2465: Updated gated ratio to 0.9051 (progress: 94.9%)
Step 2465: Updated gated ratio to 0.9051 (progress: 94.9%)
Step 2465: Updated gated ratio to 0.9051 (progress: 94.9%)
Step 2465: Updated gated ratio to 0.9051 (progress: 94.9%)
Step 2465: Updated gated ratio to 0.9051 (progress: 94.9%)
{'loss': 1.1069, 'grad_norm': 1.5586825609207153, 'learning_rate': 1.131710127444433e-05, 'epoch': 0.47}
{'loss': 1.1883, 'grad_norm': 1.6711159944534302, 'learning_rate': 1.1310924454397187e-05, 'epoch': 0.47}
{'loss': 1.0468, 'grad_norm': 1.622916579246521, 'learning_rate': 1.1304747125404031e-05, 'epoch': 0.47}
{'loss': 1.1943, 'grad_norm': 1.563905119895935, 'learning_rate': 1.129856928986312e-05, 'epoch': 0.47}
{'loss': 1.0619, 'grad_norm': 1.5529297590255737, 'learning_rate': 1.12923909501729e-05, 'epoch': 0.48}
Step 2470: Updated gated ratio to 0.9049 (progress: 95.1%)
Step 2470: Updated gated ratio to 0.9049 (progress: 95.1%)
Step 2470: Updated gated ratio to 0.9049 (progress: 95.1%)
Step 2470: Updated gated ratio to 0.9049 (progress: 95.1%)
Step 2470: Updated gated ratio to 0.9049 (progress: 95.1%)
Step 2470: Updated gated ratio to 0.9049 (progress: 95.1%)
Step 2470: Updated gated ratio to 0.9049 (progress: 95.1%)
Step 2470: Updated gated ratio to 0.9049 (progress: 95.1%)
Step 2470: Updated gated ratio to 0.9049 (progress: 95.1%)
Step 2470: Updated gated ratio to 0.9049 (progress: 95.1%)
Step 2470: Updated gated ratio to 0.9049 (progress: 95.1%)
Step 2470: Updated gated ratio to 0.9049 (progress: 95.1%)
Step 2470: Updated gated ratio to 0.9049 (progress: 95.1%)
Step 2470: Updated gated ratio to 0.9049 (progress: 95.1%)
Step 2470: Updated gated ratio to 0.9049 (progress: 95.1%)
Step 2470: Updated gated ratio to 0.9049 (progress: 95.1%)
{'loss': 1.1479, 'grad_norm': 1.6192272901535034, 'learning_rate': 1.1286212108732015e-05, 'epoch': 0.48}
WARNING: tokenization mismatch: 0 vs. 676. (ignored)
{'loss': 1.2037, 'grad_norm': 1.5725412368774414, 'learning_rate': 1.1280032767939302e-05, 'epoch': 0.48}
{'loss': 1.2626, 'grad_norm': 1.5297642946243286, 'learning_rate': 1.1273852930193798e-05, 'epoch': 0.48}
{'loss': 1.2399, 'grad_norm': 1.4444624185562134, 'learning_rate': 1.1267672597894725e-05, 'epoch': 0.48}
{'loss': 1.1662, 'grad_norm': 1.5993051528930664, 'learning_rate': 1.12614917734415e-05, 'epoch': 0.48}
Step 2475: Updated gated ratio to 0.9047 (progress: 95.3%)
Step 2475: Updated gated ratio to 0.9047 (progress: 95.3%)
Step 2475: Updated gated ratio to 0.9047 (progress: 95.3%)
Step 2475: Updated gated ratio to 0.9047 (progress: 95.3%)
Step 2475: Updated gated ratio to 0.9047 (progress: 95.3%)
Step 2475: Updated gated ratio to 0.9047 (progress: 95.3%)
Step 2475: Updated gated ratio to 0.9047 (progress: 95.3%)
Step 2475: Updated gated ratio to 0.9047 (progress: 95.3%)
Step 2475: Updated gated ratio to 0.9047 (progress: 95.3%)
Step 2475: Updated gated ratio to 0.9047 (progress: 95.3%)
Step 2475: Updated gated ratio to 0.9047 (progress: 95.3%)
Step 2475: Updated gated ratio to 0.9047 (progress: 95.3%)
Step 2475: Updated gated ratio to 0.9047 (progress: 95.3%)
Step 2475: Updated gated ratio to 0.9047 (progress: 95.3%)
Step 2475: Updated gated ratio to 0.9047 (progress: 95.3%)
Step 2475: Updated gated ratio to 0.9047 (progress: 95.3%)
{'loss': 1.0898, 'grad_norm': 1.560355544090271, 'learning_rate': 1.1255310459233737e-05, 'epoch': 0.48}
{'loss': 1.1079, 'grad_norm': 1.6848304271697998, 'learning_rate': 1.1249128657671233e-05, 'epoch': 0.48}
{'loss': 1.1075, 'grad_norm': 1.548728585243225, 'learning_rate': 1.1242946371153974e-05, 'epoch': 0.48}
{'loss': 1.0937, 'grad_norm': 1.6230379343032837, 'learning_rate': 1.1236763602082136e-05, 'epoch': 0.48}
{'loss': 1.0081, 'grad_norm': 1.547179102897644, 'learning_rate': 1.1230580352856088e-05, 'epoch': 0.48}
Step 2480: Updated gated ratio to 0.9045 (progress: 95.5%)
Step 2480: Updated gated ratio to 0.9045 (progress: 95.5%)
Step 2480: Updated gated ratio to 0.9045 (progress: 95.5%)
Step 2480: Updated gated ratio to 0.9045 (progress: 95.5%)
Step 2480: Updated gated ratio to 0.9045 (progress: 95.5%)
Step 2480: Updated gated ratio to 0.9045 (progress: 95.5%)
Step 2480: Updated gated ratio to 0.9045 (progress: 95.5%)
Step 2480: Updated gated ratio to 0.9045 (progress: 95.5%)
Step 2480: Updated gated ratio to 0.9045 (progress: 95.5%)
Step 2480: Updated gated ratio to 0.9045 (progress: 95.5%)
Step 2480: Updated gated ratio to 0.9045 (progress: 95.5%)
Step 2480: Updated gated ratio to 0.9045 (progress: 95.5%)
Step 2480: Updated gated ratio to 0.9045 (progress: 95.5%)
Step 2480: Updated gated ratio to 0.9045 (progress: 95.5%)Step 2480: Updated gated ratio to 0.9045 (progress: 95.5%)

Step 2480: Updated gated ratio to 0.9045 (progress: 95.5%)
{'loss': 1.2334, 'grad_norm': 1.579396367073059, 'learning_rate': 1.1224396625876375e-05, 'epoch': 0.48}
{'loss': 1.0944, 'grad_norm': 1.5187900066375732, 'learning_rate': 1.1218212423543734e-05, 'epoch': 0.48}
{'loss': 1.1797, 'grad_norm': 1.5369049310684204, 'learning_rate': 1.1212027748259086e-05, 'epoch': 0.48}
{'loss': 1.0943, 'grad_norm': 1.4427990913391113, 'learning_rate': 1.1205842602423537e-05, 'epoch': 0.48}
{'loss': 1.1999, 'grad_norm': 1.6246615648269653, 'learning_rate': 1.1199656988438373e-05, 'epoch': 0.48}
Step 2485: Updated gated ratio to 0.9043 (progress: 95.7%)
Step 2485: Updated gated ratio to 0.9043 (progress: 95.7%)
Step 2485: Updated gated ratio to 0.9043 (progress: 95.7%)
Step 2485: Updated gated ratio to 0.9043 (progress: 95.7%)
Step 2485: Updated gated ratio to 0.9043 (progress: 95.7%)
Step 2485: Updated gated ratio to 0.9043 (progress: 95.7%)
Step 2485: Updated gated ratio to 0.9043 (progress: 95.7%)
Step 2485: Updated gated ratio to 0.9043 (progress: 95.7%)
Step 2485: Updated gated ratio to 0.9043 (progress: 95.7%)
Step 2485: Updated gated ratio to 0.9043 (progress: 95.7%)
Step 2485: Updated gated ratio to 0.9043 (progress: 95.7%)
Step 2485: Updated gated ratio to 0.9043 (progress: 95.7%)
Step 2485: Updated gated ratio to 0.9043 (progress: 95.7%)
Step 2485: Updated gated ratio to 0.9043 (progress: 95.7%)
Step 2485: Updated gated ratio to 0.9043 (progress: 95.7%)
Step 2485: Updated gated ratio to 0.9043 (progress: 95.7%)
{'loss': 1.123, 'grad_norm': 1.6048967838287354, 'learning_rate': 1.1193470908705055e-05, 'epoch': 0.48}
{'loss': 1.1317, 'grad_norm': 1.5237691402435303, 'learning_rate': 1.1187284365625241e-05, 'epoch': 0.48}
{'loss': 1.1023, 'grad_norm': 1.5643891096115112, 'learning_rate': 1.1181097361600754e-05, 'epoch': 0.48}
{'loss': 1.1602, 'grad_norm': 1.4545053243637085, 'learning_rate': 1.1174909899033608e-05, 'epoch': 0.48}
{'loss': 1.2707, 'grad_norm': 1.377805233001709, 'learning_rate': 1.1168721980325987e-05, 'epoch': 0.48}
{'loss': 1.2213, 'grad_norm': 1.535077691078186, 'learning_rate': 1.1162533607880251e-05, 'epoch': 0.48}
Step 2491: Updated gated ratio to 0.9041 (progress: 95.9%)
Step 2491: Updated gated ratio to 0.9041 (progress: 95.9%)
Step 2491: Updated gated ratio to 0.9041 (progress: 95.9%)
Step 2491: Updated gated ratio to 0.9041 (progress: 95.9%)
Step 2491: Updated gated ratio to 0.9041 (progress: 95.9%)
Step 2491: Updated gated ratio to 0.9041 (progress: 95.9%)
Step 2491: Updated gated ratio to 0.9041 (progress: 95.9%)
Step 2491: Updated gated ratio to 0.9041 (progress: 95.9%)
Step 2491: Updated gated ratio to 0.9041 (progress: 95.9%)
Step 2491: Updated gated ratio to 0.9041 (progress: 95.9%)
Step 2491: Updated gated ratio to 0.9041 (progress: 95.9%)
Step 2491: Updated gated ratio to 0.9041 (progress: 95.9%)
Step 2491: Updated gated ratio to 0.9041 (progress: 95.9%)
Step 2491: Updated gated ratio to 0.9041 (progress: 95.9%)
Step 2491: Updated gated ratio to 0.9041 (progress: 95.9%)
Step 2491: Updated gated ratio to 0.9041 (progress: 95.9%)
{'loss': 1.1061, 'grad_norm': 1.6668003797531128, 'learning_rate': 1.1156344784098942e-05, 'epoch': 0.48}
{'loss': 1.0145, 'grad_norm': 1.4955087900161743, 'learning_rate': 1.1150155511384772e-05, 'epoch': 0.48}
{'loss': 1.0927, 'grad_norm': 1.5699166059494019, 'learning_rate': 1.1143965792140631e-05, 'epoch': 0.48}
{'loss': 1.2521, 'grad_norm': 1.627436637878418, 'learning_rate': 1.1137775628769584e-05, 'epoch': 0.48}
{'loss': 1.0887, 'grad_norm': 1.5379483699798584, 'learning_rate': 1.1131585023674863e-05, 'epoch': 0.48}
Step 2496: Updated gated ratio to 0.9039 (progress: 96.1%)Step 2496: Updated gated ratio to 0.9039 (progress: 96.1%)
Step 2496: Updated gated ratio to 0.9039 (progress: 96.1%)
Step 2496: Updated gated ratio to 0.9039 (progress: 96.1%)
Step 2496: Updated gated ratio to 0.9039 (progress: 96.1%)
Step 2496: Updated gated ratio to 0.9039 (progress: 96.1%)
Step 2496: Updated gated ratio to 0.9039 (progress: 96.1%)
Step 2496: Updated gated ratio to 0.9039 (progress: 96.1%)
Step 2496: Updated gated ratio to 0.9039 (progress: 96.1%)
Step 2496: Updated gated ratio to 0.9039 (progress: 96.1%)
Step 2496: Updated gated ratio to 0.9039 (progress: 96.1%)
Step 2496: Updated gated ratio to 0.9039 (progress: 96.1%)Step 2496: Updated gated ratio to 0.9039 (progress: 96.1%)


Step 2496: Updated gated ratio to 0.9039 (progress: 96.1%)
Step 2496: Updated gated ratio to 0.9039 (progress: 96.1%)
Step 2496: Updated gated ratio to 0.9039 (progress: 96.1%)
{'loss': 1.0987, 'grad_norm': 1.5413213968276978, 'learning_rate': 1.1125393979259874e-05, 'epoch': 0.48}
{'loss': 1.0993, 'grad_norm': 1.516106128692627, 'learning_rate': 1.1119202497928192e-05, 'epoch': 0.48}
{'loss': 1.086, 'grad_norm': 1.5134122371673584, 'learning_rate': 1.1113010582083568e-05, 'epoch': 0.48}
{'loss': 1.0887, 'grad_norm': 1.5844250917434692, 'learning_rate': 1.1106818234129913e-05, 'epoch': 0.48}
{'loss': 1.1051, 'grad_norm': 1.6509283781051636, 'learning_rate': 1.1100625456471307e-05, 'epoch': 0.48}
Step 2501: Updated gated ratio to 0.9037 (progress: 96.3%)
Step 2501: Updated gated ratio to 0.9037 (progress: 96.3%)
Step 2501: Updated gated ratio to 0.9037 (progress: 96.3%)
Step 2501: Updated gated ratio to 0.9037 (progress: 96.3%)
Step 2501: Updated gated ratio to 0.9037 (progress: 96.3%)
Step 2501: Updated gated ratio to 0.9037 (progress: 96.3%)
Step 2501: Updated gated ratio to 0.9037 (progress: 96.3%)
Step 2501: Updated gated ratio to 0.9037 (progress: 96.3%)
Step 2501: Updated gated ratio to 0.9037 (progress: 96.3%)Step 2501: Updated gated ratio to 0.9037 (progress: 96.3%)

Step 2501: Updated gated ratio to 0.9037 (progress: 96.3%)
Step 2501: Updated gated ratio to 0.9037 (progress: 96.3%)
Step 2501: Updated gated ratio to 0.9037 (progress: 96.3%)Step 2501: Updated gated ratio to 0.9037 (progress: 96.3%)

Step 2501: Updated gated ratio to 0.9037 (progress: 96.3%)
Step 2501: Updated gated ratio to 0.9037 (progress: 96.3%)
{'loss': 1.0348, 'grad_norm': 1.6288753747940063, 'learning_rate': 1.1094432251512006e-05, 'epoch': 0.48}
{'loss': 1.2217, 'grad_norm': 1.4323962926864624, 'learning_rate': 1.1088238621656422e-05, 'epoch': 0.48}
{'loss': 1.1398, 'grad_norm': 1.5348520278930664, 'learning_rate': 1.1082044569309138e-05, 'epoch': 0.48}
{'loss': 1.264, 'grad_norm': 1.3518849611282349, 'learning_rate': 1.1075850096874894e-05, 'epoch': 0.48}
{'loss': 1.0522, 'grad_norm': 1.5392497777938843, 'learning_rate': 1.1069655206758603e-05, 'epoch': 0.48}
Step 2506: Updated gated ratio to 0.9035 (progress: 96.5%)
Step 2506: Updated gated ratio to 0.9035 (progress: 96.5%)
Step 2506: Updated gated ratio to 0.9035 (progress: 96.5%)
Step 2506: Updated gated ratio to 0.9035 (progress: 96.5%)
Step 2506: Updated gated ratio to 0.9035 (progress: 96.5%)
Step 2506: Updated gated ratio to 0.9035 (progress: 96.5%)
Step 2506: Updated gated ratio to 0.9035 (progress: 96.5%)
Step 2506: Updated gated ratio to 0.9035 (progress: 96.5%)
Step 2506: Updated gated ratio to 0.9035 (progress: 96.5%)
Step 2506: Updated gated ratio to 0.9035 (progress: 96.5%)
Step 2506: Updated gated ratio to 0.9035 (progress: 96.5%)
Step 2506: Updated gated ratio to 0.9035 (progress: 96.5%)
Step 2506: Updated gated ratio to 0.9035 (progress: 96.5%)
Step 2506: Updated gated ratio to 0.9035 (progress: 96.5%)
Step 2506: Updated gated ratio to 0.9035 (progress: 96.5%)
Step 2506: Updated gated ratio to 0.9035 (progress: 96.5%)
{'loss': 1.0857, 'grad_norm': 1.6277931928634644, 'learning_rate': 1.1063459901365325e-05, 'epoch': 0.48}
{'loss': 1.1295, 'grad_norm': 1.5179439783096313, 'learning_rate': 1.1057264183100303e-05, 'epoch': 0.48}
{'loss': 1.1842, 'grad_norm': 1.5363458395004272, 'learning_rate': 1.1051068054368921e-05, 'epoch': 0.48}
{'loss': 1.1358, 'grad_norm': 1.5596094131469727, 'learning_rate': 1.104487151757673e-05, 'epoch': 0.48}
{'loss': 1.2096, 'grad_norm': 1.5800750255584717, 'learning_rate': 1.1038674575129442e-05, 'epoch': 0.48}
Step 2511: Updated gated ratio to 0.9033 (progress: 96.7%)Step 2511: Updated gated ratio to 0.9033 (progress: 96.7%)
Step 2511: Updated gated ratio to 0.9033 (progress: 96.7%)
Step 2511: Updated gated ratio to 0.9033 (progress: 96.7%)
Step 2511: Updated gated ratio to 0.9033 (progress: 96.7%)
Step 2511: Updated gated ratio to 0.9033 (progress: 96.7%)
Step 2511: Updated gated ratio to 0.9033 (progress: 96.7%)
Step 2511: Updated gated ratio to 0.9033 (progress: 96.7%)
Step 2511: Updated gated ratio to 0.9033 (progress: 96.7%)
Step 2511: Updated gated ratio to 0.9033 (progress: 96.7%)

Step 2511: Updated gated ratio to 0.9033 (progress: 96.7%)
Step 2511: Updated gated ratio to 0.9033 (progress: 96.7%)
Step 2511: Updated gated ratio to 0.9033 (progress: 96.7%)
Step 2511: Updated gated ratio to 0.9033 (progress: 96.7%)
Step 2511: Updated gated ratio to 0.9033 (progress: 96.7%)
Step 2511: Updated gated ratio to 0.9033 (progress: 96.7%)
{'loss': 1.1355, 'grad_norm': 1.6493805646896362, 'learning_rate': 1.1032477229432921e-05, 'epoch': 0.48}
{'loss': 1.0839, 'grad_norm': 1.7196582555770874, 'learning_rate': 1.1026279482893187e-05, 'epoch': 0.48}
{'loss': 1.0386, 'grad_norm': 1.4534114599227905, 'learning_rate': 1.1020081337916425e-05, 'epoch': 0.48}
{'loss': 1.0301, 'grad_norm': 1.5669296979904175, 'learning_rate': 1.1013882796908963e-05, 'epoch': 0.48}
{'loss': 1.1999, 'grad_norm': 1.1998424530029297, 'learning_rate': 1.1007683862277292e-05, 'epoch': 0.48}
{'loss': 1.124, 'grad_norm': 1.6270731687545776, 'learning_rate': 1.1001484536428052e-05, 'epoch': 0.48}
Step 2517: Updated gated ratio to 0.9031 (progress: 96.9%)
Step 2517: Updated gated ratio to 0.9031 (progress: 96.9%)
Step 2517: Updated gated ratio to 0.9031 (progress: 96.9%)
Step 2517: Updated gated ratio to 0.9031 (progress: 96.9%)
Step 2517: Updated gated ratio to 0.9031 (progress: 96.9%)
Step 2517: Updated gated ratio to 0.9031 (progress: 96.9%)
Step 2517: Updated gated ratio to 0.9031 (progress: 96.9%)Step 2517: Updated gated ratio to 0.9031 (progress: 96.9%)

Step 2517: Updated gated ratio to 0.9031 (progress: 96.9%)
Step 2517: Updated gated ratio to 0.9031 (progress: 96.9%)
Step 2517: Updated gated ratio to 0.9031 (progress: 96.9%)Step 2517: Updated gated ratio to 0.9031 (progress: 96.9%)

Step 2517: Updated gated ratio to 0.9031 (progress: 96.9%)Step 2517: Updated gated ratio to 0.9031 (progress: 96.9%)

Step 2517: Updated gated ratio to 0.9031 (progress: 96.9%)
Step 2517: Updated gated ratio to 0.9031 (progress: 96.9%)
{'loss': 1.0649, 'grad_norm': 1.5899327993392944, 'learning_rate': 1.0995284821768029e-05, 'epoch': 0.48}
{'loss': 1.0609, 'grad_norm': 1.5732094049453735, 'learning_rate': 1.098908472070417e-05, 'epoch': 0.48}
{'loss': 1.2117, 'grad_norm': 1.616701364517212, 'learning_rate': 1.0982884235643567e-05, 'epoch': 0.48}
{'loss': 1.1288, 'grad_norm': 1.5460878610610962, 'learning_rate': 1.0976683368993464e-05, 'epoch': 0.48}
{'loss': 1.1164, 'grad_norm': 1.5901020765304565, 'learning_rate': 1.0970482123161249e-05, 'epoch': 0.49}
Step 2522: Updated gated ratio to 0.9029 (progress: 97.1%)
Step 2522: Updated gated ratio to 0.9029 (progress: 97.1%)
Step 2522: Updated gated ratio to 0.9029 (progress: 97.1%)
Step 2522: Updated gated ratio to 0.9029 (progress: 97.1%)
Step 2522: Updated gated ratio to 0.9029 (progress: 97.1%)
Step 2522: Updated gated ratio to 0.9029 (progress: 97.1%)
Step 2522: Updated gated ratio to 0.9029 (progress: 97.1%)
Step 2522: Updated gated ratio to 0.9029 (progress: 97.1%)
Step 2522: Updated gated ratio to 0.9029 (progress: 97.1%)
Step 2522: Updated gated ratio to 0.9029 (progress: 97.1%)
Step 2522: Updated gated ratio to 0.9029 (progress: 97.1%)Step 2522: Updated gated ratio to 0.9029 (progress: 97.1%)
Step 2522: Updated gated ratio to 0.9029 (progress: 97.1%)
Step 2522: Updated gated ratio to 0.9029 (progress: 97.1%)
Step 2522: Updated gated ratio to 0.9029 (progress: 97.1%)

Step 2522: Updated gated ratio to 0.9029 (progress: 97.1%)
{'loss': 1.1356, 'grad_norm': 1.5085474252700806, 'learning_rate': 1.0964280500554459e-05, 'epoch': 0.49}
{'loss': 1.1693, 'grad_norm': 1.4804487228393555, 'learning_rate': 1.0958078503580776e-05, 'epoch': 0.49}
{'loss': 1.183, 'grad_norm': 1.5597100257873535, 'learning_rate': 1.0951876134648032e-05, 'epoch': 0.49}
{'loss': 1.1217, 'grad_norm': 1.6270782947540283, 'learning_rate': 1.0945673396164198e-05, 'epoch': 0.49}
{'loss': 1.0661, 'grad_norm': 1.5650763511657715, 'learning_rate': 1.0939470290537389e-05, 'epoch': 0.49}
Step 2527: Updated gated ratio to 0.9027 (progress: 97.3%)
Step 2527: Updated gated ratio to 0.9027 (progress: 97.3%)
Step 2527: Updated gated ratio to 0.9027 (progress: 97.3%)
Step 2527: Updated gated ratio to 0.9027 (progress: 97.3%)
Step 2527: Updated gated ratio to 0.9027 (progress: 97.3%)Step 2527: Updated gated ratio to 0.9027 (progress: 97.3%)
Step 2527: Updated gated ratio to 0.9027 (progress: 97.3%)
Step 2527: Updated gated ratio to 0.9027 (progress: 97.3%)
Step 2527: Updated gated ratio to 0.9027 (progress: 97.3%)
Step 2527: Updated gated ratio to 0.9027 (progress: 97.3%)
Step 2527: Updated gated ratio to 0.9027 (progress: 97.3%)

Step 2527: Updated gated ratio to 0.9027 (progress: 97.3%)
Step 2527: Updated gated ratio to 0.9027 (progress: 97.3%)
Step 2527: Updated gated ratio to 0.9027 (progress: 97.3%)
Step 2527: Updated gated ratio to 0.9027 (progress: 97.3%)
Step 2527: Updated gated ratio to 0.9027 (progress: 97.3%)
{'loss': 1.1533, 'grad_norm': 1.669898271560669, 'learning_rate': 1.0933266820175868e-05, 'epoch': 0.49}
{'loss': 1.1332, 'grad_norm': 1.5699540376663208, 'learning_rate': 1.0927062987488035e-05, 'epoch': 0.49}
{'loss': 1.1147, 'grad_norm': 1.4968749284744263, 'learning_rate': 1.0920858794882429e-05, 'epoch': 0.49}
{'loss': 1.1104, 'grad_norm': 1.8372993469238281, 'learning_rate': 1.0914654244767736e-05, 'epoch': 0.49}
{'loss': 1.0002, 'grad_norm': 1.4261655807495117, 'learning_rate': 1.0908449339552769e-05, 'epoch': 0.49}
Step 2532: Updated gated ratio to 0.9025 (progress: 97.5%)
Step 2532: Updated gated ratio to 0.9025 (progress: 97.5%)
Step 2532: Updated gated ratio to 0.9025 (progress: 97.5%)
Step 2532: Updated gated ratio to 0.9025 (progress: 97.5%)Step 2532: Updated gated ratio to 0.9025 (progress: 97.5%)

Step 2532: Updated gated ratio to 0.9025 (progress: 97.5%)
Step 2532: Updated gated ratio to 0.9025 (progress: 97.5%)
Step 2532: Updated gated ratio to 0.9025 (progress: 97.5%)
Step 2532: Updated gated ratio to 0.9025 (progress: 97.5%)
Step 2532: Updated gated ratio to 0.9025 (progress: 97.5%)
Step 2532: Updated gated ratio to 0.9025 (progress: 97.5%)
Step 2532: Updated gated ratio to 0.9025 (progress: 97.5%)
Step 2532: Updated gated ratio to 0.9025 (progress: 97.5%)Step 2532: Updated gated ratio to 0.9025 (progress: 97.5%)

Step 2532: Updated gated ratio to 0.9025 (progress: 97.5%)
Step 2532: Updated gated ratio to 0.9025 (progress: 97.5%)
{'loss': 1.1958, 'grad_norm': 1.890079379081726, 'learning_rate': 1.0902244081646489e-05, 'epoch': 0.49}
{'loss': 1.0669, 'grad_norm': 1.6147109270095825, 'learning_rate': 1.0896038473457993e-05, 'epoch': 0.49}
{'loss': 1.0776, 'grad_norm': 1.4361003637313843, 'learning_rate': 1.0889832517396511e-05, 'epoch': 0.49}
{'loss': 1.1457, 'grad_norm': 1.2427364587783813, 'learning_rate': 1.0883626215871408e-05, 'epoch': 0.49}
{'loss': 1.1282, 'grad_norm': 1.535219669342041, 'learning_rate': 1.0877419571292183e-05, 'epoch': 0.49}
Step 2537: Updated gated ratio to 0.9023 (progress: 97.7%)
Step 2537: Updated gated ratio to 0.9023 (progress: 97.7%)
Step 2537: Updated gated ratio to 0.9023 (progress: 97.7%)
Step 2537: Updated gated ratio to 0.9023 (progress: 97.7%)
Step 2537: Updated gated ratio to 0.9023 (progress: 97.7%)Step 2537: Updated gated ratio to 0.9023 (progress: 97.7%)

Step 2537: Updated gated ratio to 0.9023 (progress: 97.7%)
Step 2537: Updated gated ratio to 0.9023 (progress: 97.7%)Step 2537: Updated gated ratio to 0.9023 (progress: 97.7%)

Step 2537: Updated gated ratio to 0.9023 (progress: 97.7%)
Step 2537: Updated gated ratio to 0.9023 (progress: 97.7%)
Step 2537: Updated gated ratio to 0.9023 (progress: 97.7%)
Step 2537: Updated gated ratio to 0.9023 (progress: 97.7%)
Step 2537: Updated gated ratio to 0.9023 (progress: 97.7%)
Step 2537: Updated gated ratio to 0.9023 (progress: 97.7%)
Step 2537: Updated gated ratio to 0.9023 (progress: 97.7%)
{'loss': 1.0924, 'grad_norm': 1.4589120149612427, 'learning_rate': 1.0871212586068469e-05, 'epoch': 0.49}
{'loss': 1.2116, 'grad_norm': 1.6745811700820923, 'learning_rate': 1.0865005262610033e-05, 'epoch': 0.49}
{'loss': 1.063, 'grad_norm': 1.6139942407608032, 'learning_rate': 1.085879760332677e-05, 'epoch': 0.49}
{'loss': 1.0765, 'grad_norm': 1.6723743677139282, 'learning_rate': 1.085258961062871e-05, 'epoch': 0.49}
{'loss': 1.024, 'grad_norm': 1.6235624551773071, 'learning_rate': 1.0846381286926007e-05, 'epoch': 0.49}
{'loss': 1.0487, 'grad_norm': 1.684143304824829, 'learning_rate': 1.0840172634628948e-05, 'epoch': 0.49}
Step 2543: Updated gated ratio to 0.9021 (progress: 97.9%)
Step 2543: Updated gated ratio to 0.9021 (progress: 97.9%)
Step 2543: Updated gated ratio to 0.9021 (progress: 97.9%)
Step 2543: Updated gated ratio to 0.9021 (progress: 97.9%)Step 2543: Updated gated ratio to 0.9021 (progress: 97.9%)
Step 2543: Updated gated ratio to 0.9021 (progress: 97.9%)
Step 2543: Updated gated ratio to 0.9021 (progress: 97.9%)
Step 2543: Updated gated ratio to 0.9021 (progress: 97.9%)

Step 2543: Updated gated ratio to 0.9021 (progress: 97.9%)
Step 2543: Updated gated ratio to 0.9021 (progress: 97.9%)
Step 2543: Updated gated ratio to 0.9021 (progress: 97.9%)
Step 2543: Updated gated ratio to 0.9021 (progress: 97.9%)
Step 2543: Updated gated ratio to 0.9021 (progress: 97.9%)
Step 2543: Updated gated ratio to 0.9021 (progress: 97.9%)
Step 2543: Updated gated ratio to 0.9021 (progress: 97.9%)
Step 2543: Updated gated ratio to 0.9021 (progress: 97.9%)
{'loss': 1.1242, 'grad_norm': 1.6259547472000122, 'learning_rate': 1.0833963656147944e-05, 'epoch': 0.49}
{'loss': 1.0849, 'grad_norm': 1.6545827388763428, 'learning_rate': 1.082775435389353e-05, 'epoch': 0.49}
{'loss': 1.1738, 'grad_norm': 1.453609585762024, 'learning_rate': 1.0821544730276379e-05, 'epoch': 0.49}
{'loss': 1.1394, 'grad_norm': 1.533734679222107, 'learning_rate': 1.0815334787707277e-05, 'epoch': 0.49}
{'loss': 1.0812, 'grad_norm': 1.4658770561218262, 'learning_rate': 1.0809124528597138e-05, 'epoch': 0.49}
Step 2548: Updated gated ratio to 0.9019 (progress: 98.1%)
Step 2548: Updated gated ratio to 0.9019 (progress: 98.1%)
Step 2548: Updated gated ratio to 0.9019 (progress: 98.1%)
Step 2548: Updated gated ratio to 0.9019 (progress: 98.1%)
Step 2548: Updated gated ratio to 0.9019 (progress: 98.1%)Step 2548: Updated gated ratio to 0.9019 (progress: 98.1%)
Step 2548: Updated gated ratio to 0.9019 (progress: 98.1%)
Step 2548: Updated gated ratio to 0.9019 (progress: 98.1%)

Step 2548: Updated gated ratio to 0.9019 (progress: 98.1%)
Step 2548: Updated gated ratio to 0.9019 (progress: 98.1%)
Step 2548: Updated gated ratio to 0.9019 (progress: 98.1%)
Step 2548: Updated gated ratio to 0.9019 (progress: 98.1%)
Step 2548: Updated gated ratio to 0.9019 (progress: 98.1%)
Step 2548: Updated gated ratio to 0.9019 (progress: 98.1%)
Step 2548: Updated gated ratio to 0.9019 (progress: 98.1%)
Step 2548: Updated gated ratio to 0.9019 (progress: 98.1%)
{'loss': 1.0367, 'grad_norm': 1.5127760171890259, 'learning_rate': 1.0802913955356998e-05, 'epoch': 0.49}
{'loss': 1.1174, 'grad_norm': 1.5517293214797974, 'learning_rate': 1.0796703070398016e-05, 'epoch': 0.49}
{'loss': 1.0867, 'grad_norm': 1.6895684003829956, 'learning_rate': 1.079049187613147e-05, 'epoch': 0.49}
{'loss': 1.0725, 'grad_norm': 1.6607149839401245, 'learning_rate': 1.0784280374968761e-05, 'epoch': 0.49}
{'loss': 1.0791, 'grad_norm': 1.569205641746521, 'learning_rate': 1.0778068569321403e-05, 'epoch': 0.49}
Step 2553: Updated gated ratio to 0.9017 (progress: 98.3%)
Step 2553: Updated gated ratio to 0.9017 (progress: 98.3%)
Step 2553: Updated gated ratio to 0.9017 (progress: 98.3%)
Step 2553: Updated gated ratio to 0.9017 (progress: 98.3%)
Step 2553: Updated gated ratio to 0.9017 (progress: 98.3%)
Step 2553: Updated gated ratio to 0.9017 (progress: 98.3%)
Step 2553: Updated gated ratio to 0.9017 (progress: 98.3%)
Step 2553: Updated gated ratio to 0.9017 (progress: 98.3%)
Step 2553: Updated gated ratio to 0.9017 (progress: 98.3%)
Step 2553: Updated gated ratio to 0.9017 (progress: 98.3%)
Step 2553: Updated gated ratio to 0.9017 (progress: 98.3%)
Step 2553: Updated gated ratio to 0.9017 (progress: 98.3%)
Step 2553: Updated gated ratio to 0.9017 (progress: 98.3%)
Step 2553: Updated gated ratio to 0.9017 (progress: 98.3%)
Step 2553: Updated gated ratio to 0.9017 (progress: 98.3%)
Step 2553: Updated gated ratio to 0.9017 (progress: 98.3%)
{'loss': 1.0187, 'grad_norm': 1.4949246644973755, 'learning_rate': 1.077185646160104e-05, 'epoch': 0.49}
{'loss': 1.2236, 'grad_norm': 1.4267271757125854, 'learning_rate': 1.0765644054219422e-05, 'epoch': 0.49}
{'loss': 1.0201, 'grad_norm': 1.6442296504974365, 'learning_rate': 1.0759431349588421e-05, 'epoch': 0.49}
{'loss': 1.1333, 'grad_norm': 1.5753700733184814, 'learning_rate': 1.0753218350120023e-05, 'epoch': 0.49}
{'loss': 1.2334, 'grad_norm': 1.824912428855896, 'learning_rate': 1.0747005058226325e-05, 'epoch': 0.49}
Step 2558: Updated gated ratio to 0.9015 (progress: 98.5%)
Step 2558: Updated gated ratio to 0.9015 (progress: 98.5%)Step 2558: Updated gated ratio to 0.9015 (progress: 98.5%)
Step 2558: Updated gated ratio to 0.9015 (progress: 98.5%)
Step 2558: Updated gated ratio to 0.9015 (progress: 98.5%)
Step 2558: Updated gated ratio to 0.9015 (progress: 98.5%)
Step 2558: Updated gated ratio to 0.9015 (progress: 98.5%)

Step 2558: Updated gated ratio to 0.9015 (progress: 98.5%)
Step 2558: Updated gated ratio to 0.9015 (progress: 98.5%)
Step 2558: Updated gated ratio to 0.9015 (progress: 98.5%)
Step 2558: Updated gated ratio to 0.9015 (progress: 98.5%)
Step 2558: Updated gated ratio to 0.9015 (progress: 98.5%)
Step 2558: Updated gated ratio to 0.9015 (progress: 98.5%)
Step 2558: Updated gated ratio to 0.9015 (progress: 98.5%)
Step 2558: Updated gated ratio to 0.9015 (progress: 98.5%)
Step 2558: Updated gated ratio to 0.9015 (progress: 98.5%)
{'loss': 1.1363, 'grad_norm': 1.5012154579162598, 'learning_rate': 1.0740791476319543e-05, 'epoch': 0.49}
{'loss': 1.0615, 'grad_norm': 1.5999799966812134, 'learning_rate': 1.0734577606812007e-05, 'epoch': 0.49}
{'loss': 1.1876, 'grad_norm': 1.5882840156555176, 'learning_rate': 1.0728363452116149e-05, 'epoch': 0.49}
{'loss': 1.0871, 'grad_norm': 1.5385245084762573, 'learning_rate': 1.0722149014644523e-05, 'epoch': 0.49}
{'loss': 0.9995, 'grad_norm': 1.4920240640640259, 'learning_rate': 1.0715934296809782e-05, 'epoch': 0.49}
Step 2563: Updated gated ratio to 0.9013 (progress: 98.7%)
Step 2563: Updated gated ratio to 0.9013 (progress: 98.7%)
Step 2563: Updated gated ratio to 0.9013 (progress: 98.7%)
Step 2563: Updated gated ratio to 0.9013 (progress: 98.7%)
Step 2563: Updated gated ratio to 0.9013 (progress: 98.7%)
Step 2563: Updated gated ratio to 0.9013 (progress: 98.7%)
Step 2563: Updated gated ratio to 0.9013 (progress: 98.7%)
Step 2563: Updated gated ratio to 0.9013 (progress: 98.7%)
Step 2563: Updated gated ratio to 0.9013 (progress: 98.7%)
Step 2563: Updated gated ratio to 0.9013 (progress: 98.7%)
Step 2563: Updated gated ratio to 0.9013 (progress: 98.7%)
Step 2563: Updated gated ratio to 0.9013 (progress: 98.7%)
Step 2563: Updated gated ratio to 0.9013 (progress: 98.7%)
Step 2563: Updated gated ratio to 0.9013 (progress: 98.7%)
Step 2563: Updated gated ratio to 0.9013 (progress: 98.7%)
Step 2563: Updated gated ratio to 0.9013 (progress: 98.7%)
{'loss': 1.1648, 'grad_norm': 1.7245324850082397, 'learning_rate': 1.0709719301024698e-05, 'epoch': 0.49}
{'loss': 1.0492, 'grad_norm': 1.4982025623321533, 'learning_rate': 1.0703504029702148e-05, 'epoch': 0.49}
{'loss': 1.1508, 'grad_norm': 1.4960100650787354, 'learning_rate': 1.0697288485255107e-05, 'epoch': 0.49}
{'loss': 1.1302, 'grad_norm': 1.652541160583496, 'learning_rate': 1.0691072670096669e-05, 'epoch': 0.49}
{'loss': 1.2161, 'grad_norm': 1.3217893838882446, 'learning_rate': 1.0684856586640026e-05, 'epoch': 0.49}
{'loss': 1.119, 'grad_norm': 1.6267876625061035, 'learning_rate': 1.0678640237298476e-05, 'epoch': 0.49}
Step 2569: Updated gated ratio to 0.9011 (progress: 98.9%)
Step 2569: Updated gated ratio to 0.9011 (progress: 98.9%)
Step 2569: Updated gated ratio to 0.9011 (progress: 98.9%)
Step 2569: Updated gated ratio to 0.9011 (progress: 98.9%)
Step 2569: Updated gated ratio to 0.9011 (progress: 98.9%)
Step 2569: Updated gated ratio to 0.9011 (progress: 98.9%)
Step 2569: Updated gated ratio to 0.9011 (progress: 98.9%)
Step 2569: Updated gated ratio to 0.9011 (progress: 98.9%)
Step 2569: Updated gated ratio to 0.9011 (progress: 98.9%)
Step 2569: Updated gated ratio to 0.9011 (progress: 98.9%)
Step 2569: Updated gated ratio to 0.9011 (progress: 98.9%)
Step 2569: Updated gated ratio to 0.9011 (progress: 98.9%)Step 2569: Updated gated ratio to 0.9011 (progress: 98.9%)
Step 2569: Updated gated ratio to 0.9011 (progress: 98.9%)
Step 2569: Updated gated ratio to 0.9011 (progress: 98.9%)

Step 2569: Updated gated ratio to 0.9011 (progress: 98.9%)
{'loss': 1.1504, 'grad_norm': 1.5680216550827026, 'learning_rate': 1.0672423624485423e-05, 'epoch': 0.49}
{'loss': 1.1306, 'grad_norm': 1.5655959844589233, 'learning_rate': 1.0666206750614363e-05, 'epoch': 0.49}
{'loss': 1.1722, 'grad_norm': 1.565417766571045, 'learning_rate': 1.0659989618098904e-05, 'epoch': 0.49}
{'loss': 1.191, 'grad_norm': 1.3837270736694336, 'learning_rate': 1.065377222935275e-05, 'epoch': 0.49}
{'loss': 1.0928, 'grad_norm': 1.5767472982406616, 'learning_rate': 1.0647554586789708e-05, 'epoch': 0.5}
Step 2574: Updated gated ratio to 0.9009 (progress: 99.1%)
Step 2574: Updated gated ratio to 0.9009 (progress: 99.1%)
Step 2574: Updated gated ratio to 0.9009 (progress: 99.1%)
Step 2574: Updated gated ratio to 0.9009 (progress: 99.1%)
Step 2574: Updated gated ratio to 0.9009 (progress: 99.1%)
Step 2574: Updated gated ratio to 0.9009 (progress: 99.1%)Step 2574: Updated gated ratio to 0.9009 (progress: 99.1%)

Step 2574: Updated gated ratio to 0.9009 (progress: 99.1%)
Step 2574: Updated gated ratio to 0.9009 (progress: 99.1%)
Step 2574: Updated gated ratio to 0.9009 (progress: 99.1%)
Step 2574: Updated gated ratio to 0.9009 (progress: 99.1%)
Step 2574: Updated gated ratio to 0.9009 (progress: 99.1%)
Step 2574: Updated gated ratio to 0.9009 (progress: 99.1%)
Step 2574: Updated gated ratio to 0.9009 (progress: 99.1%)
Step 2574: Updated gated ratio to 0.9009 (progress: 99.1%)
Step 2574: Updated gated ratio to 0.9009 (progress: 99.1%)
{'loss': 1.1404, 'grad_norm': 1.575250506401062, 'learning_rate': 1.064133669282368e-05, 'epoch': 0.5}
{'loss': 1.1143, 'grad_norm': 1.6759600639343262, 'learning_rate': 1.0635118549868668e-05, 'epoch': 0.5}
{'loss': 1.1358, 'grad_norm': 1.5196765661239624, 'learning_rate': 1.0628900160338764e-05, 'epoch': 0.5}
{'loss': 1.0733, 'grad_norm': 1.48789644241333, 'learning_rate': 1.0622681526648167e-05, 'epoch': 0.5}
{'loss': 1.1716, 'grad_norm': 1.6681114435195923, 'learning_rate': 1.0616462651211156e-05, 'epoch': 0.5}
Step 2579: Updated gated ratio to 0.9007 (progress: 99.3%)
Step 2579: Updated gated ratio to 0.9007 (progress: 99.3%)
Step 2579: Updated gated ratio to 0.9007 (progress: 99.3%)
Step 2579: Updated gated ratio to 0.9007 (progress: 99.3%)
Step 2579: Updated gated ratio to 0.9007 (progress: 99.3%)
Step 2579: Updated gated ratio to 0.9007 (progress: 99.3%)
Step 2579: Updated gated ratio to 0.9007 (progress: 99.3%)
Step 2579: Updated gated ratio to 0.9007 (progress: 99.3%)
Step 2579: Updated gated ratio to 0.9007 (progress: 99.3%)
Step 2579: Updated gated ratio to 0.9007 (progress: 99.3%)
Step 2579: Updated gated ratio to 0.9007 (progress: 99.3%)
Step 2579: Updated gated ratio to 0.9007 (progress: 99.3%)
Step 2579: Updated gated ratio to 0.9007 (progress: 99.3%)
Step 2579: Updated gated ratio to 0.9007 (progress: 99.3%)
Step 2579: Updated gated ratio to 0.9007 (progress: 99.3%)
Step 2579: Updated gated ratio to 0.9007 (progress: 99.3%)
{'loss': 1.0671, 'grad_norm': 1.5557630062103271, 'learning_rate': 1.0610243536442125e-05, 'epoch': 0.5}
{'loss': 1.2329, 'grad_norm': 1.2942091226577759, 'learning_rate': 1.0604024184755539e-05, 'epoch': 0.5}
{'loss': 1.1118, 'grad_norm': 1.5958410501480103, 'learning_rate': 1.0597804598565969e-05, 'epoch': 0.5}
{'loss': 1.1129, 'grad_norm': 1.574385166168213, 'learning_rate': 1.0591584780288069e-05, 'epoch': 0.5}
{'loss': 1.1326, 'grad_norm': 1.4773246049880981, 'learning_rate': 1.0585364732336587e-05, 'epoch': 0.5}
Step 2584: Updated gated ratio to 0.9005 (progress: 99.5%)
Step 2584: Updated gated ratio to 0.9005 (progress: 99.5%)
Step 2584: Updated gated ratio to 0.9005 (progress: 99.5%)
Step 2584: Updated gated ratio to 0.9005 (progress: 99.5%)Step 2584: Updated gated ratio to 0.9005 (progress: 99.5%)

Step 2584: Updated gated ratio to 0.9005 (progress: 99.5%)
Step 2584: Updated gated ratio to 0.9005 (progress: 99.5%)
Step 2584: Updated gated ratio to 0.9005 (progress: 99.5%)
Step 2584: Updated gated ratio to 0.9005 (progress: 99.5%)Step 2584: Updated gated ratio to 0.9005 (progress: 99.5%)

Step 2584: Updated gated ratio to 0.9005 (progress: 99.5%)
Step 2584: Updated gated ratio to 0.9005 (progress: 99.5%)
Step 2584: Updated gated ratio to 0.9005 (progress: 99.5%)
Step 2584: Updated gated ratio to 0.9005 (progress: 99.5%)
Step 2584: Updated gated ratio to 0.9005 (progress: 99.5%)
Step 2584: Updated gated ratio to 0.9005 (progress: 99.5%)
{'loss': 1.1751, 'grad_norm': 1.5822707414627075, 'learning_rate': 1.0579144457126365e-05, 'epoch': 0.5}
{'loss': 1.165, 'grad_norm': 1.5515843629837036, 'learning_rate': 1.057292395707232e-05, 'epoch': 0.5}
{'loss': 1.0859, 'grad_norm': 1.5700187683105469, 'learning_rate': 1.0566703234589471e-05, 'epoch': 0.5}
{'loss': 1.0438, 'grad_norm': 1.4020018577575684, 'learning_rate': 1.0560482292092912e-05, 'epoch': 0.5}
{'loss': 1.108, 'grad_norm': 1.5846716165542603, 'learning_rate': 1.0554261131997833e-05, 'epoch': 0.5}
Step 2589: Updated gated ratio to 0.9003 (progress: 99.7%)
Step 2589: Updated gated ratio to 0.9003 (progress: 99.7%)Step 2589: Updated gated ratio to 0.9003 (progress: 99.7%)

Step 2589: Updated gated ratio to 0.9003 (progress: 99.7%)
Step 2589: Updated gated ratio to 0.9003 (progress: 99.7%)
Step 2589: Updated gated ratio to 0.9003 (progress: 99.7%)
Step 2589: Updated gated ratio to 0.9003 (progress: 99.7%)
Step 2589: Updated gated ratio to 0.9003 (progress: 99.7%)
Step 2589: Updated gated ratio to 0.9003 (progress: 99.7%)
Step 2589: Updated gated ratio to 0.9003 (progress: 99.7%)
Step 2589: Updated gated ratio to 0.9003 (progress: 99.7%)
Step 2589: Updated gated ratio to 0.9003 (progress: 99.7%)
Step 2589: Updated gated ratio to 0.9003 (progress: 99.7%)
Step 2589: Updated gated ratio to 0.9003 (progress: 99.7%)
Step 2589: Updated gated ratio to 0.9003 (progress: 99.7%)
Step 2589: Updated gated ratio to 0.9003 (progress: 99.7%)
{'loss': 1.1309, 'grad_norm': 1.5278400182724, 'learning_rate': 1.0548039756719497e-05, 'epoch': 0.5}
{'loss': 1.2403, 'grad_norm': 1.3260997533798218, 'learning_rate': 1.054181816867326e-05, 'epoch': 0.5}
{'loss': 1.0905, 'grad_norm': 1.742751121520996, 'learning_rate': 1.053559637027455e-05, 'epoch': 0.5}
{'loss': 1.1433, 'grad_norm': 1.5911227464675903, 'learning_rate': 1.0529374363938888e-05, 'epoch': 0.5}
{'loss': 1.08, 'grad_norm': 1.4766021966934204, 'learning_rate': 1.0523152152081875e-05, 'epoch': 0.5}
{'loss': 1.1974, 'grad_norm': 1.7918314933776855, 'learning_rate': 1.051692973711918e-05, 'epoch': 0.5}
Step 2595: Updated gated ratio to 0.9001 (progress: 99.9%)
Step 2595: Updated gated ratio to 0.9001 (progress: 99.9%)
Step 2595: Updated gated ratio to 0.9001 (progress: 99.9%)
Step 2595: Updated gated ratio to 0.9001 (progress: 99.9%)
Step 2595: Updated gated ratio to 0.9001 (progress: 99.9%)
Step 2595: Updated gated ratio to 0.9001 (progress: 99.9%)
Step 2595: Updated gated ratio to 0.9001 (progress: 99.9%)
Step 2595: Updated gated ratio to 0.9001 (progress: 99.9%)
Step 2595: Updated gated ratio to 0.9001 (progress: 99.9%)
Step 2595: Updated gated ratio to 0.9001 (progress: 99.9%)
Step 2595: Updated gated ratio to 0.9001 (progress: 99.9%)
Step 2595: Updated gated ratio to 0.9001 (progress: 99.9%)
Step 2595: Updated gated ratio to 0.9001 (progress: 99.9%)
Step 2595: Updated gated ratio to 0.9001 (progress: 99.9%)
Step 2595: Updated gated ratio to 0.9001 (progress: 99.9%)
Step 2595: Updated gated ratio to 0.9001 (progress: 99.9%)
{'loss': 1.107, 'grad_norm': 1.5055865049362183, 'learning_rate': 1.0510707121466568e-05, 'epoch': 0.5}
{'loss': 1.1293, 'grad_norm': 1.552209734916687, 'learning_rate': 1.0504484307539864e-05, 'epoch': 0.5}
{'loss': 1.0408, 'grad_norm': 1.4204201698303223, 'learning_rate': 1.0498261297754984e-05, 'epoch': 0.5}
{'loss': 1.1848, 'grad_norm': 1.2239309549331665, 'learning_rate': 1.0492038094527907e-05, 'epoch': 0.5}
Step 2599: Transitioning to Stage 2 - Reparameterization
Step 2599: Transitioning to Stage 2 - Reparameterization
Step 2599: Transitioning to Stage 2 - Reparameterization
Step 2599: Transitioning to Stage 2 - ReparameterizationStep 2599: Transitioning to Stage 2 - Reparameterization

Step 2599: Transitioning to Stage 2 - Reparameterization
Step 2599: Transitioning to Stage 2 - Reparameterization
Step 2599: Transitioning to Stage 2 - Reparameterization
Step 2599: Transitioning to Stage 2 - Reparameterization
Step 2599: Transitioning to Stage 2 - ReparameterizationStep 2599: Transitioning to Stage 2 - Reparameterization
Step 2599: Transitioning to Stage 2 - Reparameterization
Step 2599: Transitioning to Stage 2 - Reparameterization

Step 2599: Transitioning to Stage 2 - Reparameterization
Step 2599: Transitioning to Stage 2 - Reparameterization
Step 2599: Transitioning to Stage 2 - Reparameterization
  Calling reparam_moe_layers()...
  Calling reparam_moe_layers()...
  Calling reparam_moe_layers()...
  Calling reparam_moe_layers()...
  Calling reparam_moe_layers()...
  Calling reparam_moe_layers()...
  Calling reparam_moe_layers()...
  Calling reparam_moe_layers()...
  Calling reparam_moe_layers()...
  Calling reparam_moe_layers()...
  Calling reparam_moe_layers()...
  Calling reparam_moe_layers()...
  Calling reparam_moe_layers()...
  Calling reparam_moe_layers()...
  Calling reparam_moe_layers()...
  Calling reparam_moe_layers()...
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
  Reparameterization completed
  Updating optimizer after reparameterization...
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
  Handling DeepSpeed ZeRO optimizer reparameterization...
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
  Reparameterization completed
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
  Updating optimizer after reparameterization...
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
  Handling DeepSpeed ZeRO optimizer reparameterization...
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
  Reparameterization completed
✓ DeepSpeed MoE requirements satisfied: 9 MoE groups found
  Updating optimizer after reparameterization...
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
  Handling DeepSpeed ZeRO optimizer reparameterization...
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
RePaMoE reparameterized: created new expert from 4 experts
  Reparameterization completed
    Successfully recreated DeepSpeed optimizer with 11 groups
  Updating optimizer after reparameterization...
RePaMoE reparameterized: created new expert from 4 experts
  Reparameterization completed
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
  Handling DeepSpeed ZeRO optimizer reparameterization...
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
RePaMoE reparameterized: created new expert from 4 experts
  Reparameterization completed
  Updating optimizer after reparameterization...
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
  Handling DeepSpeed ZeRO optimizer reparameterization...
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
  Updating optimizer after reparameterization...
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
  Handling DeepSpeed ZeRO optimizer reparameterization...
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
✓ DeepSpeed MoE requirements satisfied: 9 MoE groups found
RePaMoE reparameterized: created new expert from 4 experts
    Successfully recreated DeepSpeed optimizer with 11 groups
RePaMoE reparameterized: created new expert from 4 experts
  Reparameterization completed
RePaMoE reparameterized: created new expert from 4 experts
  Reparameterization completed
  Updating optimizer after reparameterization...
  Updating optimizer after reparameterization...
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
  Handling DeepSpeed ZeRO optimizer reparameterization...
  Handling DeepSpeed ZeRO optimizer reparameterization...
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
    Initialized 472 parameter states with step count 2599.0
    DeepSpeed optimizer reparameterization completed successfully
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
  Reparameterization completed
RePaMoE reparameterized: created new expert from 4 experts
  Reparameterization completed
  Updating optimizer after reparameterization...
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
  Updating optimizer after reparameterization...
  Handling DeepSpeed ZeRO optimizer reparameterization...
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
  Handling DeepSpeed ZeRO optimizer reparameterization...
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
✓ DeepSpeed MoE requirements satisfied: 9 MoE groups found
✓ Optimizer parameter sync verified: 472 parameters
Step 2599: Successfully transitioned to Stage 2
    Successfully recreated DeepSpeed optimizer with 11 groups
RePaMoE reparameterized: created new expert from 4 experts
  Reparameterization completed
✓ DeepSpeed MoE requirements satisfied: 9 MoE groups found
    Initialized 472 parameter states with step count 2599.0
    DeepSpeed optimizer reparameterization completed successfully
  Updating optimizer after reparameterization...
RePaMoE reparameterized: created new expert from 4 experts
  Reparameterization completed
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
  Handling DeepSpeed ZeRO optimizer reparameterization...
    Successfully recreated DeepSpeed optimizer with 11 groups    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0

✓ DeepSpeed MoE requirements satisfied: 9 MoE groups found
✓ DeepSpeed MoE requirements satisfied: 9 MoE groups found
  Updating optimizer after reparameterization...
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
  Handling DeepSpeed ZeRO optimizer reparameterization...
✓ Optimizer parameter sync verified: 472 parameters
Step 2599: Successfully transitioned to Stage 2
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
    Successfully recreated DeepSpeed optimizer with 11 groups
    Successfully recreated DeepSpeed optimizer with 11 groups
RePaMoE reparameterized: created new expert from 4 experts
✓ DeepSpeed MoE requirements satisfied: 9 MoE groups found
✓ DeepSpeed MoE requirements satisfied: 9 MoE groups found
RePaMoE reparameterized: created new expert from 4 experts
  Reparameterization completed
    Successfully recreated DeepSpeed optimizer with 11 groups
    Initialized 472 parameter states with step count 2599.0
    DeepSpeed optimizer reparameterization completed successfully
    Successfully recreated DeepSpeed optimizer with 11 groups
  Updating optimizer after reparameterization...
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
  Handling DeepSpeed ZeRO optimizer reparameterization...
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
✓ Optimizer parameter sync verified: 472 parameters
    Initialized 472 parameter states with step count 2599.0
Step 2599: Successfully transitioned to Stage 2
    DeepSpeed optimizer reparameterization completed successfully
✓ DeepSpeed MoE requirements satisfied: 9 MoE groups found
✓ DeepSpeed MoE requirements satisfied: 9 MoE groups found
    Initialized 472 parameter states with step count 2599.0
    DeepSpeed optimizer reparameterization completed successfully
    Successfully recreated DeepSpeed optimizer with 11 groups
    Initialized 472 parameter states with step count 2599.0
    Successfully recreated DeepSpeed optimizer with 11 groups
    DeepSpeed optimizer reparameterization completed successfully
✓ Optimizer parameter sync verified: 472 parameters
Step 2599: Successfully transitioned to Stage 2
✓ Optimizer parameter sync verified: 472 parameters
Step 2599: Successfully transitioned to Stage 2
✓ DeepSpeed MoE requirements satisfied: 9 MoE groups found
✓ Optimizer parameter sync verified: 472 parameters
Step 2599: Successfully transitioned to Stage 2
    Successfully recreated DeepSpeed optimizer with 11 groups
✓ DeepSpeed MoE requirements satisfied: 9 MoE groups found
RePaMoE reparameterized: created new expert from 4 experts
  Reparameterization completed
    Initialized 472 parameter states with step count 2599.0
    DeepSpeed optimizer reparameterization completed successfully
    Successfully recreated DeepSpeed optimizer with 11 groups
  Updating optimizer after reparameterization...
    Initialized 472 parameter states with step count 2599.0
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
    DeepSpeed optimizer reparameterization completed successfully
  Handling DeepSpeed ZeRO optimizer reparameterization...
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
✓ Optimizer parameter sync verified: 472 parameters
Step 2599: Successfully transitioned to Stage 2
✓ Optimizer parameter sync verified: 472 parameters
Step 2599: Successfully transitioned to Stage 2
    Initialized 472 parameter states with step count 2599.0
    DeepSpeed optimizer reparameterization completed successfully
    Initialized 472 parameter states with step count 2599.0
    DeepSpeed optimizer reparameterization completed successfully
RePaMoE reparameterized: created new expert from 4 experts
  Reparameterization completed
✓ DeepSpeed MoE requirements satisfied: 9 MoE groups found
  Updating optimizer after reparameterization...
✓ Optimizer parameter sync verified: 472 parameters
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
Step 2599: Successfully transitioned to Stage 2
  Handling DeepSpeed ZeRO optimizer reparameterization...
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
    Successfully recreated DeepSpeed optimizer with 11 groups
✓ Optimizer parameter sync verified: 472 parameters
Step 2599: Successfully transitioned to Stage 2
    Initialized 472 parameter states with step count 2599.0
    DeepSpeed optimizer reparameterization completed successfully
    Initialized 472 parameter states with step count 2599.0
    DeepSpeed optimizer reparameterization completed successfully
✓ Optimizer parameter sync verified: 472 parameters
Step 2599: Successfully transitioned to Stage 2
✓ Optimizer parameter sync verified: 472 parameters
Step 2599: Successfully transitioned to Stage 2
✓ DeepSpeed MoE requirements satisfied: 9 MoE groups found
    Successfully recreated DeepSpeed optimizer with 11 groups
    Initialized 472 parameter states with step count 2599.0
    DeepSpeed optimizer reparameterization completed successfully
RePaMoE reparameterized: created new expert from 4 experts
✓ Optimizer parameter sync verified: 472 parameters
Step 2599: Successfully transitioned to Stage 2
✓ DeepSpeed MoE requirements satisfied: 9 MoE groups found
    Successfully recreated DeepSpeed optimizer with 11 groups
    Initialized 472 parameter states with step count 2599.0
    DeepSpeed optimizer reparameterization completed successfully
✓ Optimizer parameter sync verified: 472 parameters
Step 2599: Successfully transitioned to Stage 2
    Initialized 472 parameter states with step count 2599.0
    DeepSpeed optimizer reparameterization completed successfully
✓ Optimizer parameter sync verified: 472 parameters
Step 2599: Successfully transitioned to Stage 2
RePaMoE reparameterized: created new expert from 4 experts
RePaMoE reparameterized: created new expert from 4 experts
  Reparameterization completed
  Updating optimizer after reparameterization...
    Detected DeepSpeed ZeRO optimizer, using specialized handling...
  Handling DeepSpeed ZeRO optimizer reparameterization...
    Extracted: lr=1.0485814700274706e-05, wd=0.0, betas=(0.9, 0.999), eps=1e-08, step=2599.0
✓ DeepSpeed MoE requirements satisfied: 9 MoE groups found
    Successfully recreated DeepSpeed optimizer with 11 groups
    Initialized 472 parameter states with step count 2599.0
    DeepSpeed optimizer reparameterization completed successfully
✓ Optimizer parameter sync verified: 472 parameters
Step 2599: Successfully transitioned to Stage 2
{'loss': 1.102, 'grad_norm': 1.3995983600616455, 'learning_rate': 1.0485814700274706e-05, 'epoch': 0.5}
{'loss': 1.2331, 'grad_norm': 1.5700757503509521, 'learning_rate': 1.047959111741151e-05, 'epoch': 0.5}
{'loss': 1.027, 'grad_norm': 1.5697027444839478, 'learning_rate': 1.0473367348354529e-05, 'epoch': 0.5}
{'loss': 1.1945, 'grad_norm': 1.3585894107818604, 'learning_rate': 1.0467143395520044e-05, 'epoch': 0.5}
{'loss': 1.1034, 'grad_norm': 1.4204083681106567, 'learning_rate': 1.046091926132441e-05, 'epoch': 0.5}
{'loss': 1.2616, 'grad_norm': 1.20291006565094, 'learning_rate': 1.0454694948184045e-05, 'epoch': 0.5}
{'loss': 1.1607, 'grad_norm': 1.6302614212036133, 'learning_rate': 1.044847045851545e-05, 'epoch': 0.5}
{'loss': 1.164, 'grad_norm': 1.4979236125946045, 'learning_rate': 1.044224579473518e-05, 'epoch': 0.5}
{'loss': 1.1327, 'grad_norm': 1.312455415725708, 'learning_rate': 1.0436020959259862e-05, 'epoch': 0.5}
{'loss': 1.1493, 'grad_norm': 1.3016129732131958, 'learning_rate': 1.0429795954506203e-05, 'epoch': 0.5}
{'loss': 1.1835, 'grad_norm': 1.5942708253860474, 'learning_rate': 1.0423570782890951e-05, 'epoch': 0.5}
{'loss': 1.1468, 'grad_norm': 1.4847345352172852, 'learning_rate': 1.0417345446830938e-05, 'epoch': 0.5}
{'loss': 1.1904, 'grad_norm': 1.5661892890930176, 'learning_rate': 1.0411119948743052e-05, 'epoch': 0.5}
{'loss': 1.1311, 'grad_norm': 1.5409741401672363, 'learning_rate': 1.0404894291044247e-05, 'epoch': 0.5}
{'loss': 1.2005, 'grad_norm': 1.3488558530807495, 'learning_rate': 1.0398668476151538e-05, 'epoch': 0.5}
{'loss': 1.0649, 'grad_norm': 1.3516160249710083, 'learning_rate': 1.0392442506482e-05, 'epoch': 0.5}
{'loss': 1.1874, 'grad_norm': 1.6193503141403198, 'learning_rate': 1.038621638445277e-05, 'epoch': 0.5}
{'loss': 1.0284, 'grad_norm': 1.418351411819458, 'learning_rate': 1.037999011248104e-05, 'epoch': 0.5}
{'loss': 1.12, 'grad_norm': 1.468278408050537, 'learning_rate': 1.0373763692984062e-05, 'epoch': 0.5}
{'loss': 1.1481, 'grad_norm': 1.2115991115570068, 'learning_rate': 1.0367537128379154e-05, 'epoch': 0.5}
{'loss': 1.0777, 'grad_norm': 1.4453613758087158, 'learning_rate': 1.0361310421083677e-05, 'epoch': 0.5}
{'loss': 1.1689, 'grad_norm': 1.4805197715759277, 'learning_rate': 1.0355083573515052e-05, 'epoch': 0.5}
{'loss': 1.2138, 'grad_norm': 1.4708553552627563, 'learning_rate': 1.0348856588090764e-05, 'epoch': 0.5}
{'loss': 1.2033, 'grad_norm': 1.2930423021316528, 'learning_rate': 1.0342629467228331e-05, 'epoch': 0.5}
{'loss': 1.1537, 'grad_norm': 1.4723106622695923, 'learning_rate': 1.0336402213345345e-05, 'epoch': 0.5}
{'loss': 1.2432, 'grad_norm': 1.3141818046569824, 'learning_rate': 1.0330174828859434e-05, 'epoch': 0.51}
{'loss': 1.098, 'grad_norm': 1.451026201248169, 'learning_rate': 1.0323947316188288e-05, 'epoch': 0.51}
{'loss': 1.1714, 'grad_norm': 1.511203408241272, 'learning_rate': 1.031771967774964e-05, 'epoch': 0.51}
{'loss': 1.1532, 'grad_norm': 1.4855639934539795, 'learning_rate': 1.0311491915961271e-05, 'epoch': 0.51}
{'loss': 1.126, 'grad_norm': 1.4707908630371094, 'learning_rate': 1.030526403324102e-05, 'epoch': 0.51}
{'loss': 1.0592, 'grad_norm': 1.488594889640808, 'learning_rate': 1.0299036032006759e-05, 'epoch': 0.51}
{'loss': 1.1298, 'grad_norm': 1.450242042541504, 'learning_rate': 1.0292807914676412e-05, 'epoch': 0.51}
{'loss': 1.0883, 'grad_norm': 1.491779088973999, 'learning_rate': 1.0286579683667952e-05, 'epoch': 0.51}
{'loss': 1.1365, 'grad_norm': 1.6083623170852661, 'learning_rate': 1.0280351341399392e-05, 'epoch': 0.51}
{'loss': 1.1404, 'grad_norm': 1.5265005826950073, 'learning_rate': 1.027412289028879e-05, 'epoch': 0.51}
{'loss': 1.0376, 'grad_norm': 1.2727601528167725, 'learning_rate': 1.0267894332754243e-05, 'epoch': 0.51}
{'loss': 1.075, 'grad_norm': 1.403577208518982, 'learning_rate': 1.0261665671213891e-05, 'epoch': 0.51}
{'loss': 1.1075, 'grad_norm': 1.4981420040130615, 'learning_rate': 1.0255436908085919e-05, 'epoch': 0.51}
{'loss': 1.0886, 'grad_norm': 1.5476192235946655, 'learning_rate': 1.024920804578854e-05, 'epoch': 0.51}
{'loss': 1.0459, 'grad_norm': 1.4700027704238892, 'learning_rate': 1.0242979086740019e-05, 'epoch': 0.51}
{'loss': 1.1042, 'grad_norm': 1.5967531204223633, 'learning_rate': 1.023675003335865e-05, 'epoch': 0.51}
{'loss': 1.0738, 'grad_norm': 1.4672623872756958, 'learning_rate': 1.0230520888062765e-05, 'epoch': 0.51}
{'loss': 1.1535, 'grad_norm': 1.7114545106887817, 'learning_rate': 1.0224291653270739e-05, 'epoch': 0.51}
{'loss': 1.1101, 'grad_norm': 1.6240575313568115, 'learning_rate': 1.0218062331400969e-05, 'epoch': 0.51}
{'loss': 1.0908, 'grad_norm': 1.4925576448440552, 'learning_rate': 1.0211832924871889e-05, 'epoch': 0.51}
{'loss': 1.1062, 'grad_norm': 1.4744393825531006, 'learning_rate': 1.0205603436101978e-05, 'epoch': 0.51}
{'loss': 1.1839, 'grad_norm': 1.4719312191009521, 'learning_rate': 1.0199373867509734e-05, 'epoch': 0.51}
{'loss': 1.2093, 'grad_norm': 1.3092451095581055, 'learning_rate': 1.019314422151369e-05, 'epoch': 0.51}
{'loss': 1.1574, 'grad_norm': 1.4946401119232178, 'learning_rate': 1.0186914500532408e-05, 'epoch': 0.51}
{'loss': 1.1035, 'grad_norm': 1.5394277572631836, 'learning_rate': 1.0180684706984483e-05, 'epoch': 0.51}
{'loss': 1.1361, 'grad_norm': 1.482796311378479, 'learning_rate': 1.0174454843288533e-05, 'epoch': 0.51}
{'loss': 1.0759, 'grad_norm': 1.3574776649475098, 'learning_rate': 1.0168224911863205e-05, 'epoch': 0.51}
{'loss': 1.2233, 'grad_norm': 1.273767352104187, 'learning_rate': 1.0161994915127173e-05, 'epoch': 0.51}
{'loss': 1.1665, 'grad_norm': 1.5440850257873535, 'learning_rate': 1.015576485549914e-05, 'epoch': 0.51}
{'loss': 1.131, 'grad_norm': 1.6258094310760498, 'learning_rate': 1.0149534735397823e-05, 'epoch': 0.51}
{'loss': 1.1316, 'grad_norm': 1.4795597791671753, 'learning_rate': 1.0143304557241979e-05, 'epoch': 0.51}
{'loss': 1.067, 'grad_norm': 1.4428974390029907, 'learning_rate': 1.0137074323450372e-05, 'epoch': 0.51}
{'loss': 1.1275, 'grad_norm': 1.557079792022705, 'learning_rate': 1.0130844036441787e-05, 'epoch': 0.51}
{'loss': 1.1141, 'grad_norm': 1.5867408514022827, 'learning_rate': 1.0124613698635043e-05, 'epoch': 0.51}
{'loss': 1.0763, 'grad_norm': 1.461104154586792, 'learning_rate': 1.0118383312448973e-05, 'epoch': 0.51}
{'loss': 1.1538, 'grad_norm': 1.4279471635818481, 'learning_rate': 1.0112152880302426e-05, 'epoch': 0.51}
{'loss': 1.1023, 'grad_norm': 1.422985315322876, 'learning_rate': 1.0105922404614265e-05, 'epoch': 0.51}
{'loss': 1.1453, 'grad_norm': 1.6522345542907715, 'learning_rate': 1.0099691887803385e-05, 'epoch': 0.51}
{'loss': 1.2364, 'grad_norm': 1.2182331085205078, 'learning_rate': 1.0093461332288678e-05, 'epoch': 0.51}
{'loss': 1.2906, 'grad_norm': 1.3450887203216553, 'learning_rate': 1.0087230740489065e-05, 'epoch': 0.51}
{'loss': 1.0869, 'grad_norm': 1.3947327136993408, 'learning_rate': 1.0081000114823473e-05, 'epoch': 0.51}
{'loss': 1.058, 'grad_norm': 1.4739545583724976, 'learning_rate': 1.007476945771085e-05, 'epoch': 0.51}
{'loss': 1.0705, 'grad_norm': 1.4616351127624512, 'learning_rate': 1.006853877157015e-05, 'epoch': 0.51}
{'loss': 1.0583, 'grad_norm': 1.5014512538909912, 'learning_rate': 1.0062308058820337e-05, 'epoch': 0.51}
{'loss': 1.2309, 'grad_norm': 1.3612138032913208, 'learning_rate': 1.0056077321880393e-05, 'epoch': 0.51}
{'loss': 1.0729, 'grad_norm': 1.464916706085205, 'learning_rate': 1.0049846563169297e-05, 'epoch': 0.51}
{'loss': 1.091, 'grad_norm': 1.4806883335113525, 'learning_rate': 1.0043615785106051e-05, 'epoch': 0.51}
{'loss': 1.0525, 'grad_norm': 1.3566479682922363, 'learning_rate': 1.0037384990109658e-05, 'epoch': 0.51}
{'loss': 1.1334, 'grad_norm': 1.4326790571212769, 'learning_rate': 1.0031154180599123e-05, 'epoch': 0.51}
{'loss': 1.0089, 'grad_norm': 1.3892507553100586, 'learning_rate': 1.0024923358993458e-05, 'epoch': 0.51}
{'loss': 1.1074, 'grad_norm': 1.5180097818374634, 'learning_rate': 1.0018692527711695e-05, 'epoch': 0.51}
{'loss': 1.2291, 'grad_norm': 1.5117584466934204, 'learning_rate': 1.0012461689172846e-05, 'epoch': 0.51}
{'loss': 1.0781, 'grad_norm': 1.7201237678527832, 'learning_rate': 1.0006230845795937e-05, 'epoch': 0.52}
{'loss': 1.0529, 'grad_norm': 1.5378719568252563, 'learning_rate': 1e-05, 'epoch': 0.52}
{'loss': 1.1235, 'grad_norm': 1.4658780097961426, 'learning_rate': 9.993769154204063e-06, 'epoch': 0.52}
{'loss': 1.1262, 'grad_norm': 1.535138487815857, 'learning_rate': 9.987538310827159e-06, 'epoch': 0.52}
{'loss': 1.074, 'grad_norm': 1.4015026092529297, 'learning_rate': 9.981307472288308e-06, 'epoch': 0.52}
{'loss': 1.0707, 'grad_norm': 1.4839495420455933, 'learning_rate': 9.975076641006542e-06, 'epoch': 0.52}
{'loss': 1.0802, 'grad_norm': 1.4477505683898926, 'learning_rate': 9.968845819400883e-06, 'epoch': 0.52}
{'loss': 1.0445, 'grad_norm': 1.4451924562454224, 'learning_rate': 9.962615009890346e-06, 'epoch': 0.52}
{'loss': 1.1298, 'grad_norm': 1.451359510421753, 'learning_rate': 9.956384214893949e-06, 'epoch': 0.52}
{'loss': 1.0704, 'grad_norm': 1.4329161643981934, 'learning_rate': 9.950153436830707e-06, 'epoch': 0.52}
{'loss': 1.1151, 'grad_norm': 1.4250696897506714, 'learning_rate': 9.94392267811961e-06, 'epoch': 0.52}
{'loss': 1.1546, 'grad_norm': 1.7057898044586182, 'learning_rate': 9.937691941179665e-06, 'epoch': 0.52}
{'loss': 1.1052, 'grad_norm': 1.5463337898254395, 'learning_rate': 9.931461228429856e-06, 'epoch': 0.52}
{'loss': 1.119, 'grad_norm': 1.4319818019866943, 'learning_rate': 9.925230542289151e-06, 'epoch': 0.52}
{'loss': 1.1037, 'grad_norm': 1.498417854309082, 'learning_rate': 9.91899988517653e-06, 'epoch': 0.52}
{'loss': 1.0296, 'grad_norm': 1.348607063293457, 'learning_rate': 9.912769259510938e-06, 'epoch': 0.52}
{'loss': 1.1308, 'grad_norm': 1.5397183895111084, 'learning_rate': 9.906538667711324e-06, 'epoch': 0.52}
{'loss': 1.2296, 'grad_norm': 1.258561611175537, 'learning_rate': 9.90030811219662e-06, 'epoch': 0.52}
{'loss': 1.161, 'grad_norm': 1.5981857776641846, 'learning_rate': 9.894077595385736e-06, 'epoch': 0.52}
{'loss': 1.2046, 'grad_norm': 1.6443649530410767, 'learning_rate': 9.887847119697577e-06, 'epoch': 0.52}
{'loss': 1.1086, 'grad_norm': 1.4327895641326904, 'learning_rate': 9.881616687551032e-06, 'epoch': 0.52}
{'loss': 1.1436, 'grad_norm': 1.6301943063735962, 'learning_rate': 9.875386301364958e-06, 'epoch': 0.52}
{'loss': 1.1349, 'grad_norm': 1.5242953300476074, 'learning_rate': 9.869155963558215e-06, 'epoch': 0.52}
{'loss': 1.0588, 'grad_norm': 1.4064857959747314, 'learning_rate': 9.862925676549635e-06, 'epoch': 0.52}
{'loss': 0.986, 'grad_norm': 1.4929804801940918, 'learning_rate': 9.856695442758023e-06, 'epoch': 0.52}
{'loss': 1.1119, 'grad_norm': 1.6525461673736572, 'learning_rate': 9.850465264602175e-06, 'epoch': 0.52}
{'loss': 1.1506, 'grad_norm': 1.6510834693908691, 'learning_rate': 9.844235144500865e-06, 'epoch': 0.52}
{'loss': 1.2735, 'grad_norm': 1.3852967023849487, 'learning_rate': 9.83800508487283e-06, 'epoch': 0.52}
{'loss': 1.2115, 'grad_norm': 1.2875078916549683, 'learning_rate': 9.831775088136797e-06, 'epoch': 0.52}
{'loss': 1.0799, 'grad_norm': 1.5239596366882324, 'learning_rate': 9.82554515671147e-06, 'epoch': 0.52}
{'loss': 1.1647, 'grad_norm': 1.4142882823944092, 'learning_rate': 9.819315293015519e-06, 'epoch': 0.52}
{'loss': 1.2084, 'grad_norm': 1.6540733575820923, 'learning_rate': 9.813085499467594e-06, 'epoch': 0.52}
{'loss': 1.068, 'grad_norm': 1.3804279565811157, 'learning_rate': 9.806855778486314e-06, 'epoch': 0.52}
{'loss': 1.1405, 'grad_norm': 1.5108599662780762, 'learning_rate': 9.800626132490268e-06, 'epoch': 0.52}
{'loss': 1.1191, 'grad_norm': 1.5274605751037598, 'learning_rate': 9.794396563898022e-06, 'epoch': 0.52}
{'loss': 1.1949, 'grad_norm': 1.646856427192688, 'learning_rate': 9.788167075128113e-06, 'epoch': 0.52}
{'loss': 1.1651, 'grad_norm': 1.5119843482971191, 'learning_rate': 9.781937668599035e-06, 'epoch': 0.52}
{'loss': 1.1324, 'grad_norm': 1.5074269771575928, 'learning_rate': 9.775708346729263e-06, 'epoch': 0.52}
{'loss': 1.073, 'grad_norm': 1.5394091606140137, 'learning_rate': 9.769479111937238e-06, 'epoch': 0.52}
{'loss': 1.0953, 'grad_norm': 1.4500255584716797, 'learning_rate': 9.763249966641352e-06, 'epoch': 0.52}
{'loss': 1.0764, 'grad_norm': 1.3729338645935059, 'learning_rate': 9.757020913259986e-06, 'epoch': 0.52}
{'loss': 1.1852, 'grad_norm': 1.1964633464813232, 'learning_rate': 9.750791954211464e-06, 'epoch': 0.52}
{'loss': 1.0958, 'grad_norm': 1.5434356927871704, 'learning_rate': 9.744563091914085e-06, 'epoch': 0.52}
{'loss': 1.1742, 'grad_norm': 1.5555171966552734, 'learning_rate': 9.738334328786114e-06, 'epoch': 0.52}
{'loss': 1.0769, 'grad_norm': 1.460344672203064, 'learning_rate': 9.732105667245759e-06, 'epoch': 0.52}
{'loss': 1.0812, 'grad_norm': 1.4387531280517578, 'learning_rate': 9.725877109711212e-06, 'epoch': 0.52}
{'loss': 1.1245, 'grad_norm': 1.53522789478302, 'learning_rate': 9.719648658600611e-06, 'epoch': 0.52}
{'loss': 1.0902, 'grad_norm': 1.609618067741394, 'learning_rate': 9.71342031633205e-06, 'epoch': 0.52}
{'loss': 1.0989, 'grad_norm': 1.4018346071243286, 'learning_rate': 9.70719208532359e-06, 'epoch': 0.52}
{'loss': 1.1758, 'grad_norm': 1.5882213115692139, 'learning_rate': 9.700963967993246e-06, 'epoch': 0.52}
{'loss': 1.1257, 'grad_norm': 1.487628698348999, 'learning_rate': 9.694735966758982e-06, 'epoch': 0.52}
{'loss': 1.0451, 'grad_norm': 1.4558004140853882, 'learning_rate': 9.688508084038729e-06, 'epoch': 0.52}
{'loss': 1.1498, 'grad_norm': 1.5774145126342773, 'learning_rate': 9.682280322250365e-06, 'epoch': 0.53}
{'loss': 1.2606, 'grad_norm': 1.2565425634384155, 'learning_rate': 9.676052683811715e-06, 'epoch': 0.53}
{'loss': 1.1672, 'grad_norm': 1.2328108549118042, 'learning_rate': 9.669825171140568e-06, 'epoch': 0.53}
{'loss': 1.2534, 'grad_norm': 1.2464547157287598, 'learning_rate': 9.66359778665466e-06, 'epoch': 0.53}
{'loss': 1.1623, 'grad_norm': 1.6828972101211548, 'learning_rate': 9.657370532771672e-06, 'epoch': 0.53}
{'loss': 1.134, 'grad_norm': 1.469030499458313, 'learning_rate': 9.651143411909241e-06, 'epoch': 0.53}
{'loss': 1.1819, 'grad_norm': 1.5172741413116455, 'learning_rate': 9.64491642648495e-06, 'epoch': 0.53}
{'loss': 1.1116, 'grad_norm': 1.5199005603790283, 'learning_rate': 9.638689578916326e-06, 'epoch': 0.53}
{'loss': 1.2099, 'grad_norm': 1.2500184774398804, 'learning_rate': 9.632462871620847e-06, 'epoch': 0.53}
{'loss': 1.1429, 'grad_norm': 1.451077938079834, 'learning_rate': 9.62623630701594e-06, 'epoch': 0.53}
{'loss': 1.1474, 'grad_norm': 1.1891177892684937, 'learning_rate': 9.620009887518963e-06, 'epoch': 0.53}
{'loss': 1.1255, 'grad_norm': 1.5308008193969727, 'learning_rate': 9.613783615547233e-06, 'epoch': 0.53}
{'loss': 1.1039, 'grad_norm': 1.4725078344345093, 'learning_rate': 9.607557493518006e-06, 'epoch': 0.53}
{'loss': 1.077, 'grad_norm': 1.5651588439941406, 'learning_rate': 9.601331523848464e-06, 'epoch': 0.53}
{'loss': 1.0908, 'grad_norm': 1.4541796445846558, 'learning_rate': 9.595105708955758e-06, 'epoch': 0.53}
{'loss': 1.1464, 'grad_norm': 1.5110033750534058, 'learning_rate': 9.588880051256951e-06, 'epoch': 0.53}
{'loss': 1.1929, 'grad_norm': 1.5939149856567383, 'learning_rate': 9.582654553169064e-06, 'epoch': 0.53}
{'loss': 1.0855, 'grad_norm': 1.4957799911499023, 'learning_rate': 9.576429217109054e-06, 'epoch': 0.53}
{'loss': 1.1216, 'grad_norm': 1.5352904796600342, 'learning_rate': 9.5702040454938e-06, 'epoch': 0.53}
{'loss': 1.2465, 'grad_norm': 1.442527413368225, 'learning_rate': 9.563979040740138e-06, 'epoch': 0.53}
{'loss': 1.048, 'grad_norm': 1.6372370719909668, 'learning_rate': 9.557754205264826e-06, 'epoch': 0.53}
{'loss': 1.1512, 'grad_norm': 1.5868169069290161, 'learning_rate': 9.551529541484554e-06, 'epoch': 0.53}
{'loss': 1.0973, 'grad_norm': 1.49946129322052, 'learning_rate': 9.545305051815957e-06, 'epoch': 0.53}
{'loss': 1.1845, 'grad_norm': 1.5239717960357666, 'learning_rate': 9.539080738675597e-06, 'epoch': 0.53}
{'loss': 1.1077, 'grad_norm': 1.5695825815200806, 'learning_rate': 9.53285660447996e-06, 'epoch': 0.53}
{'loss': 1.1575, 'grad_norm': 1.4568278789520264, 'learning_rate': 9.526632651645476e-06, 'epoch': 0.53}
{'loss': 1.1968, 'grad_norm': 1.2077686786651611, 'learning_rate': 9.520408882588497e-06, 'epoch': 0.53}
{'loss': 1.1591, 'grad_norm': 1.194953441619873, 'learning_rate': 9.514185299725299e-06, 'epoch': 0.53}
{'loss': 1.1295, 'grad_norm': 1.5621178150177002, 'learning_rate': 9.507961905472093e-06, 'epoch': 0.53}
{'loss': 1.1816, 'grad_norm': 1.2900805473327637, 'learning_rate': 9.501738702245023e-06, 'epoch': 0.53}
{'loss': 1.1419, 'grad_norm': 1.4791582822799683, 'learning_rate': 9.495515692460138e-06, 'epoch': 0.53}
{'loss': 1.0211, 'grad_norm': 1.4969547986984253, 'learning_rate': 9.489292878533436e-06, 'epoch': 0.53}
{'loss': 1.1413, 'grad_norm': 1.5217406749725342, 'learning_rate': 9.483070262880823e-06, 'epoch': 0.53}
{'loss': 1.0827, 'grad_norm': 1.5739785432815552, 'learning_rate': 9.476847847918126e-06, 'epoch': 0.53}
{'loss': 1.1745, 'grad_norm': 1.278719186782837, 'learning_rate': 9.47062563606111e-06, 'epoch': 0.53}
{'loss': 1.2024, 'grad_norm': 1.4873963594436646, 'learning_rate': 9.464403629725454e-06, 'epoch': 0.53}
{'loss': 1.0757, 'grad_norm': 1.3853880167007446, 'learning_rate': 9.458181831326744e-06, 'epoch': 0.53}
{'loss': 1.1441, 'grad_norm': 1.6449724435806274, 'learning_rate': 9.451960243280506e-06, 'epoch': 0.53}
{'loss': 1.2192, 'grad_norm': 1.2182625532150269, 'learning_rate': 9.44573886800217e-06, 'epoch': 0.53}
{'loss': 1.165, 'grad_norm': 1.4196584224700928, 'learning_rate': 9.43951770790709e-06, 'epoch': 0.53}
{'loss': 1.1435, 'grad_norm': 1.54525625705719, 'learning_rate': 9.433296765410534e-06, 'epoch': 0.53}
{'loss': 1.2259, 'grad_norm': 1.3068389892578125, 'learning_rate': 9.427076042927683e-06, 'epoch': 0.53}
{'loss': 1.0594, 'grad_norm': 1.5536468029022217, 'learning_rate': 9.420855542873638e-06, 'epoch': 0.53}
{'loss': 1.0904, 'grad_norm': 1.4488732814788818, 'learning_rate': 9.414635267663416e-06, 'epoch': 0.53}
{'loss': 1.1509, 'grad_norm': 1.6678810119628906, 'learning_rate': 9.408415219711934e-06, 'epoch': 0.53}
{'loss': 1.0743, 'grad_norm': 1.4405391216278076, 'learning_rate': 9.402195401434036e-06, 'epoch': 0.53}
{'loss': 1.0673, 'grad_norm': 1.573613166809082, 'learning_rate': 9.395975815244468e-06, 'epoch': 0.53}
{'loss': 1.2126, 'grad_norm': 1.4670753479003906, 'learning_rate': 9.389756463557878e-06, 'epoch': 0.53}
{'loss': 1.1603, 'grad_norm': 1.6479579210281372, 'learning_rate': 9.383537348788844e-06, 'epoch': 0.53}
{'loss': 1.0545, 'grad_norm': 1.4882805347442627, 'learning_rate': 9.377318473351838e-06, 'epoch': 0.53}
{'loss': 1.1552, 'grad_norm': 1.589868426322937, 'learning_rate': 9.371099839661238e-06, 'epoch': 0.53}
{'loss': 1.1226, 'grad_norm': 1.445953607559204, 'learning_rate': 9.364881450131335e-06, 'epoch': 0.53}
{'loss': 1.0606, 'grad_norm': 1.5915825366973877, 'learning_rate': 9.358663307176323e-06, 'epoch': 0.54}
{'loss': 1.1258, 'grad_norm': 1.4897968769073486, 'learning_rate': 9.352445413210294e-06, 'epoch': 0.54}
{'loss': 1.0305, 'grad_norm': 1.3909001350402832, 'learning_rate': 9.346227770647251e-06, 'epoch': 0.54}
{'loss': 0.9396, 'grad_norm': 1.461788535118103, 'learning_rate': 9.3400103819011e-06, 'epoch': 0.54}
{'loss': 1.1192, 'grad_norm': 1.691800832748413, 'learning_rate': 9.33379324938564e-06, 'epoch': 0.54}
{'loss': 1.083, 'grad_norm': 1.5479332208633423, 'learning_rate': 9.327576375514582e-06, 'epoch': 0.54}
{'loss': 1.0985, 'grad_norm': 1.5791438817977905, 'learning_rate': 9.321359762701527e-06, 'epoch': 0.54}
{'loss': 1.0899, 'grad_norm': 1.4817771911621094, 'learning_rate': 9.315143413359975e-06, 'epoch': 0.54}
{'loss': 1.1293, 'grad_norm': 1.7490473985671997, 'learning_rate': 9.308927329903333e-06, 'epoch': 0.54}
{'loss': 1.1946, 'grad_norm': 1.5311038494110107, 'learning_rate': 9.302711514744897e-06, 'epoch': 0.54}
{'loss': 1.2171, 'grad_norm': 1.3428127765655518, 'learning_rate': 9.296495970297855e-06, 'epoch': 0.54}
{'loss': 1.1431, 'grad_norm': 1.4774565696716309, 'learning_rate': 9.290280698975307e-06, 'epoch': 0.54}
{'loss': 1.1939, 'grad_norm': 1.2221235036849976, 'learning_rate': 9.284065703190221e-06, 'epoch': 0.54}
{'loss': 1.1279, 'grad_norm': 1.445085048675537, 'learning_rate': 9.27785098535548e-06, 'epoch': 0.54}
{'loss': 1.0604, 'grad_norm': 1.468639850616455, 'learning_rate': 9.271636547883856e-06, 'epoch': 0.54}
{'loss': 1.2008, 'grad_norm': 1.6105092763900757, 'learning_rate': 9.265422393187998e-06, 'epoch': 0.54}
{'loss': 1.1328, 'grad_norm': 1.225590705871582, 'learning_rate': 9.259208523680457e-06, 'epoch': 0.54}
{'loss': 1.1392, 'grad_norm': 1.4753729104995728, 'learning_rate': 9.252994941773679e-06, 'epoch': 0.54}
{'loss': 1.1025, 'grad_norm': 1.4707995653152466, 'learning_rate': 9.24678164987998e-06, 'epoch': 0.54}
{'loss': 1.2093, 'grad_norm': 1.6321808099746704, 'learning_rate': 9.24056865041158e-06, 'epoch': 0.54}
{'loss': 1.1488, 'grad_norm': 1.4665764570236206, 'learning_rate': 9.234355945780581e-06, 'epoch': 0.54}
{'loss': 1.2196, 'grad_norm': 1.4394030570983887, 'learning_rate': 9.228143538398963e-06, 'epoch': 0.54}
{'loss': 1.2259, 'grad_norm': 1.336506724357605, 'learning_rate': 9.221931430678598e-06, 'epoch': 0.54}
{'loss': 1.1485, 'grad_norm': 1.5167675018310547, 'learning_rate': 9.215719625031245e-06, 'epoch': 0.54}
{'loss': 1.1168, 'grad_norm': 1.4574401378631592, 'learning_rate': 9.209508123868534e-06, 'epoch': 0.54}
{'loss': 1.1407, 'grad_norm': 1.3930015563964844, 'learning_rate': 9.203296929601986e-06, 'epoch': 0.54}
{'loss': 0.9738, 'grad_norm': 1.4134206771850586, 'learning_rate': 9.197086044643004e-06, 'epoch': 0.54}
{'loss': 1.101, 'grad_norm': 1.5797150135040283, 'learning_rate': 9.190875471402865e-06, 'epoch': 0.54}
{'loss': 1.0291, 'grad_norm': 1.37055242061615, 'learning_rate': 9.184665212292723e-06, 'epoch': 0.54}
{'loss': 1.1389, 'grad_norm': 1.5650724172592163, 'learning_rate': 9.178455269723623e-06, 'epoch': 0.54}
{'loss': 1.1676, 'grad_norm': 1.4933013916015625, 'learning_rate': 9.172245646106471e-06, 'epoch': 0.54}
{'loss': 1.1678, 'grad_norm': 1.4804913997650146, 'learning_rate': 9.166036343852061e-06, 'epoch': 0.54}
{'loss': 1.1117, 'grad_norm': 1.2107868194580078, 'learning_rate': 9.159827365371055e-06, 'epoch': 0.54}
{'loss': 1.0917, 'grad_norm': 1.6429296731948853, 'learning_rate': 9.153618713073995e-06, 'epoch': 0.54}
{'loss': 1.1353, 'grad_norm': 1.4713331460952759, 'learning_rate': 9.14741038937129e-06, 'epoch': 0.54}
{'loss': 1.0851, 'grad_norm': 1.4593018293380737, 'learning_rate': 9.141202396673232e-06, 'epoch': 0.54}
{'loss': 1.08, 'grad_norm': 1.5291916131973267, 'learning_rate': 9.13499473738997e-06, 'epoch': 0.54}
{'loss': 1.1402, 'grad_norm': 1.6945687532424927, 'learning_rate': 9.128787413931536e-06, 'epoch': 0.54}
{'loss': 1.0631, 'grad_norm': 1.3792023658752441, 'learning_rate': 9.122580428707822e-06, 'epoch': 0.54}
Error with image file is truncated (8 bytes not processed)
{'loss': 1.123, 'grad_norm': 1.4505019187927246, 'learning_rate': 9.116373784128597e-06, 'epoch': 0.54}
{'loss': 1.156, 'grad_norm': 1.636042594909668, 'learning_rate': 9.110167482603494e-06, 'epoch': 0.54}
{'loss': 1.1245, 'grad_norm': 1.4422223567962646, 'learning_rate': 9.10396152654201e-06, 'epoch': 0.54}
{'loss': 1.2543, 'grad_norm': 1.449607491493225, 'learning_rate': 9.097755918353513e-06, 'epoch': 0.54}
{'loss': 1.1172, 'grad_norm': 1.5408226251602173, 'learning_rate': 9.091550660447236e-06, 'epoch': 0.54}
{'loss': 1.1272, 'grad_norm': 1.4537241458892822, 'learning_rate': 9.08534575523227e-06, 'epoch': 0.54}
{'loss': 1.1007, 'grad_norm': 1.4772721529006958, 'learning_rate': 9.079141205117573e-06, 'epoch': 0.54}
{'loss': 1.1522, 'grad_norm': 1.2139300107955933, 'learning_rate': 9.072937012511968e-06, 'epoch': 0.54}
{'loss': 1.1098, 'grad_norm': 1.5666172504425049, 'learning_rate': 9.066733179824134e-06, 'epoch': 0.54}
{'loss': 1.0906, 'grad_norm': 1.4181519746780396, 'learning_rate': 9.060529709462613e-06, 'epoch': 0.54}
{'loss': 1.1509, 'grad_norm': 1.6040949821472168, 'learning_rate': 9.054326603835807e-06, 'epoch': 0.54}
{'loss': 1.1198, 'grad_norm': 1.4560365676879883, 'learning_rate': 9.048123865351971e-06, 'epoch': 0.54}
{'loss': 1.1049, 'grad_norm': 1.6925047636032104, 'learning_rate': 9.041921496419225e-06, 'epoch': 0.54}
{'loss': 1.0706, 'grad_norm': 1.5947448015213013, 'learning_rate': 9.035719499445545e-06, 'epoch': 0.55}
{'loss': 1.1172, 'grad_norm': 1.57466459274292, 'learning_rate': 9.029517876838755e-06, 'epoch': 0.55}
{'loss': 1.1986, 'grad_norm': 1.2671641111373901, 'learning_rate': 9.023316631006536e-06, 'epoch': 0.55}
{'loss': 1.0919, 'grad_norm': 1.490649700164795, 'learning_rate': 9.017115764356436e-06, 'epoch': 0.55}
{'loss': 1.2337, 'grad_norm': 1.3011373281478882, 'learning_rate': 9.010915279295833e-06, 'epoch': 0.55}
{'loss': 1.0493, 'grad_norm': 1.418226957321167, 'learning_rate': 9.004715178231975e-06, 'epoch': 0.55}
{'loss': 1.184, 'grad_norm': 1.6254301071166992, 'learning_rate': 8.998515463571953e-06, 'epoch': 0.55}
{'loss': 1.1368, 'grad_norm': 1.5640572309494019, 'learning_rate': 8.992316137722711e-06, 'epoch': 0.55}
{'loss': 1.06, 'grad_norm': 1.2767467498779297, 'learning_rate': 8.986117203091042e-06, 'epoch': 0.55}
{'loss': 1.0393, 'grad_norm': 1.4569001197814941, 'learning_rate': 8.97991866208358e-06, 'epoch': 0.55}
{'loss': 1.1329, 'grad_norm': 1.5208630561828613, 'learning_rate': 8.973720517106814e-06, 'epoch': 0.55}
{'loss': 1.0652, 'grad_norm': 1.467437505722046, 'learning_rate': 8.967522770567086e-06, 'epoch': 0.55}
{'loss': 1.0798, 'grad_norm': 1.4566136598587036, 'learning_rate': 8.961325424870561e-06, 'epoch': 0.55}
{'loss': 1.0972, 'grad_norm': 1.3567653894424438, 'learning_rate': 8.955128482423271e-06, 'epoch': 0.55}
{'loss': 1.1175, 'grad_norm': 1.4760949611663818, 'learning_rate': 8.948931945631082e-06, 'epoch': 0.55}
{'loss': 1.0954, 'grad_norm': 1.393458604812622, 'learning_rate': 8.9427358168997e-06, 'epoch': 0.55}
{'loss': 1.1007, 'grad_norm': 1.4636236429214478, 'learning_rate': 8.936540098634675e-06, 'epoch': 0.55}
{'loss': 1.1505, 'grad_norm': 1.5790084600448608, 'learning_rate': 8.930344793241404e-06, 'epoch': 0.55}
{'loss': 1.0797, 'grad_norm': 1.3995271921157837, 'learning_rate': 8.924149903125108e-06, 'epoch': 0.55}
{'loss': 1.1667, 'grad_norm': 1.4802502393722534, 'learning_rate': 8.917955430690865e-06, 'epoch': 0.55}
{'loss': 1.1541, 'grad_norm': 1.3590015172958374, 'learning_rate': 8.91176137834358e-06, 'epoch': 0.55}
{'loss': 1.1121, 'grad_norm': 1.5411338806152344, 'learning_rate': 8.905567748487997e-06, 'epoch': 0.55}
{'loss': 1.101, 'grad_norm': 1.3518704175949097, 'learning_rate': 8.899374543528695e-06, 'epoch': 0.55}
{'loss': 1.1212, 'grad_norm': 1.5771194696426392, 'learning_rate': 8.893181765870094e-06, 'epoch': 0.55}
{'loss': 1.1543, 'grad_norm': 1.4692729711532593, 'learning_rate': 8.886989417916435e-06, 'epoch': 0.55}
{'loss': 1.1938, 'grad_norm': 1.4991559982299805, 'learning_rate': 8.88079750207181e-06, 'epoch': 0.55}
{'loss': 1.173, 'grad_norm': 1.4444786310195923, 'learning_rate': 8.87460602074013e-06, 'epoch': 0.55}
{'loss': 1.176, 'grad_norm': 1.5073696374893188, 'learning_rate': 8.86841497632514e-06, 'epoch': 0.55}
{'loss': 1.0705, 'grad_norm': 1.4539874792099, 'learning_rate': 8.862224371230418e-06, 'epoch': 0.55}
{'loss': 1.1615, 'grad_norm': 1.5318083763122559, 'learning_rate': 8.85603420785937e-06, 'epoch': 0.55}
{'loss': 1.1605, 'grad_norm': 1.209452509880066, 'learning_rate': 8.84984448861523e-06, 'epoch': 0.55}
{'loss': 1.0398, 'grad_norm': 1.525599479675293, 'learning_rate': 8.84365521590106e-06, 'epoch': 0.55}
{'loss': 1.1837, 'grad_norm': 1.186066746711731, 'learning_rate': 8.837466392119752e-06, 'epoch': 0.55}
{'loss': 1.176, 'grad_norm': 1.5580132007598877, 'learning_rate': 8.831278019674017e-06, 'epoch': 0.55}
{'loss': 1.0968, 'grad_norm': 1.4820061922073364, 'learning_rate': 8.825090100966396e-06, 'epoch': 0.55}
{'loss': 1.0739, 'grad_norm': 1.5272419452667236, 'learning_rate': 8.818902638399247e-06, 'epoch': 0.55}
{'loss': 1.1253, 'grad_norm': 1.6770730018615723, 'learning_rate': 8.81271563437476e-06, 'epoch': 0.55}
{'loss': 1.0943, 'grad_norm': 1.5493688583374023, 'learning_rate': 8.806529091294948e-06, 'epoch': 0.55}
{'loss': 1.0871, 'grad_norm': 1.5252370834350586, 'learning_rate': 8.800343011561633e-06, 'epoch': 0.55}
{'loss': 1.0455, 'grad_norm': 1.4592243432998657, 'learning_rate': 8.794157397576464e-06, 'epoch': 0.55}
{'loss': 1.0922, 'grad_norm': 1.5513622760772705, 'learning_rate': 8.787972251740916e-06, 'epoch': 0.55}
{'loss': 1.1105, 'grad_norm': 1.5859105587005615, 'learning_rate': 8.781787576456269e-06, 'epoch': 0.55}
{'loss': 1.2657, 'grad_norm': 1.5852093696594238, 'learning_rate': 8.775603374123627e-06, 'epoch': 0.55}
{'loss': 1.1032, 'grad_norm': 1.6065090894699097, 'learning_rate': 8.769419647143917e-06, 'epoch': 0.55}
{'loss': 1.2258, 'grad_norm': 1.270505428314209, 'learning_rate': 8.763236397917865e-06, 'epoch': 0.55}
{'loss': 1.2076, 'grad_norm': 1.700158715248108, 'learning_rate': 8.757053628846028e-06, 'epoch': 0.55}
{'loss': 1.2054, 'grad_norm': 1.5586862564086914, 'learning_rate': 8.75087134232877e-06, 'epoch': 0.55}
{'loss': 1.1244, 'grad_norm': 1.5304852724075317, 'learning_rate': 8.744689540766265e-06, 'epoch': 0.55}
{'loss': 1.0964, 'grad_norm': 1.5406242609024048, 'learning_rate': 8.738508226558499e-06, 'epoch': 0.55}
{'loss': 1.1502, 'grad_norm': 1.3540630340576172, 'learning_rate': 8.73232740210528e-06, 'epoch': 0.55}
{'loss': 1.0591, 'grad_norm': 1.4855566024780273, 'learning_rate': 8.726147069806206e-06, 'epoch': 0.55}
{'loss': 1.1812, 'grad_norm': 1.2331277132034302, 'learning_rate': 8.719967232060698e-06, 'epoch': 0.55}
{'loss': 1.0868, 'grad_norm': 1.3508405685424805, 'learning_rate': 8.713787891267988e-06, 'epoch': 0.56}
{'loss': 1.1771, 'grad_norm': 1.63689124584198, 'learning_rate': 8.707609049827102e-06, 'epoch': 0.56}
{'loss': 1.2241, 'grad_norm': 1.5313493013381958, 'learning_rate': 8.70143071013688e-06, 'epoch': 0.56}
{'loss': 1.1297, 'grad_norm': 1.5773823261260986, 'learning_rate': 8.695252874595972e-06, 'epoch': 0.56}
{'loss': 1.1405, 'grad_norm': 1.4234410524368286, 'learning_rate': 8.689075545602816e-06, 'epoch': 0.56}
{'loss': 1.1771, 'grad_norm': 1.6119675636291504, 'learning_rate': 8.68289872555567e-06, 'epoch': 0.56}
{'loss': 1.1068, 'grad_norm': 1.5130882263183594, 'learning_rate': 8.676722416852594e-06, 'epoch': 0.56}
{'loss': 1.1329, 'grad_norm': 1.4715690612792969, 'learning_rate': 8.670546621891434e-06, 'epoch': 0.56}
{'loss': 1.0786, 'grad_norm': 1.3975684642791748, 'learning_rate': 8.66437134306985e-06, 'epoch': 0.56}
{'loss': 1.1591, 'grad_norm': 1.4106178283691406, 'learning_rate': 8.658196582785297e-06, 'epoch': 0.56}
{'loss': 1.0792, 'grad_norm': 1.3984791040420532, 'learning_rate': 8.652022343435027e-06, 'epoch': 0.56}
{'loss': 1.1013, 'grad_norm': 1.4110441207885742, 'learning_rate': 8.645848627416102e-06, 'epoch': 0.56}
{'loss': 1.0934, 'grad_norm': 1.40589439868927, 'learning_rate': 8.63967543712536e-06, 'epoch': 0.56}
{'loss': 1.0618, 'grad_norm': 1.4185606241226196, 'learning_rate': 8.633502774959453e-06, 'epoch': 0.56}
{'loss': 1.0803, 'grad_norm': 1.4863407611846924, 'learning_rate': 8.627330643314818e-06, 'epoch': 0.56}
{'loss': 1.1465, 'grad_norm': 1.4733141660690308, 'learning_rate': 8.62115904458769e-06, 'epoch': 0.56}
{'loss': 1.1717, 'grad_norm': 1.515143871307373, 'learning_rate': 8.614987981174093e-06, 'epoch': 0.56}
{'loss': 1.0787, 'grad_norm': 1.580575942993164, 'learning_rate': 8.608817455469854e-06, 'epoch': 0.56}
{'loss': 1.0149, 'grad_norm': 1.4514331817626953, 'learning_rate': 8.602647469870573e-06, 'epoch': 0.56}
{'loss': 1.0992, 'grad_norm': 1.5481438636779785, 'learning_rate': 8.596478026771658e-06, 'epoch': 0.56}
{'loss': 0.9598, 'grad_norm': 1.4788508415222168, 'learning_rate': 8.590309128568303e-06, 'epoch': 0.56}
{'loss': 1.1389, 'grad_norm': 1.5011340379714966, 'learning_rate': 8.584140777655476e-06, 'epoch': 0.56}
{'loss': 1.0662, 'grad_norm': 1.5298488140106201, 'learning_rate': 8.57797297642795e-06, 'epoch': 0.56}
{'loss': 1.2696, 'grad_norm': 1.3464373350143433, 'learning_rate': 8.571805727280278e-06, 'epoch': 0.56}
{'loss': 1.228, 'grad_norm': 1.3608858585357666, 'learning_rate': 8.565639032606794e-06, 'epoch': 0.56}
{'loss': 1.115, 'grad_norm': 1.435651183128357, 'learning_rate': 8.559472894801623e-06, 'epoch': 0.56}
{'loss': 1.1418, 'grad_norm': 1.6222598552703857, 'learning_rate': 8.553307316258678e-06, 'epoch': 0.56}
{'loss': 1.1527, 'grad_norm': 1.529994010925293, 'learning_rate': 8.547142299371642e-06, 'epoch': 0.56}
{'loss': 1.1173, 'grad_norm': 1.4044389724731445, 'learning_rate': 8.540977846533986e-06, 'epoch': 0.56}
{'loss': 1.1661, 'grad_norm': 1.1594902276992798, 'learning_rate': 8.534813960138968e-06, 'epoch': 0.56}
{'loss': 1.1513, 'grad_norm': 1.5912795066833496, 'learning_rate': 8.528650642579618e-06, 'epoch': 0.56}
{'loss': 1.1186, 'grad_norm': 1.4667972326278687, 'learning_rate': 8.52248789624875e-06, 'epoch': 0.56}
{'loss': 1.084, 'grad_norm': 1.4788328409194946, 'learning_rate': 8.516325723538949e-06, 'epoch': 0.56}
{'loss': 1.1663, 'grad_norm': 1.4692950248718262, 'learning_rate': 8.510164126842591e-06, 'epoch': 0.56}
{'loss': 1.1038, 'grad_norm': 1.5136629343032837, 'learning_rate': 8.504003108551814e-06, 'epoch': 0.56}
{'loss': 1.1338, 'grad_norm': 1.4686585664749146, 'learning_rate': 8.497842671058539e-06, 'epoch': 0.56}
{'loss': 1.0499, 'grad_norm': 1.553274393081665, 'learning_rate': 8.491682816754456e-06, 'epoch': 0.56}
{'loss': 1.0482, 'grad_norm': 1.5224791765213013, 'learning_rate': 8.485523548031044e-06, 'epoch': 0.56}
{'loss': 1.1568, 'grad_norm': 1.4180269241333008, 'learning_rate': 8.479364867279529e-06, 'epoch': 0.56}
{'loss': 0.958, 'grad_norm': 1.3345671892166138, 'learning_rate': 8.47320677689093e-06, 'epoch': 0.56}
{'loss': 1.1114, 'grad_norm': 1.5472017526626587, 'learning_rate': 8.467049279256034e-06, 'epoch': 0.56}
{'loss': 1.1, 'grad_norm': 1.7146581411361694, 'learning_rate': 8.460892376765387e-06, 'epoch': 0.56}
{'loss': 1.1545, 'grad_norm': 1.6972287893295288, 'learning_rate': 8.45473607180931e-06, 'epoch': 0.56}
{'loss': 1.0446, 'grad_norm': 1.5802668333053589, 'learning_rate': 8.448580366777898e-06, 'epoch': 0.56}
{'loss': 1.0645, 'grad_norm': 1.5531038045883179, 'learning_rate': 8.442425264061e-06, 'epoch': 0.56}
{'loss': 1.0962, 'grad_norm': 1.4926214218139648, 'learning_rate': 8.436270766048245e-06, 'epoch': 0.56}
{'loss': 1.2491, 'grad_norm': 1.2617326974868774, 'learning_rate': 8.430116875129023e-06, 'epoch': 0.56}
{'loss': 1.1471, 'grad_norm': 1.5724021196365356, 'learning_rate': 8.42396359369248e-06, 'epoch': 0.56}
{'loss': 1.1594, 'grad_norm': 1.619274616241455, 'learning_rate': 8.417810924127533e-06, 'epoch': 0.56}
{'loss': 1.1408, 'grad_norm': 1.4559335708618164, 'learning_rate': 8.411658868822866e-06, 'epoch': 0.56}
{'loss': 1.111, 'grad_norm': 1.6406880617141724, 'learning_rate': 8.40550743016691e-06, 'epoch': 0.56}
{'loss': 1.0646, 'grad_norm': 1.5578995943069458, 'learning_rate': 8.39935661054787e-06, 'epoch': 0.56}
{'loss': 1.3418, 'grad_norm': 1.0237128734588623, 'learning_rate': 8.393206412353709e-06, 'epoch': 0.57}
{'loss': 1.2599, 'grad_norm': 1.2673108577728271, 'learning_rate': 8.38705683797214e-06, 'epoch': 0.57}
{'loss': 1.1019, 'grad_norm': 1.506417155265808, 'learning_rate': 8.38090788979064e-06, 'epoch': 0.57}
{'loss': 1.0391, 'grad_norm': 1.562506914138794, 'learning_rate': 8.374759570196448e-06, 'epoch': 0.57}
{'loss': 1.106, 'grad_norm': 1.5858858823776245, 'learning_rate': 8.368611881576547e-06, 'epoch': 0.57}
{'loss': 1.0946, 'grad_norm': 1.5162986516952515, 'learning_rate': 8.362464826317687e-06, 'epoch': 0.57}
{'loss': 1.1818, 'grad_norm': 1.3176366090774536, 'learning_rate': 8.35631840680636e-06, 'epoch': 0.57}
{'loss': 1.1014, 'grad_norm': 1.5623598098754883, 'learning_rate': 8.35017262542882e-06, 'epoch': 0.57}
{'loss': 1.0241, 'grad_norm': 1.6845167875289917, 'learning_rate': 8.344027484571075e-06, 'epoch': 0.57}
{'loss': 1.1734, 'grad_norm': 1.6021255254745483, 'learning_rate': 8.337882986618877e-06, 'epoch': 0.57}
{'loss': 1.0932, 'grad_norm': 1.4179515838623047, 'learning_rate': 8.331739133957729e-06, 'epoch': 0.57}
{'loss': 1.1968, 'grad_norm': 1.6045806407928467, 'learning_rate': 8.325595928972894e-06, 'epoch': 0.57}
{'loss': 1.1067, 'grad_norm': 1.4600833654403687, 'learning_rate': 8.319453374049367e-06, 'epoch': 0.57}
{'loss': 1.1402, 'grad_norm': 1.4549965858459473, 'learning_rate': 8.313311471571903e-06, 'epoch': 0.57}
{'loss': 1.1377, 'grad_norm': 1.5905941724777222, 'learning_rate': 8.307170223925003e-06, 'epoch': 0.57}
{'loss': 1.1188, 'grad_norm': 1.496539831161499, 'learning_rate': 8.301029633492907e-06, 'epoch': 0.57}
{'loss': 1.1408, 'grad_norm': 1.3863096237182617, 'learning_rate': 8.294889702659602e-06, 'epoch': 0.57}
{'loss': 1.1487, 'grad_norm': 1.4512317180633545, 'learning_rate': 8.288750433808828e-06, 'epoch': 0.57}
{'loss': 1.1378, 'grad_norm': 1.4131537675857544, 'learning_rate': 8.282611829324049e-06, 'epoch': 0.57}
Error with image file is truncated (43 bytes not processed)
{'loss': 1.0939, 'grad_norm': 1.4419760704040527, 'learning_rate': 8.276473891588492e-06, 'epoch': 0.57}
{'loss': 1.2363, 'grad_norm': 1.6572829484939575, 'learning_rate': 8.270336622985116e-06, 'epoch': 0.57}
{'loss': 1.0875, 'grad_norm': 1.5328937768936157, 'learning_rate': 8.264200025896616e-06, 'epoch': 0.57}
{'loss': 1.0705, 'grad_norm': 1.5467532873153687, 'learning_rate': 8.258064102705428e-06, 'epoch': 0.57}
{'loss': 1.1272, 'grad_norm': 1.508118987083435, 'learning_rate': 8.251928855793736e-06, 'epoch': 0.57}
{'loss': 1.1058, 'grad_norm': 1.5346016883850098, 'learning_rate': 8.245794287543447e-06, 'epoch': 0.57}
{'loss': 1.1157, 'grad_norm': 1.5776222944259644, 'learning_rate': 8.239660400336213e-06, 'epoch': 0.57}
{'loss': 1.0463, 'grad_norm': 1.5710422992706299, 'learning_rate': 8.233527196553428e-06, 'epoch': 0.57}
{'loss': 1.1735, 'grad_norm': 1.246695637702942, 'learning_rate': 8.227394678576204e-06, 'epoch': 0.57}
{'loss': 1.0663, 'grad_norm': 1.560119390487671, 'learning_rate': 8.221262848785395e-06, 'epoch': 0.57}
{'loss': 1.1315, 'grad_norm': 1.6302968263626099, 'learning_rate': 8.215131709561597e-06, 'epoch': 0.57}
{'loss': 1.1019, 'grad_norm': 1.6203923225402832, 'learning_rate': 8.20900126328512e-06, 'epoch': 0.57}
{'loss': 1.0326, 'grad_norm': 1.433400273323059, 'learning_rate': 8.202871512336023e-06, 'epoch': 0.57}
{'loss': 1.1179, 'grad_norm': 1.5206440687179565, 'learning_rate': 8.196742459094079e-06, 'epoch': 0.57}
{'loss': 1.2425, 'grad_norm': 1.2208411693572998, 'learning_rate': 8.190614105938796e-06, 'epoch': 0.57}
{'loss': 1.0412, 'grad_norm': 1.5253866910934448, 'learning_rate': 8.184486455249424e-06, 'epoch': 0.57}
{'loss': 1.009, 'grad_norm': 1.5041688680648804, 'learning_rate': 8.178359509404916e-06, 'epoch': 0.57}
{'loss': 1.0839, 'grad_norm': 1.5107574462890625, 'learning_rate': 8.172233270783966e-06, 'epoch': 0.57}
{'loss': 1.1733, 'grad_norm': 1.4276890754699707, 'learning_rate': 8.166107741764997e-06, 'epoch': 0.57}
{'loss': 1.1848, 'grad_norm': 1.4941277503967285, 'learning_rate': 8.15998292472614e-06, 'epoch': 0.57}
{'loss': 1.2453, 'grad_norm': 1.3903414011001587, 'learning_rate': 8.153858822045267e-06, 'epoch': 0.57}
{'loss': 1.2514, 'grad_norm': 1.5624021291732788, 'learning_rate': 8.147735436099967e-06, 'epoch': 0.57}
{'loss': 1.0891, 'grad_norm': 1.442976474761963, 'learning_rate': 8.141612769267543e-06, 'epoch': 0.57}
{'loss': 1.136, 'grad_norm': 1.660080909729004, 'learning_rate': 8.135490823925027e-06, 'epoch': 0.57}
{'loss': 1.1857, 'grad_norm': 1.584200382232666, 'learning_rate': 8.129369602449176e-06, 'epoch': 0.57}
{'loss': 1.1094, 'grad_norm': 1.5324831008911133, 'learning_rate': 8.123249107216446e-06, 'epoch': 0.57}
{'loss': 1.0252, 'grad_norm': 1.5250356197357178, 'learning_rate': 8.117129340603032e-06, 'epoch': 0.57}
{'loss': 1.0823, 'grad_norm': 1.397218942642212, 'learning_rate': 8.111010304984841e-06, 'epoch': 0.57}
{'loss': 1.1309, 'grad_norm': 1.5374643802642822, 'learning_rate': 8.104892002737488e-06, 'epoch': 0.57}
{'loss': 1.1643, 'grad_norm': 1.5168014764785767, 'learning_rate': 8.098774436236308e-06, 'epoch': 0.57}
{'loss': 1.1187, 'grad_norm': 1.6209996938705444, 'learning_rate': 8.092657607856356e-06, 'epoch': 0.57}
{'loss': 1.1293, 'grad_norm': 1.564290165901184, 'learning_rate': 8.086541519972388e-06, 'epoch': 0.57}
{'loss': 1.08, 'grad_norm': 1.5517017841339111, 'learning_rate': 8.080426174958886e-06, 'epoch': 0.57}
{'loss': 1.0594, 'grad_norm': 1.3803023099899292, 'learning_rate': 8.074311575190039e-06, 'epoch': 0.58}
{'loss': 1.0749, 'grad_norm': 1.5630621910095215, 'learning_rate': 8.068197723039738e-06, 'epoch': 0.58}
{'loss': 1.1153, 'grad_norm': 1.5903558731079102, 'learning_rate': 8.062084620881598e-06, 'epoch': 0.58}
{'loss': 1.0246, 'grad_norm': 1.6570918560028076, 'learning_rate': 8.055972271088933e-06, 'epoch': 0.58}
{'loss': 1.1555, 'grad_norm': 1.4961531162261963, 'learning_rate': 8.049860676034762e-06, 'epoch': 0.58}
{'loss': 1.167, 'grad_norm': 1.4808229207992554, 'learning_rate': 8.043749838091828e-06, 'epoch': 0.58}
{'loss': 1.0775, 'grad_norm': 1.4165065288543701, 'learning_rate': 8.037639759632558e-06, 'epoch': 0.58}
{'loss': 1.1655, 'grad_norm': 1.4961440563201904, 'learning_rate': 8.031530443029099e-06, 'epoch': 0.58}
{'loss': 1.0618, 'grad_norm': 1.5594713687896729, 'learning_rate': 8.025421890653303e-06, 'epoch': 0.58}
{'loss': 1.1464, 'grad_norm': 1.4295127391815186, 'learning_rate': 8.019314104876712e-06, 'epoch': 0.58}
WARNING: tokenization mismatch: 0 vs. 55. (ignored)
{'loss': 1.1157, 'grad_norm': 1.385453224182129, 'learning_rate': 8.013207088070582e-06, 'epoch': 0.58}
{'loss': 1.1292, 'grad_norm': 1.4925122261047363, 'learning_rate': 8.007100842605872e-06, 'epoch': 0.58}
{'loss': 1.1513, 'grad_norm': 1.51592218875885, 'learning_rate': 8.000995370853227e-06, 'epoch': 0.58}
{'loss': 1.1631, 'grad_norm': 1.6055434942245483, 'learning_rate': 7.994890675183008e-06, 'epoch': 0.58}
{'loss': 1.1664, 'grad_norm': 1.456938624382019, 'learning_rate': 7.98878675796527e-06, 'epoch': 0.58}
{'loss': 1.0895, 'grad_norm': 1.1838572025299072, 'learning_rate': 7.98268362156976e-06, 'epoch': 0.58}
{'loss': 1.2017, 'grad_norm': 1.4323831796646118, 'learning_rate': 7.976581268365924e-06, 'epoch': 0.58}
{'loss': 1.0097, 'grad_norm': 1.5333083868026733, 'learning_rate': 7.97047970072291e-06, 'epoch': 0.58}
{'loss': 1.1355, 'grad_norm': 1.5162973403930664, 'learning_rate': 7.964378921009552e-06, 'epoch': 0.58}
{'loss': 0.9819, 'grad_norm': 1.4520909786224365, 'learning_rate': 7.958278931594385e-06, 'epoch': 0.58}
{'loss': 1.1532, 'grad_norm': 1.5513859987258911, 'learning_rate': 7.952179734845642e-06, 'epoch': 0.58}
{'loss': 1.1257, 'grad_norm': 1.421854019165039, 'learning_rate': 7.946081333131227e-06, 'epoch': 0.58}
{'loss': 1.2194, 'grad_norm': 1.2172999382019043, 'learning_rate': 7.93998372881876e-06, 'epoch': 0.58}
{'loss': 1.1136, 'grad_norm': 1.3863134384155273, 'learning_rate': 7.93388692427554e-06, 'epoch': 0.58}
{'loss': 1.1194, 'grad_norm': 1.5184810161590576, 'learning_rate': 7.92779092186855e-06, 'epoch': 0.58}
{'loss': 1.2585, 'grad_norm': 1.2568913698196411, 'learning_rate': 7.921695723964473e-06, 'epoch': 0.58}
{'loss': 1.143, 'grad_norm': 1.5379247665405273, 'learning_rate': 7.915601332929678e-06, 'epoch': 0.58}
{'loss': 1.1984, 'grad_norm': 1.4157322645187378, 'learning_rate': 7.90950775113021e-06, 'epoch': 0.58}
{'loss': 1.195, 'grad_norm': 1.3665891885757446, 'learning_rate': 7.903414980931813e-06, 'epoch': 0.58}
{'loss': 1.074, 'grad_norm': 1.474389672279358, 'learning_rate': 7.897323024699907e-06, 'epoch': 0.58}
{'loss': 1.0679, 'grad_norm': 1.5058943033218384, 'learning_rate': 7.8912318847996e-06, 'epoch': 0.58}
{'loss': 1.1439, 'grad_norm': 1.5296083688735962, 'learning_rate': 7.885141563595685e-06, 'epoch': 0.58}
{'loss': 1.1682, 'grad_norm': 1.1546045541763306, 'learning_rate': 7.879052063452626e-06, 'epoch': 0.58}
{'loss': 1.0999, 'grad_norm': 1.402898907661438, 'learning_rate': 7.872963386734584e-06, 'epoch': 0.58}
{'loss': 1.1223, 'grad_norm': 1.4699184894561768, 'learning_rate': 7.866875535805394e-06, 'epoch': 0.58}
{'loss': 1.105, 'grad_norm': 1.5344568490982056, 'learning_rate': 7.860788513028566e-06, 'epoch': 0.58}
{'loss': 1.1021, 'grad_norm': 1.3757758140563965, 'learning_rate': 7.85470232076729e-06, 'epoch': 0.58}
{'loss': 1.0053, 'grad_norm': 1.6671428680419922, 'learning_rate': 7.848616961384442e-06, 'epoch': 0.58}
{'loss': 1.1059, 'grad_norm': 1.4516712427139282, 'learning_rate': 7.842532437242559e-06, 'epoch': 0.58}
{'loss': 1.2625, 'grad_norm': 1.3407176733016968, 'learning_rate': 7.83644875070387e-06, 'epoch': 0.58}
{'loss': 1.2117, 'grad_norm': 1.1498302221298218, 'learning_rate': 7.83036590413027e-06, 'epoch': 0.58}
{'loss': 1.1397, 'grad_norm': 1.5544204711914062, 'learning_rate': 7.824283899883327e-06, 'epoch': 0.58}
{'loss': 1.1124, 'grad_norm': 1.5222842693328857, 'learning_rate': 7.818202740324287e-06, 'epoch': 0.58}
{'loss': 1.0069, 'grad_norm': 1.4606921672821045, 'learning_rate': 7.812122427814068e-06, 'epoch': 0.58}
{'loss': 1.101, 'grad_norm': 1.3955429792404175, 'learning_rate': 7.806042964713248e-06, 'epoch': 0.58}
{'loss': 1.0373, 'grad_norm': 1.3727385997772217, 'learning_rate': 7.79996435338209e-06, 'epoch': 0.58}
{'loss': 1.2215, 'grad_norm': 1.2532333135604858, 'learning_rate': 7.793886596180521e-06, 'epoch': 0.58}
{'loss': 1.0977, 'grad_norm': 1.4037741422653198, 'learning_rate': 7.787809695468134e-06, 'epoch': 0.58}
{'loss': 1.1303, 'grad_norm': 1.518026351928711, 'learning_rate': 7.78173365360419e-06, 'epoch': 0.58}
{'loss': 1.1111, 'grad_norm': 1.4584087133407593, 'learning_rate': 7.775658472947623e-06, 'epoch': 0.58}
{'loss': 1.1567, 'grad_norm': 1.8380931615829468, 'learning_rate': 7.769584155857019e-06, 'epoch': 0.58}
{'loss': 1.0938, 'grad_norm': 1.3857260942459106, 'learning_rate': 7.763510704690645e-06, 'epoch': 0.58}
{'loss': 1.0709, 'grad_norm': 1.5518254041671753, 'learning_rate': 7.757438121806414e-06, 'epoch': 0.59}
{'loss': 1.0041, 'grad_norm': 1.4465945959091187, 'learning_rate': 7.75136640956192e-06, 'epoch': 0.59}
{'loss': 0.9714, 'grad_norm': 1.4819573163986206, 'learning_rate': 7.745295570314412e-06, 'epoch': 0.59}
{'loss': 1.0519, 'grad_norm': 1.5160964727401733, 'learning_rate': 7.739225606420793e-06, 'epoch': 0.59}
{'loss': 1.2152, 'grad_norm': 1.1931465864181519, 'learning_rate': 7.733156520237633e-06, 'epoch': 0.59}
{'loss': 1.2113, 'grad_norm': 1.6187516450881958, 'learning_rate': 7.727088314121165e-06, 'epoch': 0.59}
{'loss': 1.1054, 'grad_norm': 1.5017684698104858, 'learning_rate': 7.721020990427268e-06, 'epoch': 0.59}
{'loss': 1.1861, 'grad_norm': 1.2415860891342163, 'learning_rate': 7.714954551511489e-06, 'epoch': 0.59}
{'loss': 0.998, 'grad_norm': 1.4291431903839111, 'learning_rate': 7.708888999729036e-06, 'epoch': 0.59}
{'loss': 1.0923, 'grad_norm': 1.5776214599609375, 'learning_rate': 7.702824337434756e-06, 'epoch': 0.59}
{'loss': 1.1385, 'grad_norm': 1.7533855438232422, 'learning_rate': 7.69676056698316e-06, 'epoch': 0.59}
{'loss': 1.0516, 'grad_norm': 1.4511970281600952, 'learning_rate': 7.690697690728417e-06, 'epoch': 0.59}
{'loss': 1.2264, 'grad_norm': 1.2767400741577148, 'learning_rate': 7.68463571102434e-06, 'epoch': 0.59}
{'loss': 1.097, 'grad_norm': 1.5091081857681274, 'learning_rate': 7.678574630224399e-06, 'epoch': 0.59}
{'loss': 1.0434, 'grad_norm': 1.491908073425293, 'learning_rate': 7.672514450681721e-06, 'epoch': 0.59}
{'loss': 1.0323, 'grad_norm': 1.4886748790740967, 'learning_rate': 7.666455174749066e-06, 'epoch': 0.59}
{'loss': 1.0462, 'grad_norm': 1.435221791267395, 'learning_rate': 7.66039680477886e-06, 'epoch': 0.59}
{'loss': 1.0894, 'grad_norm': 1.591212511062622, 'learning_rate': 7.654339343123173e-06, 'epoch': 0.59}
{'loss': 1.04, 'grad_norm': 1.440732479095459, 'learning_rate': 7.648282792133711e-06, 'epoch': 0.59}
{'loss': 1.1016, 'grad_norm': 1.4678369760513306, 'learning_rate': 7.642227154161841e-06, 'epoch': 0.59}
{'loss': 1.0434, 'grad_norm': 1.468765377998352, 'learning_rate': 7.636172431558575e-06, 'epoch': 0.59}
{'loss': 1.2033, 'grad_norm': 1.2509591579437256, 'learning_rate': 7.630118626674557e-06, 'epoch': 0.59}
{'loss': 1.1357, 'grad_norm': 1.6094814538955688, 'learning_rate': 7.6240657418600846e-06, 'epoch': 0.59}
{'loss': 1.1003, 'grad_norm': 1.4895039796829224, 'learning_rate': 7.618013779465101e-06, 'epoch': 0.59}
{'loss': 1.0869, 'grad_norm': 1.603285789489746, 'learning_rate': 7.611962741839178e-06, 'epoch': 0.59}
{'loss': 1.0864, 'grad_norm': 1.4716604948043823, 'learning_rate': 7.6059126313315466e-06, 'epoch': 0.59}
{'loss': 1.1152, 'grad_norm': 1.5626884698867798, 'learning_rate': 7.599863450291056e-06, 'epoch': 0.59}
{'loss': 1.1003, 'grad_norm': 1.5608786344528198, 'learning_rate': 7.593815201066215e-06, 'epoch': 0.59}
{'loss': 1.0606, 'grad_norm': 1.5344634056091309, 'learning_rate': 7.587767886005164e-06, 'epoch': 0.59}
{'loss': 1.1436, 'grad_norm': 1.527512788772583, 'learning_rate': 7.581721507455672e-06, 'epoch': 0.59}
{'loss': 1.0846, 'grad_norm': 1.5159279108047485, 'learning_rate': 7.575676067765154e-06, 'epoch': 0.59}
{'loss': 1.1436, 'grad_norm': 1.5323058366775513, 'learning_rate': 7.569631569280662e-06, 'epoch': 0.59}
{'loss': 1.1267, 'grad_norm': 1.663540244102478, 'learning_rate': 7.563588014348871e-06, 'epoch': 0.59}
{'loss': 1.2071, 'grad_norm': 1.5047425031661987, 'learning_rate': 7.5575454053161e-06, 'epoch': 0.59}
{'loss': 1.1075, 'grad_norm': 1.4457958936691284, 'learning_rate': 7.551503744528304e-06, 'epoch': 0.59}
{'loss': 1.1921, 'grad_norm': 1.1640506982803345, 'learning_rate': 7.545463034331054e-06, 'epoch': 0.59}
{'loss': 1.1264, 'grad_norm': 1.381097435951233, 'learning_rate': 7.539423277069568e-06, 'epoch': 0.59}
{'loss': 1.1405, 'grad_norm': 1.5805644989013672, 'learning_rate': 7.53338447508869e-06, 'epoch': 0.59}
{'loss': 1.1438, 'grad_norm': 1.506250262260437, 'learning_rate': 7.52734663073288e-06, 'epoch': 0.59}
{'loss': 1.1221, 'grad_norm': 1.5705190896987915, 'learning_rate': 7.521309746346246e-06, 'epoch': 0.59}
{'loss': 1.1851, 'grad_norm': 1.4817471504211426, 'learning_rate': 7.515273824272516e-06, 'epoch': 0.59}
{'loss': 1.1135, 'grad_norm': 1.63341224193573, 'learning_rate': 7.509238866855033e-06, 'epoch': 0.59}
{'loss': 1.0587, 'grad_norm': 1.4199278354644775, 'learning_rate': 7.503204876436785e-06, 'epoch': 0.59}
{'loss': 1.1653, 'grad_norm': 1.5074542760849, 'learning_rate': 7.497171855360372e-06, 'epoch': 0.59}
{'loss': 1.1741, 'grad_norm': 1.2905192375183105, 'learning_rate': 7.491139805968018e-06, 'epoch': 0.59}
{'loss': 1.0758, 'grad_norm': 1.4064981937408447, 'learning_rate': 7.485108730601571e-06, 'epoch': 0.59}
{'loss': 1.0901, 'grad_norm': 1.4663313627243042, 'learning_rate': 7.4790786316025125e-06, 'epoch': 0.59}
{'loss': 1.0843, 'grad_norm': 1.4362651109695435, 'learning_rate': 7.473049511311921e-06, 'epoch': 0.59}
{'loss': 1.1118, 'grad_norm': 1.4587972164154053, 'learning_rate': 7.467021372070515e-06, 'epoch': 0.59}
{'loss': 1.0742, 'grad_norm': 1.618032693862915, 'learning_rate': 7.46099421621863e-06, 'epoch': 0.59}
{'loss': 1.1141, 'grad_norm': 1.4622396230697632, 'learning_rate': 7.4549680460962044e-06, 'epoch': 0.59}
{'loss': 1.0964, 'grad_norm': 1.5095911026000977, 'learning_rate': 7.448942864042819e-06, 'epoch': 0.59}
{'loss': 1.2502, 'grad_norm': 1.5664042234420776, 'learning_rate': 7.4429186723976425e-06, 'epoch': 0.6}
{'loss': 1.1653, 'grad_norm': 1.5552592277526855, 'learning_rate': 7.43689547349948e-06, 'epoch': 0.6}
{'loss': 1.1543, 'grad_norm': 1.4612622261047363, 'learning_rate': 7.43087326968675e-06, 'epoch': 0.6}
{'loss': 1.0581, 'grad_norm': 1.5123950242996216, 'learning_rate': 7.42485206329747e-06, 'epoch': 0.6}
{'loss': 1.0692, 'grad_norm': 1.4966621398925781, 'learning_rate': 7.418831856669286e-06, 'epoch': 0.6}
{'loss': 1.068, 'grad_norm': 1.3591076135635376, 'learning_rate': 7.41281265213945e-06, 'epoch': 0.6}
{'loss': 1.0349, 'grad_norm': 1.4393681287765503, 'learning_rate': 7.406794452044816e-06, 'epoch': 0.6}
{'loss': 1.1036, 'grad_norm': 1.4809095859527588, 'learning_rate': 7.400777258721865e-06, 'epoch': 0.6}
{'loss': 1.0998, 'grad_norm': 1.6324104070663452, 'learning_rate': 7.394761074506679e-06, 'epoch': 0.6}
{'loss': 1.1378, 'grad_norm': 1.4402660131454468, 'learning_rate': 7.3887459017349405e-06, 'epoch': 0.6}
{'loss': 1.0698, 'grad_norm': 1.4602330923080444, 'learning_rate': 7.382731742741953e-06, 'epoch': 0.6}
{'loss': 1.1147, 'grad_norm': 1.5470398664474487, 'learning_rate': 7.376718599862621e-06, 'epoch': 0.6}
{'loss': 1.1144, 'grad_norm': 1.36018705368042, 'learning_rate': 7.370706475431446e-06, 'epoch': 0.6}
{'loss': 1.1438, 'grad_norm': 1.4516022205352783, 'learning_rate': 7.364695371782547e-06, 'epoch': 0.6}
{'loss': 1.1134, 'grad_norm': 1.4617658853530884, 'learning_rate': 7.358685291249644e-06, 'epoch': 0.6}
{'loss': 1.1233, 'grad_norm': 1.322632908821106, 'learning_rate': 7.352676236166051e-06, 'epoch': 0.6}
{'loss': 1.0414, 'grad_norm': 1.5020073652267456, 'learning_rate': 7.346668208864695e-06, 'epoch': 0.6}
{'loss': 1.0618, 'grad_norm': 1.6077131032943726, 'learning_rate': 7.3406612116781e-06, 'epoch': 0.6}
{'loss': 1.0574, 'grad_norm': 1.4030417203903198, 'learning_rate': 7.33465524693838e-06, 'epoch': 0.6}
{'loss': 1.0282, 'grad_norm': 1.5251109600067139, 'learning_rate': 7.328650316977265e-06, 'epoch': 0.6}
{'loss': 1.0374, 'grad_norm': 1.4433714151382446, 'learning_rate': 7.322646424126079e-06, 'epoch': 0.6}
{'loss': 1.153, 'grad_norm': 1.5509798526763916, 'learning_rate': 7.316643570715729e-06, 'epoch': 0.6}
{'loss': 1.0461, 'grad_norm': 1.4610724449157715, 'learning_rate': 7.310641759076742e-06, 'epoch': 0.6}
{'loss': 1.1683, 'grad_norm': 1.5817453861236572, 'learning_rate': 7.304640991539216e-06, 'epoch': 0.6}
{'loss': 1.0989, 'grad_norm': 1.4704445600509644, 'learning_rate': 7.2986412704328625e-06, 'epoch': 0.6}
{'loss': 1.1762, 'grad_norm': 1.450669288635254, 'learning_rate': 7.292642598086982e-06, 'epoch': 0.6}
{'loss': 1.2267, 'grad_norm': 1.2906774282455444, 'learning_rate': 7.286644976830457e-06, 'epoch': 0.6}
{'loss': 1.1075, 'grad_norm': 1.5184515714645386, 'learning_rate': 7.280648408991775e-06, 'epoch': 0.6}
{'loss': 1.0358, 'grad_norm': 1.4593336582183838, 'learning_rate': 7.274652896899015e-06, 'epoch': 0.6}
{'loss': 1.1314, 'grad_norm': 1.530109167098999, 'learning_rate': 7.268658442879834e-06, 'epoch': 0.6}
{'loss': 1.132, 'grad_norm': 1.6475610733032227, 'learning_rate': 7.262665049261489e-06, 'epoch': 0.6}
{'loss': 1.1526, 'grad_norm': 1.6378154754638672, 'learning_rate': 7.256672718370824e-06, 'epoch': 0.6}
{'loss': 1.1237, 'grad_norm': 1.5831798315048218, 'learning_rate': 7.250681452534261e-06, 'epoch': 0.6}
{'loss': 1.1181, 'grad_norm': 1.544417142868042, 'learning_rate': 7.2446912540778196e-06, 'epoch': 0.6}
{'loss': 1.1602, 'grad_norm': 1.5439351797103882, 'learning_rate': 7.238702125327106e-06, 'epoch': 0.6}
{'loss': 1.2389, 'grad_norm': 1.3582156896591187, 'learning_rate': 7.232714068607296e-06, 'epoch': 0.6}
{'loss': 1.1276, 'grad_norm': 1.57578706741333, 'learning_rate': 7.226727086243168e-06, 'epoch': 0.6}
{'loss': 1.0133, 'grad_norm': 1.438778042793274, 'learning_rate': 7.220741180559074e-06, 'epoch': 0.6}
{'loss': 1.1116, 'grad_norm': 1.5624876022338867, 'learning_rate': 7.214756353878942e-06, 'epoch': 0.6}
{'loss': 1.1308, 'grad_norm': 1.450639009475708, 'learning_rate': 7.208772608526293e-06, 'epoch': 0.6}
{'loss': 1.0293, 'grad_norm': 1.4185442924499512, 'learning_rate': 7.202789946824227e-06, 'epoch': 0.6}
{'loss': 1.0829, 'grad_norm': 1.3359870910644531, 'learning_rate': 7.1968083710954075e-06, 'epoch': 0.6}
{'loss': 1.0881, 'grad_norm': 1.393353819847107, 'learning_rate': 7.1908278836621e-06, 'epoch': 0.6}
{'loss': 1.0608, 'grad_norm': 1.5273784399032593, 'learning_rate': 7.184848486846128e-06, 'epoch': 0.6}
{'loss': 1.0379, 'grad_norm': 1.4446029663085938, 'learning_rate': 7.178870182968904e-06, 'epoch': 0.6}
{'loss': 1.1213, 'grad_norm': 1.5415769815444946, 'learning_rate': 7.1728929743514065e-06, 'epoch': 0.6}
{'loss': 1.0778, 'grad_norm': 1.4793332815170288, 'learning_rate': 7.166916863314199e-06, 'epoch': 0.6}
{'loss': 0.9933, 'grad_norm': 1.3906689882278442, 'learning_rate': 7.1609418521774095e-06, 'epoch': 0.6}
{'loss': 1.0704, 'grad_norm': 1.5226757526397705, 'learning_rate': 7.154967943260748e-06, 'epoch': 0.6}
{'loss': 1.0904, 'grad_norm': 1.7003744840621948, 'learning_rate': 7.148995138883483e-06, 'epoch': 0.6}
{'loss': 1.1062, 'grad_norm': 1.5511223077774048, 'learning_rate': 7.143023441364471e-06, 'epoch': 0.6}
{'loss': 1.0663, 'grad_norm': 1.5165364742279053, 'learning_rate': 7.13705285302213e-06, 'epoch': 0.6}
{'loss': 1.162, 'grad_norm': 1.4233378171920776, 'learning_rate': 7.131083376174441e-06, 'epoch': 0.61}
{'loss': 1.0944, 'grad_norm': 1.5289827585220337, 'learning_rate': 7.125115013138966e-06, 'epoch': 0.61}
{'loss': 1.0764, 'grad_norm': 1.4541361331939697, 'learning_rate': 7.119147766232832e-06, 'epoch': 0.61}
{'loss': 1.1857, 'grad_norm': 1.5747745037078857, 'learning_rate': 7.113181637772721e-06, 'epoch': 0.61}
{'loss': 1.0189, 'grad_norm': 1.5493154525756836, 'learning_rate': 7.107216630074895e-06, 'epoch': 0.61}
{'loss': 1.0717, 'grad_norm': 1.5518966913223267, 'learning_rate': 7.1012527454551795e-06, 'epoch': 0.61}
{'loss': 1.0847, 'grad_norm': 1.441635251045227, 'learning_rate': 7.09528998622895e-06, 'epoch': 0.61}
{'loss': 1.1724, 'grad_norm': 1.6304357051849365, 'learning_rate': 7.089328354711159e-06, 'epoch': 0.61}
{'loss': 1.1262, 'grad_norm': 1.5491584539413452, 'learning_rate': 7.083367853216323e-06, 'epoch': 0.61}
{'loss': 1.1798, 'grad_norm': 1.5514891147613525, 'learning_rate': 7.077408484058505e-06, 'epoch': 0.61}
{'loss': 1.0422, 'grad_norm': 1.4310857057571411, 'learning_rate': 7.071450249551342e-06, 'epoch': 0.61}
{'loss': 1.1573, 'grad_norm': 1.528002142906189, 'learning_rate': 7.065493152008026e-06, 'epoch': 0.61}
{'loss': 1.0503, 'grad_norm': 1.6021264791488647, 'learning_rate': 7.059537193741306e-06, 'epoch': 0.61}
{'loss': 1.1169, 'grad_norm': 1.5859968662261963, 'learning_rate': 7.053582377063489e-06, 'epoch': 0.61}
{'loss': 1.182, 'grad_norm': 1.2100467681884766, 'learning_rate': 7.047628704286446e-06, 'epoch': 0.61}
{'loss': 1.1285, 'grad_norm': 1.422606348991394, 'learning_rate': 7.041676177721588e-06, 'epoch': 0.61}
{'loss': 1.0705, 'grad_norm': 1.330610990524292, 'learning_rate': 7.035724799679898e-06, 'epoch': 0.61}
{'loss': 1.1821, 'grad_norm': 1.518898606300354, 'learning_rate': 7.029774572471904e-06, 'epoch': 0.61}
{'loss': 1.0643, 'grad_norm': 1.5172338485717773, 'learning_rate': 7.023825498407689e-06, 'epoch': 0.61}
{'loss': 1.1073, 'grad_norm': 1.5464481115341187, 'learning_rate': 7.0178775797968855e-06, 'epoch': 0.61}
{'loss': 1.0945, 'grad_norm': 1.5856740474700928, 'learning_rate': 7.011930818948688e-06, 'epoch': 0.61}
{'loss': 1.1686, 'grad_norm': 1.2013514041900635, 'learning_rate': 7.005985218171825e-06, 'epoch': 0.61}
{'loss': 1.0287, 'grad_norm': 1.40896475315094, 'learning_rate': 7.000040779774591e-06, 'epoch': 0.61}
{'loss': 1.1288, 'grad_norm': 1.4176450967788696, 'learning_rate': 6.994097506064812e-06, 'epoch': 0.61}
{'loss': 1.1299, 'grad_norm': 1.6079909801483154, 'learning_rate': 6.9881553993498805e-06, 'epoch': 0.61}
{'loss': 0.97, 'grad_norm': 1.4498804807662964, 'learning_rate': 6.9822144619367275e-06, 'epoch': 0.61}
{'loss': 1.1109, 'grad_norm': 1.5568288564682007, 'learning_rate': 6.97627469613182e-06, 'epoch': 0.61}
{'loss': 1.1892, 'grad_norm': 1.6033215522766113, 'learning_rate': 6.970336104241186e-06, 'epoch': 0.61}
{'loss': 1.1658, 'grad_norm': 1.2541987895965576, 'learning_rate': 6.9643986885703955e-06, 'epoch': 0.61}
{'loss': 1.0885, 'grad_norm': 1.5710630416870117, 'learning_rate': 6.958462451424547e-06, 'epoch': 0.61}
{'loss': 1.1391, 'grad_norm': 1.5340197086334229, 'learning_rate': 6.952527395108302e-06, 'epoch': 0.61}
{'loss': 1.1227, 'grad_norm': 1.4680830240249634, 'learning_rate': 6.9465935219258504e-06, 'epoch': 0.61}
{'loss': 1.1787, 'grad_norm': 1.4263447523117065, 'learning_rate': 6.9406608341809215e-06, 'epoch': 0.61}
{'loss': 1.0566, 'grad_norm': 1.4898772239685059, 'learning_rate': 6.934729334176793e-06, 'epoch': 0.61}
{'loss': 1.1134, 'grad_norm': 1.6449410915374756, 'learning_rate': 6.928799024216282e-06, 'epoch': 0.61}
{'loss': 1.0769, 'grad_norm': 1.4787224531173706, 'learning_rate': 6.92286990660173e-06, 'epoch': 0.61}
{'loss': 1.1682, 'grad_norm': 1.2337232828140259, 'learning_rate': 6.91694198363503e-06, 'epoch': 0.61}
{'loss': 1.1295, 'grad_norm': 1.592017650604248, 'learning_rate': 6.911015257617606e-06, 'epoch': 0.61}
{'loss': 1.1829, 'grad_norm': 1.1591774225234985, 'learning_rate': 6.905089730850416e-06, 'epoch': 0.61}
{'loss': 1.1459, 'grad_norm': 1.5402253866195679, 'learning_rate': 6.8991654056339505e-06, 'epoch': 0.61}
{'loss': 1.064, 'grad_norm': 1.361485242843628, 'learning_rate': 6.893242284268244e-06, 'epoch': 0.61}
{'loss': 1.0828, 'grad_norm': 1.4491305351257324, 'learning_rate': 6.887320369052848e-06, 'epoch': 0.61}
{'loss': 1.1155, 'grad_norm': 1.5363141298294067, 'learning_rate': 6.8813996622868584e-06, 'epoch': 0.61}
{'loss': 1.2323, 'grad_norm': 1.4718142747879028, 'learning_rate': 6.8754801662688964e-06, 'epoch': 0.61}
{'loss': 1.2652, 'grad_norm': 1.389967441558838, 'learning_rate': 6.869561883297116e-06, 'epoch': 0.61}
{'loss': 1.1268, 'grad_norm': 1.426647424697876, 'learning_rate': 6.863644815669197e-06, 'epoch': 0.61}
{'loss': 1.1917, 'grad_norm': 1.5147897005081177, 'learning_rate': 6.857728965682344e-06, 'epoch': 0.61}
{'loss': 1.0519, 'grad_norm': 1.4476039409637451, 'learning_rate': 6.851814335633298e-06, 'epoch': 0.61}
{'loss': 1.0295, 'grad_norm': 1.4045963287353516, 'learning_rate': 6.8459009278183275e-06, 'epoch': 0.61}
{'loss': 1.1511, 'grad_norm': 1.481621265411377, 'learning_rate': 6.839988744533211e-06, 'epoch': 0.61}
{'loss': 1.0478, 'grad_norm': 1.5240529775619507, 'learning_rate': 6.834077788073268e-06, 'epoch': 0.61}
{'loss': 1.1378, 'grad_norm': 1.4469623565673828, 'learning_rate': 6.8281680607333364e-06, 'epoch': 0.61}
{'loss': 1.1498, 'grad_norm': 1.3365304470062256, 'learning_rate': 6.822259564807768e-06, 'epoch': 0.62}
{'loss': 1.0144, 'grad_norm': 1.4014118909835815, 'learning_rate': 6.81635230259045e-06, 'epoch': 0.62}
{'loss': 1.0523, 'grad_norm': 1.4885817766189575, 'learning_rate': 6.810446276374789e-06, 'epoch': 0.62}
{'loss': 1.1331, 'grad_norm': 1.4064174890518188, 'learning_rate': 6.8045414884536975e-06, 'epoch': 0.62}
{'loss': 1.1386, 'grad_norm': 1.1877739429473877, 'learning_rate': 6.7986379411196255e-06, 'epoch': 0.62}
{'loss': 1.0978, 'grad_norm': 1.4334228038787842, 'learning_rate': 6.7927356366645315e-06, 'epoch': 0.62}
{'loss': 1.1341, 'grad_norm': 1.1935067176818848, 'learning_rate': 6.786834577379893e-06, 'epoch': 0.62}
{'loss': 1.0576, 'grad_norm': 1.5107197761535645, 'learning_rate': 6.780934765556702e-06, 'epoch': 0.62}
{'loss': 1.0754, 'grad_norm': 1.6512370109558105, 'learning_rate': 6.775036203485472e-06, 'epoch': 0.62}
{'loss': 1.0294, 'grad_norm': 1.42355477809906, 'learning_rate': 6.769138893456225e-06, 'epoch': 0.62}
{'loss': 1.0792, 'grad_norm': 1.4459099769592285, 'learning_rate': 6.763242837758504e-06, 'epoch': 0.62}
{'loss': 1.0746, 'grad_norm': 1.4139808416366577, 'learning_rate': 6.757348038681357e-06, 'epoch': 0.62}
{'loss': 1.1152, 'grad_norm': 1.4490556716918945, 'learning_rate': 6.751454498513349e-06, 'epoch': 0.62}
{'loss': 1.1317, 'grad_norm': 1.5827515125274658, 'learning_rate': 6.745562219542554e-06, 'epoch': 0.62}
{'loss': 0.9836, 'grad_norm': 1.5398516654968262, 'learning_rate': 6.7396712040565625e-06, 'epoch': 0.62}
{'loss': 1.15, 'grad_norm': 1.6063936948776245, 'learning_rate': 6.733781454342463e-06, 'epoch': 0.62}
{'loss': 1.0314, 'grad_norm': 1.3562209606170654, 'learning_rate': 6.727892972686861e-06, 'epoch': 0.62}
{'loss': 1.068, 'grad_norm': 1.6150493621826172, 'learning_rate': 6.722005761375873e-06, 'epoch': 0.62}
{'loss': 1.0942, 'grad_norm': 1.495021939277649, 'learning_rate': 6.716119822695111e-06, 'epoch': 0.62}
{'loss': 1.0639, 'grad_norm': 1.5308088064193726, 'learning_rate': 6.710235158929703e-06, 'epoch': 0.62}
{'loss': 1.1391, 'grad_norm': 1.45891535282135, 'learning_rate': 6.704351772364274e-06, 'epoch': 0.62}
{'loss': 1.1004, 'grad_norm': 1.5614001750946045, 'learning_rate': 6.698469665282958e-06, 'epoch': 0.62}
{'loss': 1.1706, 'grad_norm': 1.572479009628296, 'learning_rate': 6.692588839969397e-06, 'epoch': 0.62}
{'loss': 1.1359, 'grad_norm': 1.4932081699371338, 'learning_rate': 6.6867092987067214e-06, 'epoch': 0.62}
{'loss': 1.0421, 'grad_norm': 1.4700692892074585, 'learning_rate': 6.680831043777579e-06, 'epoch': 0.62}
{'loss': 1.1477, 'grad_norm': 1.4749529361724854, 'learning_rate': 6.674954077464108e-06, 'epoch': 0.62}
{'loss': 1.0967, 'grad_norm': 1.4973888397216797, 'learning_rate': 6.6690784020479484e-06, 'epoch': 0.62}
{'loss': 1.1466, 'grad_norm': 1.5302984714508057, 'learning_rate': 6.6632040198102364e-06, 'epoch': 0.62}
{'loss': 1.0941, 'grad_norm': 1.5456299781799316, 'learning_rate': 6.657330933031619e-06, 'epoch': 0.62}
{'loss': 1.1922, 'grad_norm': 1.5242365598678589, 'learning_rate': 6.651459143992221e-06, 'epoch': 0.62}
{'loss': 1.2002, 'grad_norm': 1.2483960390090942, 'learning_rate': 6.645588654971677e-06, 'epoch': 0.62}
{'loss': 1.0908, 'grad_norm': 1.4628217220306396, 'learning_rate': 6.639719468249115e-06, 'epoch': 0.62}
{'loss': 1.1475, 'grad_norm': 1.558093786239624, 'learning_rate': 6.633851586103153e-06, 'epoch': 0.62}
{'loss': 1.1288, 'grad_norm': 1.6525753736495972, 'learning_rate': 6.627985010811903e-06, 'epoch': 0.62}
{'loss': 1.1402, 'grad_norm': 1.657989501953125, 'learning_rate': 6.622119744652977e-06, 'epoch': 0.62}
{'loss': 1.1697, 'grad_norm': 1.2269518375396729, 'learning_rate': 6.616255789903467e-06, 'epoch': 0.62}
{'loss': 1.1042, 'grad_norm': 1.5493756532669067, 'learning_rate': 6.610393148839964e-06, 'epoch': 0.62}
{'loss': 1.155, 'grad_norm': 1.4014028310775757, 'learning_rate': 6.6045318237385526e-06, 'epoch': 0.62}
{'loss': 1.0563, 'grad_norm': 1.548318862915039, 'learning_rate': 6.598671816874794e-06, 'epoch': 0.62}
{'loss': 1.0795, 'grad_norm': 1.4986507892608643, 'learning_rate': 6.5928131305237465e-06, 'epoch': 0.62}
{'loss': 1.1084, 'grad_norm': 1.5125371217727661, 'learning_rate': 6.586955766959958e-06, 'epoch': 0.62}
{'loss': 1.0854, 'grad_norm': 1.533982515335083, 'learning_rate': 6.581099728457451e-06, 'epoch': 0.62}
{'loss': 1.1046, 'grad_norm': 1.5180251598358154, 'learning_rate': 6.5752450172897466e-06, 'epoch': 0.62}
{'loss': 1.039, 'grad_norm': 1.5837230682373047, 'learning_rate': 6.569391635729847e-06, 'epoch': 0.62}
{'loss': 1.1313, 'grad_norm': 1.5475431680679321, 'learning_rate': 6.563539586050233e-06, 'epoch': 0.62}
{'loss': 1.2298, 'grad_norm': 1.2248204946517944, 'learning_rate': 6.557688870522871e-06, 'epoch': 0.62}
{'loss': 1.1147, 'grad_norm': 1.4207334518432617, 'learning_rate': 6.551839491419213e-06, 'epoch': 0.62}
{'loss': 1.1128, 'grad_norm': 1.5375622510910034, 'learning_rate': 6.545991451010185e-06, 'epoch': 0.62}
{'loss': 1.1552, 'grad_norm': 1.6338456869125366, 'learning_rate': 6.5401447515662065e-06, 'epoch': 0.62}
{'loss': 1.1269, 'grad_norm': 1.492333173751831, 'learning_rate': 6.5342993953571556e-06, 'epoch': 0.62}
{'loss': 1.057, 'grad_norm': 1.48716139793396, 'learning_rate': 6.52845538465241e-06, 'epoch': 0.62}
{'loss': 1.2466, 'grad_norm': 1.2070119380950928, 'learning_rate': 6.522612721720813e-06, 'epoch': 0.62}
{'loss': 1.2106, 'grad_norm': 1.2830568552017212, 'learning_rate': 6.5167714088306865e-06, 'epoch': 0.63}
{'loss': 1.2116, 'grad_norm': 1.2000341415405273, 'learning_rate': 6.51093144824983e-06, 'epoch': 0.63}
{'loss': 1.0818, 'grad_norm': 1.495696783065796, 'learning_rate': 6.505092842245519e-06, 'epoch': 0.63}
{'loss': 1.1062, 'grad_norm': 1.4083665609359741, 'learning_rate': 6.499255593084498e-06, 'epoch': 0.63}
{'loss': 1.2467, 'grad_norm': 1.1974700689315796, 'learning_rate': 6.493419703032991e-06, 'epoch': 0.63}
{'loss': 1.1139, 'grad_norm': 1.5088104009628296, 'learning_rate': 6.487585174356691e-06, 'epoch': 0.63}
{'loss': 1.1544, 'grad_norm': 1.613753080368042, 'learning_rate': 6.481752009320761e-06, 'epoch': 0.63}
{'loss': 1.1488, 'grad_norm': 1.6130908727645874, 'learning_rate': 6.4759202101898366e-06, 'epoch': 0.63}
{'loss': 1.1501, 'grad_norm': 1.5703403949737549, 'learning_rate': 6.4700897792280285e-06, 'epoch': 0.63}
{'loss': 1.1264, 'grad_norm': 1.5222867727279663, 'learning_rate': 6.464260718698902e-06, 'epoch': 0.63}
{'loss': 1.1076, 'grad_norm': 1.4352986812591553, 'learning_rate': 6.458433030865503e-06, 'epoch': 0.63}
{'loss': 1.0633, 'grad_norm': 1.563536286354065, 'learning_rate': 6.452606717990346e-06, 'epoch': 0.63}
{'loss': 1.1349, 'grad_norm': 1.45576810836792, 'learning_rate': 6.4467817823354005e-06, 'epoch': 0.63}
{'loss': 1.1323, 'grad_norm': 1.4640446901321411, 'learning_rate': 6.440958226162104e-06, 'epoch': 0.63}
{'loss': 1.2394, 'grad_norm': 1.6240086555480957, 'learning_rate': 6.43513605173137e-06, 'epoch': 0.63}
{'loss': 1.0493, 'grad_norm': 1.504216194152832, 'learning_rate': 6.4293152613035594e-06, 'epoch': 0.63}
{'loss': 1.0808, 'grad_norm': 1.5404577255249023, 'learning_rate': 6.4234958571385095e-06, 'epoch': 0.63}
{'loss': 0.9935, 'grad_norm': 1.4737794399261475, 'learning_rate': 6.4176778414955075e-06, 'epoch': 0.63}
{'loss': 1.2207, 'grad_norm': 1.328819751739502, 'learning_rate': 6.4118612166333124e-06, 'epoch': 0.63}
{'loss': 1.2034, 'grad_norm': 1.2294713258743286, 'learning_rate': 6.4060459848101354e-06, 'epoch': 0.63}
{'loss': 1.1351, 'grad_norm': 1.4673678874969482, 'learning_rate': 6.400232148283651e-06, 'epoch': 0.63}
{'loss': 1.1685, 'grad_norm': 1.6225658655166626, 'learning_rate': 6.3944197093109885e-06, 'epoch': 0.63}
{'loss': 1.1103, 'grad_norm': 1.5787862539291382, 'learning_rate': 6.388608670148741e-06, 'epoch': 0.63}
{'loss': 1.1462, 'grad_norm': 1.510629653930664, 'learning_rate': 6.38279903305295e-06, 'epoch': 0.63}
{'loss': 1.0492, 'grad_norm': 1.3654218912124634, 'learning_rate': 6.376990800279119e-06, 'epoch': 0.63}
{'loss': 1.1604, 'grad_norm': 1.4964877367019653, 'learning_rate': 6.3711839740822035e-06, 'epoch': 0.63}
{'loss': 1.1636, 'grad_norm': 1.5048145055770874, 'learning_rate': 6.3653785567166125e-06, 'epoch': 0.63}
{'loss': 1.0917, 'grad_norm': 1.5894235372543335, 'learning_rate': 6.359574550436209e-06, 'epoch': 0.63}
{'loss': 1.1234, 'grad_norm': 1.453769326210022, 'learning_rate': 6.3537719574943105e-06, 'epoch': 0.63}
{'loss': 1.141, 'grad_norm': 1.419223427772522, 'learning_rate': 6.347970780143678e-06, 'epoch': 0.63}
{'loss': 1.2126, 'grad_norm': 1.2415410280227661, 'learning_rate': 6.342171020636533e-06, 'epoch': 0.63}
{'loss': 1.1269, 'grad_norm': 1.5827354192733765, 'learning_rate': 6.336372681224543e-06, 'epoch': 0.63}
{'loss': 1.1889, 'grad_norm': 1.2285107374191284, 'learning_rate': 6.330575764158819e-06, 'epoch': 0.63}
{'loss': 1.1005, 'grad_norm': 1.4250547885894775, 'learning_rate': 6.324780271689923e-06, 'epoch': 0.63}
{'loss': 1.2581, 'grad_norm': 1.2921148538589478, 'learning_rate': 6.318986206067872e-06, 'epoch': 0.63}
{'loss': 1.1183, 'grad_norm': 1.448987364768982, 'learning_rate': 6.313193569542113e-06, 'epoch': 0.63}
{'loss': 1.104, 'grad_norm': 1.502805233001709, 'learning_rate': 6.30740236436155e-06, 'epoch': 0.63}
{'loss': 1.2241, 'grad_norm': 1.3288589715957642, 'learning_rate': 6.301612592774533e-06, 'epoch': 0.63}
{'loss': 1.0508, 'grad_norm': 1.562335729598999, 'learning_rate': 6.295824257028844e-06, 'epoch': 0.63}
{'loss': 1.1511, 'grad_norm': 1.6406152248382568, 'learning_rate': 6.290037359371717e-06, 'epoch': 0.63}
{'loss': 1.1116, 'grad_norm': 1.4086298942565918, 'learning_rate': 6.284251902049827e-06, 'epoch': 0.63}
{'loss': 1.1719, 'grad_norm': 1.236392855644226, 'learning_rate': 6.278467887309283e-06, 'epoch': 0.63}
{'loss': 1.001, 'grad_norm': 1.5426121950149536, 'learning_rate': 6.272685317395644e-06, 'epoch': 0.63}
{'loss': 1.0211, 'grad_norm': 1.6233677864074707, 'learning_rate': 6.266904194553896e-06, 'epoch': 0.63}
{'loss': 1.1125, 'grad_norm': 1.442818284034729, 'learning_rate': 6.261124521028477e-06, 'epoch': 0.63}
{'loss': 1.0985, 'grad_norm': 1.4667925834655762, 'learning_rate': 6.255346299063252e-06, 'epoch': 0.63}
{'loss': 1.0848, 'grad_norm': 1.4926456212997437, 'learning_rate': 6.249569530901525e-06, 'epoch': 0.63}
{'loss': 1.1306, 'grad_norm': 1.5508782863616943, 'learning_rate': 6.243794218786034e-06, 'epoch': 0.63}
{'loss': 1.0686, 'grad_norm': 1.5672473907470703, 'learning_rate': 6.238020364958964e-06, 'epoch': 0.63}
{'loss': 1.1196, 'grad_norm': 1.4253742694854736, 'learning_rate': 6.232247971661912e-06, 'epoch': 0.63}
{'loss': 1.1581, 'grad_norm': 1.6293846368789673, 'learning_rate': 6.2264770411359256e-06, 'epoch': 0.63}
{'loss': 1.1486, 'grad_norm': 1.5685508251190186, 'learning_rate': 6.22070757562148e-06, 'epoch': 0.63}
{'loss': 1.0727, 'grad_norm': 1.5587379932403564, 'learning_rate': 6.214939577358479e-06, 'epoch': 0.64}
{'loss': 1.0968, 'grad_norm': 1.627699613571167, 'learning_rate': 6.209173048586253e-06, 'epoch': 0.64}
{'loss': 1.0588, 'grad_norm': 1.3899534940719604, 'learning_rate': 6.203407991543577e-06, 'epoch': 0.64}
{'loss': 0.9934, 'grad_norm': 1.5161771774291992, 'learning_rate': 6.197644408468635e-06, 'epoch': 0.64}
{'loss': 1.1874, 'grad_norm': 1.5746608972549438, 'learning_rate': 6.191882301599052e-06, 'epoch': 0.64}
{'loss': 1.1619, 'grad_norm': 1.5873366594314575, 'learning_rate': 6.186121673171882e-06, 'epoch': 0.64}
{'loss': 1.0945, 'grad_norm': 1.4446860551834106, 'learning_rate': 6.180362525423591e-06, 'epoch': 0.64}
{'loss': 1.1921, 'grad_norm': 1.5094157457351685, 'learning_rate': 6.174604860590081e-06, 'epoch': 0.64}
{'loss': 1.1265, 'grad_norm': 1.5043320655822754, 'learning_rate': 6.168848680906678e-06, 'epoch': 0.64}
{'loss': 1.1242, 'grad_norm': 1.4684224128723145, 'learning_rate': 6.163093988608127e-06, 'epoch': 0.64}
{'loss': 1.0564, 'grad_norm': 1.4641717672348022, 'learning_rate': 6.157340785928595e-06, 'epoch': 0.64}
{'loss': 1.1145, 'grad_norm': 1.6007753610610962, 'learning_rate': 6.151589075101681e-06, 'epoch': 0.64}
{'loss': 1.1977, 'grad_norm': 1.1837958097457886, 'learning_rate': 6.145838858360391e-06, 'epoch': 0.64}
{'loss': 1.0341, 'grad_norm': 1.4399802684783936, 'learning_rate': 6.140090137937158e-06, 'epoch': 0.64}
{'loss': 1.1876, 'grad_norm': 1.3761396408081055, 'learning_rate': 6.134342916063838e-06, 'epoch': 0.64}
{'loss': 1.0553, 'grad_norm': 1.366804838180542, 'learning_rate': 6.128597194971691e-06, 'epoch': 0.64}
{'loss': 1.0289, 'grad_norm': 1.537946343421936, 'learning_rate': 6.122852976891413e-06, 'epoch': 0.64}
{'loss': 1.0945, 'grad_norm': 1.494315505027771, 'learning_rate': 6.117110264053101e-06, 'epoch': 0.64}
{'loss': 1.1384, 'grad_norm': 1.5589691400527954, 'learning_rate': 6.111369058686276e-06, 'epoch': 0.64}
{'loss': 1.1192, 'grad_norm': 1.6161638498306274, 'learning_rate': 6.105629363019875e-06, 'epoch': 0.64}
{'loss': 1.0752, 'grad_norm': 1.538906455039978, 'learning_rate': 6.099891179282242e-06, 'epoch': 0.64}
{'loss': 1.1521, 'grad_norm': 1.2473227977752686, 'learning_rate': 6.094154509701133e-06, 'epoch': 0.64}
{'loss': 1.0902, 'grad_norm': 1.5711103677749634, 'learning_rate': 6.088419356503732e-06, 'epoch': 0.64}
{'loss': 1.0807, 'grad_norm': 1.5150270462036133, 'learning_rate': 6.082685721916612e-06, 'epoch': 0.64}
{'loss': 1.1112, 'grad_norm': 1.2898714542388916, 'learning_rate': 6.076953608165772e-06, 'epoch': 0.64}
{'loss': 1.1425, 'grad_norm': 1.5477344989776611, 'learning_rate': 6.07122301747662e-06, 'epoch': 0.64}
{'loss': 1.1328, 'grad_norm': 1.5491410493850708, 'learning_rate': 6.065493952073961e-06, 'epoch': 0.64}
{'loss': 1.1199, 'grad_norm': 1.502450942993164, 'learning_rate': 6.0597664141820176e-06, 'epoch': 0.64}
{'loss': 1.2025, 'grad_norm': 1.572259783744812, 'learning_rate': 6.054040406024422e-06, 'epoch': 0.64}
{'loss': 1.2027, 'grad_norm': 1.152518630027771, 'learning_rate': 6.0483159298242e-06, 'epoch': 0.64}
{'loss': 1.0637, 'grad_norm': 1.4358768463134766, 'learning_rate': 6.042592987803796e-06, 'epoch': 0.64}
{'loss': 1.1132, 'grad_norm': 1.364415168762207, 'learning_rate': 6.036871582185054e-06, 'epoch': 0.64}
{'loss': 1.1595, 'grad_norm': 1.6148252487182617, 'learning_rate': 6.031151715189217e-06, 'epoch': 0.64}
{'loss': 1.1571, 'grad_norm': 1.7232364416122437, 'learning_rate': 6.025433389036935e-06, 'epoch': 0.64}
{'loss': 1.0258, 'grad_norm': 1.471319556236267, 'learning_rate': 6.019716605948261e-06, 'epoch': 0.64}
{'loss': 1.0934, 'grad_norm': 1.3796945810317993, 'learning_rate': 6.014001368142643e-06, 'epoch': 0.64}
{'loss': 1.0626, 'grad_norm': 1.4505552053451538, 'learning_rate': 6.008287677838937e-06, 'epoch': 0.64}
{'loss': 1.0645, 'grad_norm': 1.528401494026184, 'learning_rate': 6.002575537255395e-06, 'epoch': 0.64}
{'loss': 1.0127, 'grad_norm': 1.4803401231765747, 'learning_rate': 5.996864948609662e-06, 'epoch': 0.64}
{'loss': 1.2245, 'grad_norm': 1.5817054510116577, 'learning_rate': 5.9911559141187924e-06, 'epoch': 0.64}
{'loss': 1.0754, 'grad_norm': 1.1546577215194702, 'learning_rate': 5.9854484359992235e-06, 'epoch': 0.64}
{'loss': 1.1055, 'grad_norm': 1.529931902885437, 'learning_rate': 5.979742516466793e-06, 'epoch': 0.64}
{'loss': 1.0978, 'grad_norm': 1.4726331233978271, 'learning_rate': 5.974038157736746e-06, 'epoch': 0.64}
{'loss': 1.079, 'grad_norm': 1.415191411972046, 'learning_rate': 5.968335362023697e-06, 'epoch': 0.64}
{'loss': 1.1378, 'grad_norm': 1.6082055568695068, 'learning_rate': 5.962634131541676e-06, 'epoch': 0.64}
{'loss': 1.1357, 'grad_norm': 1.590867519378662, 'learning_rate': 5.956934468504101e-06, 'epoch': 0.64}
{'loss': 1.0847, 'grad_norm': 1.5091021060943604, 'learning_rate': 5.951236375123768e-06, 'epoch': 0.64}
{'loss': 1.1059, 'grad_norm': 1.5861362218856812, 'learning_rate': 5.945539853612876e-06, 'epoch': 0.64}
{'loss': 1.0608, 'grad_norm': 1.5690637826919556, 'learning_rate': 5.939844906183016e-06, 'epoch': 0.64}
{'loss': 1.095, 'grad_norm': 1.5532736778259277, 'learning_rate': 5.934151535045156e-06, 'epoch': 0.64}
{'loss': 1.1575, 'grad_norm': 1.397834300994873, 'learning_rate': 5.92845974240966e-06, 'epoch': 0.64}
{'loss': 1.0773, 'grad_norm': 1.500437617301941, 'learning_rate': 5.922769530486283e-06, 'epoch': 0.64}
{'loss': 1.1209, 'grad_norm': 1.4372304677963257, 'learning_rate': 5.917080901484156e-06, 'epoch': 0.65}
{'loss': 1.1065, 'grad_norm': 1.5531750917434692, 'learning_rate': 5.9113938576118e-06, 'epoch': 0.65}
{'loss': 1.0731, 'grad_norm': 1.5385984182357788, 'learning_rate': 5.905708401077128e-06, 'epoch': 0.65}
{'loss': 1.0131, 'grad_norm': 1.4172998666763306, 'learning_rate': 5.900024534087421e-06, 'epoch': 0.65}
{'loss': 1.0883, 'grad_norm': 1.5969476699829102, 'learning_rate': 5.894342258849355e-06, 'epoch': 0.65}
{'loss': 1.1895, 'grad_norm': 1.2657926082611084, 'learning_rate': 5.88866157756899e-06, 'epoch': 0.65}
{'loss': 1.0343, 'grad_norm': 1.58344304561615, 'learning_rate': 5.882982492451757e-06, 'epoch': 0.65}
{'loss': 1.0252, 'grad_norm': 1.4684600830078125, 'learning_rate': 5.877305005702471e-06, 'epoch': 0.65}
{'loss': 1.0512, 'grad_norm': 1.4472229480743408, 'learning_rate': 5.871629119525335e-06, 'epoch': 0.65}
{'loss': 1.1146, 'grad_norm': 1.5401403903961182, 'learning_rate': 5.865954836123915e-06, 'epoch': 0.65}
{'loss': 1.1417, 'grad_norm': 1.51010262966156, 'learning_rate': 5.860282157701167e-06, 'epoch': 0.65}
{'loss': 1.2329, 'grad_norm': 1.330370545387268, 'learning_rate': 5.854611086459423e-06, 'epoch': 0.65}
{'loss': 0.9563, 'grad_norm': 1.4989012479782104, 'learning_rate': 5.8489416246003814e-06, 'epoch': 0.65}
{'loss': 1.0275, 'grad_norm': 1.3876687288284302, 'learning_rate': 5.8432737743251315e-06, 'epoch': 0.65}
{'loss': 1.2472, 'grad_norm': 1.3001829385757446, 'learning_rate': 5.8376075378341194e-06, 'epoch': 0.65}
{'loss': 1.1922, 'grad_norm': 1.5256072282791138, 'learning_rate': 5.831942917327172e-06, 'epoch': 0.65}
{'loss': 1.1106, 'grad_norm': 1.5482277870178223, 'learning_rate': 5.826279915003503e-06, 'epoch': 0.65}
{'loss': 1.12, 'grad_norm': 1.4804584980010986, 'learning_rate': 5.8206185330616725e-06, 'epoch': 0.65}
{'loss': 1.0906, 'grad_norm': 1.546730875968933, 'learning_rate': 5.814958773699625e-06, 'epoch': 0.65}
{'loss': 1.1, 'grad_norm': 1.554448127746582, 'learning_rate': 5.809300639114683e-06, 'epoch': 0.65}
{'loss': 1.1234, 'grad_norm': 1.5150612592697144, 'learning_rate': 5.803644131503516e-06, 'epoch': 0.65}
{'loss': 1.1344, 'grad_norm': 1.4926762580871582, 'learning_rate': 5.797989253062186e-06, 'epoch': 0.65}
{'loss': 1.0679, 'grad_norm': 1.659403681755066, 'learning_rate': 5.792336005986105e-06, 'epoch': 0.65}
{'loss': 1.154, 'grad_norm': 1.6794803142547607, 'learning_rate': 5.786684392470064e-06, 'epoch': 0.65}
{'loss': 1.1684, 'grad_norm': 1.5407227277755737, 'learning_rate': 5.781034414708208e-06, 'epoch': 0.65}
{'loss': 1.0558, 'grad_norm': 1.4786403179168701, 'learning_rate': 5.775386074894058e-06, 'epoch': 0.65}
{'loss': 1.1118, 'grad_norm': 1.5250011682510376, 'learning_rate': 5.769739375220489e-06, 'epoch': 0.65}
{'loss': 1.1605, 'grad_norm': 1.548361897468567, 'learning_rate': 5.7640943178797445e-06, 'epoch': 0.65}
{'loss': 1.0541, 'grad_norm': 1.4137665033340454, 'learning_rate': 5.7584509050634395e-06, 'epoch': 0.65}
{'loss': 1.1061, 'grad_norm': 1.5430340766906738, 'learning_rate': 5.752809138962525e-06, 'epoch': 0.65}
{'loss': 1.1799, 'grad_norm': 1.206579327583313, 'learning_rate': 5.747169021767342e-06, 'epoch': 0.65}
{'loss': 1.1211, 'grad_norm': 1.3694186210632324, 'learning_rate': 5.7415305556675805e-06, 'epoch': 0.65}
{'loss': 1.1256, 'grad_norm': 1.6833285093307495, 'learning_rate': 5.73589374285227e-06, 'epoch': 0.65}
{'loss': 1.1613, 'grad_norm': 1.5058677196502686, 'learning_rate': 5.730258585509832e-06, 'epoch': 0.65}
{'loss': 1.0446, 'grad_norm': 1.553121566772461, 'learning_rate': 5.724625085828022e-06, 'epoch': 0.65}
{'loss': 1.1361, 'grad_norm': 1.6504838466644287, 'learning_rate': 5.718993245993958e-06, 'epoch': 0.65}
{'loss': 1.1064, 'grad_norm': 1.5116604566574097, 'learning_rate': 5.713363068194115e-06, 'epoch': 0.65}
{'loss': 1.1895, 'grad_norm': 1.1923277378082275, 'learning_rate': 5.7077345546143235e-06, 'epoch': 0.65}
{'loss': 1.0218, 'grad_norm': 1.4578850269317627, 'learning_rate': 5.702107707439766e-06, 'epoch': 0.65}
{'loss': 1.0517, 'grad_norm': 1.455459713935852, 'learning_rate': 5.6964825288549745e-06, 'epoch': 0.65}
{'loss': 1.1231, 'grad_norm': 1.5826444625854492, 'learning_rate': 5.690859021043842e-06, 'epoch': 0.65}
{'loss': 1.2596, 'grad_norm': 1.4633095264434814, 'learning_rate': 5.685237186189601e-06, 'epoch': 0.65}
{'loss': 1.0545, 'grad_norm': 1.4060487747192383, 'learning_rate': 5.679617026474853e-06, 'epoch': 0.65}
{'loss': 1.0708, 'grad_norm': 1.5305161476135254, 'learning_rate': 5.673998544081527e-06, 'epoch': 0.65}
{'loss': 1.1311, 'grad_norm': 1.6591922044754028, 'learning_rate': 5.6683817411909114e-06, 'epoch': 0.65}
{'loss': 1.0667, 'grad_norm': 1.4165935516357422, 'learning_rate': 5.662766619983653e-06, 'epoch': 0.65}
{'loss': 1.0869, 'grad_norm': 1.5325740575790405, 'learning_rate': 5.65715318263972e-06, 'epoch': 0.65}
{'loss': 1.201, 'grad_norm': 1.6141407489776611, 'learning_rate': 5.651541431338454e-06, 'epoch': 0.65}
{'loss': 1.0773, 'grad_norm': 1.5729323625564575, 'learning_rate': 5.645931368258527e-06, 'epoch': 0.65}
{'loss': 1.2728, 'grad_norm': 1.293096899986267, 'learning_rate': 5.640322995577958e-06, 'epoch': 0.65}
{'loss': 1.1204, 'grad_norm': 1.5774884223937988, 'learning_rate': 5.634716315474109e-06, 'epoch': 0.65}
{'loss': 1.0538, 'grad_norm': 1.4451828002929688, 'learning_rate': 5.629111330123689e-06, 'epoch': 0.65}
{'loss': 1.1006, 'grad_norm': 1.5845218896865845, 'learning_rate': 5.623508041702743e-06, 'epoch': 0.66}
{'loss': 1.1095, 'grad_norm': 1.2833220958709717, 'learning_rate': 5.617906452386659e-06, 'epoch': 0.66}
{'loss': 1.1572, 'grad_norm': 1.5708730220794678, 'learning_rate': 5.612306564350179e-06, 'epoch': 0.66}
{'loss': 1.0772, 'grad_norm': 1.6037602424621582, 'learning_rate': 5.6067083797673535e-06, 'epoch': 0.66}
{'loss': 1.1102, 'grad_norm': 1.392209529876709, 'learning_rate': 5.601111900811607e-06, 'epoch': 0.66}
{'loss': 1.2523, 'grad_norm': 1.2075819969177246, 'learning_rate': 5.595517129655681e-06, 'epoch': 0.66}
{'loss': 1.0752, 'grad_norm': 1.460389494895935, 'learning_rate': 5.589924068471648e-06, 'epoch': 0.66}
{'loss': 1.1386, 'grad_norm': 1.40939462184906, 'learning_rate': 5.58433271943094e-06, 'epoch': 0.66}
{'loss': 1.1892, 'grad_norm': 1.1094534397125244, 'learning_rate': 5.578743084704306e-06, 'epoch': 0.66}
{'loss': 1.1374, 'grad_norm': 1.5184681415557861, 'learning_rate': 5.573155166461833e-06, 'epoch': 0.66}
{'loss': 1.1177, 'grad_norm': 1.4368526935577393, 'learning_rate': 5.567568966872947e-06, 'epoch': 0.66}
{'loss': 1.1579, 'grad_norm': 1.513704776763916, 'learning_rate': 5.5619844881064e-06, 'epoch': 0.66}
{'loss': 1.0913, 'grad_norm': 1.5208964347839355, 'learning_rate': 5.556401732330281e-06, 'epoch': 0.66}
{'loss': 1.1208, 'grad_norm': 1.4319288730621338, 'learning_rate': 5.550820701712007e-06, 'epoch': 0.66}
{'loss': 1.1706, 'grad_norm': 1.2873420715332031, 'learning_rate': 5.545241398418326e-06, 'epoch': 0.66}
{'loss': 1.1247, 'grad_norm': 1.4188071489334106, 'learning_rate': 5.539663824615312e-06, 'epoch': 0.66}
{'loss': 1.059, 'grad_norm': 1.3063489198684692, 'learning_rate': 5.534087982468384e-06, 'epoch': 0.66}
{'loss': 1.0229, 'grad_norm': 1.4921098947525024, 'learning_rate': 5.5285138741422615e-06, 'epoch': 0.66}
{'loss': 1.0358, 'grad_norm': 1.5526915788650513, 'learning_rate': 5.522941501801008e-06, 'epoch': 0.66}
{'loss': 1.055, 'grad_norm': 1.5219578742980957, 'learning_rate': 5.517370867608021e-06, 'epoch': 0.66}
{'loss': 1.128, 'grad_norm': 1.2018280029296875, 'learning_rate': 5.511801973725997e-06, 'epoch': 0.66}
{'loss': 1.1975, 'grad_norm': 1.351594090461731, 'learning_rate': 5.506234822316983e-06, 'epoch': 0.66}
Error with image file is truncated (2 bytes not processed)
{'loss': 1.1841, 'grad_norm': 1.565230131149292, 'learning_rate': 5.500669415542336e-06, 'epoch': 0.66}
{'loss': 1.2243, 'grad_norm': 1.24180006980896, 'learning_rate': 5.495105755562738e-06, 'epoch': 0.66}
{'loss': 0.9981, 'grad_norm': 1.3780146837234497, 'learning_rate': 5.4895438445381945e-06, 'epoch': 0.66}
{'loss': 1.1002, 'grad_norm': 1.452630639076233, 'learning_rate': 5.48398368462803e-06, 'epoch': 0.66}
{'loss': 1.1365, 'grad_norm': 1.6519477367401123, 'learning_rate': 5.4784252779908905e-06, 'epoch': 0.66}
{'loss': 1.0703, 'grad_norm': 1.3834569454193115, 'learning_rate': 5.4728686267847354e-06, 'epoch': 0.66}
{'loss': 1.126, 'grad_norm': 1.5219910144805908, 'learning_rate': 5.467313733166863e-06, 'epoch': 0.66}
{'loss': 1.1168, 'grad_norm': 1.4415040016174316, 'learning_rate': 5.461760599293855e-06, 'epoch': 0.66}
{'loss': 1.1448, 'grad_norm': 1.490383505821228, 'learning_rate': 5.456209227321643e-06, 'epoch': 0.66}
{'loss': 1.152, 'grad_norm': 1.4879282712936401, 'learning_rate': 5.450659619405458e-06, 'epoch': 0.66}
{'loss': 1.1481, 'grad_norm': 1.477332592010498, 'learning_rate': 5.445111777699842e-06, 'epoch': 0.66}
{'loss': 1.1352, 'grad_norm': 1.2981067895889282, 'learning_rate': 5.439565704358667e-06, 'epoch': 0.66}
{'loss': 1.1345, 'grad_norm': 1.4793095588684082, 'learning_rate': 5.434021401535105e-06, 'epoch': 0.66}
{'loss': 1.0545, 'grad_norm': 1.5877633094787598, 'learning_rate': 5.428478871381646e-06, 'epoch': 0.66}
{'loss': 1.0462, 'grad_norm': 1.5358470678329468, 'learning_rate': 5.422938116050092e-06, 'epoch': 0.66}
{'loss': 1.0982, 'grad_norm': 1.6644995212554932, 'learning_rate': 5.417399137691552e-06, 'epoch': 0.66}
{'loss': 1.0845, 'grad_norm': 1.4214931726455688, 'learning_rate': 5.411861938456453e-06, 'epoch': 0.66}
{'loss': 1.0155, 'grad_norm': 1.441238284111023, 'learning_rate': 5.406326520494522e-06, 'epoch': 0.66}
{'loss': 1.0929, 'grad_norm': 1.4422085285186768, 'learning_rate': 5.400792885954802e-06, 'epoch': 0.66}
{'loss': 1.0945, 'grad_norm': 1.480884075164795, 'learning_rate': 5.395261036985635e-06, 'epoch': 0.66}
{'loss': 1.054, 'grad_norm': 1.5242412090301514, 'learning_rate': 5.389730975734686e-06, 'epoch': 0.66}
{'loss': 1.017, 'grad_norm': 1.3717111349105835, 'learning_rate': 5.384202704348902e-06, 'epoch': 0.66}
{'loss': 1.1354, 'grad_norm': 1.5277549028396606, 'learning_rate': 5.378676224974557e-06, 'epoch': 0.66}
{'loss': 1.1379, 'grad_norm': 1.3927100896835327, 'learning_rate': 5.373151539757224e-06, 'epoch': 0.66}
{'loss': 1.0945, 'grad_norm': 1.4985779523849487, 'learning_rate': 5.367628650841761e-06, 'epoch': 0.66}
{'loss': 1.1016, 'grad_norm': 1.4212547540664673, 'learning_rate': 5.362107560372358e-06, 'epoch': 0.66}
{'loss': 1.1916, 'grad_norm': 1.4858239889144897, 'learning_rate': 5.356588270492487e-06, 'epoch': 0.66}
{'loss': 1.0753, 'grad_norm': 1.4702430963516235, 'learning_rate': 5.351070783344926e-06, 'epoch': 0.66}
{'loss': 1.1143, 'grad_norm': 1.5692577362060547, 'learning_rate': 5.3455551010717545e-06, 'epoch': 0.66}
{'loss': 1.104, 'grad_norm': 1.4187089204788208, 'learning_rate': 5.34004122581435e-06, 'epoch': 0.66}
{'loss': 1.2501, 'grad_norm': 1.2264604568481445, 'learning_rate': 5.334529159713389e-06, 'epoch': 0.67}
{'loss': 1.2369, 'grad_norm': 1.249633550643921, 'learning_rate': 5.329018904908841e-06, 'epoch': 0.67}
{'loss': 1.1075, 'grad_norm': 1.3979041576385498, 'learning_rate': 5.323510463539989e-06, 'epoch': 0.67}
{'loss': 1.0867, 'grad_norm': 1.574545979499817, 'learning_rate': 5.318003837745382e-06, 'epoch': 0.67}
{'loss': 1.0736, 'grad_norm': 1.4615122079849243, 'learning_rate': 5.3124990296628974e-06, 'epoch': 0.67}
{'loss': 1.1341, 'grad_norm': 1.6117045879364014, 'learning_rate': 5.306996041429688e-06, 'epoch': 0.67}
{'loss': 1.0755, 'grad_norm': 1.5373972654342651, 'learning_rate': 5.301494875182192e-06, 'epoch': 0.67}
{'loss': 1.207, 'grad_norm': 1.5325067043304443, 'learning_rate': 5.295995533056162e-06, 'epoch': 0.67}
{'loss': 1.0238, 'grad_norm': 1.4392287731170654, 'learning_rate': 5.290498017186631e-06, 'epoch': 0.67}
{'loss': 1.1677, 'grad_norm': 1.527980089187622, 'learning_rate': 5.2850023297079235e-06, 'epoch': 0.67}
{'loss': 1.0944, 'grad_norm': 1.5991367101669312, 'learning_rate': 5.279508472753654e-06, 'epoch': 0.67}
{'loss': 1.1579, 'grad_norm': 1.4618306159973145, 'learning_rate': 5.274016448456725e-06, 'epoch': 0.67}
{'loss': 1.0653, 'grad_norm': 1.4985324144363403, 'learning_rate': 5.2685262589493314e-06, 'epoch': 0.67}
{'loss': 1.1199, 'grad_norm': 1.5372217893600464, 'learning_rate': 5.263037906362953e-06, 'epoch': 0.67}
{'loss': 1.082, 'grad_norm': 1.5148208141326904, 'learning_rate': 5.257551392828359e-06, 'epoch': 0.67}
{'loss': 1.1009, 'grad_norm': 1.5991222858428955, 'learning_rate': 5.252066720475597e-06, 'epoch': 0.67}
{'loss': 1.1065, 'grad_norm': 1.428342580795288, 'learning_rate': 5.246583891434018e-06, 'epoch': 0.67}
{'loss': 1.0522, 'grad_norm': 1.4618791341781616, 'learning_rate': 5.241102907832232e-06, 'epoch': 0.67}
{'loss': 1.1057, 'grad_norm': 1.5377980470657349, 'learning_rate': 5.235623771798151e-06, 'epoch': 0.67}
{'loss': 1.1743, 'grad_norm': 1.6428650617599487, 'learning_rate': 5.23014648545897e-06, 'epoch': 0.67}
{'loss': 1.2031, 'grad_norm': 1.1512746810913086, 'learning_rate': 5.224671050941146e-06, 'epoch': 0.67}
{'loss': 1.1866, 'grad_norm': 1.4914071559906006, 'learning_rate': 5.2191974703704425e-06, 'epoch': 0.67}
{'loss': 1.0997, 'grad_norm': 1.3938589096069336, 'learning_rate': 5.213725745871889e-06, 'epoch': 0.67}
{'loss': 1.1763, 'grad_norm': 1.6283150911331177, 'learning_rate': 5.208255879569799e-06, 'epoch': 0.67}
{'loss': 1.0843, 'grad_norm': 1.420296311378479, 'learning_rate': 5.20278787358776e-06, 'epoch': 0.67}
{'loss': 1.0354, 'grad_norm': 1.6152873039245605, 'learning_rate': 5.197321730048641e-06, 'epoch': 0.67}
{'loss': 1.0692, 'grad_norm': 1.4264260530471802, 'learning_rate': 5.1918574510745865e-06, 'epoch': 0.67}
{'loss': 1.075, 'grad_norm': 1.607817530632019, 'learning_rate': 5.186395038787017e-06, 'epoch': 0.67}
{'loss': 1.191, 'grad_norm': 1.2189713716506958, 'learning_rate': 5.180934495306638e-06, 'epoch': 0.67}
{'loss': 1.1856, 'grad_norm': 1.53984534740448, 'learning_rate': 5.175475822753404e-06, 'epoch': 0.67}
{'loss': 1.0636, 'grad_norm': 1.4871950149536133, 'learning_rate': 5.170019023246574e-06, 'epoch': 0.67}
{'loss': 1.0714, 'grad_norm': 1.5160491466522217, 'learning_rate': 5.16456409890466e-06, 'epoch': 0.67}
{'loss': 1.0933, 'grad_norm': 1.4821438789367676, 'learning_rate': 5.159111051845451e-06, 'epoch': 0.67}
{'loss': 1.2613, 'grad_norm': 1.356858730316162, 'learning_rate': 5.153659884186013e-06, 'epoch': 0.67}
{'loss': 1.0586, 'grad_norm': 1.4411908388137817, 'learning_rate': 5.148210598042665e-06, 'epoch': 0.67}
{'loss': 1.0317, 'grad_norm': 1.5580962896347046, 'learning_rate': 5.142763195531017e-06, 'epoch': 0.67}
{'loss': 1.2055, 'grad_norm': 1.5073672533035278, 'learning_rate': 5.137317678765939e-06, 'epoch': 0.67}
{'loss': 1.0825, 'grad_norm': 1.5130689144134521, 'learning_rate': 5.131874049861563e-06, 'epoch': 0.67}
{'loss': 1.1985, 'grad_norm': 1.2982877492904663, 'learning_rate': 5.126432310931295e-06, 'epoch': 0.67}
{'loss': 1.0544, 'grad_norm': 1.4199419021606445, 'learning_rate': 5.120992464087807e-06, 'epoch': 0.67}
{'loss': 1.1503, 'grad_norm': 1.578755497932434, 'learning_rate': 5.115554511443033e-06, 'epoch': 0.67}
{'loss': 1.1087, 'grad_norm': 1.5751436948776245, 'learning_rate': 5.1101184551081705e-06, 'epoch': 0.67}
{'loss': 1.1818, 'grad_norm': 1.5331916809082031, 'learning_rate': 5.104684297193694e-06, 'epoch': 0.67}
{'loss': 1.0704, 'grad_norm': 1.5386009216308594, 'learning_rate': 5.099252039809317e-06, 'epoch': 0.67}
{'loss': 1.2541, 'grad_norm': 1.2319461107254028, 'learning_rate': 5.09382168506404e-06, 'epoch': 0.67}
{'loss': 1.03, 'grad_norm': 1.4897810220718384, 'learning_rate': 5.088393235066114e-06, 'epoch': 0.67}
{'loss': 1.0983, 'grad_norm': 1.5989866256713867, 'learning_rate': 5.082966691923037e-06, 'epoch': 0.67}
{'loss': 1.1888, 'grad_norm': 1.1512484550476074, 'learning_rate': 5.077542057741592e-06, 'epoch': 0.67}
{'loss': 1.1409, 'grad_norm': 1.5689128637313843, 'learning_rate': 5.0721193346278066e-06, 'epoch': 0.67}
{'loss': 1.1528, 'grad_norm': 1.602500557899475, 'learning_rate': 5.066698524686966e-06, 'epoch': 0.67}
{'loss': 1.0727, 'grad_norm': 1.572717547416687, 'learning_rate': 5.061279630023618e-06, 'epoch': 0.67}
{'loss': 1.1639, 'grad_norm': 1.5662084817886353, 'learning_rate': 5.055862652741562e-06, 'epoch': 0.67}
{'loss': 1.0953, 'grad_norm': 1.539165735244751, 'learning_rate': 5.050447594943856e-06, 'epoch': 0.68}
{'loss': 1.0504, 'grad_norm': 1.3569397926330566, 'learning_rate': 5.045034458732808e-06, 'epoch': 0.68}
{'loss': 1.1383, 'grad_norm': 1.4169007539749146, 'learning_rate': 5.0396232462099945e-06, 'epoch': 0.68}
{'loss': 1.1358, 'grad_norm': 1.6517103910446167, 'learning_rate': 5.034213959476222e-06, 'epoch': 0.68}
{'loss': 1.1877, 'grad_norm': 1.5064382553100586, 'learning_rate': 5.028806600631569e-06, 'epoch': 0.68}
{'loss': 1.0457, 'grad_norm': 1.4634497165679932, 'learning_rate': 5.023401171775357e-06, 'epoch': 0.68}
{'loss': 1.1041, 'grad_norm': 1.4771573543548584, 'learning_rate': 5.017997675006161e-06, 'epoch': 0.68}
{'loss': 1.0643, 'grad_norm': 1.4336038827896118, 'learning_rate': 5.012596112421806e-06, 'epoch': 0.68}
{'loss': 1.1406, 'grad_norm': 1.1523510217666626, 'learning_rate': 5.007196486119355e-06, 'epoch': 0.68}
{'loss': 1.1054, 'grad_norm': 1.4943710565567017, 'learning_rate': 5.001798798195136e-06, 'epoch': 0.68}
{'loss': 1.0871, 'grad_norm': 1.4918583631515503, 'learning_rate': 4.996403050744719e-06, 'epoch': 0.68}
{'loss': 1.1938, 'grad_norm': 1.5450111627578735, 'learning_rate': 4.991009245862917e-06, 'epoch': 0.68}
{'loss': 1.1645, 'grad_norm': 1.4972795248031616, 'learning_rate': 4.985617385643789e-06, 'epoch': 0.68}
{'loss': 1.0685, 'grad_norm': 1.509054183959961, 'learning_rate': 4.980227472180643e-06, 'epoch': 0.68}
{'loss': 1.2153, 'grad_norm': 1.7021527290344238, 'learning_rate': 4.974839507566027e-06, 'epoch': 0.68}
{'loss': 1.1884, 'grad_norm': 1.457987904548645, 'learning_rate': 4.969453493891733e-06, 'epoch': 0.68}
{'loss': 1.0034, 'grad_norm': 1.5881192684173584, 'learning_rate': 4.9640694332488075e-06, 'epoch': 0.68}
{'loss': 1.2353, 'grad_norm': 1.387600064277649, 'learning_rate': 4.958687327727511e-06, 'epoch': 0.68}
{'loss': 1.1424, 'grad_norm': 1.4454630613327026, 'learning_rate': 4.953307179417376e-06, 'epoch': 0.68}
{'loss': 1.1576, 'grad_norm': 1.639157772064209, 'learning_rate': 4.947928990407156e-06, 'epoch': 0.68}
Error with image file is truncated (41 bytes not processed)
{'loss': 1.1626, 'grad_norm': 1.5930908918380737, 'learning_rate': 4.94255276278485e-06, 'epoch': 0.68}
{'loss': 1.101, 'grad_norm': 1.5472928285598755, 'learning_rate': 4.937178498637696e-06, 'epoch': 0.68}
{'loss': 1.1536, 'grad_norm': 1.6333246231079102, 'learning_rate': 4.931806200052165e-06, 'epoch': 0.68}
{'loss': 1.1675, 'grad_norm': 1.6899746656417847, 'learning_rate': 4.926435869113971e-06, 'epoch': 0.68}
{'loss': 1.0872, 'grad_norm': 1.5013015270233154, 'learning_rate': 4.92106750790806e-06, 'epoch': 0.68}
{'loss': 1.076, 'grad_norm': 1.4423508644104004, 'learning_rate': 4.915701118518616e-06, 'epoch': 0.68}
{'loss': 1.1716, 'grad_norm': 1.5348801612854004, 'learning_rate': 4.910336703029055e-06, 'epoch': 0.68}
{'loss': 1.125, 'grad_norm': 1.5614386796951294, 'learning_rate': 4.904974263522025e-06, 'epoch': 0.68}
{'loss': 1.2135, 'grad_norm': 1.5811184644699097, 'learning_rate': 4.899613802079419e-06, 'epoch': 0.68}
{'loss': 1.0927, 'grad_norm': 1.6096835136413574, 'learning_rate': 4.8942553207823395e-06, 'epoch': 0.68}
{'loss': 1.064, 'grad_norm': 1.4972351789474487, 'learning_rate': 4.888898821711144e-06, 'epoch': 0.68}
{'loss': 1.1782, 'grad_norm': 1.1657394170761108, 'learning_rate': 4.883544306945407e-06, 'epoch': 0.68}
{'loss': 1.1471, 'grad_norm': 1.2414512634277344, 'learning_rate': 4.878191778563934e-06, 'epoch': 0.68}
{'loss': 1.1646, 'grad_norm': 1.5711619853973389, 'learning_rate': 4.872841238644766e-06, 'epoch': 0.68}
{'loss': 1.1667, 'grad_norm': 1.431637167930603, 'learning_rate': 4.867492689265154e-06, 'epoch': 0.68}
{'loss': 1.1241, 'grad_norm': 1.551405429840088, 'learning_rate': 4.8621461325016015e-06, 'epoch': 0.68}
{'loss': 1.1143, 'grad_norm': 1.4699342250823975, 'learning_rate': 4.856801570429822e-06, 'epoch': 0.68}
{'loss': 1.1242, 'grad_norm': 1.4551349878311157, 'learning_rate': 4.851459005124759e-06, 'epoch': 0.68}
{'loss': 1.099, 'grad_norm': 1.5214112997055054, 'learning_rate': 4.846118438660578e-06, 'epoch': 0.68}
{'loss': 1.0991, 'grad_norm': 1.5128954648971558, 'learning_rate': 4.840779873110675e-06, 'epoch': 0.68}
{'loss': 1.2816, 'grad_norm': 0.9909771680831909, 'learning_rate': 4.83544331054766e-06, 'epoch': 0.68}
{'loss': 1.1593, 'grad_norm': 1.4493190050125122, 'learning_rate': 4.83010875304337e-06, 'epoch': 0.68}
{'loss': 1.2016, 'grad_norm': 1.5854843854904175, 'learning_rate': 4.824776202668875e-06, 'epoch': 0.68}
{'loss': 1.1324, 'grad_norm': 1.3640261888504028, 'learning_rate': 4.819445661494437e-06, 'epoch': 0.68}
{'loss': 1.2227, 'grad_norm': 1.185651421546936, 'learning_rate': 4.8141171315895694e-06, 'epoch': 0.68}
{'loss': 1.2132, 'grad_norm': 1.4442689418792725, 'learning_rate': 4.808790615022987e-06, 'epoch': 0.68}
{'loss': 1.1586, 'grad_norm': 1.1791869401931763, 'learning_rate': 4.803466113862626e-06, 'epoch': 0.68}
{'loss': 1.0501, 'grad_norm': 1.394614815711975, 'learning_rate': 4.798143630175642e-06, 'epoch': 0.68}
{'loss': 1.1177, 'grad_norm': 1.5593736171722412, 'learning_rate': 4.792823166028405e-06, 'epoch': 0.68}
Error with image file is truncated (60 bytes not processed)
{'loss': 1.1475, 'grad_norm': 1.5886961221694946, 'learning_rate': 4.787504723486505e-06, 'epoch': 0.68}
{'loss': 1.0611, 'grad_norm': 1.4644780158996582, 'learning_rate': 4.7821883046147414e-06, 'epoch': 0.68}
{'loss': 1.2138, 'grad_norm': 1.250220775604248, 'learning_rate': 4.776873911477133e-06, 'epoch': 0.68}
{'loss': 1.1848, 'grad_norm': 1.5430891513824463, 'learning_rate': 4.771561546136908e-06, 'epoch': 0.69}
{'loss': 0.9754, 'grad_norm': 1.2890735864639282, 'learning_rate': 4.766251210656509e-06, 'epoch': 0.69}
{'loss': 1.118, 'grad_norm': 1.4397293329238892, 'learning_rate': 4.760942907097601e-06, 'epoch': 0.69}
{'loss': 1.2115, 'grad_norm': 1.299729824066162, 'learning_rate': 4.755636637521035e-06, 'epoch': 0.69}
{'loss': 1.0479, 'grad_norm': 1.4629144668579102, 'learning_rate': 4.750332403986902e-06, 'epoch': 0.69}
{'loss': 1.1452, 'grad_norm': 1.6502937078475952, 'learning_rate': 4.7450302085544735e-06, 'epoch': 0.69}
{'loss': 1.1373, 'grad_norm': 1.4817496538162231, 'learning_rate': 4.739730053282255e-06, 'epoch': 0.69}
{'loss': 1.0664, 'grad_norm': 1.6007055044174194, 'learning_rate': 4.734431940227951e-06, 'epoch': 0.69}
{'loss': 1.1495, 'grad_norm': 1.5766268968582153, 'learning_rate': 4.7291358714484594e-06, 'epoch': 0.69}
{'loss': 1.123, 'grad_norm': 1.4859638214111328, 'learning_rate': 4.723841848999907e-06, 'epoch': 0.69}
{'loss': 1.1824, 'grad_norm': 1.2697709798812866, 'learning_rate': 4.718549874937612e-06, 'epoch': 0.69}
{'loss': 1.0041, 'grad_norm': 1.4681367874145508, 'learning_rate': 4.713259951316103e-06, 'epoch': 0.69}
{'loss': 1.1221, 'grad_norm': 1.467551589012146, 'learning_rate': 4.707972080189106e-06, 'epoch': 0.69}
{'loss': 1.1032, 'grad_norm': 1.5734872817993164, 'learning_rate': 4.702686263609559e-06, 'epoch': 0.69}
{'loss': 1.0185, 'grad_norm': 1.4516632556915283, 'learning_rate': 4.697402503629596e-06, 'epoch': 0.69}
{'loss': 1.0569, 'grad_norm': 1.5036394596099854, 'learning_rate': 4.69212080230055e-06, 'epoch': 0.69}
{'loss': 1.0829, 'grad_norm': 1.5515100955963135, 'learning_rate': 4.686841161672974e-06, 'epoch': 0.69}
{'loss': 1.0823, 'grad_norm': 1.5545825958251953, 'learning_rate': 4.681563583796587e-06, 'epoch': 0.69}
{'loss': 1.0551, 'grad_norm': 1.465766191482544, 'learning_rate': 4.67628807072034e-06, 'epoch': 0.69}
{'loss': 1.1301, 'grad_norm': 1.5012307167053223, 'learning_rate': 4.6710146244923645e-06, 'epoch': 0.69}
{'loss': 1.1324, 'grad_norm': 1.636125087738037, 'learning_rate': 4.665743247159995e-06, 'epoch': 0.69}
{'loss': 1.1112, 'grad_norm': 1.5622918605804443, 'learning_rate': 4.660473940769761e-06, 'epoch': 0.69}
{'loss': 1.0786, 'grad_norm': 1.5188215970993042, 'learning_rate': 4.655206707367388e-06, 'epoch': 0.69}
{'loss': 1.1721, 'grad_norm': 1.5613105297088623, 'learning_rate': 4.649941548997797e-06, 'epoch': 0.69}
{'loss': 1.1199, 'grad_norm': 1.5228058099746704, 'learning_rate': 4.644678467705101e-06, 'epoch': 0.69}
{'loss': 1.0627, 'grad_norm': 1.4778721332550049, 'learning_rate': 4.639417465532622e-06, 'epoch': 0.69}
{'loss': 1.0929, 'grad_norm': 1.426178216934204, 'learning_rate': 4.634158544522849e-06, 'epoch': 0.69}
{'loss': 1.1071, 'grad_norm': 1.423383116722107, 'learning_rate': 4.628901706717476e-06, 'epoch': 0.69}
{'loss': 1.0612, 'grad_norm': 1.4614812135696411, 'learning_rate': 4.623646954157399e-06, 'epoch': 0.69}
{'loss': 1.0567, 'grad_norm': 1.4544780254364014, 'learning_rate': 4.618394288882681e-06, 'epoch': 0.69}
{'loss': 1.093, 'grad_norm': 1.5363069772720337, 'learning_rate': 4.613143712932603e-06, 'epoch': 0.69}
{'loss': 1.132, 'grad_norm': 1.461888074874878, 'learning_rate': 4.607895228345603e-06, 'epoch': 0.69}
{'loss': 1.1827, 'grad_norm': 1.6189918518066406, 'learning_rate': 4.602648837159333e-06, 'epoch': 0.69}
{'loss': 1.1151, 'grad_norm': 1.4428960084915161, 'learning_rate': 4.597404541410622e-06, 'epoch': 0.69}
{'loss': 1.0196, 'grad_norm': 1.487526535987854, 'learning_rate': 4.592162343135483e-06, 'epoch': 0.69}
{'loss': 1.0763, 'grad_norm': 1.4606941938400269, 'learning_rate': 4.586922244369122e-06, 'epoch': 0.69}
{'loss': 1.0646, 'grad_norm': 1.1360396146774292, 'learning_rate': 4.5816842471459224e-06, 'epoch': 0.69}
{'loss': 1.0697, 'grad_norm': 1.3621503114700317, 'learning_rate': 4.576448353499457e-06, 'epoch': 0.69}
{'loss': 0.9798, 'grad_norm': 1.4182807207107544, 'learning_rate': 4.571214565462477e-06, 'epoch': 0.69}
{'loss': 1.1986, 'grad_norm': 1.2645440101623535, 'learning_rate': 4.565982885066923e-06, 'epoch': 0.69}
{'loss': 1.0743, 'grad_norm': 1.5537132024765015, 'learning_rate': 4.560753314343912e-06, 'epoch': 0.69}
{'loss': 1.0835, 'grad_norm': 1.5412853956222534, 'learning_rate': 4.555525855323738e-06, 'epoch': 0.69}
{'loss': 1.0867, 'grad_norm': 1.5147606134414673, 'learning_rate': 4.5503005100358945e-06, 'epoch': 0.69}
{'loss': 1.0548, 'grad_norm': 1.5578036308288574, 'learning_rate': 4.545077280509022e-06, 'epoch': 0.69}
{'loss': 1.0032, 'grad_norm': 1.3881127834320068, 'learning_rate': 4.539856168770974e-06, 'epoch': 0.69}
{'loss': 1.2413, 'grad_norm': 1.35164475440979, 'learning_rate': 4.534637176848758e-06, 'epoch': 0.69}
{'loss': 1.1009, 'grad_norm': 1.5118290185928345, 'learning_rate': 4.52942030676857e-06, 'epoch': 0.69}
{'loss': 1.1435, 'grad_norm': 1.4961458444595337, 'learning_rate': 4.524205560555774e-06, 'epoch': 0.69}
{'loss': 1.1421, 'grad_norm': 1.5241968631744385, 'learning_rate': 4.5189929402349175e-06, 'epoch': 0.69}
{'loss': 1.0596, 'grad_norm': 1.5091001987457275, 'learning_rate': 4.513782447829717e-06, 'epoch': 0.69}
{'loss': 1.2252, 'grad_norm': 1.3288516998291016, 'learning_rate': 4.508574085363065e-06, 'epoch': 0.69}
{'loss': 1.0439, 'grad_norm': 1.6223812103271484, 'learning_rate': 4.503367854857035e-06, 'epoch': 0.69}
{'loss': 1.0933, 'grad_norm': 1.5953702926635742, 'learning_rate': 4.498163758332853e-06, 'epoch': 0.7}
{'loss': 1.1344, 'grad_norm': 1.6338915824890137, 'learning_rate': 4.492961797810932e-06, 'epoch': 0.7}
{'loss': 1.244, 'grad_norm': 1.3330649137496948, 'learning_rate': 4.4877619753108605e-06, 'epoch': 0.7}
{'loss': 1.1256, 'grad_norm': 1.6349211931228638, 'learning_rate': 4.4825642928513746e-06, 'epoch': 0.7}
{'loss': 1.1473, 'grad_norm': 1.5387790203094482, 'learning_rate': 4.477368752450409e-06, 'epoch': 0.7}
{'loss': 1.0571, 'grad_norm': 1.4988802671432495, 'learning_rate': 4.472175356125036e-06, 'epoch': 0.7}
{'loss': 1.1124, 'grad_norm': 1.5357946157455444, 'learning_rate': 4.466984105891521e-06, 'epoch': 0.7}
{'loss': 1.1251, 'grad_norm': 1.5651935338974, 'learning_rate': 4.461795003765285e-06, 'epoch': 0.7}
{'loss': 1.1056, 'grad_norm': 1.4981861114501953, 'learning_rate': 4.456608051760914e-06, 'epoch': 0.7}
{'loss': 1.1245, 'grad_norm': 1.5346623659133911, 'learning_rate': 4.45142325189216e-06, 'epoch': 0.7}
{'loss': 1.1353, 'grad_norm': 1.5352230072021484, 'learning_rate': 4.446240606171945e-06, 'epoch': 0.7}
{'loss': 1.0866, 'grad_norm': 1.4165575504302979, 'learning_rate': 4.4410601166123475e-06, 'epoch': 0.7}
{'loss': 1.114, 'grad_norm': 1.5883859395980835, 'learning_rate': 4.4358817852246124e-06, 'epoch': 0.7}
{'loss': 1.1, 'grad_norm': 1.5870004892349243, 'learning_rate': 4.430705614019147e-06, 'epoch': 0.7}
{'loss': 1.1246, 'grad_norm': 1.5333706140518188, 'learning_rate': 4.425531605005519e-06, 'epoch': 0.7}
{'loss': 1.153, 'grad_norm': 1.6374132633209229, 'learning_rate': 4.420359760192452e-06, 'epoch': 0.7}
{'loss': 1.0943, 'grad_norm': 1.3374593257904053, 'learning_rate': 4.4151900815878455e-06, 'epoch': 0.7}
{'loss': 1.0372, 'grad_norm': 1.4291383028030396, 'learning_rate': 4.410022571198734e-06, 'epoch': 0.7}
{'loss': 1.1053, 'grad_norm': 1.5339182615280151, 'learning_rate': 4.404857231031332e-06, 'epoch': 0.7}
{'loss': 1.1577, 'grad_norm': 1.6034313440322876, 'learning_rate': 4.399694063090999e-06, 'epoch': 0.7}
{'loss': 1.0795, 'grad_norm': 1.360213041305542, 'learning_rate': 4.394533069382255e-06, 'epoch': 0.7}
{'loss': 1.0843, 'grad_norm': 1.4773048162460327, 'learning_rate': 4.3893742519087754e-06, 'epoch': 0.7}
{'loss': 1.1521, 'grad_norm': 1.5706684589385986, 'learning_rate': 4.3842176126733914e-06, 'epoch': 0.7}
{'loss': 1.1489, 'grad_norm': 1.5345758199691772, 'learning_rate': 4.379063153678087e-06, 'epoch': 0.7}
{'loss': 1.081, 'grad_norm': 1.4087358713150024, 'learning_rate': 4.373910876923997e-06, 'epoch': 0.7}
{'loss': 1.1346, 'grad_norm': 1.4859530925750732, 'learning_rate': 4.368760784411423e-06, 'epoch': 0.7}
{'loss': 1.2229, 'grad_norm': 1.288067102432251, 'learning_rate': 4.363612878139799e-06, 'epoch': 0.7}
{'loss': 1.0547, 'grad_norm': 1.5109672546386719, 'learning_rate': 4.3584671601077224e-06, 'epoch': 0.7}
{'loss': 1.0771, 'grad_norm': 1.65915846824646, 'learning_rate': 4.353323632312938e-06, 'epoch': 0.7}
{'loss': 1.0544, 'grad_norm': 1.4836598634719849, 'learning_rate': 4.348182296752336e-06, 'epoch': 0.7}
{'loss': 1.0642, 'grad_norm': 1.63186514377594, 'learning_rate': 4.343043155421971e-06, 'epoch': 0.7}
{'loss': 1.1555, 'grad_norm': 1.5207287073135376, 'learning_rate': 4.3379062103170214e-06, 'epoch': 0.7}
{'loss': 1.1507, 'grad_norm': 1.6364243030548096, 'learning_rate': 4.332771463431837e-06, 'epoch': 0.7}
{'loss': 1.0597, 'grad_norm': 1.5274022817611694, 'learning_rate': 4.327638916759898e-06, 'epoch': 0.7}
{'loss': 1.0809, 'grad_norm': 1.544874668121338, 'learning_rate': 4.322508572293836e-06, 'epoch': 0.7}
{'loss': 1.0223, 'grad_norm': 1.546661376953125, 'learning_rate': 4.317380432025428e-06, 'epoch': 0.7}
{'loss': 1.0215, 'grad_norm': 1.4827646017074585, 'learning_rate': 4.312254497945595e-06, 'epoch': 0.7}
{'loss': 1.1035, 'grad_norm': 1.5083850622177124, 'learning_rate': 4.3071307720444015e-06, 'epoch': 0.7}
{'loss': 1.118, 'grad_norm': 1.4599170684814453, 'learning_rate': 4.3020092563110485e-06, 'epoch': 0.7}
{'loss': 1.1312, 'grad_norm': 1.5848332643508911, 'learning_rate': 4.2968899527338984e-06, 'epoch': 0.7}
{'loss': 1.1628, 'grad_norm': 1.6679000854492188, 'learning_rate': 4.291772863300428e-06, 'epoch': 0.7}
{'loss': 1.0523, 'grad_norm': 1.4177583456039429, 'learning_rate': 4.2866579899972686e-06, 'epoch': 0.7}
{'loss': 0.9725, 'grad_norm': 1.5044983625411987, 'learning_rate': 4.281545334810201e-06, 'epoch': 0.7}
{'loss': 1.2064, 'grad_norm': 1.7154988050460815, 'learning_rate': 4.276434899724119e-06, 'epoch': 0.7}
{'loss': 1.0779, 'grad_norm': 1.44008469581604, 'learning_rate': 4.27132668672308e-06, 'epoch': 0.7}
{'loss': 1.0296, 'grad_norm': 1.4551552534103394, 'learning_rate': 4.266220697790266e-06, 'epoch': 0.7}
{'loss': 1.0992, 'grad_norm': 1.5155541896820068, 'learning_rate': 4.2611169349079985e-06, 'epoch': 0.7}
{'loss': 1.1658, 'grad_norm': 1.7189104557037354, 'learning_rate': 4.25601540005773e-06, 'epoch': 0.7}
{'loss': 1.0402, 'grad_norm': 1.5250197649002075, 'learning_rate': 4.250916095220056e-06, 'epoch': 0.7}
{'loss': 1.084, 'grad_norm': 1.4750651121139526, 'learning_rate': 4.2458190223747e-06, 'epoch': 0.7}
{'loss': 1.0869, 'grad_norm': 1.4970622062683105, 'learning_rate': 4.240724183500518e-06, 'epoch': 0.7}
{'loss': 1.0878, 'grad_norm': 1.4980179071426392, 'learning_rate': 4.2356315805755135e-06, 'epoch': 0.7}
{'loss': 1.1117, 'grad_norm': 1.5353437662124634, 'learning_rate': 4.230541215576798e-06, 'epoch': 0.71}
{'loss': 1.1209, 'grad_norm': 1.4381107091903687, 'learning_rate': 4.225453090480631e-06, 'epoch': 0.71}
{'loss': 1.0409, 'grad_norm': 1.453485369682312, 'learning_rate': 4.220367207262398e-06, 'epoch': 0.71}
{'loss': 1.1415, 'grad_norm': 1.535987377166748, 'learning_rate': 4.21528356789661e-06, 'epoch': 0.71}
{'loss': 1.0923, 'grad_norm': 1.5048326253890991, 'learning_rate': 4.210202174356922e-06, 'epoch': 0.71}
{'loss': 1.1172, 'grad_norm': 1.591342806816101, 'learning_rate': 4.20512302861609e-06, 'epoch': 0.71}
{'loss': 1.0604, 'grad_norm': 1.5487115383148193, 'learning_rate': 4.2000461326460274e-06, 'epoch': 0.71}
{'loss': 1.2358, 'grad_norm': 1.2269827127456665, 'learning_rate': 4.194971488417753e-06, 'epoch': 0.71}
{'loss': 1.1963, 'grad_norm': 1.2244545221328735, 'learning_rate': 4.189899097901421e-06, 'epoch': 0.71}
{'loss': 1.18, 'grad_norm': 1.5274540185928345, 'learning_rate': 4.184828963066305e-06, 'epoch': 0.71}
{'loss': 1.1767, 'grad_norm': 1.5292041301727295, 'learning_rate': 4.179761085880809e-06, 'epoch': 0.71}
{'loss': 1.1519, 'grad_norm': 1.4911808967590332, 'learning_rate': 4.174695468312456e-06, 'epoch': 0.71}
{'loss': 1.0555, 'grad_norm': 1.572706699371338, 'learning_rate': 4.16963211232789e-06, 'epoch': 0.71}
{'loss': 1.0969, 'grad_norm': 1.3905423879623413, 'learning_rate': 4.16457101989289e-06, 'epoch': 0.71}
{'loss': 1.028, 'grad_norm': 1.4789295196533203, 'learning_rate': 4.159512192972337e-06, 'epoch': 0.71}
{'loss': 1.1023, 'grad_norm': 1.3826329708099365, 'learning_rate': 4.15445563353024e-06, 'epoch': 0.71}
{'loss': 1.0643, 'grad_norm': 1.5081170797348022, 'learning_rate': 4.149401343529742e-06, 'epoch': 0.71}
{'loss': 1.0841, 'grad_norm': 1.5043798685073853, 'learning_rate': 4.144349324933077e-06, 'epoch': 0.71}
{'loss': 1.1247, 'grad_norm': 1.613824725151062, 'learning_rate': 4.139299579701623e-06, 'epoch': 0.71}
{'loss': 1.1728, 'grad_norm': 1.228042721748352, 'learning_rate': 4.134252109795863e-06, 'epoch': 0.71}
{'loss': 1.0295, 'grad_norm': 1.4681615829467773, 'learning_rate': 4.129206917175397e-06, 'epoch': 0.71}
{'loss': 1.2099, 'grad_norm': 1.2444449663162231, 'learning_rate': 4.124164003798944e-06, 'epoch': 0.71}
{'loss': 1.019, 'grad_norm': 1.4040045738220215, 'learning_rate': 4.119123371624335e-06, 'epoch': 0.71}
{'loss': 1.1757, 'grad_norm': 1.3559778928756714, 'learning_rate': 4.114085022608517e-06, 'epoch': 0.71}
{'loss': 1.0558, 'grad_norm': 1.3399137258529663, 'learning_rate': 4.109048958707552e-06, 'epoch': 0.71}
{'loss': 1.0471, 'grad_norm': 1.5153276920318604, 'learning_rate': 4.104015181876613e-06, 'epoch': 0.71}
{'loss': 1.0646, 'grad_norm': 1.5725266933441162, 'learning_rate': 4.09898369406998e-06, 'epoch': 0.71}
{'loss': 1.0075, 'grad_norm': 1.5198849439620972, 'learning_rate': 4.0939544972410636e-06, 'epoch': 0.71}
{'loss': 1.1155, 'grad_norm': 1.4950275421142578, 'learning_rate': 4.0889275933423576e-06, 'epoch': 0.71}
{'loss': 1.1496, 'grad_norm': 1.4545010328292847, 'learning_rate': 4.0839029843254815e-06, 'epoch': 0.71}
{'loss': 1.1708, 'grad_norm': 1.512024164199829, 'learning_rate': 4.078880672141171e-06, 'epoch': 0.71}
{'loss': 1.1023, 'grad_norm': 1.5876553058624268, 'learning_rate': 4.073860658739246e-06, 'epoch': 0.71}
{'loss': 1.1819, 'grad_norm': 1.299709677696228, 'learning_rate': 4.068842946068661e-06, 'epoch': 0.71}
{'loss': 1.141, 'grad_norm': 1.5806604623794556, 'learning_rate': 4.063827536077459e-06, 'epoch': 0.71}
{'loss': 1.1011, 'grad_norm': 1.5012273788452148, 'learning_rate': 4.058814430712796e-06, 'epoch': 0.71}
{'loss': 1.1409, 'grad_norm': 1.6018650531768799, 'learning_rate': 4.0538036319209325e-06, 'epoch': 0.71}
{'loss': 1.1837, 'grad_norm': 1.4976551532745361, 'learning_rate': 4.0487951416472324e-06, 'epoch': 0.71}
{'loss': 1.0896, 'grad_norm': 1.5252360105514526, 'learning_rate': 4.043788961836164e-06, 'epoch': 0.71}
{'loss': 1.1211, 'grad_norm': 1.5429449081420898, 'learning_rate': 4.038785094431295e-06, 'epoch': 0.71}
{'loss': 1.1863, 'grad_norm': 1.551306128501892, 'learning_rate': 4.0337835413753116e-06, 'epoch': 0.71}
{'loss': 1.125, 'grad_norm': 1.6568171977996826, 'learning_rate': 4.0287843046099765e-06, 'epoch': 0.71}
{'loss': 1.1402, 'grad_norm': 1.4415761232376099, 'learning_rate': 4.0237873860761645e-06, 'epoch': 0.71}
{'loss': 1.0942, 'grad_norm': 1.5721509456634521, 'learning_rate': 4.018792787713865e-06, 'epoch': 0.71}
{'loss': 1.0428, 'grad_norm': 1.5175888538360596, 'learning_rate': 4.013800511462135e-06, 'epoch': 0.71}
{'loss': 1.1611, 'grad_norm': 1.171040654182434, 'learning_rate': 4.008810559259162e-06, 'epoch': 0.71}
{'loss': 1.0335, 'grad_norm': 1.6733105182647705, 'learning_rate': 4.003822933042213e-06, 'epoch': 0.71}
{'loss': 1.0361, 'grad_norm': 1.6732292175292969, 'learning_rate': 3.998837634747655e-06, 'epoch': 0.71}
{'loss': 1.1545, 'grad_norm': 1.6220500469207764, 'learning_rate': 3.993854666310955e-06, 'epoch': 0.71}
{'loss': 1.1669, 'grad_norm': 1.5273195505142212, 'learning_rate': 3.98887402966667e-06, 'epoch': 0.71}
{'loss': 1.1833, 'grad_norm': 1.5615745782852173, 'learning_rate': 3.983895726748455e-06, 'epoch': 0.71}
{'loss': 1.1305, 'grad_norm': 1.6034917831420898, 'learning_rate': 3.97891975948906e-06, 'epoch': 0.71}
{'loss': 0.9908, 'grad_norm': 1.58769953250885, 'learning_rate': 3.973946129820326e-06, 'epoch': 0.71}
{'loss': 1.1264, 'grad_norm': 1.4882925748825073, 'learning_rate': 3.968974839673186e-06, 'epoch': 0.72}
{'loss': 1.1045, 'grad_norm': 1.4643867015838623, 'learning_rate': 3.964005890977672e-06, 'epoch': 0.72}
{'loss': 1.1237, 'grad_norm': 1.5639811754226685, 'learning_rate': 3.9590392856628946e-06, 'epoch': 0.72}
{'loss': 1.0985, 'grad_norm': 1.5213547945022583, 'learning_rate': 3.954075025657058e-06, 'epoch': 0.72}
{'loss': 1.168, 'grad_norm': 1.5437134504318237, 'learning_rate': 3.949113112887471e-06, 'epoch': 0.72}
{'loss': 1.043, 'grad_norm': 1.5389162302017212, 'learning_rate': 3.944153549280506e-06, 'epoch': 0.72}
{'loss': 1.1824, 'grad_norm': 1.4240946769714355, 'learning_rate': 3.939196336761645e-06, 'epoch': 0.72}
{'loss': 1.0577, 'grad_norm': 1.4488605260849, 'learning_rate': 3.934241477255445e-06, 'epoch': 0.72}
{'loss': 1.221, 'grad_norm': 1.438951849937439, 'learning_rate': 3.929288972685555e-06, 'epoch': 0.72}
{'loss': 1.0857, 'grad_norm': 1.4926743507385254, 'learning_rate': 3.924338824974705e-06, 'epoch': 0.72}
{'loss': 1.0846, 'grad_norm': 1.4894624948501587, 'learning_rate': 3.919391036044715e-06, 'epoch': 0.72}
{'loss': 1.0898, 'grad_norm': 1.5183756351470947, 'learning_rate': 3.914445607816486e-06, 'epoch': 0.72}
{'loss': 1.0843, 'grad_norm': 1.4346152544021606, 'learning_rate': 3.909502542210001e-06, 'epoch': 0.72}
{'loss': 1.0141, 'grad_norm': 1.4766716957092285, 'learning_rate': 3.904561841144338e-06, 'epoch': 0.72}
{'loss': 1.1711, 'grad_norm': 1.4612349271774292, 'learning_rate': 3.899623506537635e-06, 'epoch': 0.72}
{'loss': 1.19, 'grad_norm': 1.2975952625274658, 'learning_rate': 3.894687540307127e-06, 'epoch': 0.72}
{'loss': 1.1073, 'grad_norm': 1.4312193393707275, 'learning_rate': 3.8897539443691355e-06, 'epoch': 0.72}
{'loss': 1.2058, 'grad_norm': 1.212660312652588, 'learning_rate': 3.884822720639036e-06, 'epoch': 0.72}
{'loss': 1.0672, 'grad_norm': 1.6318542957305908, 'learning_rate': 3.879893871031314e-06, 'epoch': 0.72}
{'loss': 1.1472, 'grad_norm': 1.6696524620056152, 'learning_rate': 3.874967397459511e-06, 'epoch': 0.72}
{'loss': 1.0891, 'grad_norm': 1.396245002746582, 'learning_rate': 3.870043301836256e-06, 'epoch': 0.72}
{'loss': 1.0922, 'grad_norm': 1.6289459466934204, 'learning_rate': 3.86512158607325e-06, 'epoch': 0.72}
{'loss': 1.1118, 'grad_norm': 1.6117732524871826, 'learning_rate': 3.860202252081276e-06, 'epoch': 0.72}
{'loss': 1.0633, 'grad_norm': 1.661739468574524, 'learning_rate': 3.855285301770188e-06, 'epoch': 0.72}
{'loss': 1.2173, 'grad_norm': 1.2646620273590088, 'learning_rate': 3.850370737048913e-06, 'epoch': 0.72}
{'loss': 1.1704, 'grad_norm': 1.332438588142395, 'learning_rate': 3.8454585598254565e-06, 'epoch': 0.72}
{'loss': 1.3103, 'grad_norm': 1.371824860572815, 'learning_rate': 3.840548772006891e-06, 'epoch': 0.72}
{'loss': 1.1206, 'grad_norm': 1.4302502870559692, 'learning_rate': 3.835641375499375e-06, 'epoch': 0.72}
{'loss': 1.1119, 'grad_norm': 1.5215606689453125, 'learning_rate': 3.830736372208118e-06, 'epoch': 0.72}
{'loss': 0.96, 'grad_norm': 1.492748498916626, 'learning_rate': 3.8258337640374125e-06, 'epoch': 0.72}
{'loss': 1.101, 'grad_norm': 1.454884648323059, 'learning_rate': 3.820933552890629e-06, 'epoch': 0.72}
{'loss': 1.1658, 'grad_norm': 1.6341315507888794, 'learning_rate': 3.816035740670185e-06, 'epoch': 0.72}
{'loss': 1.1418, 'grad_norm': 1.5948184728622437, 'learning_rate': 3.811140329277591e-06, 'epoch': 0.72}
{'loss': 1.1145, 'grad_norm': 1.5752443075180054, 'learning_rate': 3.8062473206134088e-06, 'epoch': 0.72}
{'loss': 1.2031, 'grad_norm': 1.5433779954910278, 'learning_rate': 3.8013567165772735e-06, 'epoch': 0.72}
{'loss': 1.0399, 'grad_norm': 1.555578351020813, 'learning_rate': 3.7964685190678874e-06, 'epoch': 0.72}
{'loss': 1.172, 'grad_norm': 1.5309019088745117, 'learning_rate': 3.7915827299830154e-06, 'epoch': 0.72}
{'loss': 0.9738, 'grad_norm': 1.4639006853103638, 'learning_rate': 3.7866993512194895e-06, 'epoch': 0.72}
{'loss': 1.1594, 'grad_norm': 1.4562711715698242, 'learning_rate': 3.7818183846732024e-06, 'epoch': 0.72}
{'loss': 1.0787, 'grad_norm': 1.5004210472106934, 'learning_rate': 3.776939832239125e-06, 'epoch': 0.72}
{'loss': 1.1555, 'grad_norm': 1.2222521305084229, 'learning_rate': 3.7720636958112623e-06, 'epoch': 0.72}
{'loss': 1.1513, 'grad_norm': 1.5218453407287598, 'learning_rate': 3.7671899772827113e-06, 'epoch': 0.72}
{'loss': 1.2596, 'grad_norm': 1.3205362558364868, 'learning_rate': 3.7623186785456156e-06, 'epoch': 0.72}
{'loss': 1.1542, 'grad_norm': 1.205426573753357, 'learning_rate': 3.757449801491172e-06, 'epoch': 0.72}
{'loss': 1.0875, 'grad_norm': 1.5228865146636963, 'learning_rate': 3.7525833480096575e-06, 'epoch': 0.72}
{'loss': 1.1145, 'grad_norm': 1.4608250856399536, 'learning_rate': 3.7477193199903903e-06, 'epoch': 0.72}
{'loss': 1.0758, 'grad_norm': 1.4348679780960083, 'learning_rate': 3.7428577193217563e-06, 'epoch': 0.72}
{'loss': 1.1083, 'grad_norm': 1.5433282852172852, 'learning_rate': 3.737998547891195e-06, 'epoch': 0.72}
{'loss': 1.0979, 'grad_norm': 1.5082472562789917, 'learning_rate': 3.7331418075852053e-06, 'epoch': 0.72}
{'loss': 0.9826, 'grad_norm': 1.4755597114562988, 'learning_rate': 3.728287500289339e-06, 'epoch': 0.72}
{'loss': 1.1208, 'grad_norm': 1.5055230855941772, 'learning_rate': 3.7234356278882076e-06, 'epoch': 0.72}
{'loss': 1.0277, 'grad_norm': 1.069103479385376, 'learning_rate': 3.718586192265473e-06, 'epoch': 0.72}
{'loss': 1.1166, 'grad_norm': 1.385332465171814, 'learning_rate': 3.7137391953038516e-06, 'epoch': 0.73}
{'loss': 1.0277, 'grad_norm': 1.4144331216812134, 'learning_rate': 3.7088946388851223e-06, 'epoch': 0.73}
{'loss': 1.1586, 'grad_norm': 1.4045270681381226, 'learning_rate': 3.7040525248901003e-06, 'epoch': 0.73}
{'loss': 1.2548, 'grad_norm': 1.2431718111038208, 'learning_rate': 3.6992128551986617e-06, 'epoch': 0.73}
{'loss': 1.1043, 'grad_norm': 1.501964807510376, 'learning_rate': 3.6943756316897406e-06, 'epoch': 0.73}
{'loss': 1.1413, 'grad_norm': 1.4764182567596436, 'learning_rate': 3.6895408562413027e-06, 'epoch': 0.73}
{'loss': 1.0381, 'grad_norm': 1.4548312425613403, 'learning_rate': 3.684708530730382e-06, 'epoch': 0.73}
{'loss': 1.1961, 'grad_norm': 1.5277085304260254, 'learning_rate': 3.6798786570330526e-06, 'epoch': 0.73}
{'loss': 1.164, 'grad_norm': 1.5192052125930786, 'learning_rate': 3.6750512370244363e-06, 'epoch': 0.73}
{'loss': 1.0229, 'grad_norm': 1.535355567932129, 'learning_rate': 3.670226272578704e-06, 'epoch': 0.73}
{'loss': 1.0404, 'grad_norm': 1.3711100816726685, 'learning_rate': 3.6654037655690732e-06, 'epoch': 0.73}
{'loss': 1.1226, 'grad_norm': 1.4748084545135498, 'learning_rate': 3.660583717867807e-06, 'epoch': 0.73}
{'loss': 1.0571, 'grad_norm': 1.571979284286499, 'learning_rate': 3.655766131346211e-06, 'epoch': 0.73}
{'loss': 1.0752, 'grad_norm': 1.4500850439071655, 'learning_rate': 3.650951007874648e-06, 'epoch': 0.73}
{'loss': 1.1356, 'grad_norm': 1.5090035200119019, 'learning_rate': 3.6461383493225012e-06, 'epoch': 0.73}
{'loss': 1.0324, 'grad_norm': 1.3975871801376343, 'learning_rate': 3.6413281575582194e-06, 'epoch': 0.73}
{'loss': 1.2241, 'grad_norm': 1.2594621181488037, 'learning_rate': 3.6365204344492867e-06, 'epoch': 0.73}
{'loss': 1.1074, 'grad_norm': 1.4732184410095215, 'learning_rate': 3.6317151818622154e-06, 'epoch': 0.73}
{'loss': 1.1027, 'grad_norm': 1.4494160413742065, 'learning_rate': 3.62691240166258e-06, 'epoch': 0.73}
{'loss': 1.0904, 'grad_norm': 1.3861926794052124, 'learning_rate': 3.6221120957149826e-06, 'epoch': 0.73}
{'loss': 1.1938, 'grad_norm': 1.7051458358764648, 'learning_rate': 3.617314265883066e-06, 'epoch': 0.73}
{'loss': 1.0627, 'grad_norm': 1.4035496711730957, 'learning_rate': 3.612518914029515e-06, 'epoch': 0.73}
{'loss': 1.0799, 'grad_norm': 1.5508617162704468, 'learning_rate': 3.6077260420160487e-06, 'epoch': 0.73}
{'loss': 1.0844, 'grad_norm': 1.547322392463684, 'learning_rate': 3.602935651703424e-06, 'epoch': 0.73}
{'loss': 1.1492, 'grad_norm': 1.5872502326965332, 'learning_rate': 3.598147744951438e-06, 'epoch': 0.73}
{'loss': 1.0148, 'grad_norm': 1.4762086868286133, 'learning_rate': 3.5933623236189198e-06, 'epoch': 0.73}
{'loss': 1.0674, 'grad_norm': 1.4437459707260132, 'learning_rate': 3.58857938956373e-06, 'epoch': 0.73}
{'loss': 1.1326, 'grad_norm': 1.2589479684829712, 'learning_rate': 3.58379894464278e-06, 'epoch': 0.73}
{'loss': 1.0982, 'grad_norm': 1.4493963718414307, 'learning_rate': 3.57902099071199e-06, 'epoch': 0.73}
{'loss': 1.058, 'grad_norm': 1.4082016944885254, 'learning_rate': 3.5742455296263346e-06, 'epoch': 0.73}
{'loss': 1.2583, 'grad_norm': 1.153273582458496, 'learning_rate': 3.569472563239814e-06, 'epoch': 0.73}
{'loss': 1.0267, 'grad_norm': 1.4129385948181152, 'learning_rate': 3.5647020934054465e-06, 'epoch': 0.73}
{'loss': 1.2021, 'grad_norm': 1.566744089126587, 'learning_rate': 3.559934121975304e-06, 'epoch': 0.73}
{'loss': 1.1375, 'grad_norm': 1.4646527767181396, 'learning_rate': 3.5551686508004735e-06, 'epoch': 0.73}
{'loss': 1.0385, 'grad_norm': 1.3882930278778076, 'learning_rate': 3.550405681731074e-06, 'epoch': 0.73}
{'loss': 1.0954, 'grad_norm': 1.5239028930664062, 'learning_rate': 3.5456452166162547e-06, 'epoch': 0.73}
{'loss': 1.2388, 'grad_norm': 1.5204777717590332, 'learning_rate': 3.540887257304193e-06, 'epoch': 0.73}
{'loss': 1.0539, 'grad_norm': 1.5557478666305542, 'learning_rate': 3.5361318056420925e-06, 'epoch': 0.73}
{'loss': 1.0584, 'grad_norm': 1.5146418809890747, 'learning_rate': 3.531378863476178e-06, 'epoch': 0.73}
{'loss': 1.031, 'grad_norm': 1.4951109886169434, 'learning_rate': 3.5266284326517165e-06, 'epoch': 0.73}
{'loss': 1.0936, 'grad_norm': 1.5811995267868042, 'learning_rate': 3.5218805150129755e-06, 'epoch': 0.73}
{'loss': 1.0994, 'grad_norm': 1.5718632936477661, 'learning_rate': 3.5171351124032703e-06, 'epoch': 0.73}
{'loss': 1.0629, 'grad_norm': 1.428598165512085, 'learning_rate': 3.51239222666493e-06, 'epoch': 0.73}
{'loss': 1.1959, 'grad_norm': 1.1724880933761597, 'learning_rate': 3.507651859639295e-06, 'epoch': 0.73}
{'loss': 1.104, 'grad_norm': 1.4186197519302368, 'learning_rate': 3.5029140131667493e-06, 'epoch': 0.73}
{'loss': 1.2009, 'grad_norm': 1.1690841913223267, 'learning_rate': 3.4981786890866853e-06, 'epoch': 0.73}
{'loss': 1.0855, 'grad_norm': 1.5117815732955933, 'learning_rate': 3.493445889237518e-06, 'epoch': 0.73}
{'loss': 1.1141, 'grad_norm': 1.575371503829956, 'learning_rate': 3.4887156154566847e-06, 'epoch': 0.73}
{'loss': 1.1069, 'grad_norm': 1.5626256465911865, 'learning_rate': 3.4839878695806385e-06, 'epoch': 0.73}
{'loss': 1.0906, 'grad_norm': 1.6326571702957153, 'learning_rate': 3.4792626534448547e-06, 'epoch': 0.73}
{'loss': 1.1364, 'grad_norm': 1.4564446210861206, 'learning_rate': 3.4745399688838243e-06, 'epoch': 0.73}
{'loss': 1.2064, 'grad_norm': 1.4162606000900269, 'learning_rate': 3.469819817731056e-06, 'epoch': 0.73}
{'loss': 1.0854, 'grad_norm': 1.4778940677642822, 'learning_rate': 3.4651022018190715e-06, 'epoch': 0.74}
{'loss': 1.004, 'grad_norm': 1.4086676836013794, 'learning_rate': 3.460387122979423e-06, 'epoch': 0.74}
{'loss': 1.0803, 'grad_norm': 1.427317500114441, 'learning_rate': 3.455674583042652e-06, 'epoch': 0.74}
{'loss': 1.0568, 'grad_norm': 1.5104308128356934, 'learning_rate': 3.4509645838383386e-06, 'epoch': 0.74}
{'loss': 1.1186, 'grad_norm': 1.3843247890472412, 'learning_rate': 3.4462571271950674e-06, 'epoch': 0.74}
{'loss': 1.0909, 'grad_norm': 1.4496089220046997, 'learning_rate': 3.4415522149404233e-06, 'epoch': 0.74}
{'loss': 1.1562, 'grad_norm': 1.5607830286026, 'learning_rate': 3.436849848901028e-06, 'epoch': 0.74}
{'loss': 1.153, 'grad_norm': 1.5700575113296509, 'learning_rate': 3.432150030902497e-06, 'epoch': 0.74}
{'loss': 1.067, 'grad_norm': 1.440885066986084, 'learning_rate': 3.427452762769462e-06, 'epoch': 0.74}
{'loss': 1.1782, 'grad_norm': 1.5970592498779297, 'learning_rate': 3.4227580463255628e-06, 'epoch': 0.74}
{'loss': 1.059, 'grad_norm': 1.5574992895126343, 'learning_rate': 3.4180658833934523e-06, 'epoch': 0.74}
{'loss': 1.1136, 'grad_norm': 1.59811270236969, 'learning_rate': 3.4133762757947873e-06, 'epoch': 0.74}
{'loss': 1.0703, 'grad_norm': 1.532109022140503, 'learning_rate': 3.4086892253502344e-06, 'epoch': 0.74}
{'loss': 1.1136, 'grad_norm': 1.400883674621582, 'learning_rate': 3.4040047338794756e-06, 'epoch': 0.74}
{'loss': 1.1489, 'grad_norm': 1.5357801914215088, 'learning_rate': 3.3993228032011784e-06, 'epoch': 0.74}
{'loss': 1.0783, 'grad_norm': 1.4704869985580444, 'learning_rate': 3.3946434351330415e-06, 'epoch': 0.74}
{'loss': 1.0682, 'grad_norm': 1.5377416610717773, 'learning_rate': 3.3899666314917512e-06, 'epoch': 0.74}
{'loss': 1.0922, 'grad_norm': 1.5569374561309814, 'learning_rate': 3.385292394093006e-06, 'epoch': 0.74}
{'loss': 1.1889, 'grad_norm': 1.5618995428085327, 'learning_rate': 3.3806207247515068e-06, 'epoch': 0.74}
{'loss': 1.0592, 'grad_norm': 1.4277613162994385, 'learning_rate': 3.375951625280948e-06, 'epoch': 0.74}
{'loss': 1.0304, 'grad_norm': 1.396933674812317, 'learning_rate': 3.3712850974940437e-06, 'epoch': 0.74}
{'loss': 0.9803, 'grad_norm': 1.4603739976882935, 'learning_rate': 3.3666211432024974e-06, 'epoch': 0.74}
{'loss': 1.136, 'grad_norm': 1.4449752569198608, 'learning_rate': 3.361959764217018e-06, 'epoch': 0.74}
{'loss': 1.0986, 'grad_norm': 1.5912667512893677, 'learning_rate': 3.357300962347313e-06, 'epoch': 0.74}
{'loss': 1.1202, 'grad_norm': 1.7424427270889282, 'learning_rate': 3.3526447394020887e-06, 'epoch': 0.74}
{'loss': 1.109, 'grad_norm': 1.3512107133865356, 'learning_rate': 3.3479910971890516e-06, 'epoch': 0.74}
{'loss': 1.1293, 'grad_norm': 1.515830397605896, 'learning_rate': 3.343340037514903e-06, 'epoch': 0.74}
{'loss': 1.1389, 'grad_norm': 1.4113677740097046, 'learning_rate': 3.3386915621853533e-06, 'epoch': 0.74}
{'loss': 1.0826, 'grad_norm': 1.4834296703338623, 'learning_rate': 3.3340456730050887e-06, 'epoch': 0.74}
{'loss': 1.0792, 'grad_norm': 1.4537866115570068, 'learning_rate': 3.3294023717778122e-06, 'epoch': 0.74}
{'loss': 1.1188, 'grad_norm': 1.5155960321426392, 'learning_rate': 3.324761660306215e-06, 'epoch': 0.74}
{'loss': 1.0623, 'grad_norm': 1.3918427228927612, 'learning_rate': 3.3201235403919683e-06, 'epoch': 0.74}
{'loss': 1.1906, 'grad_norm': 1.3005306720733643, 'learning_rate': 3.3154880138357626e-06, 'epoch': 0.74}
{'loss': 1.0526, 'grad_norm': 1.4655959606170654, 'learning_rate': 3.3108550824372632e-06, 'epoch': 0.74}
{'loss': 1.055, 'grad_norm': 1.5004690885543823, 'learning_rate': 3.306224747995136e-06, 'epoch': 0.74}
{'loss': 1.0042, 'grad_norm': 1.5619229078292847, 'learning_rate': 3.301597012307034e-06, 'epoch': 0.74}
{'loss': 1.064, 'grad_norm': 1.4215384721755981, 'learning_rate': 3.2969718771696047e-06, 'epoch': 0.74}
{'loss': 1.1929, 'grad_norm': 1.2436904907226562, 'learning_rate': 3.292349344378486e-06, 'epoch': 0.74}
{'loss': 1.1185, 'grad_norm': 1.2906783819198608, 'learning_rate': 3.287729415728298e-06, 'epoch': 0.74}
{'loss': 1.1102, 'grad_norm': 1.5924814939498901, 'learning_rate': 3.283112093012669e-06, 'epoch': 0.74}
{'loss': 1.0472, 'grad_norm': 1.5164742469787598, 'learning_rate': 3.278497378024187e-06, 'epoch': 0.74}
{'loss': 1.068, 'grad_norm': 1.4534947872161865, 'learning_rate': 3.2738852725544547e-06, 'epoch': 0.74}
{'loss': 1.1505, 'grad_norm': 1.5848175287246704, 'learning_rate': 3.2692757783940467e-06, 'epoch': 0.74}
{'loss': 1.1372, 'grad_norm': 1.59868323802948, 'learning_rate': 3.264668897332527e-06, 'epoch': 0.74}
{'loss': 0.9928, 'grad_norm': 1.5939297676086426, 'learning_rate': 3.2600646311584494e-06, 'epoch': 0.74}
{'loss': 1.1672, 'grad_norm': 1.2600740194320679, 'learning_rate': 3.2554629816593375e-06, 'epoch': 0.74}
{'loss': 1.1039, 'grad_norm': 1.5328048467636108, 'learning_rate': 3.250863950621721e-06, 'epoch': 0.74}
{'loss': 1.0467, 'grad_norm': 1.4128222465515137, 'learning_rate': 3.2462675398310984e-06, 'epoch': 0.74}
{'loss': 0.9907, 'grad_norm': 1.3187901973724365, 'learning_rate': 3.241673751071954e-06, 'epoch': 0.74}
{'loss': 1.0201, 'grad_norm': 1.5590951442718506, 'learning_rate': 3.2370825861277567e-06, 'epoch': 0.74}
{'loss': 1.1823, 'grad_norm': 1.5462702512741089, 'learning_rate': 3.2324940467809527e-06, 'epoch': 0.74}
{'loss': 1.1761, 'grad_norm': 1.5305696725845337, 'learning_rate': 3.2279081348129713e-06, 'epoch': 0.74}
{'loss': 1.1675, 'grad_norm': 1.3473061323165894, 'learning_rate': 3.223324852004219e-06, 'epoch': 0.75}
{'loss': 1.1505, 'grad_norm': 1.5754117965698242, 'learning_rate': 3.2187442001340942e-06, 'epoch': 0.75}
{'loss': 1.0371, 'grad_norm': 1.4119281768798828, 'learning_rate': 3.21416618098095e-06, 'epoch': 0.75}
{'loss': 1.1454, 'grad_norm': 1.544151782989502, 'learning_rate': 3.2095907963221396e-06, 'epoch': 0.75}
{'loss': 1.0287, 'grad_norm': 1.3590507507324219, 'learning_rate': 3.2050180479339865e-06, 'epoch': 0.75}
{'loss': 1.0471, 'grad_norm': 1.5570690631866455, 'learning_rate': 3.2004479375917783e-06, 'epoch': 0.75}
{'loss': 1.1953, 'grad_norm': 1.6174798011779785, 'learning_rate': 3.1958804670698008e-06, 'epoch': 0.75}
{'loss': 1.036, 'grad_norm': 1.500956654548645, 'learning_rate': 3.191315638141297e-06, 'epoch': 0.75}
{'loss': 1.1397, 'grad_norm': 1.4699965715408325, 'learning_rate': 3.1867534525784937e-06, 'epoch': 0.75}
{'loss': 1.149, 'grad_norm': 1.5073962211608887, 'learning_rate': 3.182193912152586e-06, 'epoch': 0.75}
{'loss': 1.0923, 'grad_norm': 1.720709204673767, 'learning_rate': 3.177637018633746e-06, 'epoch': 0.75}
{'loss': 1.1363, 'grad_norm': 1.6884229183197021, 'learning_rate': 3.1730827737911163e-06, 'epoch': 0.75}
{'loss': 1.0262, 'grad_norm': 1.507565975189209, 'learning_rate': 3.1685311793928077e-06, 'epoch': 0.75}
{'loss': 1.0371, 'grad_norm': 1.488067626953125, 'learning_rate': 3.163982237205917e-06, 'epoch': 0.75}
{'loss': 1.0496, 'grad_norm': 1.4416892528533936, 'learning_rate': 3.1594359489964853e-06, 'epoch': 0.75}
{'loss': 1.0483, 'grad_norm': 1.5742942094802856, 'learning_rate': 3.15489231652955e-06, 'epoch': 0.75}
{'loss': 1.0878, 'grad_norm': 1.3668248653411865, 'learning_rate': 3.150351341569101e-06, 'epoch': 0.75}
{'loss': 1.1233, 'grad_norm': 1.6031057834625244, 'learning_rate': 3.1458130258781006e-06, 'epoch': 0.75}
{'loss': 1.044, 'grad_norm': 1.4523524045944214, 'learning_rate': 3.141277371218484e-06, 'epoch': 0.75}
{'loss': 1.1802, 'grad_norm': 1.2128359079360962, 'learning_rate': 3.136744379351139e-06, 'epoch': 0.75}
{'loss': 1.1826, 'grad_norm': 1.2467631101608276, 'learning_rate': 3.1322140520359366e-06, 'epoch': 0.75}
{'loss': 1.0402, 'grad_norm': 1.4421592950820923, 'learning_rate': 3.1276863910317057e-06, 'epoch': 0.75}
{'loss': 1.1439, 'grad_norm': 1.5979881286621094, 'learning_rate': 3.1231613980962373e-06, 'epoch': 0.75}
{'loss': 1.1452, 'grad_norm': 1.4680452346801758, 'learning_rate': 3.1186390749862904e-06, 'epoch': 0.75}
{'loss': 1.2098, 'grad_norm': 1.2439326047897339, 'learning_rate': 3.1141194234575878e-06, 'epoch': 0.75}
{'loss': 1.076, 'grad_norm': 1.5195488929748535, 'learning_rate': 3.1096024452648123e-06, 'epoch': 0.75}
{'loss': 1.1328, 'grad_norm': 1.5328866243362427, 'learning_rate': 3.1050881421616076e-06, 'epoch': 0.75}
{'loss': 1.0158, 'grad_norm': 1.6022562980651855, 'learning_rate': 3.100576515900591e-06, 'epoch': 0.75}
{'loss': 0.9703, 'grad_norm': 1.434868574142456, 'learning_rate': 3.0960675682333186e-06, 'epoch': 0.75}
{'loss': 1.1539, 'grad_norm': 1.613823652267456, 'learning_rate': 3.0915613009103296e-06, 'epoch': 0.75}
{'loss': 1.1232, 'grad_norm': 1.528282880783081, 'learning_rate': 3.0870577156811077e-06, 'epoch': 0.75}
{'loss': 1.0854, 'grad_norm': 1.7574381828308105, 'learning_rate': 3.0825568142940998e-06, 'epoch': 0.75}
{'loss': 0.9773, 'grad_norm': 1.5065739154815674, 'learning_rate': 3.0780585984967113e-06, 'epoch': 0.75}
{'loss': 1.0037, 'grad_norm': 1.6124552488327026, 'learning_rate': 3.073563070035305e-06, 'epoch': 0.75}
{'loss': 1.0713, 'grad_norm': 1.415651798248291, 'learning_rate': 3.069070230655198e-06, 'epoch': 0.75}
{'loss': 1.181, 'grad_norm': 1.5021140575408936, 'learning_rate': 3.0645800821006667e-06, 'epoch': 0.75}
{'loss': 1.1461, 'grad_norm': 1.4729304313659668, 'learning_rate': 3.060092626114941e-06, 'epoch': 0.75}
{'loss': 1.1118, 'grad_norm': 1.4172898530960083, 'learning_rate': 3.0556078644402066e-06, 'epoch': 0.75}
{'loss': 1.0847, 'grad_norm': 1.5449559688568115, 'learning_rate': 3.051125798817598e-06, 'epoch': 0.75}
{'loss': 1.055, 'grad_norm': 1.4770969152450562, 'learning_rate': 3.0466464309872167e-06, 'epoch': 0.75}
{'loss': 1.0676, 'grad_norm': 1.6539103984832764, 'learning_rate': 3.042169762688096e-06, 'epoch': 0.75}
{'loss': 1.183, 'grad_norm': 1.5630630254745483, 'learning_rate': 3.0376957956582452e-06, 'epoch': 0.75}
{'loss': 1.164, 'grad_norm': 1.527295470237732, 'learning_rate': 3.0332245316346e-06, 'epoch': 0.75}
{'loss': 1.1147, 'grad_norm': 1.547542929649353, 'learning_rate': 3.0287559723530667e-06, 'epoch': 0.75}
{'loss': 1.1498, 'grad_norm': 1.5796550512313843, 'learning_rate': 3.024290119548495e-06, 'epoch': 0.75}
{'loss': 1.0541, 'grad_norm': 1.5548423528671265, 'learning_rate': 3.019826974954674e-06, 'epoch': 0.75}
{'loss': 1.0864, 'grad_norm': 1.472326397895813, 'learning_rate': 3.0153665403043586e-06, 'epoch': 0.75}
{'loss': 1.0771, 'grad_norm': 1.5478278398513794, 'learning_rate': 3.01090881732924e-06, 'epoch': 0.75}
{'loss': 1.0824, 'grad_norm': 1.5733903646469116, 'learning_rate': 3.0064538077599603e-06, 'epoch': 0.75}
{'loss': 1.0921, 'grad_norm': 1.4274524450302124, 'learning_rate': 3.002001513326107e-06, 'epoch': 0.75}
{'loss': 1.1607, 'grad_norm': 1.2222436666488647, 'learning_rate': 2.9975519357562155e-06, 'epoch': 0.75}
{'loss': 1.075, 'grad_norm': 1.4853068590164185, 'learning_rate': 2.9931050767777626e-06, 'epoch': 0.75}
{'loss': 1.043, 'grad_norm': 1.5661914348602295, 'learning_rate': 2.9886609381171703e-06, 'epoch': 0.76}
{'loss': 1.1612, 'grad_norm': 1.2735369205474854, 'learning_rate': 2.984219521499816e-06, 'epoch': 0.76}
{'loss': 1.022, 'grad_norm': 1.4609003067016602, 'learning_rate': 2.9797808286499976e-06, 'epoch': 0.76}
{'loss': 1.1497, 'grad_norm': 1.5540984869003296, 'learning_rate': 2.9753448612909775e-06, 'epoch': 0.76}
{'loss': 1.1028, 'grad_norm': 1.489466905593872, 'learning_rate': 2.9709116211449484e-06, 'epoch': 0.76}
{'loss': 1.1432, 'grad_norm': 1.4939051866531372, 'learning_rate': 2.966481109933047e-06, 'epoch': 0.76}
{'loss': 1.1344, 'grad_norm': 1.1912227869033813, 'learning_rate': 2.9620533293753495e-06, 'epoch': 0.76}
{'loss': 1.1246, 'grad_norm': 1.4967641830444336, 'learning_rate': 2.957628281190873e-06, 'epoch': 0.76}
{'loss': 1.2064, 'grad_norm': 1.4457318782806396, 'learning_rate': 2.9532059670975732e-06, 'epoch': 0.76}
{'loss': 1.0914, 'grad_norm': 1.4277390241622925, 'learning_rate': 2.948786388812346e-06, 'epoch': 0.76}
{'loss': 1.0909, 'grad_norm': 1.4905303716659546, 'learning_rate': 2.9443695480510225e-06, 'epoch': 0.76}
{'loss': 1.2416, 'grad_norm': 1.2974635362625122, 'learning_rate': 2.9399554465283742e-06, 'epoch': 0.76}
{'loss': 1.1756, 'grad_norm': 1.5221079587936401, 'learning_rate': 2.935544085958102e-06, 'epoch': 0.76}
{'loss': 1.093, 'grad_norm': 1.5148371458053589, 'learning_rate': 2.931135468052858e-06, 'epoch': 0.76}
{'loss': 1.0848, 'grad_norm': 1.4859143495559692, 'learning_rate': 2.926729594524207e-06, 'epoch': 0.76}
{'loss': 1.2391, 'grad_norm': 1.1806799173355103, 'learning_rate': 2.9223264670826746e-06, 'epoch': 0.76}
{'loss': 1.1726, 'grad_norm': 1.6447044610977173, 'learning_rate': 2.9179260874376915e-06, 'epoch': 0.76}
{'loss': 1.1579, 'grad_norm': 1.1892739534378052, 'learning_rate': 2.9135284572976486e-06, 'epoch': 0.76}
{'loss': 1.0914, 'grad_norm': 1.3588508367538452, 'learning_rate': 2.9091335783698517e-06, 'epoch': 0.76}
{'loss': 1.0826, 'grad_norm': 1.5797417163848877, 'learning_rate': 2.9047414523605467e-06, 'epoch': 0.76}
{'loss': 1.21, 'grad_norm': 1.6786044836044312, 'learning_rate': 2.9003520809749053e-06, 'epoch': 0.76}
{'loss': 1.114, 'grad_norm': 1.48721444606781, 'learning_rate': 2.8959654659170354e-06, 'epoch': 0.76}
{'loss': 1.1026, 'grad_norm': 1.5973498821258545, 'learning_rate': 2.8915816088899696e-06, 'epoch': 0.76}
{'loss': 1.1453, 'grad_norm': 1.5667333602905273, 'learning_rate': 2.8872005115956746e-06, 'epoch': 0.76}
{'loss': 1.0567, 'grad_norm': 1.695347785949707, 'learning_rate': 2.8828221757350406e-06, 'epoch': 0.76}
{'loss': 1.0623, 'grad_norm': 1.5507779121398926, 'learning_rate': 2.8784466030078905e-06, 'epoch': 0.76}
{'loss': 1.0934, 'grad_norm': 1.7143419981002808, 'learning_rate': 2.874073795112967e-06, 'epoch': 0.76}
{'loss': 1.0499, 'grad_norm': 1.664878010749817, 'learning_rate': 2.8697037537479565e-06, 'epoch': 0.76}
{'loss': 1.0702, 'grad_norm': 1.5804699659347534, 'learning_rate': 2.8653364806094454e-06, 'epoch': 0.76}
{'loss': 1.1933, 'grad_norm': 1.2511696815490723, 'learning_rate': 2.86097197739297e-06, 'epoch': 0.76}
{'loss': 1.0344, 'grad_norm': 1.562401294708252, 'learning_rate': 2.856610245792976e-06, 'epoch': 0.76}
{'loss': 1.1341, 'grad_norm': 1.6115436553955078, 'learning_rate': 2.8522512875028396e-06, 'epoch': 0.76}
{'loss': 1.0614, 'grad_norm': 1.5936402082443237, 'learning_rate': 2.847895104214856e-06, 'epoch': 0.76}
{'loss': 1.2011, 'grad_norm': 1.4705291986465454, 'learning_rate': 2.843541697620249e-06, 'epoch': 0.76}
{'loss': 1.1284, 'grad_norm': 1.4672282934188843, 'learning_rate': 2.8391910694091584e-06, 'epoch': 0.76}
{'loss': 1.1254, 'grad_norm': 1.0800598859786987, 'learning_rate': 2.8348432212706443e-06, 'epoch': 0.76}
{'loss': 1.1579, 'grad_norm': 1.4301120042800903, 'learning_rate': 2.8304981548927025e-06, 'epoch': 0.76}
{'loss': 1.1073, 'grad_norm': 1.524330973625183, 'learning_rate': 2.826155871962227e-06, 'epoch': 0.76}
{'loss': 0.9907, 'grad_norm': 1.5705468654632568, 'learning_rate': 2.8218163741650415e-06, 'epoch': 0.76}
{'loss': 1.2228, 'grad_norm': 1.2148263454437256, 'learning_rate': 2.817479663185898e-06, 'epoch': 0.76}
{'loss': 1.1117, 'grad_norm': 1.5375789403915405, 'learning_rate': 2.813145740708445e-06, 'epoch': 0.76}
{'loss': 1.1248, 'grad_norm': 1.5199824571609497, 'learning_rate': 2.808814608415271e-06, 'epoch': 0.76}
{'loss': 1.2033, 'grad_norm': 1.2423877716064453, 'learning_rate': 2.8044862679878605e-06, 'epoch': 0.76}
{'loss': 1.1212, 'grad_norm': 1.5325855016708374, 'learning_rate': 2.800160721106633e-06, 'epoch': 0.76}
WARNING: tokenization mismatch: 0 vs. 62. (ignored)
{'loss': 1.0769, 'grad_norm': 1.4833768606185913, 'learning_rate': 2.7958379694509108e-06, 'epoch': 0.76}
{'loss': 1.0734, 'grad_norm': 1.571000099182129, 'learning_rate': 2.791518014698935e-06, 'epoch': 0.76}
{'loss': 1.0956, 'grad_norm': 1.581436276435852, 'learning_rate': 2.787200858527862e-06, 'epoch': 0.76}
{'loss': 1.075, 'grad_norm': 1.6674264669418335, 'learning_rate': 2.7828865026137584e-06, 'epoch': 0.76}
{'loss': 1.1035, 'grad_norm': 1.5817115306854248, 'learning_rate': 2.7785749486316085e-06, 'epoch': 0.76}
{'loss': 1.1091, 'grad_norm': 1.4625957012176514, 'learning_rate': 2.774266198255303e-06, 'epoch': 0.76}
{'loss': 1.0751, 'grad_norm': 1.4432748556137085, 'learning_rate': 2.7699602531576496e-06, 'epoch': 0.76}
{'loss': 1.1392, 'grad_norm': 1.427322506904602, 'learning_rate': 2.765657115010364e-06, 'epoch': 0.76}
{'loss': 1.1329, 'grad_norm': 1.5700384378433228, 'learning_rate': 2.7613567854840685e-06, 'epoch': 0.77}
{'loss': 1.0755, 'grad_norm': 1.5736454725265503, 'learning_rate': 2.7570592662483086e-06, 'epoch': 0.77}
{'loss': 1.0397, 'grad_norm': 1.4650226831436157, 'learning_rate': 2.752764558971517e-06, 'epoch': 0.77}
{'loss': 1.0535, 'grad_norm': 1.5274583101272583, 'learning_rate': 2.748472665321056e-06, 'epoch': 0.77}
{'loss': 1.0309, 'grad_norm': 1.431400179862976, 'learning_rate': 2.744183586963185e-06, 'epoch': 0.77}
{'loss': 1.0815, 'grad_norm': 1.5645309686660767, 'learning_rate': 2.739897325563069e-06, 'epoch': 0.77}
{'loss': 1.1225, 'grad_norm': 1.473162293434143, 'learning_rate': 2.7356138827847856e-06, 'epoch': 0.77}
{'loss': 1.0346, 'grad_norm': 1.5032424926757812, 'learning_rate': 2.731333260291311e-06, 'epoch': 0.77}
{'loss': 1.1363, 'grad_norm': 1.5778605937957764, 'learning_rate': 2.7270554597445343e-06, 'epoch': 0.77}
{'loss': 1.1009, 'grad_norm': 1.5323505401611328, 'learning_rate': 2.7227804828052384e-06, 'epoch': 0.77}
{'loss': 1.2263, 'grad_norm': 1.5813299417495728, 'learning_rate': 2.7185083311331283e-06, 'epoch': 0.77}
{'loss': 1.0618, 'grad_norm': 1.584076166152954, 'learning_rate': 2.7142390063867896e-06, 'epoch': 0.77}
{'loss': 1.0938, 'grad_norm': 1.619052529335022, 'learning_rate': 2.709972510223725e-06, 'epoch': 0.77}
{'loss': 1.1187, 'grad_norm': 1.5112935304641724, 'learning_rate': 2.7057088443003343e-06, 'epoch': 0.77}
{'loss': 1.1021, 'grad_norm': 1.4285153150558472, 'learning_rate': 2.7014480102719174e-06, 'epoch': 0.77}
{'loss': 1.0129, 'grad_norm': 1.4356821775436401, 'learning_rate': 2.697190009792685e-06, 'epoch': 0.77}
{'loss': 1.023, 'grad_norm': 1.4718060493469238, 'learning_rate': 2.692934844515729e-06, 'epoch': 0.77}
{'loss': 1.1186, 'grad_norm': 1.5569159984588623, 'learning_rate': 2.6886825160930587e-06, 'epoch': 0.77}
{'loss': 0.9995, 'grad_norm': 1.4612833261489868, 'learning_rate': 2.6844330261755715e-06, 'epoch': 0.77}
{'loss': 1.0682, 'grad_norm': 1.4065444469451904, 'learning_rate': 2.6801863764130653e-06, 'epoch': 0.77}
{'loss': 1.12, 'grad_norm': 1.63858163356781, 'learning_rate': 2.675942568454236e-06, 'epoch': 0.77}
{'loss': 1.1568, 'grad_norm': 1.7606942653656006, 'learning_rate': 2.671701603946678e-06, 'epoch': 0.77}
{'loss': 1.1097, 'grad_norm': 1.6451208591461182, 'learning_rate': 2.667463484536876e-06, 'epoch': 0.77}
{'loss': 1.0194, 'grad_norm': 1.5599619150161743, 'learning_rate': 2.6632282118702147e-06, 'epoch': 0.77}
{'loss': 1.1179, 'grad_norm': 1.4672614336013794, 'learning_rate': 2.65899578759098e-06, 'epoch': 0.77}
{'loss': 1.094, 'grad_norm': 1.192589521408081, 'learning_rate': 2.654766213342335e-06, 'epoch': 0.77}
{'loss': 1.0795, 'grad_norm': 1.4514206647872925, 'learning_rate': 2.650539490766346e-06, 'epoch': 0.77}
{'loss': 1.1038, 'grad_norm': 1.4490797519683838, 'learning_rate': 2.646315621503983e-06, 'epoch': 0.77}
{'loss': 1.0939, 'grad_norm': 1.4754137992858887, 'learning_rate': 2.642094607195085e-06, 'epoch': 0.77}
{'loss': 1.1714, 'grad_norm': 1.560091495513916, 'learning_rate': 2.6378764494784027e-06, 'epoch': 0.77}
{'loss': 1.0167, 'grad_norm': 1.4447144269943237, 'learning_rate': 2.633661149991569e-06, 'epoch': 0.77}
{'loss': 1.0571, 'grad_norm': 1.4652228355407715, 'learning_rate': 2.6294487103711064e-06, 'epoch': 0.77}
{'loss': 1.1016, 'grad_norm': 1.587199091911316, 'learning_rate': 2.6252391322524297e-06, 'epoch': 0.77}
{'loss': 1.0382, 'grad_norm': 1.4543763399124146, 'learning_rate': 2.6210324172698432e-06, 'epoch': 0.77}
{'loss': 1.1798, 'grad_norm': 1.253894567489624, 'learning_rate': 2.6168285670565374e-06, 'epoch': 0.77}
{'loss': 1.2038, 'grad_norm': 1.2848297357559204, 'learning_rate': 2.6126275832445892e-06, 'epoch': 0.77}
{'loss': 1.1361, 'grad_norm': 1.4824984073638916, 'learning_rate': 2.6084294674649734e-06, 'epoch': 0.77}
{'loss': 1.0953, 'grad_norm': 1.5856034755706787, 'learning_rate': 2.6042342213475346e-06, 'epoch': 0.77}
{'loss': 1.1022, 'grad_norm': 1.4272184371948242, 'learning_rate': 2.6000418465210143e-06, 'epoch': 0.77}
{'loss': 1.1652, 'grad_norm': 1.4400498867034912, 'learning_rate': 2.595852344613038e-06, 'epoch': 0.77}
{'loss': 1.0685, 'grad_norm': 1.574641466140747, 'learning_rate': 2.5916657172501103e-06, 'epoch': 0.77}
{'loss': 1.0525, 'grad_norm': 1.5028809309005737, 'learning_rate': 2.587481966057633e-06, 'epoch': 0.77}
{'loss': 1.064, 'grad_norm': 1.4715787172317505, 'learning_rate': 2.583301092659872e-06, 'epoch': 0.77}
{'loss': 1.0561, 'grad_norm': 1.3469609022140503, 'learning_rate': 2.5791230986799944e-06, 'epoch': 0.77}
{'loss': 1.0982, 'grad_norm': 1.5316828489303589, 'learning_rate': 2.5749479857400383e-06, 'epoch': 0.77}
{'loss': 1.1519, 'grad_norm': 1.481380820274353, 'learning_rate': 2.5707757554609247e-06, 'epoch': 0.77}
{'loss': 1.0874, 'grad_norm': 1.4412941932678223, 'learning_rate': 2.56660640946246e-06, 'epoch': 0.77}
{'loss': 1.0388, 'grad_norm': 1.3443752527236938, 'learning_rate': 2.5624399493633257e-06, 'epoch': 0.77}
{'loss': 1.0923, 'grad_norm': 1.5973258018493652, 'learning_rate': 2.558276376781086e-06, 'epoch': 0.77}
{'loss': 1.1134, 'grad_norm': 1.5605075359344482, 'learning_rate': 2.55411569333218e-06, 'epoch': 0.77}
{'loss': 1.0791, 'grad_norm': 1.5168848037719727, 'learning_rate': 2.5499579006319365e-06, 'epoch': 0.77}
{'loss': 1.1254, 'grad_norm': 1.5441845655441284, 'learning_rate': 2.5458030002945457e-06, 'epoch': 0.77}
{'loss': 1.062, 'grad_norm': 1.3692899942398071, 'learning_rate': 2.5416509939330836e-06, 'epoch': 0.78}
{'loss': 1.1514, 'grad_norm': 1.216164469718933, 'learning_rate': 2.537501883159509e-06, 'epoch': 0.78}
{'loss': 1.108, 'grad_norm': 1.7346688508987427, 'learning_rate': 2.5333556695846384e-06, 'epoch': 0.78}
{'loss': 1.1431, 'grad_norm': 1.3155721426010132, 'learning_rate': 2.5292123548181847e-06, 'epoch': 0.78}
{'loss': 1.1429, 'grad_norm': 1.5643811225891113, 'learning_rate': 2.525071940468722e-06, 'epoch': 0.78}
{'loss': 1.1394, 'grad_norm': 1.5381228923797607, 'learning_rate': 2.520934428143701e-06, 'epoch': 0.78}
{'loss': 1.0051, 'grad_norm': 1.473513126373291, 'learning_rate': 2.5167998194494468e-06, 'epoch': 0.78}
{'loss': 1.0101, 'grad_norm': 1.3883572816848755, 'learning_rate': 2.5126681159911558e-06, 'epoch': 0.78}
{'loss': 1.0587, 'grad_norm': 1.4369367361068726, 'learning_rate': 2.5085393193729e-06, 'epoch': 0.78}
{'loss': 1.1312, 'grad_norm': 1.7050909996032715, 'learning_rate': 2.5044134311976156e-06, 'epoch': 0.78}
{'loss': 1.2109, 'grad_norm': 1.6505409479141235, 'learning_rate': 2.5002904530671236e-06, 'epoch': 0.78}
{'loss': 1.1833, 'grad_norm': 1.2707971334457397, 'learning_rate': 2.4961703865820974e-06, 'epoch': 0.78}
{'loss': 1.1702, 'grad_norm': 1.256281852722168, 'learning_rate': 2.492053233342091e-06, 'epoch': 0.78}
{'loss': 1.0252, 'grad_norm': 1.6079224348068237, 'learning_rate': 2.487938994945527e-06, 'epoch': 0.78}
{'loss': 1.0893, 'grad_norm': 1.459409475326538, 'learning_rate': 2.4838276729896884e-06, 'epoch': 0.78}
{'loss': 1.1632, 'grad_norm': 1.2979172468185425, 'learning_rate': 2.479719269070743e-06, 'epoch': 0.78}
{'loss': 1.1264, 'grad_norm': 1.417757511138916, 'learning_rate': 2.4756137847837025e-06, 'epoch': 0.78}
{'loss': 1.1025, 'grad_norm': 1.5692918300628662, 'learning_rate': 2.4715112217224657e-06, 'epoch': 0.78}
{'loss': 1.2069, 'grad_norm': 1.2572569847106934, 'learning_rate': 2.467411581479786e-06, 'epoch': 0.78}
{'loss': 1.1141, 'grad_norm': 1.536160945892334, 'learning_rate': 2.463314865647286e-06, 'epoch': 0.78}
{'loss': 1.0601, 'grad_norm': 1.676383137702942, 'learning_rate': 2.45922107581545e-06, 'epoch': 0.78}
{'loss': 1.08, 'grad_norm': 1.511314034461975, 'learning_rate': 2.4551302135736287e-06, 'epoch': 0.78}
{'loss': 1.0707, 'grad_norm': 1.5745497941970825, 'learning_rate': 2.4510422805100366e-06, 'epoch': 0.78}
{'loss': 1.1837, 'grad_norm': 1.6029669046401978, 'learning_rate': 2.446957278211746e-06, 'epoch': 0.78}
{'loss': 1.1201, 'grad_norm': 1.5772055387496948, 'learning_rate': 2.4428752082647044e-06, 'epoch': 0.78}
{'loss': 1.1946, 'grad_norm': 1.200864315032959, 'learning_rate': 2.438796072253704e-06, 'epoch': 0.78}
{'loss': 1.055, 'grad_norm': 1.639488935470581, 'learning_rate': 2.4347198717624054e-06, 'epoch': 0.78}
{'loss': 1.1984, 'grad_norm': 1.2128324508666992, 'learning_rate': 2.4306466083733392e-06, 'epoch': 0.78}
{'loss': 1.0769, 'grad_norm': 1.5750631093978882, 'learning_rate': 2.426576283667873e-06, 'epoch': 0.78}
{'loss': 0.9898, 'grad_norm': 1.4919798374176025, 'learning_rate': 2.422508899226258e-06, 'epoch': 0.78}
{'loss': 1.151, 'grad_norm': 1.3538063764572144, 'learning_rate': 2.418444456627589e-06, 'epoch': 0.78}
{'loss': 1.1181, 'grad_norm': 1.4022735357284546, 'learning_rate': 2.4143829574498224e-06, 'epoch': 0.78}
{'loss': 0.9931, 'grad_norm': 1.5304862260818481, 'learning_rate': 2.4103244032697717e-06, 'epoch': 0.78}
{'loss': 1.0713, 'grad_norm': 1.4527204036712646, 'learning_rate': 2.406268795663108e-06, 'epoch': 0.78}
{'loss': 1.2249, 'grad_norm': 1.2995797395706177, 'learning_rate': 2.4022161362043574e-06, 'epoch': 0.78}
{'loss': 1.0847, 'grad_norm': 1.3381102085113525, 'learning_rate': 2.3981664264669025e-06, 'epoch': 0.78}
{'loss': 1.0814, 'grad_norm': 1.5222265720367432, 'learning_rate': 2.3941196680229794e-06, 'epoch': 0.78}
{'loss': 1.0656, 'grad_norm': 1.4139798879623413, 'learning_rate': 2.3900758624436772e-06, 'epoch': 0.78}
{'loss': 1.1921, 'grad_norm': 1.5870720148086548, 'learning_rate': 2.3860350112989473e-06, 'epoch': 0.78}
{'loss': 1.1022, 'grad_norm': 1.5514570474624634, 'learning_rate': 2.3819971161575807e-06, 'epoch': 0.78}
{'loss': 1.1178, 'grad_norm': 1.4176667928695679, 'learning_rate': 2.3779621785872252e-06, 'epoch': 0.78}
{'loss': 1.031, 'grad_norm': 1.5781254768371582, 'learning_rate': 2.3739302001543918e-06, 'epoch': 0.78}
{'loss': 1.0227, 'grad_norm': 1.4917199611663818, 'learning_rate': 2.3699011824244234e-06, 'epoch': 0.78}
{'loss': 1.2196, 'grad_norm': 1.313482403755188, 'learning_rate': 2.365875126961531e-06, 'epoch': 0.78}
{'loss': 1.1398, 'grad_norm': 1.4270380735397339, 'learning_rate': 2.3618520353287644e-06, 'epoch': 0.78}
{'loss': 1.0649, 'grad_norm': 1.4905521869659424, 'learning_rate': 2.3578319090880263e-06, 'epoch': 0.78}
{'loss': 1.0828, 'grad_norm': 1.513295292854309, 'learning_rate': 2.3538147498000695e-06, 'epoch': 0.78}
{'loss': 1.094, 'grad_norm': 1.5749903917312622, 'learning_rate': 2.349800559024492e-06, 'epoch': 0.78}
{'loss': 1.0062, 'grad_norm': 1.544312596321106, 'learning_rate': 2.3457893383197415e-06, 'epoch': 0.78}
{'loss': 0.9758, 'grad_norm': 1.4300472736358643, 'learning_rate': 2.3417810892431104e-06, 'epoch': 0.78}
{'loss': 1.1274, 'grad_norm': 1.3354544639587402, 'learning_rate': 2.3377758133507455e-06, 'epoch': 0.78}
{'loss': 1.2324, 'grad_norm': 1.2343868017196655, 'learning_rate': 2.3337735121976247e-06, 'epoch': 0.78}
{'loss': 1.1721, 'grad_norm': 1.5373122692108154, 'learning_rate': 2.32977418733758e-06, 'epoch': 0.79}
{'loss': 1.1222, 'grad_norm': 1.549160122871399, 'learning_rate': 2.3257778403232954e-06, 'epoch': 0.79}
{'loss': 1.0382, 'grad_norm': 1.5567225217819214, 'learning_rate': 2.321784472706279e-06, 'epoch': 0.79}
{'loss': 1.1047, 'grad_norm': 1.5499218702316284, 'learning_rate': 2.317794086036901e-06, 'epoch': 0.79}
{'loss': 1.1136, 'grad_norm': 1.5086336135864258, 'learning_rate': 2.3138066818643647e-06, 'epoch': 0.79}
{'loss': 1.0208, 'grad_norm': 1.4443548917770386, 'learning_rate': 2.3098222617367184e-06, 'epoch': 0.79}
{'loss': 1.1471, 'grad_norm': 1.3754231929779053, 'learning_rate': 2.30584082720085e-06, 'epoch': 0.79}
{'loss': 1.0876, 'grad_norm': 1.3983145952224731, 'learning_rate': 2.301862379802492e-06, 'epoch': 0.79}
{'loss': 1.1281, 'grad_norm': 1.521650791168213, 'learning_rate': 2.297886921086211e-06, 'epoch': 0.79}
{'loss': 1.0882, 'grad_norm': 1.45219087600708, 'learning_rate': 2.2939144525954194e-06, 'epoch': 0.79}
{'loss': 1.0702, 'grad_norm': 1.4143160581588745, 'learning_rate': 2.2899449758723657e-06, 'epoch': 0.79}
{'loss': 1.0786, 'grad_norm': 1.524633765220642, 'learning_rate': 2.285978492458134e-06, 'epoch': 0.79}
{'loss': 1.0909, 'grad_norm': 1.499186396598816, 'learning_rate': 2.282015003892659e-06, 'epoch': 0.79}
{'loss': 1.1668, 'grad_norm': 1.4121183156967163, 'learning_rate': 2.2780545117146947e-06, 'epoch': 0.79}
{'loss': 1.0092, 'grad_norm': 1.4192951917648315, 'learning_rate': 2.2740970174618405e-06, 'epoch': 0.79}
{'loss': 1.0959, 'grad_norm': 1.6457072496414185, 'learning_rate': 2.270142522670541e-06, 'epoch': 0.79}
{'loss': 1.0178, 'grad_norm': 1.3983914852142334, 'learning_rate': 2.2661910288760545e-06, 'epoch': 0.79}
{'loss': 1.1478, 'grad_norm': 1.606170654296875, 'learning_rate': 2.262242537612497e-06, 'epoch': 0.79}
{'loss': 1.08, 'grad_norm': 1.6064540147781372, 'learning_rate': 2.258297050412804e-06, 'epoch': 0.79}
{'loss': 1.1252, 'grad_norm': 1.7488865852355957, 'learning_rate': 2.254354568808752e-06, 'epoch': 0.79}
{'loss': 1.0918, 'grad_norm': 1.4587602615356445, 'learning_rate': 2.2504150943309455e-06, 'epoch': 0.79}
{'loss': 1.1684, 'grad_norm': 1.6206306219100952, 'learning_rate': 2.246478628508827e-06, 'epoch': 0.79}
{'loss': 1.1583, 'grad_norm': 1.225302815437317, 'learning_rate': 2.242545172870665e-06, 'epoch': 0.79}
{'loss': 1.0793, 'grad_norm': 1.4909909963607788, 'learning_rate': 2.238614728943561e-06, 'epoch': 0.79}
{'loss': 1.1192, 'grad_norm': 1.3597115278244019, 'learning_rate': 2.2346872982534584e-06, 'epoch': 0.79}
{'loss': 1.0048, 'grad_norm': 1.4357558488845825, 'learning_rate': 2.2307628823251083e-06, 'epoch': 0.79}
{'loss': 1.1051, 'grad_norm': 1.5556142330169678, 'learning_rate': 2.2268414826821117e-06, 'epoch': 0.79}
{'loss': 1.2397, 'grad_norm': 1.323064923286438, 'learning_rate': 2.222923100846893e-06, 'epoch': 0.79}
{'loss': 1.1064, 'grad_norm': 1.548284888267517, 'learning_rate': 2.2190077383406938e-06, 'epoch': 0.79}
{'loss': 1.0916, 'grad_norm': 1.4859364032745361, 'learning_rate': 2.2150953966835996e-06, 'epoch': 0.79}
{'loss': 1.1197, 'grad_norm': 1.537562370300293, 'learning_rate': 2.211186077394516e-06, 'epoch': 0.79}
{'loss': 1.0586, 'grad_norm': 1.4448484182357788, 'learning_rate': 2.207279781991173e-06, 'epoch': 0.79}
{'loss': 1.0456, 'grad_norm': 1.5301973819732666, 'learning_rate': 2.2033765119901294e-06, 'epoch': 0.79}
{'loss': 1.2274, 'grad_norm': 1.3325594663619995, 'learning_rate': 2.1994762689067705e-06, 'epoch': 0.79}
{'loss': 1.0009, 'grad_norm': 1.4124763011932373, 'learning_rate': 2.1955790542553036e-06, 'epoch': 0.79}
{'loss': 1.121, 'grad_norm': 1.4874217510223389, 'learning_rate': 2.1916848695487615e-06, 'epoch': 0.79}
{'loss': 1.121, 'grad_norm': 1.5776219367980957, 'learning_rate': 2.1877937162990015e-06, 'epoch': 0.79}
{'loss': 1.0508, 'grad_norm': 1.5367364883422852, 'learning_rate': 2.1839055960167e-06, 'epoch': 0.79}
{'loss': 1.2241, 'grad_norm': 1.3353825807571411, 'learning_rate': 2.180020510211367e-06, 'epoch': 0.79}
{'loss': 1.1166, 'grad_norm': 1.5199421644210815, 'learning_rate': 2.1761384603913203e-06, 'epoch': 0.79}
{'loss': 1.0832, 'grad_norm': 1.5102689266204834, 'learning_rate': 2.172259448063704e-06, 'epoch': 0.79}
{'loss': 1.0927, 'grad_norm': 1.4460134506225586, 'learning_rate': 2.1683834747344913e-06, 'epoch': 0.79}
{'loss': 1.0202, 'grad_norm': 1.5159600973129272, 'learning_rate': 2.1645105419084587e-06, 'epoch': 0.79}
{'loss': 1.1744, 'grad_norm': 1.19314706325531, 'learning_rate': 2.160640651089221e-06, 'epoch': 0.79}
{'loss': 1.0066, 'grad_norm': 1.5024124383926392, 'learning_rate': 2.1567738037791998e-06, 'epoch': 0.79}
Error with image file is truncated (35 bytes not processed)
{'loss': 1.0301, 'grad_norm': 1.362157940864563, 'learning_rate': 2.152910001479638e-06, 'epoch': 0.79}
{'loss': 1.0462, 'grad_norm': 1.3547744750976562, 'learning_rate': 2.1490492456905964e-06, 'epoch': 0.79}
{'loss': 1.0664, 'grad_norm': 1.4939500093460083, 'learning_rate': 2.1451915379109546e-06, 'epoch': 0.79}
{'loss': 1.0476, 'grad_norm': 1.5101923942565918, 'learning_rate': 2.141336879638406e-06, 'epoch': 0.79}
{'loss': 1.0634, 'grad_norm': 1.5319113731384277, 'learning_rate': 2.1374852723694595e-06, 'epoch': 0.79}
{'loss': 1.0354, 'grad_norm': 1.5178889036178589, 'learning_rate': 2.133636717599451e-06, 'epoch': 0.79}
{'loss': 1.2213, 'grad_norm': 1.6010721921920776, 'learning_rate': 2.1297912168225086e-06, 'epoch': 0.79}
{'loss': 1.2249, 'grad_norm': 1.1814407110214233, 'learning_rate': 2.1259487715316e-06, 'epoch': 0.8}
{'loss': 1.2295, 'grad_norm': 1.216188907623291, 'learning_rate': 2.1221093832184903e-06, 'epoch': 0.8}
{'loss': 1.0343, 'grad_norm': 1.50619375705719, 'learning_rate': 2.118273053373757e-06, 'epoch': 0.8}
{'loss': 1.1439, 'grad_norm': 1.4656753540039062, 'learning_rate': 2.1144397834868034e-06, 'epoch': 0.8}
{'loss': 1.171, 'grad_norm': 1.232334017753601, 'learning_rate': 2.1106095750458332e-06, 'epoch': 0.8}
{'loss': 1.118, 'grad_norm': 1.6145899295806885, 'learning_rate': 2.106782429537866e-06, 'epoch': 0.8}
{'loss': 1.0275, 'grad_norm': 1.5623606443405151, 'learning_rate': 2.1029583484487315e-06, 'epoch': 0.8}
{'loss': 1.0217, 'grad_norm': 1.389736533164978, 'learning_rate': 2.0991373332630683e-06, 'epoch': 0.8}
{'loss': 1.1305, 'grad_norm': 1.570239543914795, 'learning_rate': 2.0953193854643274e-06, 'epoch': 0.8}
{'loss': 1.1119, 'grad_norm': 1.737870693206787, 'learning_rate': 2.0915045065347673e-06, 'epoch': 0.8}
{'loss': 1.0976, 'grad_norm': 1.5063804388046265, 'learning_rate': 2.0876926979554545e-06, 'epoch': 0.8}
{'loss': 1.1063, 'grad_norm': 1.444495439529419, 'learning_rate': 2.0838839612062633e-06, 'epoch': 0.8}
{'loss': 1.0481, 'grad_norm': 1.5663928985595703, 'learning_rate': 2.080078297765884e-06, 'epoch': 0.8}
{'loss': 1.1485, 'grad_norm': 1.419445514678955, 'learning_rate': 2.0762757091117937e-06, 'epoch': 0.8}
{'loss': 1.0943, 'grad_norm': 1.4685323238372803, 'learning_rate': 2.0724761967202987e-06, 'epoch': 0.8}
{'loss': 1.053, 'grad_norm': 1.4787263870239258, 'learning_rate': 2.0686797620664987e-06, 'epoch': 0.8}
{'loss': 1.1838, 'grad_norm': 1.6116037368774414, 'learning_rate': 2.0648864066242937e-06, 'epoch': 0.8}
{'loss': 1.1482, 'grad_norm': 1.3703107833862305, 'learning_rate': 2.0610961318664013e-06, 'epoch': 0.8}
{'loss': 1.1188, 'grad_norm': 1.5086687803268433, 'learning_rate': 2.0573089392643362e-06, 'epoch': 0.8}
{'loss': 1.1476, 'grad_norm': 1.1353819370269775, 'learning_rate': 2.0535248302884147e-06, 'epoch': 0.8}
{'loss': 1.1598, 'grad_norm': 1.572373390197754, 'learning_rate': 2.0497438064077603e-06, 'epoch': 0.8}
{'loss': 1.0668, 'grad_norm': 1.4691119194030762, 'learning_rate': 2.045965869090295e-06, 'epoch': 0.8}
{'loss': 1.0716, 'grad_norm': 1.4292032718658447, 'learning_rate': 2.0421910198027452e-06, 'epoch': 0.8}
{'loss': 1.1321, 'grad_norm': 1.5161700248718262, 'learning_rate': 2.0384192600106335e-06, 'epoch': 0.8}
{'loss': 1.1458, 'grad_norm': 1.5175853967666626, 'learning_rate': 2.0346505911782956e-06, 'epoch': 0.8}
{'loss': 1.1175, 'grad_norm': 1.4831202030181885, 'learning_rate': 2.0308850147688484e-06, 'epoch': 0.8}
{'loss': 1.117, 'grad_norm': 1.4401841163635254, 'learning_rate': 2.0271225322442255e-06, 'epoch': 0.8}
{'loss': 1.1409, 'grad_norm': 1.4249144792556763, 'learning_rate': 2.0233631450651525e-06, 'epoch': 0.8}
{'loss': 1.025, 'grad_norm': 1.5191987752914429, 'learning_rate': 2.019606854691145e-06, 'epoch': 0.8}
{'loss': 1.1462, 'grad_norm': 1.6185922622680664, 'learning_rate': 2.0158536625805325e-06, 'epoch': 0.8}
{'loss': 1.1097, 'grad_norm': 1.479183554649353, 'learning_rate': 2.01210357019043e-06, 'epoch': 0.8}
{'loss': 1.14, 'grad_norm': 1.4575514793395996, 'learning_rate': 2.008356578976752e-06, 'epoch': 0.8}
{'loss': 1.0316, 'grad_norm': 1.5264172554016113, 'learning_rate': 2.004612690394212e-06, 'epoch': 0.8}
{'loss': 1.1943, 'grad_norm': 1.651825189590454, 'learning_rate': 2.0008719058963144e-06, 'epoch': 0.8}
{'loss': 1.0816, 'grad_norm': 1.3983094692230225, 'learning_rate': 1.997134226935361e-06, 'epoch': 0.8}
{'loss': 1.1258, 'grad_norm': 1.650178074836731, 'learning_rate': 1.9933996549624468e-06, 'epoch': 0.8}
{'loss': 1.0564, 'grad_norm': 1.4617987871170044, 'learning_rate': 1.9896681914274616e-06, 'epoch': 0.8}
{'loss': 1.1638, 'grad_norm': 1.4979822635650635, 'learning_rate': 1.9859398377790872e-06, 'epoch': 0.8}
{'loss': 1.1582, 'grad_norm': 1.644710898399353, 'learning_rate': 1.982214595464804e-06, 'epoch': 0.8}
{'loss': 1.1038, 'grad_norm': 1.6519496440887451, 'learning_rate': 1.97849246593087e-06, 'epoch': 0.8}
{'loss': 1.1087, 'grad_norm': 1.4097034931182861, 'learning_rate': 1.9747734506223525e-06, 'epoch': 0.8}
{'loss': 1.1848, 'grad_norm': 1.6105870008468628, 'learning_rate': 1.9710575509831008e-06, 'epoch': 0.8}
{'loss': 1.1914, 'grad_norm': 1.2083971500396729, 'learning_rate': 1.967344768455747e-06, 'epoch': 0.8}
{'loss': 1.1389, 'grad_norm': 1.463090181350708, 'learning_rate': 1.9636351044817292e-06, 'epoch': 0.8}
{'loss': 1.1178, 'grad_norm': 1.6332364082336426, 'learning_rate': 1.9599285605012643e-06, 'epoch': 0.8}
{'loss': 1.0488, 'grad_norm': 1.407472848892212, 'learning_rate': 1.9562251379533593e-06, 'epoch': 0.8}
{'loss': 1.1299, 'grad_norm': 1.6137990951538086, 'learning_rate': 1.952524838275811e-06, 'epoch': 0.8}
{'loss': 1.1638, 'grad_norm': 1.4439935684204102, 'learning_rate': 1.9488276629052026e-06, 'epoch': 0.8}
{'loss': 1.1023, 'grad_norm': 1.4704807996749878, 'learning_rate': 1.945133613276907e-06, 'epoch': 0.8}
{'loss': 0.9876, 'grad_norm': 1.5193135738372803, 'learning_rate': 1.941442690825076e-06, 'epoch': 0.8}
{'loss': 1.0603, 'grad_norm': 1.4606331586837769, 'learning_rate': 1.937754896982663e-06, 'epoch': 0.8}
{'loss': 1.1286, 'grad_norm': 1.4916186332702637, 'learning_rate': 1.9340702331813842e-06, 'epoch': 0.8}
{'loss': 1.1586, 'grad_norm': 1.534453272819519, 'learning_rate': 1.9303887008517618e-06, 'epoch': 0.81}
{'loss': 1.0114, 'grad_norm': 1.4509299993515015, 'learning_rate': 1.9267103014230935e-06, 'epoch': 0.81}
{'loss': 1.0407, 'grad_norm': 1.4229421615600586, 'learning_rate': 1.923035036323452e-06, 'epoch': 0.81}
{'loss': 1.205, 'grad_norm': 1.2755041122436523, 'learning_rate': 1.91936290697971e-06, 'epoch': 0.81}
{'loss': 1.1694, 'grad_norm': 1.6154212951660156, 'learning_rate': 1.9156939148175125e-06, 'epoch': 0.81}
{'loss': 1.0576, 'grad_norm': 1.5432884693145752, 'learning_rate': 1.9120280612612873e-06, 'epoch': 0.81}
{'loss': 1.0758, 'grad_norm': 1.3909204006195068, 'learning_rate': 1.9083653477342467e-06, 'epoch': 0.81}
{'loss': 1.0238, 'grad_norm': 1.4669266939163208, 'learning_rate': 1.904705775658381e-06, 'epoch': 0.81}
{'loss': 1.0562, 'grad_norm': 1.5921387672424316, 'learning_rate': 1.9010493464544621e-06, 'epoch': 0.81}
{'loss': 1.2304, 'grad_norm': 1.2300747632980347, 'learning_rate': 1.8973960615420416e-06, 'epoch': 0.81}
{'loss': 1.1542, 'grad_norm': 1.5305215120315552, 'learning_rate': 1.8937459223394517e-06, 'epoch': 0.81}
{'loss': 1.1364, 'grad_norm': 1.3039159774780273, 'learning_rate': 1.8900989302637985e-06, 'epoch': 0.81}
{'loss': 1.0557, 'grad_norm': 1.464746117591858, 'learning_rate': 1.8864550867309771e-06, 'epoch': 0.81}
{'loss': 1.0846, 'grad_norm': 1.5290805101394653, 'learning_rate': 1.8828143931556442e-06, 'epoch': 0.81}
{'loss': 1.0582, 'grad_norm': 1.442352533340454, 'learning_rate': 1.8791768509512487e-06, 'epoch': 0.81}
{'loss': 1.1432, 'grad_norm': 1.499404788017273, 'learning_rate': 1.875542461530011e-06, 'epoch': 0.81}
{'loss': 1.1947, 'grad_norm': 1.2930171489715576, 'learning_rate': 1.871911226302917e-06, 'epoch': 0.81}
{'loss': 1.2293, 'grad_norm': 1.2377206087112427, 'learning_rate': 1.868283146679747e-06, 'epoch': 0.81}
{'loss': 1.0085, 'grad_norm': 1.4916510581970215, 'learning_rate': 1.8646582240690414e-06, 'epoch': 0.81}
{'loss': 1.1386, 'grad_norm': 1.440614104270935, 'learning_rate': 1.8610364598781227e-06, 'epoch': 0.81}
{'loss': 1.1553, 'grad_norm': 1.6676393747329712, 'learning_rate': 1.8574178555130818e-06, 'epoch': 0.81}
{'loss': 1.0579, 'grad_norm': 1.3885066509246826, 'learning_rate': 1.8538024123787868e-06, 'epoch': 0.81}
{'loss': 1.213, 'grad_norm': 1.5246632099151611, 'learning_rate': 1.8501901318788773e-06, 'epoch': 0.81}
{'loss': 1.0815, 'grad_norm': 1.4838075637817383, 'learning_rate': 1.8465810154157626e-06, 'epoch': 0.81}
{'loss': 1.0409, 'grad_norm': 1.4526236057281494, 'learning_rate': 1.8429750643906331e-06, 'epoch': 0.81}
{'loss': 1.0368, 'grad_norm': 1.5230506658554077, 'learning_rate': 1.8393722802034331e-06, 'epoch': 0.81}
{'loss': 1.1294, 'grad_norm': 1.4611181020736694, 'learning_rate': 1.835772664252895e-06, 'epoch': 0.81}
{'loss': 1.1857, 'grad_norm': 1.5097144842147827, 'learning_rate': 1.832176217936511e-06, 'epoch': 0.81}
{'loss': 1.0721, 'grad_norm': 1.4611103534698486, 'learning_rate': 1.8285829426505453e-06, 'epoch': 0.81}
{'loss': 1.0852, 'grad_norm': 1.5522451400756836, 'learning_rate': 1.8249928397900351e-06, 'epoch': 0.81}
{'loss': 1.121, 'grad_norm': 1.4914627075195312, 'learning_rate': 1.8214059107487726e-06, 'epoch': 0.81}
{'loss': 1.0925, 'grad_norm': 1.5678863525390625, 'learning_rate': 1.8178221569193343e-06, 'epoch': 0.81}
{'loss': 1.1238, 'grad_norm': 1.5502204895019531, 'learning_rate': 1.8142415796930568e-06, 'epoch': 0.81}
{'loss': 1.1375, 'grad_norm': 1.6015772819519043, 'learning_rate': 1.8106641804600411e-06, 'epoch': 0.81}
{'loss': 1.076, 'grad_norm': 1.6536108255386353, 'learning_rate': 1.8070899606091586e-06, 'epoch': 0.81}
{'loss': 1.0722, 'grad_norm': 1.3953840732574463, 'learning_rate': 1.8035189215280423e-06, 'epoch': 0.81}
{'loss': 0.9505, 'grad_norm': 1.4489940404891968, 'learning_rate': 1.799951064603095e-06, 'epoch': 0.81}
{'loss': 1.0803, 'grad_norm': 1.435149908065796, 'learning_rate': 1.7963863912194768e-06, 'epoch': 0.81}
{'loss': 1.1911, 'grad_norm': 1.6049494743347168, 'learning_rate': 1.7928249027611255e-06, 'epoch': 0.81}
{'loss': 1.1474, 'grad_norm': 1.241999864578247, 'learning_rate': 1.789266600610724e-06, 'epoch': 0.81}
{'loss': 1.0154, 'grad_norm': 1.5243474245071411, 'learning_rate': 1.7857114861497337e-06, 'epoch': 0.81}
{'loss': 1.0605, 'grad_norm': 1.5758540630340576, 'learning_rate': 1.782159560758373e-06, 'epoch': 0.81}
{'loss': 1.1486, 'grad_norm': 1.600558876991272, 'learning_rate': 1.7786108258156154e-06, 'epoch': 0.81}
{'loss': 1.1196, 'grad_norm': 1.6890283823013306, 'learning_rate': 1.7750652826992077e-06, 'epoch': 0.81}
{'loss': 0.9736, 'grad_norm': 1.594115972518921, 'learning_rate': 1.7715229327856498e-06, 'epoch': 0.81}
{'loss': 0.9971, 'grad_norm': 1.4802320003509521, 'learning_rate': 1.7679837774502052e-06, 'epoch': 0.81}
{'loss': 1.0452, 'grad_norm': 1.3847429752349854, 'learning_rate': 1.7644478180668945e-06, 'epoch': 0.81}
{'loss': 1.0845, 'grad_norm': 1.5519527196884155, 'learning_rate': 1.7609150560084986e-06, 'epoch': 0.81}
{'loss': 1.0618, 'grad_norm': 1.467646598815918, 'learning_rate': 1.7573854926465582e-06, 'epoch': 0.81}
{'loss': 0.9942, 'grad_norm': 1.4717116355895996, 'learning_rate': 1.7538591293513685e-06, 'epoch': 0.81}
{'loss': 1.0902, 'grad_norm': 1.5015490055084229, 'learning_rate': 1.7503359674919929e-06, 'epoch': 0.81}
{'loss': 1.0934, 'grad_norm': 1.3942370414733887, 'learning_rate': 1.746816008436234e-06, 'epoch': 0.81}
{'loss': 1.0612, 'grad_norm': 1.3951624631881714, 'learning_rate': 1.7432992535506687e-06, 'epoch': 0.82}
{'loss': 1.1571, 'grad_norm': 1.5412291288375854, 'learning_rate': 1.7397857042006194e-06, 'epoch': 0.82}
{'loss': 1.1132, 'grad_norm': 1.4102504253387451, 'learning_rate': 1.736275361750167e-06, 'epoch': 0.82}
{'loss': 0.987, 'grad_norm': 1.4558440446853638, 'learning_rate': 1.7327682275621506e-06, 'epoch': 0.82}
{'loss': 1.1169, 'grad_norm': 1.5120220184326172, 'learning_rate': 1.7292643029981525e-06, 'epoch': 0.82}
{'loss': 1.1577, 'grad_norm': 1.6091822385787964, 'learning_rate': 1.7257635894185232e-06, 'epoch': 0.82}
{'loss': 1.0358, 'grad_norm': 1.4780735969543457, 'learning_rate': 1.7222660881823594e-06, 'epoch': 0.82}
{'loss': 1.1156, 'grad_norm': 1.531753420829773, 'learning_rate': 1.7187718006475117e-06, 'epoch': 0.82}
{'loss': 1.1291, 'grad_norm': 1.5057718753814697, 'learning_rate': 1.7152807281705809e-06, 'epoch': 0.82}
{'loss': 1.057, 'grad_norm': 1.7165557146072388, 'learning_rate': 1.7117928721069233e-06, 'epoch': 0.82}
{'loss': 0.985, 'grad_norm': 1.6115809679031372, 'learning_rate': 1.708308233810644e-06, 'epoch': 0.82}
{'loss': 1.1268, 'grad_norm': 1.3996976613998413, 'learning_rate': 1.704826814634597e-06, 'epoch': 0.82}
{'loss': 1.1209, 'grad_norm': 1.6742485761642456, 'learning_rate': 1.701348615930397e-06, 'epoch': 0.82}
{'loss': 1.0523, 'grad_norm': 1.597326636314392, 'learning_rate': 1.6978736390483896e-06, 'epoch': 0.82}
{'loss': 1.1255, 'grad_norm': 1.5524539947509766, 'learning_rate': 1.6944018853376898e-06, 'epoch': 0.82}
{'loss': 1.0702, 'grad_norm': 1.199141502380371, 'learning_rate': 1.6909333561461471e-06, 'epoch': 0.82}
{'loss': 1.1823, 'grad_norm': 1.6536387205123901, 'learning_rate': 1.6874680528203657e-06, 'epoch': 0.82}
{'loss': 1.0021, 'grad_norm': 1.3988842964172363, 'learning_rate': 1.6840059767056949e-06, 'epoch': 0.82}
{'loss': 1.0497, 'grad_norm': 1.4611701965332031, 'learning_rate': 1.6805471291462316e-06, 'epoch': 0.82}
{'loss': 1.0998, 'grad_norm': 1.5974982976913452, 'learning_rate': 1.6770915114848197e-06, 'epoch': 0.82}
{'loss': 1.0502, 'grad_norm': 1.5892595052719116, 'learning_rate': 1.67363912506305e-06, 'epoch': 0.82}
{'loss': 0.9902, 'grad_norm': 1.376315951347351, 'learning_rate': 1.6701899712212565e-06, 'epoch': 0.82}
{'loss': 1.1005, 'grad_norm': 1.1686428785324097, 'learning_rate': 1.66674405129852e-06, 'epoch': 0.82}
{'loss': 1.0525, 'grad_norm': 1.5982506275177002, 'learning_rate': 1.6633013666326636e-06, 'epoch': 0.82}
{'loss': 0.9634, 'grad_norm': 1.352849006652832, 'learning_rate': 1.6598619185602616e-06, 'epoch': 0.82}
{'loss': 1.1103, 'grad_norm': 1.4159940481185913, 'learning_rate': 1.656425708416617e-06, 'epoch': 0.82}
{'loss': 1.1449, 'grad_norm': 1.2326157093048096, 'learning_rate': 1.6529927375357957e-06, 'epoch': 0.82}
{'loss': 1.1235, 'grad_norm': 1.6704645156860352, 'learning_rate': 1.6495630072505841e-06, 'epoch': 0.82}
{'loss': 1.0882, 'grad_norm': 1.5393080711364746, 'learning_rate': 1.6461365188925304e-06, 'epoch': 0.82}
{'loss': 1.0465, 'grad_norm': 1.409092903137207, 'learning_rate': 1.642713273791914e-06, 'epoch': 0.82}
{'loss': 1.057, 'grad_norm': 1.4434417486190796, 'learning_rate': 1.6392932732777489e-06, 'epoch': 0.82}
{'loss': 1.0197, 'grad_norm': 1.4783803224563599, 'learning_rate': 1.6358765186778057e-06, 'epoch': 0.82}
{'loss': 1.203, 'grad_norm': 1.3404459953308105, 'learning_rate': 1.6324630113185835e-06, 'epoch': 0.82}
{'loss': 1.0628, 'grad_norm': 1.5665959119796753, 'learning_rate': 1.629052752525323e-06, 'epoch': 0.82}
{'loss': 1.0247, 'grad_norm': 1.5505527257919312, 'learning_rate': 1.625645743622003e-06, 'epoch': 0.82}
{'loss': 1.1569, 'grad_norm': 1.611216425895691, 'learning_rate': 1.6222419859313443e-06, 'epoch': 0.82}
{'loss': 1.1192, 'grad_norm': 1.4174233675003052, 'learning_rate': 1.6188414807747999e-06, 'epoch': 0.82}
{'loss': 1.0452, 'grad_norm': 1.4879862070083618, 'learning_rate': 1.6154442294725636e-06, 'epoch': 0.82}
{'loss': 1.1135, 'grad_norm': 1.4992693662643433, 'learning_rate': 1.6120502333435695e-06, 'epoch': 0.82}
{'loss': 1.035, 'grad_norm': 1.4089837074279785, 'learning_rate': 1.6086594937054767e-06, 'epoch': 0.82}
{'loss': 1.1579, 'grad_norm': 1.4964327812194824, 'learning_rate': 1.6052720118746923e-06, 'epoch': 0.82}
{'loss': 1.1217, 'grad_norm': 1.5102591514587402, 'learning_rate': 1.6018877891663521e-06, 'epoch': 0.82}
{'loss': 1.1379, 'grad_norm': 1.197060227394104, 'learning_rate': 1.5985068268943283e-06, 'epoch': 0.82}
{'loss': 1.1586, 'grad_norm': 1.523126482963562, 'learning_rate': 1.5951291263712255e-06, 'epoch': 0.82}
{'loss': 1.133, 'grad_norm': 1.4993126392364502, 'learning_rate': 1.5917546889083834e-06, 'epoch': 0.82}
{'loss': 1.127, 'grad_norm': 1.520819067955017, 'learning_rate': 1.5883835158158767e-06, 'epoch': 0.82}
{'loss': 1.0348, 'grad_norm': 1.6161922216415405, 'learning_rate': 1.5850156084025091e-06, 'epoch': 0.82}
{'loss': 1.0352, 'grad_norm': 1.5979714393615723, 'learning_rate': 1.5816509679758185e-06, 'epoch': 0.82}
{'loss': 1.1191, 'grad_norm': 1.470554232597351, 'learning_rate': 1.578289595842074e-06, 'epoch': 0.82}
{'loss': 1.1311, 'grad_norm': 1.4777411222457886, 'learning_rate': 1.5749314933062754e-06, 'epoch': 0.82}
{'loss': 1.0518, 'grad_norm': 1.5017668008804321, 'learning_rate': 1.5715766616721584e-06, 'epoch': 0.82}
{'loss': 1.0476, 'grad_norm': 1.3788573741912842, 'learning_rate': 1.5682251022421757e-06, 'epoch': 0.82}
{'loss': 0.9771, 'grad_norm': 1.5292545557022095, 'learning_rate': 1.5648768163175277e-06, 'epoch': 0.83}
{'loss': 1.1141, 'grad_norm': 1.568281650543213, 'learning_rate': 1.5615318051981243e-06, 'epoch': 0.83}
{'loss': 1.1935, 'grad_norm': 1.198928952217102, 'learning_rate': 1.5581900701826226e-06, 'epoch': 0.83}
{'loss': 1.0932, 'grad_norm': 1.5901215076446533, 'learning_rate': 1.5548516125683976e-06, 'epoch': 0.83}
{'loss': 1.0161, 'grad_norm': 1.4741411209106445, 'learning_rate': 1.5515164336515465e-06, 'epoch': 0.83}
{'loss': 1.1712, 'grad_norm': 1.2979474067687988, 'learning_rate': 1.5481845347269077e-06, 'epoch': 0.83}
{'loss': 1.0917, 'grad_norm': 1.6051456928253174, 'learning_rate': 1.5448559170880373e-06, 'epoch': 0.83}
{'loss': 1.0827, 'grad_norm': 1.480887770652771, 'learning_rate': 1.5415305820272198e-06, 'epoch': 0.83}
{'loss': 1.057, 'grad_norm': 1.4873449802398682, 'learning_rate': 1.5382085308354633e-06, 'epoch': 0.83}
{'loss': 1.0969, 'grad_norm': 1.4467408657073975, 'learning_rate': 1.534889764802503e-06, 'epoch': 0.83}
{'loss': 1.116, 'grad_norm': 1.5049912929534912, 'learning_rate': 1.5315742852167992e-06, 'epoch': 0.83}
{'loss': 1.1357, 'grad_norm': 1.5234639644622803, 'learning_rate': 1.5282620933655312e-06, 'epoch': 0.83}
{'loss': 1.1225, 'grad_norm': 1.505434274673462, 'learning_rate': 1.5249531905346138e-06, 'epoch': 0.83}
{'loss': 1.1392, 'grad_norm': 1.5203567743301392, 'learning_rate': 1.521647578008667e-06, 'epoch': 0.83}
{'loss': 1.094, 'grad_norm': 1.5165003538131714, 'learning_rate': 1.5183452570710522e-06, 'epoch': 0.83}
{'loss': 1.1658, 'grad_norm': 1.5387279987335205, 'learning_rate': 1.5150462290038392e-06, 'epoch': 0.83}
{'loss': 1.096, 'grad_norm': 1.5709304809570312, 'learning_rate': 1.511750495087827e-06, 'epoch': 0.83}
{'loss': 1.0907, 'grad_norm': 1.5535517930984497, 'learning_rate': 1.5084580566025309e-06, 'epoch': 0.83}
{'loss': 1.0892, 'grad_norm': 1.5470190048217773, 'learning_rate': 1.5051689148261895e-06, 'epoch': 0.83}
{'loss': 1.1869, 'grad_norm': 1.2653886079788208, 'learning_rate': 1.5018830710357612e-06, 'epoch': 0.83}
{'loss': 1.1073, 'grad_norm': 1.504915714263916, 'learning_rate': 1.4986005265069204e-06, 'epoch': 0.83}
{'loss': 1.054, 'grad_norm': 1.6238101720809937, 'learning_rate': 1.4953212825140728e-06, 'epoch': 0.83}
{'loss': 1.016, 'grad_norm': 1.4615907669067383, 'learning_rate': 1.4920453403303249e-06, 'epoch': 0.83}
{'loss': 1.0701, 'grad_norm': 1.4877398014068604, 'learning_rate': 1.4887727012275112e-06, 'epoch': 0.83}
{'loss': 1.1542, 'grad_norm': 1.2560545206069946, 'learning_rate': 1.4855033664761898e-06, 'epoch': 0.83}
{'loss': 1.0877, 'grad_norm': 1.408639907836914, 'learning_rate': 1.48223733734562e-06, 'epoch': 0.83}
{'loss': 1.061, 'grad_norm': 1.4748841524124146, 'learning_rate': 1.4789746151037942e-06, 'epoch': 0.83}
{'loss': 1.0608, 'grad_norm': 1.4882110357284546, 'learning_rate': 1.475715201017407e-06, 'epoch': 0.83}
{'loss': 0.9937, 'grad_norm': 1.4254850149154663, 'learning_rate': 1.4724590963518803e-06, 'epoch': 0.83}
{'loss': 1.0575, 'grad_norm': 1.425106167793274, 'learning_rate': 1.4692063023713444e-06, 'epoch': 0.83}
{'loss': 1.1314, 'grad_norm': 1.115660309791565, 'learning_rate': 1.4659568203386464e-06, 'epoch': 0.83}
{'loss': 1.1401, 'grad_norm': 1.4657222032546997, 'learning_rate': 1.4627106515153456e-06, 'epoch': 0.83}
{'loss': 1.0555, 'grad_norm': 1.4998587369918823, 'learning_rate': 1.4594677971617178e-06, 'epoch': 0.83}
{'loss': 1.0696, 'grad_norm': 1.43612802028656, 'learning_rate': 1.4562282585367493e-06, 'epoch': 0.83}
{'loss': 1.1564, 'grad_norm': 1.3755717277526855, 'learning_rate': 1.452992036898142e-06, 'epoch': 0.83}
{'loss': 1.113, 'grad_norm': 1.4696725606918335, 'learning_rate': 1.4497591335023087e-06, 'epoch': 0.83}
{'loss': 1.1259, 'grad_norm': 1.588535189628601, 'learning_rate': 1.446529549604373e-06, 'epoch': 0.83}
{'loss': 1.0974, 'grad_norm': 1.4856173992156982, 'learning_rate': 1.4433032864581687e-06, 'epoch': 0.83}
{'loss': 1.1163, 'grad_norm': 1.5256832838058472, 'learning_rate': 1.4400803453162482e-06, 'epoch': 0.83}
{'loss': 1.1163, 'grad_norm': 1.4084237813949585, 'learning_rate': 1.4368607274298596e-06, 'epoch': 0.83}
{'loss': 1.1352, 'grad_norm': 1.66069495677948, 'learning_rate': 1.4336444340489775e-06, 'epoch': 0.83}
{'loss': 1.1425, 'grad_norm': 1.7018393278121948, 'learning_rate': 1.430431466422273e-06, 'epoch': 0.83}
{'loss': 1.1447, 'grad_norm': 1.232174277305603, 'learning_rate': 1.4272218257971327e-06, 'epoch': 0.83}
{'loss': 1.1355, 'grad_norm': 1.4914811849594116, 'learning_rate': 1.4240155134196499e-06, 'epoch': 0.83}
{'loss': 1.0496, 'grad_norm': 1.497068166732788, 'learning_rate': 1.4208125305346232e-06, 'epoch': 0.83}
{'loss': 1.1002, 'grad_norm': 1.4884542226791382, 'learning_rate': 1.4176128783855636e-06, 'epoch': 0.83}
{'loss': 1.0737, 'grad_norm': 1.4851324558258057, 'learning_rate': 1.4144165582146819e-06, 'epoch': 0.83}
{'loss': 1.1718, 'grad_norm': 1.6296031475067139, 'learning_rate': 1.4112235712629063e-06, 'epoch': 0.83}
{'loss': 1.1076, 'grad_norm': 1.5123635530471802, 'learning_rate': 1.40803391876986e-06, 'epoch': 0.83}
{'loss': 1.1508, 'grad_norm': 1.570985198020935, 'learning_rate': 1.4048476019738756e-06, 'epoch': 0.83}
{'loss': 1.0577, 'grad_norm': 1.5864837169647217, 'learning_rate': 1.4016646221119912e-06, 'epoch': 0.83}
{'loss': 0.9913, 'grad_norm': 1.4522449970245361, 'learning_rate': 1.3984849804199485e-06, 'epoch': 0.83}
{'loss': 1.1983, 'grad_norm': 1.542000412940979, 'learning_rate': 1.395308678132199e-06, 'epoch': 0.84}
{'loss': 1.0435, 'grad_norm': 1.508530616760254, 'learning_rate': 1.392135716481885e-06, 'epoch': 0.84}
{'loss': 1.0686, 'grad_norm': 1.4973863363265991, 'learning_rate': 1.3889660967008656e-06, 'epoch': 0.84}
{'loss': 1.1085, 'grad_norm': 1.3897637128829956, 'learning_rate': 1.3857998200196943e-06, 'epoch': 0.84}
{'loss': 1.1808, 'grad_norm': 1.7016499042510986, 'learning_rate': 1.3826368876676278e-06, 'epoch': 0.84}
{'loss': 1.1099, 'grad_norm': 1.684771180152893, 'learning_rate': 1.379477300872626e-06, 'epoch': 0.84}
{'loss': 1.1506, 'grad_norm': 1.5527647733688354, 'learning_rate': 1.3763210608613497e-06, 'epoch': 0.84}
{'loss': 1.1106, 'grad_norm': 1.4893393516540527, 'learning_rate': 1.3731681688591593e-06, 'epoch': 0.84}
{'loss': 1.0282, 'grad_norm': 1.5805628299713135, 'learning_rate': 1.370018626090116e-06, 'epoch': 0.84}
{'loss': 1.1204, 'grad_norm': 1.5095624923706055, 'learning_rate': 1.3668724337769823e-06, 'epoch': 0.84}
{'loss': 1.1715, 'grad_norm': 1.5304139852523804, 'learning_rate': 1.3637295931412153e-06, 'epoch': 0.84}
{'loss': 1.106, 'grad_norm': 1.4377129077911377, 'learning_rate': 1.3605901054029746e-06, 'epoch': 0.84}
{'loss': 1.0645, 'grad_norm': 1.6052018404006958, 'learning_rate': 1.3574539717811231e-06, 'epoch': 0.84}
{'loss': 1.1333, 'grad_norm': 1.3820394277572632, 'learning_rate': 1.3543211934932065e-06, 'epoch': 0.84}
{'loss': 1.0769, 'grad_norm': 1.1537868976593018, 'learning_rate': 1.3511917717554846e-06, 'epoch': 0.84}
{'loss': 1.1359, 'grad_norm': 1.6091960668563843, 'learning_rate': 1.348065707782904e-06, 'epoch': 0.84}
{'loss': 1.1634, 'grad_norm': 1.55793297290802, 'learning_rate': 1.3449430027891096e-06, 'epoch': 0.84}
{'loss': 1.1155, 'grad_norm': 1.5199406147003174, 'learning_rate': 1.3418236579864452e-06, 'epoch': 0.84}
{'loss': 1.0256, 'grad_norm': 1.5936610698699951, 'learning_rate': 1.338707674585945e-06, 'epoch': 0.84}
{'loss': 1.0895, 'grad_norm': 1.5109905004501343, 'learning_rate': 1.3355950537973438e-06, 'epoch': 0.84}
{'loss': 1.0924, 'grad_norm': 1.6424295902252197, 'learning_rate': 1.332485796829065e-06, 'epoch': 0.84}
{'loss': 1.2903, 'grad_norm': 0.9708542823791504, 'learning_rate': 1.329379904888235e-06, 'epoch': 0.84}
{'loss': 1.1025, 'grad_norm': 1.5630009174346924, 'learning_rate': 1.3262773791806617e-06, 'epoch': 0.84}
{'loss': 1.1384, 'grad_norm': 1.5187608003616333, 'learning_rate': 1.3231782209108546e-06, 'epoch': 0.84}
{'loss': 1.0648, 'grad_norm': 1.4909923076629639, 'learning_rate': 1.3200824312820137e-06, 'epoch': 0.84}
{'loss': 1.096, 'grad_norm': 1.429626703262329, 'learning_rate': 1.3169900114960298e-06, 'epoch': 0.84}
{'loss': 1.1105, 'grad_norm': 1.5258411169052124, 'learning_rate': 1.3139009627534927e-06, 'epoch': 0.84}
{'loss': 1.1957, 'grad_norm': 1.3055058717727661, 'learning_rate': 1.3108152862536683e-06, 'epoch': 0.84}
{'loss': 1.1039, 'grad_norm': 1.6025484800338745, 'learning_rate': 1.3077329831945295e-06, 'epoch': 0.84}
{'loss': 0.9633, 'grad_norm': 1.5016847848892212, 'learning_rate': 1.3046540547727305e-06, 'epoch': 0.84}
{'loss': 1.1433, 'grad_norm': 1.4793436527252197, 'learning_rate': 1.3015785021836159e-06, 'epoch': 0.84}
{'loss': 1.1097, 'grad_norm': 1.511210560798645, 'learning_rate': 1.2985063266212229e-06, 'epoch': 0.84}
{'loss': 1.1085, 'grad_norm': 1.6575229167938232, 'learning_rate': 1.295437529278275e-06, 'epoch': 0.84}
{'loss': 1.1584, 'grad_norm': 1.5525504350662231, 'learning_rate': 1.2923721113461852e-06, 'epoch': 0.84}
{'loss': 1.0342, 'grad_norm': 1.6226133108139038, 'learning_rate': 1.2893100740150522e-06, 'epoch': 0.84}
{'loss': 1.1332, 'grad_norm': 1.2587555646896362, 'learning_rate': 1.2862514184736695e-06, 'epoch': 0.84}
{'loss': 1.1321, 'grad_norm': 1.6377638578414917, 'learning_rate': 1.2831961459095088e-06, 'epoch': 0.84}
{'loss': 1.1024, 'grad_norm': 1.5525671243667603, 'learning_rate': 1.2801442575087296e-06, 'epoch': 0.84}
{'loss': 1.0929, 'grad_norm': 1.4769142866134644, 'learning_rate': 1.2770957544561868e-06, 'epoch': 0.84}
{'loss': 1.106, 'grad_norm': 1.6707954406738281, 'learning_rate': 1.274050637935408e-06, 'epoch': 0.84}
{'loss': 1.122, 'grad_norm': 1.4659639596939087, 'learning_rate': 1.2710089091286148e-06, 'epoch': 0.84}
{'loss': 1.0341, 'grad_norm': 1.4615370035171509, 'learning_rate': 1.2679705692167122e-06, 'epoch': 0.84}
{'loss': 1.1664, 'grad_norm': 1.2882429361343384, 'learning_rate': 1.2649356193792873e-06, 'epoch': 0.84}
{'loss': 1.1087, 'grad_norm': 1.5213031768798828, 'learning_rate': 1.261904060794612e-06, 'epoch': 0.84}
{'loss': 1.1198, 'grad_norm': 1.6883726119995117, 'learning_rate': 1.2588758946396417e-06, 'epoch': 0.84}
{'loss': 1.036, 'grad_norm': 1.542314887046814, 'learning_rate': 1.2558511220900138e-06, 'epoch': 0.84}
{'loss': 1.1884, 'grad_norm': 1.179614782333374, 'learning_rate': 1.2528297443200489e-06, 'epoch': 0.84}
{'loss': 1.1727, 'grad_norm': 1.4194769859313965, 'learning_rate': 1.2498117625027562e-06, 'epoch': 0.84}
{'loss': 1.1723, 'grad_norm': 1.2628669738769531, 'learning_rate': 1.246797177809812e-06, 'epoch': 0.84}
{'loss': 1.11, 'grad_norm': 1.6206406354904175, 'learning_rate': 1.2437859914115847e-06, 'epoch': 0.84}
{'loss': 1.1253, 'grad_norm': 1.543470025062561, 'learning_rate': 1.2407782044771222e-06, 'epoch': 0.84}
WARNING: tokenization mismatch: 0 vs. 680. (ignored)
{'loss': 1.048, 'grad_norm': 1.6613847017288208, 'learning_rate': 1.237773818174146e-06, 'epoch': 0.84}
{'loss': 1.1884, 'grad_norm': 1.2881205081939697, 'learning_rate': 1.23477283366907e-06, 'epoch': 0.85}
{'loss': 1.0518, 'grad_norm': 1.4531059265136719, 'learning_rate': 1.2317752521269722e-06, 'epoch': 0.85}
{'loss': 1.1281, 'grad_norm': 1.492854356765747, 'learning_rate': 1.2287810747116224e-06, 'epoch': 0.85}
{'loss': 1.0379, 'grad_norm': 1.4491831064224243, 'learning_rate': 1.225790302585461e-06, 'epoch': 0.85}
{'loss': 1.1251, 'grad_norm': 1.4900084733963013, 'learning_rate': 1.2228029369096094e-06, 'epoch': 0.85}
{'loss': 1.0738, 'grad_norm': 1.4510302543640137, 'learning_rate': 1.2198189788438652e-06, 'epoch': 0.85}
{'loss': 1.2052, 'grad_norm': 1.174682378768921, 'learning_rate': 1.216838429546704e-06, 'epoch': 0.85}
{'loss': 1.1339, 'grad_norm': 1.5143507719039917, 'learning_rate': 1.2138612901752777e-06, 'epoch': 0.85}
{'loss': 1.0037, 'grad_norm': 1.5036965608596802, 'learning_rate': 1.2108875618854122e-06, 'epoch': 0.85}
{'loss': 1.1283, 'grad_norm': 1.4785394668579102, 'learning_rate': 1.2079172458316168e-06, 'epoch': 0.85}
{'loss': 1.1416, 'grad_norm': 1.4637255668640137, 'learning_rate': 1.204950343167065e-06, 'epoch': 0.85}
{'loss': 1.0923, 'grad_norm': 1.5500746965408325, 'learning_rate': 1.2019868550436099e-06, 'epoch': 0.85}
{'loss': 1.0206, 'grad_norm': 1.5494056940078735, 'learning_rate': 1.1990267826117874e-06, 'epoch': 0.85}
{'loss': 1.1078, 'grad_norm': 1.474674105644226, 'learning_rate': 1.1960701270207885e-06, 'epoch': 0.85}
{'loss': 1.1062, 'grad_norm': 1.4158153533935547, 'learning_rate': 1.1931168894184974e-06, 'epoch': 0.85}
{'loss': 1.1368, 'grad_norm': 1.5998060703277588, 'learning_rate': 1.19016707095146e-06, 'epoch': 0.85}
{'loss': 1.0829, 'grad_norm': 1.3866796493530273, 'learning_rate': 1.187220672764897e-06, 'epoch': 0.85}
{'loss': 1.0972, 'grad_norm': 1.5929149389266968, 'learning_rate': 1.1842776960027014e-06, 'epoch': 0.85}
{'loss': 1.2207, 'grad_norm': 1.283587098121643, 'learning_rate': 1.1813381418074388e-06, 'epoch': 0.85}
{'loss': 1.0925, 'grad_norm': 1.4668012857437134, 'learning_rate': 1.1784020113203453e-06, 'epoch': 0.85}
{'loss': 1.0889, 'grad_norm': 1.4810346364974976, 'learning_rate': 1.1754693056813272e-06, 'epoch': 0.85}
{'loss': 1.0291, 'grad_norm': 1.387135624885559, 'learning_rate': 1.172540026028962e-06, 'epoch': 0.85}
{'loss': 1.0297, 'grad_norm': 1.4069935083389282, 'learning_rate': 1.169614173500494e-06, 'epoch': 0.85}
{'loss': 0.9989, 'grad_norm': 1.4513485431671143, 'learning_rate': 1.1666917492318486e-06, 'epoch': 0.85}
WARNING: tokenization mismatch: 0 vs. 1286. (ignored)
{'loss': 1.0399, 'grad_norm': 1.3833868503570557, 'learning_rate': 1.1637727543576027e-06, 'epoch': 0.85}
{'loss': 1.0993, 'grad_norm': 1.4811197519302368, 'learning_rate': 1.1608571900110122e-06, 'epoch': 0.85}
{'loss': 1.111, 'grad_norm': 1.392562985420227, 'learning_rate': 1.1579450573240058e-06, 'epoch': 0.85}
{'loss': 1.1207, 'grad_norm': 1.4519577026367188, 'learning_rate': 1.1550363574271638e-06, 'epoch': 0.85}
{'loss': 1.0931, 'grad_norm': 1.5716995000839233, 'learning_rate': 1.1521310914497518e-06, 'epoch': 0.85}
{'loss': 1.0507, 'grad_norm': 1.5682241916656494, 'learning_rate': 1.149229260519691e-06, 'epoch': 0.85}
{'loss': 1.134, 'grad_norm': 1.4017764329910278, 'learning_rate': 1.1463308657635718e-06, 'epoch': 0.85}
{'loss': 1.086, 'grad_norm': 1.3950518369674683, 'learning_rate': 1.1434359083066515e-06, 'epoch': 0.85}
{'loss': 1.0202, 'grad_norm': 1.4641655683517456, 'learning_rate': 1.140544389272853e-06, 'epoch': 0.85}
{'loss': 1.0756, 'grad_norm': 1.4550514221191406, 'learning_rate': 1.1376563097847616e-06, 'epoch': 0.85}
{'loss': 1.1646, 'grad_norm': 1.3294764757156372, 'learning_rate': 1.1347716709636282e-06, 'epoch': 0.85}
{'loss': 1.0616, 'grad_norm': 1.331484079360962, 'learning_rate': 1.1318904739293745e-06, 'epoch': 0.85}
{'loss': 1.0817, 'grad_norm': 1.4683024883270264, 'learning_rate': 1.129012719800575e-06, 'epoch': 0.85}
{'loss': 1.0698, 'grad_norm': 1.2965977191925049, 'learning_rate': 1.1261384096944728e-06, 'epoch': 0.85}
{'loss': 1.1209, 'grad_norm': 1.7442041635513306, 'learning_rate': 1.1232675447269803e-06, 'epoch': 0.85}
{'loss': 1.0467, 'grad_norm': 1.4906693696975708, 'learning_rate': 1.1204001260126574e-06, 'epoch': 0.85}
{'loss': 1.1034, 'grad_norm': 1.492292046546936, 'learning_rate': 1.1175361546647413e-06, 'epoch': 0.85}
{'loss': 1.0634, 'grad_norm': 1.5836527347564697, 'learning_rate': 1.1146756317951224e-06, 'epoch': 0.85}
{'loss': 1.1773, 'grad_norm': 1.1955814361572266, 'learning_rate': 1.1118185585143536e-06, 'epoch': 0.85}
{'loss': 1.172, 'grad_norm': 1.5726745128631592, 'learning_rate': 1.1089649359316501e-06, 'epoch': 0.85}
{'loss': 0.9488, 'grad_norm': 1.502938151359558, 'learning_rate': 1.1061147651548855e-06, 'epoch': 0.85}
{'loss': 1.1712, 'grad_norm': 1.530920147895813, 'learning_rate': 1.1032680472905932e-06, 'epoch': 0.85}
{'loss': 0.9942, 'grad_norm': 1.410452127456665, 'learning_rate': 1.1004247834439697e-06, 'epoch': 0.85}
{'loss': 1.0532, 'grad_norm': 1.5771279335021973, 'learning_rate': 1.097584974718866e-06, 'epoch': 0.85}
{'loss': 1.0686, 'grad_norm': 1.6201786994934082, 'learning_rate': 1.0947486222177928e-06, 'epoch': 0.85}
{'loss': 1.0456, 'grad_norm': 1.5290534496307373, 'learning_rate': 1.0919157270419257e-06, 'epoch': 0.85}
{'loss': 1.0514, 'grad_norm': 1.4260238409042358, 'learning_rate': 1.0890862902910849e-06, 'epoch': 0.85}
{'loss': 1.0739, 'grad_norm': 1.5088629722595215, 'learning_rate': 1.0862603130637562e-06, 'epoch': 0.85}
{'loss': 1.1153, 'grad_norm': 1.6118550300598145, 'learning_rate': 1.0834377964570863e-06, 'epoch': 0.86}
{'loss': 1.1736, 'grad_norm': 1.7041573524475098, 'learning_rate': 1.0806187415668668e-06, 'epoch': 0.86}
{'loss': 1.0769, 'grad_norm': 1.483302354812622, 'learning_rate': 1.0778031494875574e-06, 'epoch': 0.86}
{'loss': 1.1315, 'grad_norm': 1.4942681789398193, 'learning_rate': 1.0749910213122649e-06, 'epoch': 0.86}
{'loss': 1.0612, 'grad_norm': 1.5342429876327515, 'learning_rate': 1.072182358132755e-06, 'epoch': 0.86}
{'loss': 1.143, 'grad_norm': 1.6031979322433472, 'learning_rate': 1.0693771610394477e-06, 'epoch': 0.86}
{'loss': 1.0585, 'grad_norm': 1.5489435195922852, 'learning_rate': 1.066575431121417e-06, 'epoch': 0.86}
{'loss': 1.1866, 'grad_norm': 1.4612098932266235, 'learning_rate': 1.06377716946639e-06, 'epoch': 0.86}
{'loss': 1.1097, 'grad_norm': 1.567480444908142, 'learning_rate': 1.0609823771607487e-06, 'epoch': 0.86}
{'loss': 1.1051, 'grad_norm': 1.5052006244659424, 'learning_rate': 1.0581910552895302e-06, 'epoch': 0.86}
{'loss': 1.107, 'grad_norm': 1.6886128187179565, 'learning_rate': 1.055403204936416e-06, 'epoch': 0.86}
{'loss': 1.131, 'grad_norm': 1.4979063272476196, 'learning_rate': 1.0526188271837512e-06, 'epoch': 0.86}
{'loss': 0.9731, 'grad_norm': 1.3843439817428589, 'learning_rate': 1.0498379231125278e-06, 'epoch': 0.86}
{'loss': 1.1935, 'grad_norm': 1.2842406034469604, 'learning_rate': 1.047060493802381e-06, 'epoch': 0.86}
{'loss': 1.0902, 'grad_norm': 1.5772322416305542, 'learning_rate': 1.0442865403316117e-06, 'epoch': 0.86}
{'loss': 1.0265, 'grad_norm': 1.4137861728668213, 'learning_rate': 1.0415160637771604e-06, 'epoch': 0.86}
{'loss': 1.1744, 'grad_norm': 1.5142687559127808, 'learning_rate': 1.0387490652146236e-06, 'epoch': 0.86}
{'loss': 1.0512, 'grad_norm': 1.5180708169937134, 'learning_rate': 1.0359855457182455e-06, 'epoch': 0.86}
{'loss': 1.0904, 'grad_norm': 1.4768143892288208, 'learning_rate': 1.0332255063609177e-06, 'epoch': 0.86}
{'loss': 1.0235, 'grad_norm': 1.4521609544754028, 'learning_rate': 1.0304689482141839e-06, 'epoch': 0.86}
{'loss': 1.1772, 'grad_norm': 1.4724633693695068, 'learning_rate': 1.027715872348234e-06, 'epoch': 0.86}
{'loss': 1.0742, 'grad_norm': 1.4881964921951294, 'learning_rate': 1.0249662798319072e-06, 'epoch': 0.86}
{'loss': 1.0587, 'grad_norm': 1.553920030593872, 'learning_rate': 1.0222201717326885e-06, 'epoch': 0.86}
{'loss': 1.1182, 'grad_norm': 1.4561676979064941, 'learning_rate': 1.0194775491167164e-06, 'epoch': 0.86}
{'loss': 1.1353, 'grad_norm': 1.5903726816177368, 'learning_rate': 1.0167384130487667e-06, 'epoch': 0.86}
{'loss': 0.9944, 'grad_norm': 1.3206876516342163, 'learning_rate': 1.0140027645922656e-06, 'epoch': 0.86}
{'loss': 1.1067, 'grad_norm': 1.5941855907440186, 'learning_rate': 1.0112706048092924e-06, 'epoch': 0.86}
{'loss': 1.1551, 'grad_norm': 1.5786242485046387, 'learning_rate': 1.0085419347605575e-06, 'epoch': 0.86}
{'loss': 1.1476, 'grad_norm': 1.5272231101989746, 'learning_rate': 1.00581675550543e-06, 'epoch': 0.86}
{'loss': 1.1324, 'grad_norm': 1.6882174015045166, 'learning_rate': 1.003095068101917e-06, 'epoch': 0.86}
{'loss': 1.0492, 'grad_norm': 1.3508944511413574, 'learning_rate': 1.0003768736066722e-06, 'epoch': 0.86}
{'loss': 1.1765, 'grad_norm': 1.7131736278533936, 'learning_rate': 9.976621730749892e-07, 'epoch': 0.86}
{'loss': 1.1449, 'grad_norm': 1.5005444288253784, 'learning_rate': 9.949509675608115e-07, 'epoch': 0.86}
{'loss': 1.1255, 'grad_norm': 1.529317021369934, 'learning_rate': 9.922432581167207e-07, 'epoch': 0.86}
{'loss': 1.1703, 'grad_norm': 1.5486207008361816, 'learning_rate': 9.895390457939414e-07, 'epoch': 0.86}
{'loss': 1.1272, 'grad_norm': 1.4953804016113281, 'learning_rate': 9.86838331642348e-07, 'epoch': 0.86}
{'loss': 1.0358, 'grad_norm': 1.4544378519058228, 'learning_rate': 9.84141116710442e-07, 'epoch': 0.86}
{'loss': 1.0877, 'grad_norm': 1.3395416736602783, 'learning_rate': 9.814474020453824e-07, 'epoch': 0.86}
{'loss': 1.2269, 'grad_norm': 1.3655765056610107, 'learning_rate': 9.787571886929604e-07, 'epoch': 0.86}
{'loss': 1.105, 'grad_norm': 1.5690702199935913, 'learning_rate': 9.76070477697605e-07, 'epoch': 0.86}
{'loss': 1.0402, 'grad_norm': 1.446564793586731, 'learning_rate': 9.733872701023938e-07, 'epoch': 0.86}
{'loss': 1.0891, 'grad_norm': 1.4142673015594482, 'learning_rate': 9.707075669490407e-07, 'epoch': 0.86}
{'loss': 1.151, 'grad_norm': 1.6966891288757324, 'learning_rate': 9.680313692778976e-07, 'epoch': 0.86}
{'loss': 1.2169, 'grad_norm': 1.4147385358810425, 'learning_rate': 9.653586781279567e-07, 'epoch': 0.86}
{'loss': 1.0487, 'grad_norm': 1.3979928493499756, 'learning_rate': 9.626894945368492e-07, 'epoch': 0.86}
{'loss': 1.0785, 'grad_norm': 1.4987761974334717, 'learning_rate': 9.600238195408428e-07, 'epoch': 0.86}
{'loss': 0.9941, 'grad_norm': 1.5522059202194214, 'learning_rate': 9.573616541748464e-07, 'epoch': 0.86}
{'loss': 1.007, 'grad_norm': 1.3709825277328491, 'learning_rate': 9.547029994724023e-07, 'epoch': 0.86}
{'loss': 1.0898, 'grad_norm': 1.6401736736297607, 'learning_rate': 9.520478564656898e-07, 'epoch': 0.86}
{'loss': 1.0811, 'grad_norm': 1.4528206586837769, 'learning_rate': 9.49396226185535e-07, 'epoch': 0.86}
{'loss': 1.1398, 'grad_norm': 1.5456187725067139, 'learning_rate': 9.467481096613829e-07, 'epoch': 0.86}
{'loss': 1.0631, 'grad_norm': 1.43534255027771, 'learning_rate': 9.441035079213267e-07, 'epoch': 0.86}
{'loss': 1.0965, 'grad_norm': 1.437394142150879, 'learning_rate': 9.414624219920953e-07, 'epoch': 0.87}
{'loss': 1.1304, 'grad_norm': 1.6386594772338867, 'learning_rate': 9.38824852899043e-07, 'epoch': 0.87}
{'loss': 1.0713, 'grad_norm': 1.5242468118667603, 'learning_rate': 9.361908016661703e-07, 'epoch': 0.87}
{'loss': 1.0723, 'grad_norm': 1.3391486406326294, 'learning_rate': 9.335602693161039e-07, 'epoch': 0.87}
{'loss': 1.0474, 'grad_norm': 1.4805822372436523, 'learning_rate': 9.309332568701079e-07, 'epoch': 0.87}
{'loss': 1.0466, 'grad_norm': 1.475006103515625, 'learning_rate': 9.283097653480788e-07, 'epoch': 0.87}
{'loss': 1.0656, 'grad_norm': 1.3841809034347534, 'learning_rate': 9.256897957685463e-07, 'epoch': 0.87}
{'loss': 1.1172, 'grad_norm': 1.4460883140563965, 'learning_rate': 9.230733491486721e-07, 'epoch': 0.87}
{'loss': 1.1599, 'grad_norm': 1.5520527362823486, 'learning_rate': 9.204604265042505e-07, 'epoch': 0.87}
{'loss': 1.0605, 'grad_norm': 1.553734302520752, 'learning_rate': 9.178510288497123e-07, 'epoch': 0.87}
{'loss': 1.1757, 'grad_norm': 1.5957753658294678, 'learning_rate': 9.15245157198108e-07, 'epoch': 0.87}
{'loss': 0.9769, 'grad_norm': 1.3309530019760132, 'learning_rate': 9.126428125611342e-07, 'epoch': 0.87}
{'loss': 1.0466, 'grad_norm': 1.3786684274673462, 'learning_rate': 9.10043995949108e-07, 'epoch': 0.87}
{'loss': 1.0709, 'grad_norm': 1.421800971031189, 'learning_rate': 9.074487083709759e-07, 'epoch': 0.87}
{'loss': 1.0816, 'grad_norm': 1.6005594730377197, 'learning_rate': 9.04856950834323e-07, 'epoch': 0.87}
{'loss': 1.1355, 'grad_norm': 1.4920681715011597, 'learning_rate': 9.022687243453554e-07, 'epoch': 0.87}
{'loss': 1.138, 'grad_norm': 1.4764068126678467, 'learning_rate': 8.996840299089149e-07, 'epoch': 0.87}
{'loss': 1.1323, 'grad_norm': 1.538777232170105, 'learning_rate': 8.971028685284655e-07, 'epoch': 0.87}
{'loss': 1.1646, 'grad_norm': 1.1306359767913818, 'learning_rate': 8.945252412061056e-07, 'epoch': 0.87}
{'loss': 1.0739, 'grad_norm': 1.5936967134475708, 'learning_rate': 8.91951148942557e-07, 'epoch': 0.87}
{'loss': 0.9923, 'grad_norm': 1.4132126569747925, 'learning_rate': 8.893805927371724e-07, 'epoch': 0.87}
{'loss': 1.0635, 'grad_norm': 1.4341293573379517, 'learning_rate': 8.868135735879291e-07, 'epoch': 0.87}
{'loss': 1.0898, 'grad_norm': 1.4492871761322021, 'learning_rate': 8.842500924914299e-07, 'epoch': 0.87}
{'loss': 1.0175, 'grad_norm': 1.3480316400527954, 'learning_rate': 8.816901504429143e-07, 'epoch': 0.87}
{'loss': 1.0934, 'grad_norm': 1.5317903757095337, 'learning_rate': 8.791337484362305e-07, 'epoch': 0.87}
{'loss': 1.0157, 'grad_norm': 1.4687546491622925, 'learning_rate': 8.765808874638682e-07, 'epoch': 0.87}
{'loss': 1.1181, 'grad_norm': 1.495301604270935, 'learning_rate': 8.740315685169364e-07, 'epoch': 0.87}
{'loss': 1.1848, 'grad_norm': 1.3813862800598145, 'learning_rate': 8.714857925851617e-07, 'epoch': 0.87}
{'loss': 1.1016, 'grad_norm': 1.4559903144836426, 'learning_rate': 8.689435606569086e-07, 'epoch': 0.87}
{'loss': 1.0452, 'grad_norm': 1.570178508758545, 'learning_rate': 8.664048737191566e-07, 'epoch': 0.87}
{'loss': 1.0546, 'grad_norm': 1.4858319759368896, 'learning_rate': 8.638697327575108e-07, 'epoch': 0.87}
{'loss': 1.1505, 'grad_norm': 1.6525774002075195, 'learning_rate': 8.613381387562015e-07, 'epoch': 0.87}
{'loss': 1.0744, 'grad_norm': 1.6420918703079224, 'learning_rate': 8.588100926980802e-07, 'epoch': 0.87}
{'loss': 1.1169, 'grad_norm': 1.1362606287002563, 'learning_rate': 8.56285595564621e-07, 'epoch': 0.87}
{'loss': 1.2474, 'grad_norm': 1.2071210145950317, 'learning_rate': 8.537646483359185e-07, 'epoch': 0.87}
{'loss': 1.1502, 'grad_norm': 1.5759998559951782, 'learning_rate': 8.512472519906978e-07, 'epoch': 0.87}
{'loss': 1.1231, 'grad_norm': 1.780132532119751, 'learning_rate': 8.487334075062914e-07, 'epoch': 0.87}
{'loss': 1.1215, 'grad_norm': 1.498095154762268, 'learning_rate': 8.462231158586654e-07, 'epoch': 0.87}
{'loss': 0.9985, 'grad_norm': 1.4870665073394775, 'learning_rate': 8.437163780224011e-07, 'epoch': 0.87}
{'loss': 1.1452, 'grad_norm': 1.5442272424697876, 'learning_rate': 8.412131949706958e-07, 'epoch': 0.87}
{'loss': 1.0186, 'grad_norm': 1.4002997875213623, 'learning_rate': 8.387135676753755e-07, 'epoch': 0.87}
{'loss': 1.1441, 'grad_norm': 1.5460621118545532, 'learning_rate': 8.362174971068804e-07, 'epoch': 0.87}
{'loss': 1.2222, 'grad_norm': 1.3089354038238525, 'learning_rate': 8.337249842342721e-07, 'epoch': 0.87}
{'loss': 1.2114, 'grad_norm': 1.2642712593078613, 'learning_rate': 8.312360300252287e-07, 'epoch': 0.87}
{'loss': 1.1763, 'grad_norm': 1.4223946332931519, 'learning_rate': 8.287506354460484e-07, 'epoch': 0.87}
{'loss': 1.0219, 'grad_norm': 1.570813775062561, 'learning_rate': 8.26268801461646e-07, 'epoch': 0.87}
{'loss': 1.1593, 'grad_norm': 1.5467274188995361, 'learning_rate': 8.237905290355563e-07, 'epoch': 0.87}
{'loss': 1.0436, 'grad_norm': 1.565125584602356, 'learning_rate': 8.213158191299297e-07, 'epoch': 0.87}
{'loss': 1.0271, 'grad_norm': 1.4709073305130005, 'learning_rate': 8.188446727055311e-07, 'epoch': 0.87}
{'loss': 1.0917, 'grad_norm': 1.4639009237289429, 'learning_rate': 8.163770907217506e-07, 'epoch': 0.87}
{'loss': 1.0908, 'grad_norm': 1.4788801670074463, 'learning_rate': 8.139130741365819e-07, 'epoch': 0.87}
{'loss': 1.096, 'grad_norm': 1.5230672359466553, 'learning_rate': 8.114526239066456e-07, 'epoch': 0.87}
{'loss': 1.154, 'grad_norm': 1.5799756050109863, 'learning_rate': 8.08995740987173e-07, 'epoch': 0.88}
{'loss': 1.1054, 'grad_norm': 1.5262271165847778, 'learning_rate': 8.065424263320054e-07, 'epoch': 0.88}
{'loss': 1.0446, 'grad_norm': 1.4572675228118896, 'learning_rate': 8.040926808936112e-07, 'epoch': 0.88}
{'loss': 1.082, 'grad_norm': 1.5597978830337524, 'learning_rate': 8.016465056230616e-07, 'epoch': 0.88}
{'loss': 1.013, 'grad_norm': 1.2879799604415894, 'learning_rate': 7.99203901470047e-07, 'epoch': 0.88}
{'loss': 1.1728, 'grad_norm': 1.5155694484710693, 'learning_rate': 7.967648693828712e-07, 'epoch': 0.88}
{'loss': 1.2027, 'grad_norm': 1.2391279935836792, 'learning_rate': 7.943294103084487e-07, 'epoch': 0.88}
{'loss': 1.0826, 'grad_norm': 1.395744800567627, 'learning_rate': 7.9189752519231e-07, 'epoch': 0.88}
{'loss': 1.1473, 'grad_norm': 1.590868592262268, 'learning_rate': 7.894692149785954e-07, 'epoch': 0.88}
{'loss': 1.0439, 'grad_norm': 1.4954472780227661, 'learning_rate': 7.870444806100619e-07, 'epoch': 0.88}
{'loss': 1.0868, 'grad_norm': 1.5053033828735352, 'learning_rate': 7.846233230280698e-07, 'epoch': 0.88}
{'loss': 1.1393, 'grad_norm': 1.519044280052185, 'learning_rate': 7.822057431725994e-07, 'epoch': 0.88}
{'loss': 1.0379, 'grad_norm': 1.4116313457489014, 'learning_rate': 7.797917419822377e-07, 'epoch': 0.88}
{'loss': 1.1784, 'grad_norm': 1.3390021324157715, 'learning_rate': 7.773813203941827e-07, 'epoch': 0.88}
{'loss': 1.0658, 'grad_norm': 1.3788901567459106, 'learning_rate': 7.749744793442448e-07, 'epoch': 0.88}
{'loss': 1.0222, 'grad_norm': 1.5176323652267456, 'learning_rate': 7.725712197668378e-07, 'epoch': 0.88}
{'loss': 1.1248, 'grad_norm': 1.589166522026062, 'learning_rate': 7.701715425949952e-07, 'epoch': 0.88}
{'loss': 1.1204, 'grad_norm': 1.3562636375427246, 'learning_rate': 7.677754487603517e-07, 'epoch': 0.88}
{'loss': 1.0528, 'grad_norm': 1.520772099494934, 'learning_rate': 7.653829391931533e-07, 'epoch': 0.88}
{'loss': 1.1683, 'grad_norm': 1.5451338291168213, 'learning_rate': 7.629940148222559e-07, 'epoch': 0.88}
{'loss': 1.0519, 'grad_norm': 1.4234986305236816, 'learning_rate': 7.606086765751209e-07, 'epoch': 0.88}
{'loss': 1.169, 'grad_norm': 1.5651333332061768, 'learning_rate': 7.582269253778185e-07, 'epoch': 0.88}
{'loss': 1.225, 'grad_norm': 1.1989473104476929, 'learning_rate': 7.55848762155027e-07, 'epoch': 0.88}
{'loss': 1.1153, 'grad_norm': 1.4948934316635132, 'learning_rate': 7.534741878300333e-07, 'epoch': 0.88}
{'loss': 1.1256, 'grad_norm': 1.439788818359375, 'learning_rate': 7.511032033247256e-07, 'epoch': 0.88}
{'loss': 1.1265, 'grad_norm': 1.6148176193237305, 'learning_rate': 7.487358095596031e-07, 'epoch': 0.88}
{'loss': 1.0273, 'grad_norm': 1.4184390306472778, 'learning_rate': 7.463720074537728e-07, 'epoch': 0.88}
{'loss': 1.0247, 'grad_norm': 1.4222908020019531, 'learning_rate': 7.440117979249362e-07, 'epoch': 0.88}
{'loss': 1.0757, 'grad_norm': 1.4926800727844238, 'learning_rate': 7.416551818894158e-07, 'epoch': 0.88}
{'loss': 1.0246, 'grad_norm': 1.4311707019805908, 'learning_rate': 7.393021602621264e-07, 'epoch': 0.88}
{'loss': 1.1477, 'grad_norm': 1.250788688659668, 'learning_rate': 7.369527339565951e-07, 'epoch': 0.88}
{'loss': 1.0396, 'grad_norm': 1.4332891702651978, 'learning_rate': 7.346069038849469e-07, 'epoch': 0.88}
{'loss': 0.9919, 'grad_norm': 1.5421925783157349, 'learning_rate': 7.322646709579173e-07, 'epoch': 0.88}
{'loss': 1.0132, 'grad_norm': 1.4845561981201172, 'learning_rate': 7.299260360848382e-07, 'epoch': 0.88}
{'loss': 1.0401, 'grad_norm': 1.545668363571167, 'learning_rate': 7.275910001736497e-07, 'epoch': 0.88}
{'loss': 1.0597, 'grad_norm': 1.4338033199310303, 'learning_rate': 7.252595641308957e-07, 'epoch': 0.88}
{'loss': 1.1998, 'grad_norm': 1.2511873245239258, 'learning_rate': 7.229317288617144e-07, 'epoch': 0.88}
{'loss': 1.0499, 'grad_norm': 1.5282297134399414, 'learning_rate': 7.20607495269856e-07, 'epoch': 0.88}
{'loss': 1.0475, 'grad_norm': 1.608583688735962, 'learning_rate': 7.182868642576679e-07, 'epoch': 0.88}
{'loss': 1.1216, 'grad_norm': 1.5599461793899536, 'learning_rate': 7.15969836726097e-07, 'epoch': 0.88}
{'loss': 1.2137, 'grad_norm': 1.211451530456543, 'learning_rate': 7.13656413574696e-07, 'epoch': 0.88}
{'loss': 1.1052, 'grad_norm': 1.37778902053833, 'learning_rate': 7.113465957016097e-07, 'epoch': 0.88}
{'loss': 1.1435, 'grad_norm': 1.6032519340515137, 'learning_rate': 7.090403840035942e-07, 'epoch': 0.88}
{'loss': 1.0215, 'grad_norm': 1.435454249382019, 'learning_rate': 7.067377793759999e-07, 'epoch': 0.88}
{'loss': 1.05, 'grad_norm': 1.580167531967163, 'learning_rate': 7.044387827127752e-07, 'epoch': 0.88}
{'loss': 1.0835, 'grad_norm': 1.63945734500885, 'learning_rate': 7.021433949064704e-07, 'epoch': 0.88}
{'loss': 1.0869, 'grad_norm': 1.462030291557312, 'learning_rate': 6.99851616848235e-07, 'epoch': 0.88}
{'loss': 1.1363, 'grad_norm': 1.5836013555526733, 'learning_rate': 6.975634494278149e-07, 'epoch': 0.88}
{'loss': 1.1651, 'grad_norm': 1.5354584455490112, 'learning_rate': 6.952788935335541e-07, 'epoch': 0.88}
{'loss': 1.1087, 'grad_norm': 1.6056187152862549, 'learning_rate': 6.92997950052402e-07, 'epoch': 0.88}
{'loss': 1.2082, 'grad_norm': 1.605830430984497, 'learning_rate': 6.907206198698912e-07, 'epoch': 0.88}
{'loss': 1.1164, 'grad_norm': 1.5569325685501099, 'learning_rate': 6.884469038701646e-07, 'epoch': 0.88}
{'loss': 1.1417, 'grad_norm': 1.6338374614715576, 'learning_rate': 6.861768029359595e-07, 'epoch': 0.89}
{'loss': 1.0766, 'grad_norm': 1.464821219444275, 'learning_rate': 6.839103179485995e-07, 'epoch': 0.89}
{'loss': 1.1856, 'grad_norm': 1.5319936275482178, 'learning_rate': 6.816474497880177e-07, 'epoch': 0.89}
{'loss': 1.071, 'grad_norm': 1.47760808467865, 'learning_rate': 6.793881993327366e-07, 'epoch': 0.89}
{'loss': 1.0837, 'grad_norm': 1.5438567399978638, 'learning_rate': 6.77132567459875e-07, 'epoch': 0.89}
{'loss': 1.1232, 'grad_norm': 1.3992691040039062, 'learning_rate': 6.748805550451453e-07, 'epoch': 0.89}
{'loss': 1.0507, 'grad_norm': 1.4955775737762451, 'learning_rate': 6.726321629628585e-07, 'epoch': 0.89}
{'loss': 1.1258, 'grad_norm': 1.5055209398269653, 'learning_rate': 6.703873920859161e-07, 'epoch': 0.89}
{'loss': 1.0268, 'grad_norm': 1.4681766033172607, 'learning_rate': 6.681462432858154e-07, 'epoch': 0.89}
{'loss': 1.1203, 'grad_norm': 1.5457981824874878, 'learning_rate': 6.659087174326506e-07, 'epoch': 0.89}
{'loss': 1.0642, 'grad_norm': 1.4331626892089844, 'learning_rate': 6.636748153951e-07, 'epoch': 0.89}
{'loss': 1.1176, 'grad_norm': 1.568070888519287, 'learning_rate': 6.614445380404478e-07, 'epoch': 0.89}
Error with cannot identify image file '/scratch3/li309/data/llava_data/train_data/llava_image_tune/ocr_vqa/images/376028076.jpg'
{'loss': 1.2424, 'grad_norm': 1.172731637954712, 'learning_rate': 6.592178862345622e-07, 'epoch': 0.89}
{'loss': 1.1656, 'grad_norm': 1.4908556938171387, 'learning_rate': 6.569948608419041e-07, 'epoch': 0.89}
{'loss': 1.1258, 'grad_norm': 1.4369696378707886, 'learning_rate': 6.547754627255332e-07, 'epoch': 0.89}
{'loss': 1.1024, 'grad_norm': 1.5210380554199219, 'learning_rate': 6.52559692747089e-07, 'epoch': 0.89}
{'loss': 1.0752, 'grad_norm': 1.4349479675292969, 'learning_rate': 6.503475517668168e-07, 'epoch': 0.89}
{'loss': 1.0538, 'grad_norm': 1.5502398014068604, 'learning_rate': 6.481390406435417e-07, 'epoch': 0.89}
{'loss': 1.0759, 'grad_norm': 1.7085673809051514, 'learning_rate': 6.459341602346858e-07, 'epoch': 0.89}
{'loss': 1.0406, 'grad_norm': 1.55671226978302, 'learning_rate': 6.437329113962576e-07, 'epoch': 0.89}
{'loss': 1.0567, 'grad_norm': 1.3962129354476929, 'learning_rate': 6.415352949828601e-07, 'epoch': 0.89}
{'loss': 1.0475, 'grad_norm': 1.5591760873794556, 'learning_rate': 6.393413118476821e-07, 'epoch': 0.89}
{'loss': 1.0599, 'grad_norm': 1.4065874814987183, 'learning_rate': 6.371509628425021e-07, 'epoch': 0.89}
{'loss': 1.2005, 'grad_norm': 1.3584120273590088, 'learning_rate': 6.349642488176943e-07, 'epoch': 0.89}
{'loss': 1.1198, 'grad_norm': 1.4596600532531738, 'learning_rate': 6.327811706222097e-07, 'epoch': 0.89}
{'loss': 1.1223, 'grad_norm': 1.5899510383605957, 'learning_rate': 6.306017291035981e-07, 'epoch': 0.89}
{'loss': 1.2637, 'grad_norm': 1.2461216449737549, 'learning_rate': 6.284259251079939e-07, 'epoch': 0.89}
{'loss': 1.1265, 'grad_norm': 1.4620269536972046, 'learning_rate': 6.262537594801177e-07, 'epoch': 0.89}
{'loss': 1.0573, 'grad_norm': 1.4732545614242554, 'learning_rate': 6.240852330632796e-07, 'epoch': 0.89}
{'loss': 1.1236, 'grad_norm': 1.4640254974365234, 'learning_rate': 6.219203466993762e-07, 'epoch': 0.89}
{'loss': 0.9927, 'grad_norm': 1.5132100582122803, 'learning_rate': 6.197591012288918e-07, 'epoch': 0.89}
{'loss': 1.0909, 'grad_norm': 1.5634280443191528, 'learning_rate': 6.17601497490895e-07, 'epoch': 0.89}
{'loss': 1.1521, 'grad_norm': 1.6001640558242798, 'learning_rate': 6.154475363230417e-07, 'epoch': 0.89}
{'loss': 1.0655, 'grad_norm': 1.486588954925537, 'learning_rate': 6.132972185615749e-07, 'epoch': 0.89}
{'loss': 1.1158, 'grad_norm': 1.4014451503753662, 'learning_rate': 6.111505450413202e-07, 'epoch': 0.89}
{'loss': 1.0959, 'grad_norm': 1.6039721965789795, 'learning_rate': 6.090075165956943e-07, 'epoch': 0.89}
{'loss': 1.2263, 'grad_norm': 1.3314436674118042, 'learning_rate': 6.068681340566896e-07, 'epoch': 0.89}
{'loss': 1.1433, 'grad_norm': 1.5242115259170532, 'learning_rate': 6.047323982548924e-07, 'epoch': 0.89}
{'loss': 1.1479, 'grad_norm': 1.507501482963562, 'learning_rate': 6.026003100194633e-07, 'epoch': 0.89}
{'loss': 1.1461, 'grad_norm': 1.5882790088653564, 'learning_rate': 6.004718701781575e-07, 'epoch': 0.89}
{'loss': 1.0817, 'grad_norm': 1.4602831602096558, 'learning_rate': 5.983470795573088e-07, 'epoch': 0.89}
{'loss': 1.0432, 'grad_norm': 1.4875339269638062, 'learning_rate': 5.962259389818292e-07, 'epoch': 0.89}
{'loss': 1.1533, 'grad_norm': 1.5843876600265503, 'learning_rate': 5.941084492752236e-07, 'epoch': 0.89}
{'loss': 1.04, 'grad_norm': 1.5582958459854126, 'learning_rate': 5.91994611259572e-07, 'epoch': 0.89}
{'loss': 1.1627, 'grad_norm': 1.5214060544967651, 'learning_rate': 5.898844257555392e-07, 'epoch': 0.89}
{'loss': 1.076, 'grad_norm': 1.4176656007766724, 'learning_rate': 5.87777893582372e-07, 'epoch': 0.89}
{'loss': 1.1421, 'grad_norm': 1.5284423828125, 'learning_rate': 5.856750155578983e-07, 'epoch': 0.89}
{'loss': 1.149, 'grad_norm': 1.5769352912902832, 'learning_rate': 5.835757924985286e-07, 'epoch': 0.89}
{'loss': 1.1144, 'grad_norm': 1.5488797426223755, 'learning_rate': 5.81480225219252e-07, 'epoch': 0.89}
{'loss': 1.073, 'grad_norm': 1.4248217344284058, 'learning_rate': 5.793883145336443e-07, 'epoch': 0.89}
{'loss': 1.0412, 'grad_norm': 1.4400168657302856, 'learning_rate': 5.773000612538505e-07, 'epoch': 0.89}
{'loss': 1.0967, 'grad_norm': 1.5444319248199463, 'learning_rate': 5.752154661906085e-07, 'epoch': 0.89}
{'loss': 1.1232, 'grad_norm': 1.111415982246399, 'learning_rate': 5.731345301532265e-07, 'epoch': 0.9}
{'loss': 1.1071, 'grad_norm': 1.5452275276184082, 'learning_rate': 5.710572539495962e-07, 'epoch': 0.9}
{'loss': 1.0631, 'grad_norm': 1.474677562713623, 'learning_rate': 5.68983638386188e-07, 'epoch': 0.9}
{'loss': 1.2158, 'grad_norm': 1.4502990245819092, 'learning_rate': 5.669136842680512e-07, 'epoch': 0.9}
{'loss': 1.1392, 'grad_norm': 1.5994961261749268, 'learning_rate': 5.648473923988129e-07, 'epoch': 0.9}
{'loss': 1.1169, 'grad_norm': 1.5668745040893555, 'learning_rate': 5.627847635806771e-07, 'epoch': 0.9}
{'loss': 0.9932, 'grad_norm': 1.5163413286209106, 'learning_rate': 5.607257986144321e-07, 'epoch': 0.9}
{'loss': 1.1456, 'grad_norm': 1.6707327365875244, 'learning_rate': 5.58670498299434e-07, 'epoch': 0.9}
{'loss': 1.133, 'grad_norm': 1.6484733819961548, 'learning_rate': 5.566188634336212e-07, 'epoch': 0.9}
{'loss': 1.1363, 'grad_norm': 1.669238567352295, 'learning_rate': 5.545708948135142e-07, 'epoch': 0.9}
{'loss': 1.0152, 'grad_norm': 1.5414611101150513, 'learning_rate': 5.525265932341984e-07, 'epoch': 0.9}
{'loss': 1.0724, 'grad_norm': 1.4205985069274902, 'learning_rate': 5.504859594893475e-07, 'epoch': 0.9}
{'loss': 1.0819, 'grad_norm': 1.51101553440094, 'learning_rate': 5.484489943712013e-07, 'epoch': 0.9}
{'loss': 1.2502, 'grad_norm': 1.157713770866394, 'learning_rate': 5.464156986705826e-07, 'epoch': 0.9}
{'loss': 1.1474, 'grad_norm': 1.5082330703735352, 'learning_rate': 5.443860731768869e-07, 'epoch': 0.9}
{'loss': 1.0658, 'grad_norm': 1.4169731140136719, 'learning_rate': 5.423601186780836e-07, 'epoch': 0.9}
{'loss': 1.1028, 'grad_norm': 1.5896012783050537, 'learning_rate': 5.403378359607181e-07, 'epoch': 0.9}
{'loss': 1.0696, 'grad_norm': 1.6041814088821411, 'learning_rate': 5.383192258099113e-07, 'epoch': 0.9}
{'loss': 1.0599, 'grad_norm': 1.4916952848434448, 'learning_rate': 5.36304289009355e-07, 'epoch': 0.9}
{'loss': 1.2336, 'grad_norm': 1.3769766092300415, 'learning_rate': 5.342930263413193e-07, 'epoch': 0.9}
{'loss': 1.1689, 'grad_norm': 1.2470371723175049, 'learning_rate': 5.322854385866439e-07, 'epoch': 0.9}
{'loss': 1.104, 'grad_norm': 1.4627145528793335, 'learning_rate': 5.302815265247452e-07, 'epoch': 0.9}
Error with image file is truncated (5 bytes not processed)
{'loss': 1.0493, 'grad_norm': 1.5003803968429565, 'learning_rate': 5.282812909336077e-07, 'epoch': 0.9}
{'loss': 1.0682, 'grad_norm': 1.5488252639770508, 'learning_rate': 5.262847325897968e-07, 'epoch': 0.9}
{'loss': 1.1305, 'grad_norm': 1.543055772781372, 'learning_rate': 5.242918522684392e-07, 'epoch': 0.9}
{'loss': 1.1721, 'grad_norm': 1.5738614797592163, 'learning_rate': 5.22302650743245e-07, 'epoch': 0.9}
{'loss': 1.1676, 'grad_norm': 1.4071440696716309, 'learning_rate': 5.203171287864872e-07, 'epoch': 0.9}
{'loss': 1.1441, 'grad_norm': 1.4800063371658325, 'learning_rate': 5.183352871690162e-07, 'epoch': 0.9}
{'loss': 1.1434, 'grad_norm': 1.5582290887832642, 'learning_rate': 5.163571266602485e-07, 'epoch': 0.9}
{'loss': 1.0825, 'grad_norm': 1.2373491525650024, 'learning_rate': 5.143826480281778e-07, 'epoch': 0.9}
{'loss': 1.187, 'grad_norm': 1.6041500568389893, 'learning_rate': 5.124118520393606e-07, 'epoch': 0.9}
{'loss': 1.1699, 'grad_norm': 1.2537918090820312, 'learning_rate': 5.104447394589295e-07, 'epoch': 0.9}
{'loss': 0.9855, 'grad_norm': 1.5614455938339233, 'learning_rate': 5.084813110505871e-07, 'epoch': 0.9}
{'loss': 1.0358, 'grad_norm': 1.5590966939926147, 'learning_rate': 5.065215675766023e-07, 'epoch': 0.9}
{'loss': 1.1031, 'grad_norm': 1.3262882232666016, 'learning_rate': 5.045655097978131e-07, 'epoch': 0.9}
{'loss': 1.1213, 'grad_norm': 1.2363072633743286, 'learning_rate': 5.026131384736321e-07, 'epoch': 0.9}
{'loss': 1.0372, 'grad_norm': 1.3373188972473145, 'learning_rate': 5.006644543620342e-07, 'epoch': 0.9}
{'loss': 1.0543, 'grad_norm': 1.5375694036483765, 'learning_rate': 4.987194582195687e-07, 'epoch': 0.9}
{'loss': 1.0254, 'grad_norm': 1.4960891008377075, 'learning_rate': 4.967781508013459e-07, 'epoch': 0.9}
{'loss': 1.1908, 'grad_norm': 1.5336207151412964, 'learning_rate': 4.948405328610506e-07, 'epoch': 0.9}
{'loss': 1.0985, 'grad_norm': 1.5035573244094849, 'learning_rate': 4.929066051509346e-07, 'epoch': 0.9}
{'loss': 1.1096, 'grad_norm': 1.5250928401947021, 'learning_rate': 4.909763684218116e-07, 'epoch': 0.9}
{'loss': 1.0792, 'grad_norm': 1.3792445659637451, 'learning_rate': 4.890498234230689e-07, 'epoch': 0.9}
{'loss': 1.1461, 'grad_norm': 1.2414528131484985, 'learning_rate': 4.871269709026561e-07, 'epoch': 0.9}
{'loss': 1.1223, 'grad_norm': 1.4689133167266846, 'learning_rate': 4.852078116070902e-07, 'epoch': 0.9}
{'loss': 1.1316, 'grad_norm': 1.4001370668411255, 'learning_rate': 4.832923462814565e-07, 'epoch': 0.9}
{'loss': 1.1803, 'grad_norm': 1.572390079498291, 'learning_rate': 4.813805756694035e-07, 'epoch': 0.9}
{'loss': 1.1109, 'grad_norm': 1.436679482460022, 'learning_rate': 4.794725005131462e-07, 'epoch': 0.9}
{'loss': 1.1557, 'grad_norm': 1.5366652011871338, 'learning_rate': 4.775681215534656e-07, 'epoch': 0.9}
{'loss': 1.2418, 'grad_norm': 1.221998929977417, 'learning_rate': 4.7566743952970894e-07, 'epoch': 0.9}
{'loss': 1.185, 'grad_norm': 1.4521512985229492, 'learning_rate': 4.7377045517978173e-07, 'epoch': 0.9}
{'loss': 1.0728, 'grad_norm': 1.4566478729248047, 'learning_rate': 4.7187716924016355e-07, 'epoch': 0.9}
{'loss': 1.0806, 'grad_norm': 1.4567056894302368, 'learning_rate': 4.6998758244588995e-07, 'epoch': 0.91}
{'loss': 1.1304, 'grad_norm': 1.4680734872817993, 'learning_rate': 4.6810169553056616e-07, 'epoch': 0.91}
{'loss': 1.0949, 'grad_norm': 1.494175672531128, 'learning_rate': 4.662195092263566e-07, 'epoch': 0.91}
{'loss': 1.0178, 'grad_norm': 1.4754419326782227, 'learning_rate': 4.643410242639912e-07, 'epoch': 0.91}
{'loss': 1.1186, 'grad_norm': 1.4578288793563843, 'learning_rate': 4.6246624137276206e-07, 'epoch': 0.91}
{'loss': 1.0238, 'grad_norm': 1.5428643226623535, 'learning_rate': 4.605951612805237e-07, 'epoch': 0.91}
{'loss': 1.0609, 'grad_norm': 1.403978705406189, 'learning_rate': 4.587277847136984e-07, 'epoch': 0.91}
{'loss': 1.0969, 'grad_norm': 1.6433515548706055, 'learning_rate': 4.568641123972606e-07, 'epoch': 0.91}
{'loss': 1.1178, 'grad_norm': 1.527845025062561, 'learning_rate': 4.550041450547549e-07, 'epoch': 0.91}
{'loss': 1.0832, 'grad_norm': 1.6772797107696533, 'learning_rate': 4.5314788340828365e-07, 'epoch': 0.91}
{'loss': 1.2181, 'grad_norm': 1.338916301727295, 'learning_rate': 4.512953281785104e-07, 'epoch': 0.91}
{'loss': 1.0296, 'grad_norm': 1.4751447439193726, 'learning_rate': 4.494464800846654e-07, 'epoch': 0.91}
{'loss': 1.1376, 'grad_norm': 1.4483275413513184, 'learning_rate': 4.476013398445289e-07, 'epoch': 0.91}
{'loss': 1.0869, 'grad_norm': 1.5026071071624756, 'learning_rate': 4.4575990817445234e-07, 'epoch': 0.91}
{'loss': 1.1236, 'grad_norm': 1.2175990343093872, 'learning_rate': 4.4392218578934164e-07, 'epoch': 0.91}
{'loss': 1.0818, 'grad_norm': 1.533267855644226, 'learning_rate': 4.4208817340266385e-07, 'epoch': 0.91}
{'loss': 1.1063, 'grad_norm': 1.3731228113174438, 'learning_rate': 4.4025787172644495e-07, 'epoch': 0.91}
{'loss': 1.1091, 'grad_norm': 1.5751733779907227, 'learning_rate': 4.384312814712721e-07, 'epoch': 0.91}
{'loss': 1.1155, 'grad_norm': 1.5983034372329712, 'learning_rate': 4.366084033462914e-07, 'epoch': 0.91}
{'loss': 1.0821, 'grad_norm': 1.5334392786026, 'learning_rate': 4.3478923805920335e-07, 'epoch': 0.91}
{'loss': 1.0335, 'grad_norm': 1.4404330253601074, 'learning_rate': 4.329737863162753e-07, 'epoch': 0.91}
{'loss': 1.1089, 'grad_norm': 1.475403904914856, 'learning_rate': 4.311620488223256e-07, 'epoch': 0.91}
{'loss': 1.0858, 'grad_norm': 1.4552611112594604, 'learning_rate': 4.2935402628073166e-07, 'epoch': 0.91}
{'loss': 1.1614, 'grad_norm': 1.4392579793930054, 'learning_rate': 4.27549719393433e-07, 'epoch': 0.91}
{'loss': 1.0258, 'grad_norm': 1.5093265771865845, 'learning_rate': 4.2574912886092166e-07, 'epoch': 0.91}
{'loss': 1.0792, 'grad_norm': 1.4956254959106445, 'learning_rate': 4.239522553822495e-07, 'epoch': 0.91}
{'loss': 1.0014, 'grad_norm': 1.435840368270874, 'learning_rate': 4.221590996550251e-07, 'epoch': 0.91}
{'loss': 1.06, 'grad_norm': 1.412122368812561, 'learning_rate': 4.203696623754139e-07, 'epoch': 0.91}
{'loss': 1.1342, 'grad_norm': 1.6149193048477173, 'learning_rate': 4.1858394423813563e-07, 'epoch': 0.91}
{'loss': 1.0764, 'grad_norm': 1.5926156044006348, 'learning_rate': 4.1680194593646696e-07, 'epoch': 0.91}
{'loss': 1.1342, 'grad_norm': 1.5331882238388062, 'learning_rate': 4.1502366816224327e-07, 'epoch': 0.91}
{'loss': 1.0678, 'grad_norm': 1.4404067993164062, 'learning_rate': 4.1324911160585014e-07, 'epoch': 0.91}
{'loss': 1.0953, 'grad_norm': 1.6330140829086304, 'learning_rate': 4.1147827695623643e-07, 'epoch': 0.91}
{'loss': 1.1021, 'grad_norm': 1.4618074893951416, 'learning_rate': 4.097111649008967e-07, 'epoch': 0.91}
{'loss': 1.1176, 'grad_norm': 1.5411511659622192, 'learning_rate': 4.0794777612588543e-07, 'epoch': 0.91}
{'loss': 1.1253, 'grad_norm': 1.603196382522583, 'learning_rate': 4.061881113158117e-07, 'epoch': 0.91}
{'loss': 1.1674, 'grad_norm': 1.591191291809082, 'learning_rate': 4.044321711538368e-07, 'epoch': 0.91}
{'loss': 1.1749, 'grad_norm': 1.320285677909851, 'learning_rate': 4.02679956321681e-07, 'epoch': 0.91}
{'loss': 1.0492, 'grad_norm': 1.3519190549850464, 'learning_rate': 4.00931467499609e-07, 'epoch': 0.91}
{'loss': 1.1048, 'grad_norm': 1.4546552896499634, 'learning_rate': 3.9918670536644776e-07, 'epoch': 0.91}
{'loss': 1.0346, 'grad_norm': 1.603074550628662, 'learning_rate': 3.974456705995733e-07, 'epoch': 0.91}
{'loss': 1.1728, 'grad_norm': 1.5303072929382324, 'learning_rate': 3.9570836387491487e-07, 'epoch': 0.91}
{'loss': 1.1494, 'grad_norm': 1.4139349460601807, 'learning_rate': 3.9397478586695513e-07, 'epoch': 0.91}
{'loss': 1.0561, 'grad_norm': 1.5232954025268555, 'learning_rate': 3.9224493724872915e-07, 'epoch': 0.91}
{'loss': 1.1363, 'grad_norm': 1.5975420475006104, 'learning_rate': 3.90518818691823e-07, 'epoch': 0.91}
{'loss': 1.1253, 'grad_norm': 1.421247959136963, 'learning_rate': 3.8879643086637384e-07, 'epoch': 0.91}
{'loss': 1.0879, 'grad_norm': 1.475723385810852, 'learning_rate': 3.8707777444107697e-07, 'epoch': 0.91}
{'loss': 1.1461, 'grad_norm': 1.581655740737915, 'learning_rate': 3.8536285008316854e-07, 'epoch': 0.91}
{'loss': 1.1043, 'grad_norm': 1.4809120893478394, 'learning_rate': 3.8365165845844266e-07, 'epoch': 0.91}
{'loss': 1.0663, 'grad_norm': 1.5550435781478882, 'learning_rate': 3.819442002312457e-07, 'epoch': 0.91}
{'loss': 1.1279, 'grad_norm': 1.5615075826644897, 'learning_rate': 3.8024047606446736e-07, 'epoch': 0.91}
{'loss': 1.1379, 'grad_norm': 1.4728484153747559, 'learning_rate': 3.785404866195552e-07, 'epoch': 0.91}
{'loss': 1.056, 'grad_norm': 1.6002124547958374, 'learning_rate': 3.768442325565036e-07, 'epoch': 0.92}
{'loss': 1.0845, 'grad_norm': 1.4824252128601074, 'learning_rate': 3.751517145338546e-07, 'epoch': 0.92}
{'loss': 1.0963, 'grad_norm': 1.485798716545105, 'learning_rate': 3.7346293320870363e-07, 'epoch': 0.92}
{'loss': 1.1178, 'grad_norm': 1.588598370552063, 'learning_rate': 3.717778892366941e-07, 'epoch': 0.92}
{'loss': 1.1308, 'grad_norm': 1.4090783596038818, 'learning_rate': 3.700965832720171e-07, 'epoch': 0.92}
{'loss': 1.0732, 'grad_norm': 1.508621335029602, 'learning_rate': 3.684190159674117e-07, 'epoch': 0.92}
{'loss': 1.1805, 'grad_norm': 1.291253685951233, 'learning_rate': 3.6674518797417236e-07, 'epoch': 0.92}
{'loss': 1.175, 'grad_norm': 1.4664947986602783, 'learning_rate': 3.6507509994213155e-07, 'epoch': 0.92}
{'loss': 1.1761, 'grad_norm': 1.2150644063949585, 'learning_rate': 3.6340875251967946e-07, 'epoch': 0.92}
{'loss': 1.1307, 'grad_norm': 1.5191395282745361, 'learning_rate': 3.617461463537464e-07, 'epoch': 0.92}
{'loss': 1.1063, 'grad_norm': 1.4789409637451172, 'learning_rate': 3.6008728208981157e-07, 'epoch': 0.92}
{'loss': 1.0773, 'grad_norm': 1.6225992441177368, 'learning_rate': 3.5843216037190873e-07, 'epoch': 0.92}
{'loss': 1.0466, 'grad_norm': 1.4931808710098267, 'learning_rate': 3.5678078184260834e-07, 'epoch': 0.92}
{'loss': 1.0867, 'grad_norm': 1.627149224281311, 'learning_rate': 3.5513314714303524e-07, 'epoch': 0.92}
{'loss': 1.1738, 'grad_norm': 1.5774402618408203, 'learning_rate': 3.5348925691285675e-07, 'epoch': 0.92}
{'loss': 1.027, 'grad_norm': 1.4742436408996582, 'learning_rate': 3.518491117902878e-07, 'epoch': 0.92}
{'loss': 1.053, 'grad_norm': 1.5238786935806274, 'learning_rate': 3.502127124120891e-07, 'epoch': 0.92}
{'loss': 1.0701, 'grad_norm': 1.6086479425430298, 'learning_rate': 3.48580059413568e-07, 'epoch': 0.92}
{'loss': 1.1639, 'grad_norm': 1.4890532493591309, 'learning_rate': 3.4695115342857524e-07, 'epoch': 0.92}
{'loss': 1.0047, 'grad_norm': 1.4413046836853027, 'learning_rate': 3.4532599508950826e-07, 'epoch': 0.92}
{'loss': 1.2899, 'grad_norm': 0.988153338432312, 'learning_rate': 3.437045850273113e-07, 'epoch': 0.92}
{'loss': 1.0631, 'grad_norm': 1.4352835416793823, 'learning_rate': 3.420869238714708e-07, 'epoch': 0.92}
{'loss': 1.0757, 'grad_norm': 1.5235885381698608, 'learning_rate': 3.404730122500155e-07, 'epoch': 0.92}
{'loss': 1.1726, 'grad_norm': 1.4978193044662476, 'learning_rate': 3.3886285078952753e-07, 'epoch': 0.92}
{'loss': 1.1059, 'grad_norm': 1.4581245183944702, 'learning_rate': 3.3725644011512125e-07, 'epoch': 0.92}
{'loss': 1.002, 'grad_norm': 1.446616530418396, 'learning_rate': 3.356537808504634e-07, 'epoch': 0.92}
{'loss': 1.0155, 'grad_norm': 1.4809561967849731, 'learning_rate': 3.3405487361776177e-07, 'epoch': 0.92}
{'loss': 1.0925, 'grad_norm': 1.4503360986709595, 'learning_rate': 3.3245971903776654e-07, 'epoch': 0.92}
{'loss': 1.2078, 'grad_norm': 1.2305057048797607, 'learning_rate': 3.308683177297711e-07, 'epoch': 0.92}
{'loss': 1.1085, 'grad_norm': 1.459165096282959, 'learning_rate': 3.292806703116125e-07, 'epoch': 0.92}
{'loss': 1.0748, 'grad_norm': 1.536534309387207, 'learning_rate': 3.2769677739966975e-07, 'epoch': 0.92}
{'loss': 1.0594, 'grad_norm': 1.5118744373321533, 'learning_rate': 3.2611663960886665e-07, 'epoch': 0.92}
{'loss': 1.205, 'grad_norm': 1.2940152883529663, 'learning_rate': 3.245402575526646e-07, 'epoch': 0.92}
{'loss': 1.1419, 'grad_norm': 1.5637091398239136, 'learning_rate': 3.2296763184306965e-07, 'epoch': 0.92}
{'loss': 1.0899, 'grad_norm': 1.4809120893478394, 'learning_rate': 3.2139876309063233e-07, 'epoch': 0.92}
{'loss': 1.0909, 'grad_norm': 1.4964708089828491, 'learning_rate': 3.198336519044376e-07, 'epoch': 0.92}
{'loss': 1.1277, 'grad_norm': 1.428187370300293, 'learning_rate': 3.182722988921161e-07, 'epoch': 0.92}
{'loss': 1.0264, 'grad_norm': 1.6059439182281494, 'learning_rate': 3.167147046598418e-07, 'epoch': 0.92}
{'loss': 1.0459, 'grad_norm': 1.3498144149780273, 'learning_rate': 3.151608698123232e-07, 'epoch': 0.92}
{'loss': 1.137, 'grad_norm': 1.6523611545562744, 'learning_rate': 3.1361079495281443e-07, 'epoch': 0.92}
{'loss': 1.0015, 'grad_norm': 1.4782739877700806, 'learning_rate': 3.1206448068310635e-07, 'epoch': 0.92}
{'loss': 0.9857, 'grad_norm': 1.4996747970581055, 'learning_rate': 3.1052192760353316e-07, 'epoch': 0.92}
{'loss': 1.125, 'grad_norm': 1.5953339338302612, 'learning_rate': 3.0898313631296586e-07, 'epoch': 0.92}
{'loss': 1.1565, 'grad_norm': 1.4041156768798828, 'learning_rate': 3.0744810740881646e-07, 'epoch': 0.92}
{'loss': 1.1263, 'grad_norm': 1.2530940771102905, 'learning_rate': 3.0591684148703617e-07, 'epoch': 0.92}
{'loss': 1.0735, 'grad_norm': 1.444579839706421, 'learning_rate': 3.043893391421149e-07, 'epoch': 0.92}
{'loss': 0.9504, 'grad_norm': 1.4598066806793213, 'learning_rate': 3.0286560096708275e-07, 'epoch': 0.92}
{'loss': 1.0863, 'grad_norm': 1.4464178085327148, 'learning_rate': 3.013456275535054e-07, 'epoch': 0.92}
{'loss': 1.0847, 'grad_norm': 1.522002935409546, 'learning_rate': 2.998294194914897e-07, 'epoch': 0.92}
{'loss': 1.0924, 'grad_norm': 1.648598074913025, 'learning_rate': 2.983169773696815e-07, 'epoch': 0.92}
{'loss': 1.095, 'grad_norm': 1.519312858581543, 'learning_rate': 2.968083017752599e-07, 'epoch': 0.92}
{'loss': 1.0613, 'grad_norm': 1.4687983989715576, 'learning_rate': 2.953033932939464e-07, 'epoch': 0.92}
{'loss': 1.0622, 'grad_norm': 1.6481170654296875, 'learning_rate': 2.938022525099982e-07, 'epoch': 0.93}
{'loss': 1.1596, 'grad_norm': 1.620964765548706, 'learning_rate': 2.9230488000621003e-07, 'epoch': 0.93}
{'loss': 1.0757, 'grad_norm': 1.521796703338623, 'learning_rate': 2.908112763639137e-07, 'epoch': 0.93}
{'loss': 1.1179, 'grad_norm': 1.5191045999526978, 'learning_rate': 2.8932144216297643e-07, 'epoch': 0.93}
{'loss': 1.0968, 'grad_norm': 1.5748494863510132, 'learning_rate': 2.878353779818044e-07, 'epoch': 0.93}
{'loss': 1.116, 'grad_norm': 1.2293400764465332, 'learning_rate': 2.863530843973372e-07, 'epoch': 0.93}
{'loss': 1.1063, 'grad_norm': 1.6089149713516235, 'learning_rate': 2.848745619850546e-07, 'epoch': 0.93}
{'loss': 1.0079, 'grad_norm': 1.3786770105361938, 'learning_rate': 2.833998113189662e-07, 'epoch': 0.93}
{'loss': 1.0609, 'grad_norm': 1.499120831489563, 'learning_rate': 2.8192883297162634e-07, 'epoch': 0.93}
{'loss': 1.0006, 'grad_norm': 1.6274546384811401, 'learning_rate': 2.804616275141148e-07, 'epoch': 0.93}
{'loss': 1.1363, 'grad_norm': 1.575074553489685, 'learning_rate': 2.7899819551605256e-07, 'epoch': 0.93}
{'loss': 1.209, 'grad_norm': 1.2351733446121216, 'learning_rate': 2.7753853754559634e-07, 'epoch': 0.93}
Error with image file is truncated (57 bytes not processed)
{'loss': 1.2058, 'grad_norm': 1.3287584781646729, 'learning_rate': 2.760826541694328e-07, 'epoch': 0.93}
{'loss': 1.1685, 'grad_norm': 1.532366394996643, 'learning_rate': 2.746305459527876e-07, 'epoch': 0.93}
{'loss': 1.2087, 'grad_norm': 1.270002841949463, 'learning_rate': 2.7318221345941865e-07, 'epoch': 0.93}
{'loss': 0.9736, 'grad_norm': 1.3054341077804565, 'learning_rate': 2.717376572516184e-07, 'epoch': 0.93}
{'loss': 1.0884, 'grad_norm': 1.4749499559402466, 'learning_rate': 2.7029687789021377e-07, 'epoch': 0.93}
{'loss': 1.0236, 'grad_norm': 1.4347785711288452, 'learning_rate': 2.688598759345651e-07, 'epoch': 0.93}
{'loss': 1.1869, 'grad_norm': 1.5304758548736572, 'learning_rate': 2.67426651942565e-07, 'epoch': 0.93}
{'loss': 1.0774, 'grad_norm': 1.3925083875656128, 'learning_rate': 2.659972064706406e-07, 'epoch': 0.93}
{'loss': 1.1054, 'grad_norm': 1.4436520338058472, 'learning_rate': 2.645715400737536e-07, 'epoch': 0.93}
{'loss': 1.0764, 'grad_norm': 1.5923967361450195, 'learning_rate': 2.631496533053934e-07, 'epoch': 0.93}
{'loss': 1.1108, 'grad_norm': 1.5548601150512695, 'learning_rate': 2.6173154671758847e-07, 'epoch': 0.93}
{'loss': 1.155, 'grad_norm': 1.41002357006073, 'learning_rate': 2.603172208608962e-07, 'epoch': 0.93}
{'loss': 1.1778, 'grad_norm': 1.283107042312622, 'learning_rate': 2.589066762844039e-07, 'epoch': 0.93}
{'loss': 1.1159, 'grad_norm': 1.5731767416000366, 'learning_rate': 2.57499913535737e-07, 'epoch': 0.93}
{'loss': 1.0863, 'grad_norm': 1.5267815589904785, 'learning_rate': 2.5609693316104745e-07, 'epoch': 0.93}
{'loss': 1.0898, 'grad_norm': 1.5645910501480103, 'learning_rate': 2.5469773570502063e-07, 'epoch': 0.93}
{'loss': 1.1025, 'grad_norm': 1.5291539430618286, 'learning_rate': 2.5330232171087433e-07, 'epoch': 0.93}
{'loss': 1.1577, 'grad_norm': 1.5676244497299194, 'learning_rate': 2.51910691720354e-07, 'epoch': 0.93}
{'loss': 1.1735, 'grad_norm': 1.5998388528823853, 'learning_rate': 2.5052284627374077e-07, 'epoch': 0.93}
{'loss': 1.1383, 'grad_norm': 1.4987119436264038, 'learning_rate': 2.491387859098426e-07, 'epoch': 0.93}
{'loss': 1.11, 'grad_norm': 1.5261226892471313, 'learning_rate': 2.477585111659997e-07, 'epoch': 0.93}
{'loss': 1.0788, 'grad_norm': 1.6446478366851807, 'learning_rate': 2.463820225780811e-07, 'epoch': 0.93}
{'loss': 1.0971, 'grad_norm': 1.5161339044570923, 'learning_rate': 2.4500932068049046e-07, 'epoch': 0.93}
{'loss': 1.0984, 'grad_norm': 1.4181609153747559, 'learning_rate': 2.4364040600615477e-07, 'epoch': 0.93}
{'loss': 1.1133, 'grad_norm': 1.5139575004577637, 'learning_rate': 2.422752790865346e-07, 'epoch': 0.93}
{'loss': 1.1496, 'grad_norm': 1.6208857297897339, 'learning_rate': 2.409139404516203e-07, 'epoch': 0.93}
{'loss': 1.1262, 'grad_norm': 1.465480923652649, 'learning_rate': 2.3955639062992696e-07, 'epoch': 0.93}
{'loss': 1.0576, 'grad_norm': 1.577123761177063, 'learning_rate': 2.3820263014850741e-07, 'epoch': 0.93}
{'loss': 1.1193, 'grad_norm': 1.5002273321151733, 'learning_rate': 2.3685265953293345e-07, 'epoch': 0.93}
{'loss': 1.1129, 'grad_norm': 1.4607949256896973, 'learning_rate': 2.3550647930731362e-07, 'epoch': 0.93}
{'loss': 1.0972, 'grad_norm': 1.3829870223999023, 'learning_rate': 2.3416408999427876e-07, 'epoch': 0.93}
{'loss': 1.0067, 'grad_norm': 1.544791579246521, 'learning_rate': 2.3282549211499307e-07, 'epoch': 0.93}
{'loss': 1.1813, 'grad_norm': 1.587941288948059, 'learning_rate': 2.3149068618914417e-07, 'epoch': 0.93}
{'loss': 1.1083, 'grad_norm': 1.4649912118911743, 'learning_rate': 2.3015967273494867e-07, 'epoch': 0.93}
{'loss': 1.1507, 'grad_norm': 1.473507046699524, 'learning_rate': 2.2883245226915652e-07, 'epoch': 0.93}
{'loss': 1.1589, 'grad_norm': 1.520318627357483, 'learning_rate': 2.2750902530703667e-07, 'epoch': 0.93}
{'loss': 1.1324, 'grad_norm': 1.4525271654129028, 'learning_rate': 2.2618939236238924e-07, 'epoch': 0.93}
{'loss': 1.1181, 'grad_norm': 1.588853120803833, 'learning_rate': 2.2487355394754328e-07, 'epoch': 0.93}
{'loss': 1.0792, 'grad_norm': 1.517266869544983, 'learning_rate': 2.2356151057334908e-07, 'epoch': 0.93}
{'loss': 1.0816, 'grad_norm': 1.4867219924926758, 'learning_rate': 2.2225326274919135e-07, 'epoch': 0.93}
{'loss': 1.0899, 'grad_norm': 1.5088516473770142, 'learning_rate': 2.209488109829727e-07, 'epoch': 0.94}
{'loss': 1.0302, 'grad_norm': 1.4295862913131714, 'learning_rate': 2.196481557811303e-07, 'epoch': 0.94}
{'loss': 1.1018, 'grad_norm': 1.628671646118164, 'learning_rate': 2.1835129764861907e-07, 'epoch': 0.94}
{'loss': 1.0853, 'grad_norm': 1.5122116804122925, 'learning_rate': 2.1705823708892737e-07, 'epoch': 0.94}
{'loss': 1.1271, 'grad_norm': 1.6253472566604614, 'learning_rate': 2.1576897460406477e-07, 'epoch': 0.94}
{'loss': 1.1555, 'grad_norm': 1.7098537683486938, 'learning_rate': 2.144835106945664e-07, 'epoch': 0.94}
{'loss': 1.1005, 'grad_norm': 1.5087610483169556, 'learning_rate': 2.1320184585949532e-07, 'epoch': 0.94}
{'loss': 1.0997, 'grad_norm': 1.5872960090637207, 'learning_rate': 2.119239805964357e-07, 'epoch': 0.94}
{'loss': 1.1098, 'grad_norm': 1.467262625694275, 'learning_rate': 2.106499154015018e-07, 'epoch': 0.94}
{'loss': 1.1269, 'grad_norm': 1.3936353921890259, 'learning_rate': 2.0937965076932576e-07, 'epoch': 0.94}
{'loss': 1.0888, 'grad_norm': 1.476940393447876, 'learning_rate': 2.0811318719307194e-07, 'epoch': 0.94}
{'loss': 1.1013, 'grad_norm': 1.437599539756775, 'learning_rate': 2.0685052516442373e-07, 'epoch': 0.94}
{'loss': 1.0502, 'grad_norm': 1.6440765857696533, 'learning_rate': 2.0559166517358787e-07, 'epoch': 0.94}
{'loss': 1.0499, 'grad_norm': 1.5405595302581787, 'learning_rate': 2.0433660770930009e-07, 'epoch': 0.94}
{'loss': 1.0697, 'grad_norm': 1.5299570560455322, 'learning_rate': 2.0308535325881616e-07, 'epoch': 0.94}
{'loss': 1.146, 'grad_norm': 1.5117435455322266, 'learning_rate': 2.0183790230791532e-07, 'epoch': 0.94}
{'loss': 1.1334, 'grad_norm': 1.6005245447158813, 'learning_rate': 2.0059425534090128e-07, 'epoch': 0.94}
{'loss': 1.0474, 'grad_norm': 1.507087230682373, 'learning_rate': 1.9935441284059998e-07, 'epoch': 0.94}
{'loss': 1.0886, 'grad_norm': 1.540594458580017, 'learning_rate': 1.981183752883631e-07, 'epoch': 0.94}
{'loss': 1.1036, 'grad_norm': 1.4040497541427612, 'learning_rate': 1.9688614316406006e-07, 'epoch': 0.94}
{'loss': 1.0513, 'grad_norm': 1.5823081731796265, 'learning_rate': 1.9565771694608937e-07, 'epoch': 0.94}
{'loss': 0.9968, 'grad_norm': 1.4338757991790771, 'learning_rate': 1.9443309711136393e-07, 'epoch': 0.94}
{'loss': 1.1447, 'grad_norm': 1.2137378454208374, 'learning_rate': 1.9321228413532788e-07, 'epoch': 0.94}
{'loss': 1.1797, 'grad_norm': 1.5420687198638916, 'learning_rate': 1.9199527849194098e-07, 'epoch': 0.94}
{'loss': 1.0948, 'grad_norm': 1.5421462059020996, 'learning_rate': 1.907820806536842e-07, 'epoch': 0.94}
{'loss': 1.1614, 'grad_norm': 1.6289258003234863, 'learning_rate': 1.895726910915663e-07, 'epoch': 0.94}
{'loss': 1.0737, 'grad_norm': 1.6866191625595093, 'learning_rate': 1.883671102751128e-07, 'epoch': 0.94}
{'loss': 1.005, 'grad_norm': 1.5476573705673218, 'learning_rate': 1.8716533867237153e-07, 'epoch': 0.94}
{'loss': 1.069, 'grad_norm': 1.3905247449874878, 'learning_rate': 1.859673767499115e-07, 'epoch': 0.94}
{'loss': 1.0551, 'grad_norm': 1.4906044006347656, 'learning_rate': 1.847732249728218e-07, 'epoch': 0.94}
{'loss': 1.0019, 'grad_norm': 1.4782549142837524, 'learning_rate': 1.83582883804716e-07, 'epoch': 0.94}
{'loss': 1.0436, 'grad_norm': 1.553826928138733, 'learning_rate': 1.8239635370772223e-07, 'epoch': 0.94}
{'loss': 1.0141, 'grad_norm': 1.364982008934021, 'learning_rate': 1.8121363514249534e-07, 'epoch': 0.94}
{'loss': 1.1095, 'grad_norm': 1.4574693441390991, 'learning_rate': 1.8003472856820469e-07, 'epoch': 0.94}
{'loss': 1.0377, 'grad_norm': 1.4437006711959839, 'learning_rate': 1.7885963444254528e-07, 'epoch': 0.94}
{'loss': 1.083, 'grad_norm': 1.5000110864639282, 'learning_rate': 1.7768835322172552e-07, 'epoch': 0.94}
{'loss': 1.0693, 'grad_norm': 1.5154922008514404, 'learning_rate': 1.7652088536048052e-07, 'epoch': 0.94}
{'loss': 1.0409, 'grad_norm': 1.5310688018798828, 'learning_rate': 1.7535723131206106e-07, 'epoch': 0.94}
{'loss': 1.1527, 'grad_norm': 1.2015480995178223, 'learning_rate': 1.7419739152823468e-07, 'epoch': 0.94}
{'loss': 1.1002, 'grad_norm': 1.5342378616333008, 'learning_rate': 1.7304136645929448e-07, 'epoch': 0.94}
{'loss': 1.1038, 'grad_norm': 1.4258878231048584, 'learning_rate': 1.7188915655404814e-07, 'epoch': 0.94}
{'loss': 1.0381, 'grad_norm': 1.4159538745880127, 'learning_rate': 1.707407622598223e-07, 'epoch': 0.94}
{'loss': 1.0786, 'grad_norm': 1.554136037826538, 'learning_rate': 1.695961840224636e-07, 'epoch': 0.94}
{'loss': 0.9936, 'grad_norm': 1.448433518409729, 'learning_rate': 1.6845542228633772e-07, 'epoch': 0.94}
{'loss': 1.1646, 'grad_norm': 1.533661961555481, 'learning_rate': 1.6731847749432705e-07, 'epoch': 0.94}
{'loss': 1.0941, 'grad_norm': 1.4828705787658691, 'learning_rate': 1.6618535008783075e-07, 'epoch': 0.94}
{'loss': 1.1011, 'grad_norm': 1.407428503036499, 'learning_rate': 1.6505604050677249e-07, 'epoch': 0.94}
{'loss': 1.1303, 'grad_norm': 1.488613486289978, 'learning_rate': 1.6393054918958373e-07, 'epoch': 0.94}
{'loss': 1.1811, 'grad_norm': 1.1972477436065674, 'learning_rate': 1.6280887657322276e-07, 'epoch': 0.94}
{'loss': 1.2127, 'grad_norm': 1.2795217037200928, 'learning_rate': 1.616910230931612e-07, 'epoch': 0.94}
{'loss': 1.0673, 'grad_norm': 1.2938300371170044, 'learning_rate': 1.6057698918338526e-07, 'epoch': 0.94}
{'loss': 1.0794, 'grad_norm': 1.5220412015914917, 'learning_rate': 1.5946677527640563e-07, 'epoch': 0.94}
{'loss': 1.1155, 'grad_norm': 1.4992024898529053, 'learning_rate': 1.5836038180324198e-07, 'epoch': 0.95}
{'loss': 1.05, 'grad_norm': 1.5173821449279785, 'learning_rate': 1.5725780919343624e-07, 'epoch': 0.95}
{'loss': 1.0575, 'grad_norm': 1.5576016902923584, 'learning_rate': 1.561590578750438e-07, 'epoch': 0.95}
{'loss': 1.0807, 'grad_norm': 1.5995622873306274, 'learning_rate': 1.55064128274639e-07, 'epoch': 0.95}
{'loss': 1.0843, 'grad_norm': 1.475182056427002, 'learning_rate': 1.5397302081731069e-07, 'epoch': 0.95}
{'loss': 1.0939, 'grad_norm': 1.4735267162322998, 'learning_rate': 1.5288573592666445e-07, 'epoch': 0.95}
{'loss': 1.1853, 'grad_norm': 0.8139126896858215, 'learning_rate': 1.518022740248215e-07, 'epoch': 0.95}
{'loss': 1.1597, 'grad_norm': 1.5835597515106201, 'learning_rate': 1.5072263553241872e-07, 'epoch': 0.95}
{'loss': 1.0957, 'grad_norm': 1.6123816967010498, 'learning_rate': 1.4964682086861082e-07, 'epoch': 0.95}
{'loss': 1.0624, 'grad_norm': 1.5838828086853027, 'learning_rate': 1.4857483045106258e-07, 'epoch': 0.95}
{'loss': 0.9531, 'grad_norm': 1.4296163320541382, 'learning_rate': 1.475066646959611e-07, 'epoch': 0.95}
{'loss': 1.1617, 'grad_norm': 1.558847427368164, 'learning_rate': 1.4644232401800352e-07, 'epoch': 0.95}
{'loss': 1.0397, 'grad_norm': 1.575762152671814, 'learning_rate': 1.4538180883040264e-07, 'epoch': 0.95}
{'loss': 1.2055, 'grad_norm': 0.8716954588890076, 'learning_rate': 1.4432511954488915e-07, 'epoch': 0.95}
{'loss': 1.1202, 'grad_norm': 1.4292263984680176, 'learning_rate': 1.4327225657170485e-07, 'epoch': 0.95}
{'loss': 1.1212, 'grad_norm': 1.4930576086044312, 'learning_rate': 1.4222322031960723e-07, 'epoch': 0.95}
{'loss': 1.0035, 'grad_norm': 1.6528174877166748, 'learning_rate': 1.411780111958694e-07, 'epoch': 0.95}
{'loss': 1.1391, 'grad_norm': 1.5261788368225098, 'learning_rate': 1.4013662960627562e-07, 'epoch': 0.95}
{'loss': 1.1194, 'grad_norm': 1.5591260194778442, 'learning_rate': 1.3909907595512806e-07, 'epoch': 0.95}
{'loss': 1.1131, 'grad_norm': 1.7575572729110718, 'learning_rate': 1.3806535064524006e-07, 'epoch': 0.95}
{'loss': 1.1357, 'grad_norm': 1.4769287109375, 'learning_rate': 1.3703545407793951e-07, 'epoch': 0.95}
{'loss': 1.1254, 'grad_norm': 1.4479533433914185, 'learning_rate': 1.360093866530665e-07, 'epoch': 0.95}
{'loss': 1.085, 'grad_norm': 1.5642166137695312, 'learning_rate': 1.34987148768978e-07, 'epoch': 0.95}
{'loss': 1.0483, 'grad_norm': 1.4581166505813599, 'learning_rate': 1.3396874082253986e-07, 'epoch': 0.95}
{'loss': 1.2543, 'grad_norm': 1.3024895191192627, 'learning_rate': 1.3295416320913357e-07, 'epoch': 0.95}
{'loss': 1.1216, 'grad_norm': 1.5079439878463745, 'learning_rate': 1.3194341632265518e-07, 'epoch': 0.95}
{'loss': 1.1477, 'grad_norm': 1.639076828956604, 'learning_rate': 1.3093650055550855e-07, 'epoch': 0.95}
{'loss': 1.0513, 'grad_norm': 1.5789247751235962, 'learning_rate': 1.2993341629861432e-07, 'epoch': 0.95}
{'loss': 1.0402, 'grad_norm': 1.44447922706604, 'learning_rate': 1.2893416394140323e-07, 'epoch': 0.95}
{'loss': 1.119, 'grad_norm': 1.3496037721633911, 'learning_rate': 1.279387438718216e-07, 'epoch': 0.95}
{'loss': 1.1202, 'grad_norm': 1.5105869770050049, 'learning_rate': 1.269471564763247e-07, 'epoch': 0.95}
{'loss': 1.116, 'grad_norm': 1.556692361831665, 'learning_rate': 1.2595940213988024e-07, 'epoch': 0.95}
{'loss': 1.0062, 'grad_norm': 1.3835430145263672, 'learning_rate': 1.2497548124597026e-07, 'epoch': 0.95}
{'loss': 1.204, 'grad_norm': 1.5231150388717651, 'learning_rate': 1.2399539417658368e-07, 'epoch': 0.95}
{'loss': 1.1974, 'grad_norm': 1.6233103275299072, 'learning_rate': 1.2301914131222726e-07, 'epoch': 0.95}
{'loss': 1.0457, 'grad_norm': 1.4945051670074463, 'learning_rate': 1.2204672303191335e-07, 'epoch': 0.95}
{'loss': 1.15, 'grad_norm': 1.5953234434127808, 'learning_rate': 1.2107813971317106e-07, 'epoch': 0.95}
{'loss': 1.244, 'grad_norm': 1.396527886390686, 'learning_rate': 1.201133917320363e-07, 'epoch': 0.95}
{'loss': 0.9843, 'grad_norm': 1.3843121528625488, 'learning_rate': 1.1915247946305498e-07, 'epoch': 0.95}
{'loss': 1.0962, 'grad_norm': 1.444260597229004, 'learning_rate': 1.1819540327929092e-07, 'epoch': 0.95}
{'loss': 1.05, 'grad_norm': 1.5816582441329956, 'learning_rate': 1.1724216355231022e-07, 'epoch': 0.95}
{'loss': 1.078, 'grad_norm': 1.682815432548523, 'learning_rate': 1.1629276065219575e-07, 'epoch': 0.95}
{'loss': 1.0976, 'grad_norm': 1.5117549896240234, 'learning_rate': 1.1534719494753821e-07, 'epoch': 0.95}
{'loss': 1.0383, 'grad_norm': 1.36614191532135, 'learning_rate': 1.144054668054373e-07, 'epoch': 0.95}
{'loss': 1.0495, 'grad_norm': 1.4944010972976685, 'learning_rate': 1.1346757659150498e-07, 'epoch': 0.95}
{'loss': 1.1055, 'grad_norm': 1.5470530986785889, 'learning_rate': 1.1253352466986334e-07, 'epoch': 0.95}
{'loss': 1.0977, 'grad_norm': 1.4423433542251587, 'learning_rate': 1.116033114031434e-07, 'epoch': 0.95}
{'loss': 1.1218, 'grad_norm': 1.1314011812210083, 'learning_rate': 1.1067693715248406e-07, 'epoch': 0.95}
{'loss': 1.0614, 'grad_norm': 1.5094494819641113, 'learning_rate': 1.0975440227753764e-07, 'epoch': 0.95}
{'loss': 1.0736, 'grad_norm': 1.3995169401168823, 'learning_rate': 1.0883570713646318e-07, 'epoch': 0.95}
{'loss': 1.0425, 'grad_norm': 1.4908103942871094, 'learning_rate': 1.0792085208593095e-07, 'epoch': 0.95}
{'loss': 1.0878, 'grad_norm': 1.469302773475647, 'learning_rate': 1.0700983748111792e-07, 'epoch': 0.95}
{'loss': 1.1778, 'grad_norm': 1.600010871887207, 'learning_rate': 1.061026636757101e-07, 'epoch': 0.96}
{'loss': 1.1331, 'grad_norm': 1.323808193206787, 'learning_rate': 1.0519933102190682e-07, 'epoch': 0.96}
{'loss': 1.1991, 'grad_norm': 1.6753255128860474, 'learning_rate': 1.0429983987041092e-07, 'epoch': 0.96}
{'loss': 1.0464, 'grad_norm': 1.534961223602295, 'learning_rate': 1.0340419057043527e-07, 'epoch': 0.96}
{'loss': 1.0096, 'grad_norm': 1.5880811214447021, 'learning_rate': 1.0251238346970393e-07, 'epoch': 0.96}
{'loss': 1.0184, 'grad_norm': 1.497521996498108, 'learning_rate': 1.0162441891444441e-07, 'epoch': 0.96}
{'loss': 1.1008, 'grad_norm': 1.5430527925491333, 'learning_rate': 1.007402972493976e-07, 'epoch': 0.96}
{'loss': 1.1468, 'grad_norm': 1.7275893688201904, 'learning_rate': 9.986001881780783e-08, 'epoch': 0.96}
{'loss': 1.2384, 'grad_norm': 1.2397040128707886, 'learning_rate': 9.898358396143171e-08, 'epoch': 0.96}
{'loss': 1.0652, 'grad_norm': 1.6085797548294067, 'learning_rate': 9.811099302052928e-08, 'epoch': 0.96}
{'loss': 1.2269, 'grad_norm': 1.2036240100860596, 'learning_rate': 9.72422463338718e-08, 'epoch': 0.96}
{'loss': 1.0846, 'grad_norm': 1.4837361574172974, 'learning_rate': 9.637734423873612e-08, 'epoch': 0.96}
{'loss': 1.0566, 'grad_norm': 1.3642407655715942, 'learning_rate': 9.55162870709081e-08, 'epoch': 0.96}
{'loss': 1.1722, 'grad_norm': 1.289382815361023, 'learning_rate': 9.465907516467698e-08, 'epoch': 0.96}
{'loss': 1.1447, 'grad_norm': 1.3291876316070557, 'learning_rate': 9.380570885284546e-08, 'epoch': 0.96}
{'loss': 1.1013, 'grad_norm': 1.5018305778503418, 'learning_rate': 9.295618846671739e-08, 'epoch': 0.96}
{'loss': 1.0147, 'grad_norm': 1.3553627729415894, 'learning_rate': 9.211051433610674e-08, 'epoch': 0.96}
{'loss': 1.1023, 'grad_norm': 1.4560152292251587, 'learning_rate': 9.126868678933198e-08, 'epoch': 0.96}
{'loss': 1.0812, 'grad_norm': 1.4726651906967163, 'learning_rate': 9.04307061532217e-08, 'epoch': 0.96}
{'loss': 1.0114, 'grad_norm': 1.5363906621932983, 'learning_rate': 8.959657275310674e-08, 'epoch': 0.96}
{'loss': 1.1354, 'grad_norm': 1.5724207162857056, 'learning_rate': 8.876628691282918e-08, 'epoch': 0.96}
{'loss': 1.2176, 'grad_norm': 1.2680304050445557, 'learning_rate': 8.793984895473117e-08, 'epoch': 0.96}
{'loss': 1.1102, 'grad_norm': 1.5452954769134521, 'learning_rate': 8.711725919966718e-08, 'epoch': 0.96}
{'loss': 1.0483, 'grad_norm': 1.393678069114685, 'learning_rate': 8.629851796699284e-08, 'epoch': 0.96}
{'loss': 1.159, 'grad_norm': 1.167966604232788, 'learning_rate': 8.54836255745728e-08, 'epoch': 0.96}
{'loss': 1.0758, 'grad_norm': 1.5231893062591553, 'learning_rate': 8.467258233877728e-08, 'epoch': 0.96}
{'loss': 1.1378, 'grad_norm': 1.5412770509719849, 'learning_rate': 8.386538857447779e-08, 'epoch': 0.96}
{'loss': 1.0871, 'grad_norm': 1.4435516595840454, 'learning_rate': 8.306204459505807e-08, 'epoch': 0.96}
{'loss': 1.0063, 'grad_norm': 1.4648642539978027, 'learning_rate': 8.226255071240308e-08, 'epoch': 0.96}
{'loss': 1.1566, 'grad_norm': 1.5693683624267578, 'learning_rate': 8.146690723690342e-08, 'epoch': 0.96}
{'loss': 1.0811, 'grad_norm': 1.461702823638916, 'learning_rate': 8.067511447745535e-08, 'epoch': 0.96}
{'loss': 1.0404, 'grad_norm': 1.5498244762420654, 'learning_rate': 7.988717274146074e-08, 'epoch': 0.96}
{'loss': 1.1661, 'grad_norm': 1.2370905876159668, 'learning_rate': 7.910308233482488e-08, 'epoch': 0.96}
{'loss': 1.1386, 'grad_norm': 1.5902975797653198, 'learning_rate': 7.832284356195764e-08, 'epoch': 0.96}
{'loss': 1.1933, 'grad_norm': 1.2684497833251953, 'learning_rate': 7.754645672577776e-08, 'epoch': 0.96}
{'loss': 1.0699, 'grad_norm': 1.5205283164978027, 'learning_rate': 7.677392212770196e-08, 'epoch': 0.96}
{'loss': 1.1555, 'grad_norm': 1.6555204391479492, 'learning_rate': 7.600524006765808e-08, 'epoch': 0.96}
{'loss': 1.0601, 'grad_norm': 1.481169581413269, 'learning_rate': 7.524041084407185e-08, 'epoch': 0.96}
{'loss': 1.1374, 'grad_norm': 1.6080900430679321, 'learning_rate': 7.447943475387797e-08, 'epoch': 0.96}
{'loss': 1.0915, 'grad_norm': 1.421107530593872, 'learning_rate': 7.372231209251346e-08, 'epoch': 0.96}
{'loss': 1.1432, 'grad_norm': 1.5693591833114624, 'learning_rate': 7.296904315391873e-08, 'epoch': 0.96}
{'loss': 1.0773, 'grad_norm': 1.563380241394043, 'learning_rate': 7.221962823053874e-08, 'epoch': 0.96}
{'loss': 1.0743, 'grad_norm': 1.4462693929672241, 'learning_rate': 7.147406761332298e-08, 'epoch': 0.96}
{'loss': 1.1325, 'grad_norm': 1.4564787149429321, 'learning_rate': 7.073236159172325e-08, 'epoch': 0.96}
Error with image file is truncated (36 bytes not processed)
{'loss': 1.1122, 'grad_norm': 1.5506339073181152, 'learning_rate': 6.999451045369587e-08, 'epoch': 0.96}
{'loss': 1.2051, 'grad_norm': 1.232382893562317, 'learning_rate': 6.926051448569948e-08, 'epoch': 0.96}
{'loss': 0.9765, 'grad_norm': 1.417688250541687, 'learning_rate': 6.853037397269724e-08, 'epoch': 0.96}
{'loss': 0.9799, 'grad_norm': 1.4605469703674316, 'learning_rate': 6.78040891981524e-08, 'epoch': 0.96}
{'loss': 1.1226, 'grad_norm': 1.4894155263900757, 'learning_rate': 6.70816604440383e-08, 'epoch': 0.96}
{'loss': 1.1756, 'grad_norm': 1.4018409252166748, 'learning_rate': 6.63630879908217e-08, 'epoch': 0.96}
{'loss': 1.2008, 'grad_norm': 1.2889426946640015, 'learning_rate': 6.564837211748054e-08, 'epoch': 0.96}
{'loss': 1.0864, 'grad_norm': 1.4819327592849731, 'learning_rate': 6.493751310149177e-08, 'epoch': 0.96}
{'loss': 1.1449, 'grad_norm': 1.536072015762329, 'learning_rate': 6.42305112188335e-08, 'epoch': 0.97}
{'loss': 1.1064, 'grad_norm': 1.622137427330017, 'learning_rate': 6.352736674398951e-08, 'epoch': 0.97}
{'loss': 1.1935, 'grad_norm': 1.262982726097107, 'learning_rate': 6.282807994994477e-08, 'epoch': 0.97}
{'loss': 0.9596, 'grad_norm': 1.316372036933899, 'learning_rate': 6.213265110818656e-08, 'epoch': 0.97}
{'loss': 1.193, 'grad_norm': 1.23525071144104, 'learning_rate': 6.144108048870335e-08, 'epoch': 0.97}
{'loss': 1.0621, 'grad_norm': 1.7123245000839233, 'learning_rate': 6.075336835998813e-08, 'epoch': 0.97}
{'loss': 1.0766, 'grad_norm': 1.46040678024292, 'learning_rate': 6.00695149890329e-08, 'epoch': 0.97}
{'loss': 1.0767, 'grad_norm': 1.4918458461761475, 'learning_rate': 5.938952064133419e-08, 'epoch': 0.97}
{'loss': 1.0887, 'grad_norm': 1.6691575050354004, 'learning_rate': 5.871338558088857e-08, 'epoch': 0.97}
{'loss': 1.1293, 'grad_norm': 1.4943372011184692, 'learning_rate': 5.8041110070194976e-08, 'epoch': 0.97}
{'loss': 1.0658, 'grad_norm': 1.427114486694336, 'learning_rate': 5.7372694370254614e-08, 'epoch': 0.97}
{'loss': 1.1278, 'grad_norm': 1.4966150522232056, 'learning_rate': 5.67081387405688e-08, 'epoch': 0.97}
{'loss': 1.0747, 'grad_norm': 1.4506556987762451, 'learning_rate': 5.6047443439141146e-08, 'epoch': 0.97}
{'loss': 1.1234, 'grad_norm': 1.5232187509536743, 'learning_rate': 5.539060872247537e-08, 'epoch': 0.97}
{'loss': 1.0071, 'grad_norm': 1.397278070449829, 'learning_rate': 5.47376348455797e-08, 'epoch': 0.97}
{'loss': 1.1174, 'grad_norm': 1.4599099159240723, 'learning_rate': 5.408852206195914e-08, 'epoch': 0.97}
{'loss': 1.0001, 'grad_norm': 1.520208477973938, 'learning_rate': 5.344327062362098e-08, 'epoch': 0.97}
{'loss': 1.0669, 'grad_norm': 1.470380187034607, 'learning_rate': 5.2801880781075954e-08, 'epoch': 0.97}
{'loss': 1.0723, 'grad_norm': 1.4043906927108765, 'learning_rate': 5.216435278333376e-08, 'epoch': 0.97}
{'loss': 1.1001, 'grad_norm': 1.4411983489990234, 'learning_rate': 5.153068687790197e-08, 'epoch': 0.97}
{'loss': 1.0982, 'grad_norm': 1.5053681135177612, 'learning_rate': 5.0900883310794903e-08, 'epoch': 0.97}
{'loss': 1.1617, 'grad_norm': 1.539210319519043, 'learning_rate': 5.0274942326521414e-08, 'epoch': 0.97}
{'loss': 1.0973, 'grad_norm': 1.440451979637146, 'learning_rate': 4.9652864168096e-08, 'epoch': 0.97}
{'loss': 1.0641, 'grad_norm': 1.5640215873718262, 'learning_rate': 4.9034649077027706e-08, 'epoch': 0.97}
{'loss': 1.2161, 'grad_norm': 1.6388353109359741, 'learning_rate': 4.84202972933312e-08, 'epoch': 0.97}
{'loss': 1.1188, 'grad_norm': 1.3941240310668945, 'learning_rate': 4.7809809055517906e-08, 'epoch': 0.97}
{'loss': 1.0986, 'grad_norm': 1.5315260887145996, 'learning_rate': 4.720318460060047e-08, 'epoch': 0.97}
{'loss': 1.0649, 'grad_norm': 1.4360533952713013, 'learning_rate': 4.6600424164091606e-08, 'epoch': 0.97}
{'loss': 1.0449, 'grad_norm': 1.458165168762207, 'learning_rate': 4.6001527980004125e-08, 'epoch': 0.97}
{'loss': 1.1395, 'grad_norm': 1.487078309059143, 'learning_rate': 4.54064962808487e-08, 'epoch': 0.97}
{'loss': 1.0435, 'grad_norm': 1.6017611026763916, 'learning_rate': 4.4815329297639434e-08, 'epoch': 0.97}
{'loss': 1.1349, 'grad_norm': 1.4681905508041382, 'learning_rate': 4.422802725988606e-08, 'epoch': 0.97}
{'loss': 1.0726, 'grad_norm': 1.3760133981704712, 'learning_rate': 4.364459039559843e-08, 'epoch': 0.97}
{'loss': 1.1522, 'grad_norm': 1.513654112815857, 'learning_rate': 4.3065018931289784e-08, 'epoch': 0.97}
{'loss': 1.1445, 'grad_norm': 1.4927647113800049, 'learning_rate': 4.248931309196791e-08, 'epoch': 0.97}
{'loss': 1.0809, 'grad_norm': 1.401402235031128, 'learning_rate': 4.1917473101140696e-08, 'epoch': 0.97}
{'loss': 1.1028, 'grad_norm': 1.4748934507369995, 'learning_rate': 4.134949918081832e-08, 'epoch': 0.97}
{'loss': 1.0955, 'grad_norm': 1.455357313156128, 'learning_rate': 4.0785391551506626e-08, 'epoch': 0.97}
{'loss': 1.0849, 'grad_norm': 1.4131145477294922, 'learning_rate': 4.022515043221154e-08, 'epoch': 0.97}
{'loss': 1.1489, 'grad_norm': 1.4573081731796265, 'learning_rate': 3.966877604043795e-08, 'epoch': 0.97}
Error with image file is truncated (33 bytes not processed)
{'loss': 1.116, 'grad_norm': 1.600618839263916, 'learning_rate': 3.9116268592189755e-08, 'epoch': 0.97}
{'loss': 1.0756, 'grad_norm': 1.569637656211853, 'learning_rate': 3.8567628301969806e-08, 'epoch': 0.97}
{'loss': 1.1429, 'grad_norm': 1.5448602437973022, 'learning_rate': 3.802285538277772e-08, 'epoch': 0.97}
{'loss': 1.1215, 'grad_norm': 1.6355888843536377, 'learning_rate': 3.748195004611543e-08, 'epoch': 0.97}
{'loss': 1.1616, 'grad_norm': 1.1411164999008179, 'learning_rate': 3.69449125019794e-08, 'epoch': 0.97}
{'loss': 1.0639, 'grad_norm': 1.4147881269454956, 'learning_rate': 3.6411742958866184e-08, 'epoch': 0.97}
{'loss': 1.0904, 'grad_norm': 1.5761847496032715, 'learning_rate': 3.588244162377019e-08, 'epoch': 0.97}
{'loss': 1.1214, 'grad_norm': 1.630840539932251, 'learning_rate': 3.5357008702185945e-08, 'epoch': 0.97}
{'loss': 1.0944, 'grad_norm': 1.5384896993637085, 'learning_rate': 3.483544439810249e-08, 'epoch': 0.97}
{'loss': 1.0404, 'grad_norm': 1.5650774240493774, 'learning_rate': 3.4317748914011187e-08, 'epoch': 0.97}
{'loss': 1.02, 'grad_norm': 1.463195562362671, 'learning_rate': 3.3803922450897917e-08, 'epoch': 0.97}
{'loss': 1.2043, 'grad_norm': 1.5879801511764526, 'learning_rate': 3.329396520824757e-08, 'epoch': 0.97}
{'loss': 1.0845, 'grad_norm': 1.4656274318695068, 'learning_rate': 3.2787877384045095e-08, 'epoch': 0.98}
{'loss': 1.0828, 'grad_norm': 1.44140625, 'learning_rate': 3.228565917476889e-08, 'epoch': 0.98}
{'loss': 1.0415, 'grad_norm': 1.6675549745559692, 'learning_rate': 3.178731077539743e-08, 'epoch': 0.98}
{'loss': 1.1132, 'grad_norm': 1.421318531036377, 'learning_rate': 3.129283237940928e-08, 'epoch': 0.98}
{'loss': 1.1142, 'grad_norm': 1.458146333694458, 'learning_rate': 3.080222417877421e-08, 'epoch': 0.98}
{'loss': 1.0726, 'grad_norm': 1.7409141063690186, 'learning_rate': 3.031548636396764e-08, 'epoch': 0.98}
{'loss': 1.1517, 'grad_norm': 1.4912399053573608, 'learning_rate': 2.983261912395397e-08, 'epoch': 0.98}
{'loss': 1.0546, 'grad_norm': 1.4399058818817139, 'learning_rate': 2.9353622646199898e-08, 'epoch': 0.98}
{'loss': 1.0816, 'grad_norm': 1.566598653793335, 'learning_rate': 2.8878497116671124e-08, 'epoch': 0.98}
{'loss': 1.1537, 'grad_norm': 1.5748543739318848, 'learning_rate': 2.8407242719823424e-08, 'epoch': 0.98}
{'loss': 1.1537, 'grad_norm': 1.5472817420959473, 'learning_rate': 2.7939859638617118e-08, 'epoch': 0.98}
{'loss': 1.1433, 'grad_norm': 1.3928042650222778, 'learning_rate': 2.7476348054504832e-08, 'epoch': 0.98}
{'loss': 1.2071, 'grad_norm': 1.6603626012802124, 'learning_rate': 2.7016708147439285e-08, 'epoch': 0.98}
{'loss': 1.007, 'grad_norm': 1.5150928497314453, 'learning_rate': 2.6560940095866626e-08, 'epoch': 0.98}
{'loss': 1.0649, 'grad_norm': 1.4160670042037964, 'learning_rate': 2.6109044076733092e-08, 'epoch': 0.98}
{'loss': 0.9857, 'grad_norm': 1.412131428718567, 'learning_rate': 2.5661020265479452e-08, 'epoch': 0.98}
{'loss': 1.1851, 'grad_norm': 1.1462160348892212, 'learning_rate': 2.5216868836043242e-08, 'epoch': 0.98}
{'loss': 1.1076, 'grad_norm': 1.4840114116668701, 'learning_rate': 2.4776589960862074e-08, 'epoch': 0.98}
{'loss': 1.1071, 'grad_norm': 1.3717961311340332, 'learning_rate': 2.434018381086589e-08, 'epoch': 0.98}
{'loss': 1.1359, 'grad_norm': 1.4371421337127686, 'learning_rate': 2.3907650555481387e-08, 'epoch': 0.98}
{'loss': 1.0911, 'grad_norm': 1.4968230724334717, 'learning_rate': 2.3478990362634235e-08, 'epoch': 0.98}
{'loss': 1.071, 'grad_norm': 1.4088258743286133, 'learning_rate': 2.3054203398743537e-08, 'epoch': 0.98}
{'loss': 1.1574, 'grad_norm': 1.4850330352783203, 'learning_rate': 2.263328982872959e-08, 'epoch': 0.98}
{'loss': 1.154, 'grad_norm': 1.5882686376571655, 'learning_rate': 2.221624981600168e-08, 'epoch': 0.98}
{'loss': 1.1108, 'grad_norm': 1.5565764904022217, 'learning_rate': 2.1803083522471402e-08, 'epoch': 0.98}
{'loss': 1.103, 'grad_norm': 1.4960013628005981, 'learning_rate': 2.1393791108542672e-08, 'epoch': 0.98}
{'loss': 1.0786, 'grad_norm': 1.5873260498046875, 'learning_rate': 2.098837273311838e-08, 'epoch': 0.98}
WARNING: tokenization mismatch: 0 vs. 1275. (ignored)
{'loss': 1.0444, 'grad_norm': 1.5243841409683228, 'learning_rate': 2.058682855359595e-08, 'epoch': 0.98}
{'loss': 1.1639, 'grad_norm': 1.58492910861969, 'learning_rate': 2.0189158725867353e-08, 'epoch': 0.98}
{'loss': 1.0556, 'grad_norm': 1.4823830127716064, 'learning_rate': 1.979536340432131e-08, 'epoch': 0.98}
{'loss': 1.0883, 'grad_norm': 1.4801976680755615, 'learning_rate': 1.9405442741844415e-08, 'epoch': 0.98}
{'loss': 1.0571, 'grad_norm': 1.4903409481048584, 'learning_rate': 1.9019396889816688e-08, 'epoch': 0.98}
{'loss': 1.0998, 'grad_norm': 1.464288592338562, 'learning_rate': 1.8637225998114904e-08, 'epoch': 0.98}
{'loss': 1.1441, 'grad_norm': 1.5055359601974487, 'learning_rate': 1.825893021510927e-08, 'epoch': 0.98}
{'loss': 1.0866, 'grad_norm': 1.4861701726913452, 'learning_rate': 1.7884509687668972e-08, 'epoch': 0.98}
{'loss': 1.1091, 'grad_norm': 1.5939784049987793, 'learning_rate': 1.7513964561156617e-08, 'epoch': 0.98}
{'loss': 1.1455, 'grad_norm': 1.6220980882644653, 'learning_rate': 1.714729497942935e-08, 'epoch': 0.98}
{'loss': 1.0677, 'grad_norm': 1.5255769491195679, 'learning_rate': 1.6784501084843307e-08, 'epoch': 0.98}
{'loss': 1.1284, 'grad_norm': 1.4450647830963135, 'learning_rate': 1.6425583018244706e-08, 'epoch': 0.98}
{'loss': 1.075, 'grad_norm': 1.4922378063201904, 'learning_rate': 1.607054091897986e-08, 'epoch': 0.98}
{'loss': 1.0663, 'grad_norm': 1.4315731525421143, 'learning_rate': 1.57193749248874e-08, 'epoch': 0.98}
{'loss': 1.0816, 'grad_norm': 1.5090564489364624, 'learning_rate': 1.537208517230271e-08, 'epoch': 0.98}
{'loss': 0.9512, 'grad_norm': 1.3224555253982544, 'learning_rate': 1.5028671796055715e-08, 'epoch': 0.98}
{'loss': 1.1491, 'grad_norm': 1.2215914726257324, 'learning_rate': 1.4689134929470884e-08, 'epoch': 0.98}
{'loss': 1.1962, 'grad_norm': 1.2396454811096191, 'learning_rate': 1.435347470436832e-08, 'epoch': 0.98}
{'loss': 1.2372, 'grad_norm': 1.1994060277938843, 'learning_rate': 1.4021691251062675e-08, 'epoch': 0.98}
{'loss': 1.075, 'grad_norm': 1.509069800376892, 'learning_rate': 1.3693784698363133e-08, 'epoch': 0.98}
{'loss': 1.0624, 'grad_norm': 1.481851577758789, 'learning_rate': 1.3369755173575639e-08, 'epoch': 0.98}
{'loss': 1.091, 'grad_norm': 1.4918161630630493, 'learning_rate': 1.3049602802498451e-08, 'epoch': 0.98}
{'loss': 1.0871, 'grad_norm': 1.5630842447280884, 'learning_rate': 1.273332770942659e-08, 'epoch': 0.98}
{'loss': 1.0158, 'grad_norm': 1.5082528591156006, 'learning_rate': 1.2420930017148503e-08, 'epoch': 0.98}
{'loss': 1.1406, 'grad_norm': 1.6514514684677124, 'learning_rate': 1.2112409846947171e-08, 'epoch': 0.98}
{'loss': 1.0448, 'grad_norm': 1.4704314470291138, 'learning_rate': 1.1807767318602337e-08, 'epoch': 0.99}
Error with image file is truncated (44 bytes not processed)
{'loss': 1.0981, 'grad_norm': 1.4337689876556396, 'learning_rate': 1.150700255038606e-08, 'epoch': 0.99}
{'loss': 1.0918, 'grad_norm': 1.4294383525848389, 'learning_rate': 1.1210115659063825e-08, 'epoch': 0.99}
{'loss': 1.0545, 'grad_norm': 1.486962914466858, 'learning_rate': 1.0917106759900097e-08, 'epoch': 0.99}
{'loss': 1.1019, 'grad_norm': 1.4546349048614502, 'learning_rate': 1.0627975966649439e-08, 'epoch': 0.99}
{'loss': 1.0089, 'grad_norm': 1.4163087606430054, 'learning_rate': 1.034272339156206e-08, 'epoch': 0.99}
{'loss': 1.1773, 'grad_norm': 1.5638091564178467, 'learning_rate': 1.0061349145383814e-08, 'epoch': 0.99}
{'loss': 0.9846, 'grad_norm': 1.4672002792358398, 'learning_rate': 9.783853337353987e-09, 'epoch': 0.99}
{'loss': 1.0351, 'grad_norm': 1.5073344707489014, 'learning_rate': 9.510236075205292e-09, 'epoch': 0.99}
{'loss': 1.1192, 'grad_norm': 1.480486273765564, 'learning_rate': 9.240497465164978e-09, 'epoch': 0.99}
{'loss': 1.0562, 'grad_norm': 1.5728576183319092, 'learning_rate': 8.974637611955939e-09, 'epoch': 0.99}
{'loss': 1.0817, 'grad_norm': 1.3273260593414307, 'learning_rate': 8.712656618793391e-09, 'epoch': 0.99}
{'loss': 1.1339, 'grad_norm': 1.5690298080444336, 'learning_rate': 8.454554587388198e-09, 'epoch': 0.99}
{'loss': 1.0781, 'grad_norm': 1.4186211824417114, 'learning_rate': 8.200331617943535e-09, 'epoch': 0.99}
{'loss': 1.0301, 'grad_norm': 1.479000449180603, 'learning_rate': 7.949987809158232e-09, 'epoch': 0.99}
{'loss': 1.0502, 'grad_norm': 1.5369749069213867, 'learning_rate': 7.703523258223433e-09, 'epoch': 0.99}
{'loss': 1.1075, 'grad_norm': 1.3763353824615479, 'learning_rate': 7.460938060825929e-09, 'epoch': 0.99}
{'loss': 1.095, 'grad_norm': 1.4837431907653809, 'learning_rate': 7.222232311145938e-09, 'epoch': 0.99}
{'loss': 1.0736, 'grad_norm': 1.5354881286621094, 'learning_rate': 6.987406101855998e-09, 'epoch': 0.99}
{'loss': 1.1423, 'grad_norm': 1.4976686239242554, 'learning_rate': 6.756459524125403e-09, 'epoch': 0.99}
{'loss': 1.0469, 'grad_norm': 1.53030264377594, 'learning_rate': 6.5293926676135434e-09, 'epoch': 0.99}
{'loss': 1.106, 'grad_norm': 1.4276963472366333, 'learning_rate': 6.306205620477679e-09, 'epoch': 0.99}
{'loss': 1.2073, 'grad_norm': 1.2999484539031982, 'learning_rate': 6.086898469365166e-09, 'epoch': 0.99}
{'loss': 1.1137, 'grad_norm': 1.5787509679794312, 'learning_rate': 5.871471299419007e-09, 'epoch': 0.99}
{'loss': 1.1319, 'grad_norm': 1.6379574537277222, 'learning_rate': 5.6599241942767445e-09, 'epoch': 0.99}
{'loss': 1.2222, 'grad_norm': 1.3044514656066895, 'learning_rate': 5.452257236066017e-09, 'epoch': 0.99}
{'loss': 1.0559, 'grad_norm': 1.488976001739502, 'learning_rate': 5.248470505412328e-09, 'epoch': 0.99}
{'loss': 1.1725, 'grad_norm': 1.4523409605026245, 'learning_rate': 5.0485640814312844e-09, 'epoch': 0.99}
{'loss': 1.0988, 'grad_norm': 1.4316843748092651, 'learning_rate': 4.8525380417330234e-09, 'epoch': 0.99}
{'loss': 1.0202, 'grad_norm': 1.5290417671203613, 'learning_rate': 4.660392462424446e-09, 'epoch': 0.99}
{'loss': 1.1329, 'grad_norm': 1.1891131401062012, 'learning_rate': 4.472127418099215e-09, 'epoch': 0.99}
{'loss': 1.0864, 'grad_norm': 1.471580982208252, 'learning_rate': 4.287742981851084e-09, 'epoch': 0.99}
{'loss': 1.0694, 'grad_norm': 1.3239567279815674, 'learning_rate': 4.1072392252639034e-09, 'epoch': 0.99}
{'loss': 1.0877, 'grad_norm': 1.5297547578811646, 'learning_rate': 3.930616218414951e-09, 'epoch': 0.99}
{'loss': 1.0425, 'grad_norm': 1.4555505514144897, 'learning_rate': 3.757874029874931e-09, 'epoch': 0.99}
{'loss': 1.0089, 'grad_norm': 1.4053043127059937, 'learning_rate': 3.5890127267090844e-09, 'epoch': 0.99}
{'loss': 1.0811, 'grad_norm': 1.5973111391067505, 'learning_rate': 3.424032374476083e-09, 'epoch': 0.99}
{'loss': 1.1322, 'grad_norm': 1.3206851482391357, 'learning_rate': 3.2629330372246915e-09, 'epoch': 0.99}
{'loss': 1.0483, 'grad_norm': 1.5351792573928833, 'learning_rate': 3.105714777501545e-09, 'epoch': 0.99}
{'loss': 1.1863, 'grad_norm': 1.579208493232727, 'learning_rate': 2.9523776563422644e-09, 'epoch': 0.99}
{'loss': 1.2145, 'grad_norm': 1.2143622636795044, 'learning_rate': 2.802921733278119e-09, 'epoch': 0.99}
{'loss': 1.1671, 'grad_norm': 1.2018280029296875, 'learning_rate': 2.657347066333804e-09, 'epoch': 0.99}
{'loss': 1.0892, 'grad_norm': 1.5901354551315308, 'learning_rate': 2.5156537120263335e-09, 'epoch': 0.99}
{'loss': 1.3268, 'grad_norm': 1.4405957460403442, 'learning_rate': 2.3778417253650376e-09, 'epoch': 0.99}
{'loss': 1.0995, 'grad_norm': 1.5579640865325928, 'learning_rate': 2.2439111598537844e-09, 'epoch': 0.99}
{'loss': 1.0477, 'grad_norm': 1.4940330982208252, 'learning_rate': 2.113862067488759e-09, 'epoch': 0.99}
{'loss': 1.0588, 'grad_norm': 1.425783395767212, 'learning_rate': 1.987694498760684e-09, 'epoch': 0.99}
{'loss': 1.0309, 'grad_norm': 1.6526851654052734, 'learning_rate': 1.865408502650379e-09, 'epoch': 0.99}
{'loss': 1.0889, 'grad_norm': 1.5026851892471313, 'learning_rate': 1.747004126635421e-09, 'epoch': 0.99}
Error with image file is truncated (49 bytes not processed)
{'loss': 1.0043, 'grad_norm': 1.523848533630371, 'learning_rate': 1.6324814166823744e-09, 'epoch': 0.99}
{'loss': 1.03, 'grad_norm': 1.5273339748382568, 'learning_rate': 1.5218404172545609e-09, 'epoch': 0.99}
{'loss': 1.0373, 'grad_norm': 1.5238873958587646, 'learning_rate': 1.415081171305399e-09, 'epoch': 0.99}
{'loss': 1.0396, 'grad_norm': 1.491800308227539, 'learning_rate': 1.3122037202828452e-09, 'epoch': 1.0}
{'loss': 1.1632, 'grad_norm': 1.6219146251678467, 'learning_rate': 1.2132081041282829e-09, 'epoch': 1.0}
{'loss': 1.0693, 'grad_norm': 1.4761191606521606, 'learning_rate': 1.1180943612754124e-09, 'epoch': 1.0}
{'loss': 1.1539, 'grad_norm': 1.524552345275879, 'learning_rate': 1.026862528649142e-09, 'epoch': 1.0}
{'loss': 1.0174, 'grad_norm': 1.4165927171707153, 'learning_rate': 9.39512641668916e-10, 'epoch': 1.0}
{'loss': 1.1225, 'grad_norm': 1.4303399324417114, 'learning_rate': 8.560447342487177e-10, 'epoch': 1.0}
{'loss': 1.03, 'grad_norm': 1.6089361906051636, 'learning_rate': 7.764588387915161e-10, 'epoch': 1.0}
{'loss': 1.1858, 'grad_norm': 1.5575639009475708, 'learning_rate': 7.007549861970387e-10, 'epoch': 1.0}
{'loss': 1.1242, 'grad_norm': 1.557659387588501, 'learning_rate': 6.289332058551089e-10, 'epoch': 1.0}
{'loss': 1.1434, 'grad_norm': 1.3186944723129272, 'learning_rate': 5.609935256500887e-10, 'epoch': 1.0}
{'loss': 1.0728, 'grad_norm': 1.540999174118042, 'learning_rate': 4.969359719586563e-10, 'epoch': 1.0}
{'loss': 1.1908, 'grad_norm': 1.3742318153381348, 'learning_rate': 4.3676056964869764e-10, 'epoch': 1.0}
{'loss': 1.1349, 'grad_norm': 1.4358758926391602, 'learning_rate': 3.804673420837457e-10, 'epoch': 1.0}
{'loss': 1.0863, 'grad_norm': 1.454450011253357, 'learning_rate': 3.2805631111743064e-10, 'epoch': 1.0}
{'loss': 1.1118, 'grad_norm': 1.594244122505188, 'learning_rate': 2.795274971001405e-10, 'epoch': 1.0}
{'loss': 1.0771, 'grad_norm': 1.5258865356445312, 'learning_rate': 2.3488091886902933e-10, 'epoch': 1.0}
{'loss': 1.0996, 'grad_norm': 1.4498261213302612, 'learning_rate': 1.941165937602296e-10, 'epoch': 1.0}
{'loss': 1.0308, 'grad_norm': 1.4239792823791504, 'learning_rate': 1.5723453759886042e-10, 'epoch': 1.0}
{'loss': 1.0398, 'grad_norm': 1.3468307256698608, 'learning_rate': 1.2423476470346808e-10, 'epoch': 1.0}
{'loss': 1.0687, 'grad_norm': 1.4610430002212524, 'learning_rate': 9.511728788602625e-11, 'epoch': 1.0}
{'loss': 1.0527, 'grad_norm': 1.3707797527313232, 'learning_rate': 6.988211845082582e-11, 'epoch': 1.0}
{'loss': 1.107, 'grad_norm': 1.617383599281311, 'learning_rate': 4.852926619447473e-11, 'epoch': 1.0}
{'loss': 1.0701, 'grad_norm': 1.5357463359832764, 'learning_rate': 3.105873940811854e-11, 'epoch': 1.0}
{'loss': 1.088, 'grad_norm': 1.4211505651474, 'learning_rate': 1.7470544874109706e-11, 'epoch': 1.0}
{'loss': 1.2308, 'grad_norm': 1.5870565176010132, 'learning_rate': 7.764687866007592e-12, 'epoch': 1.0}
{'loss': 1.155, 'grad_norm': 1.5345638990402222, 'learning_rate': 1.9411721552398123e-12, 'epoch': 1.0}
{'train_runtime': 18815.9442, 'train_samples_per_second': 35.358, 'train_steps_per_second': 0.276, 'train_loss': 1.1085993006120236, 'epoch': 1.0}
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA/wandb/offline-run-20250917_044814-dr6s2evx[0m
[1;34mwandb[0m: Find logs at: [1;35m../../../../../datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA/wandb/offline-run-20250917_044814-dr6s2evx/logs[0m
