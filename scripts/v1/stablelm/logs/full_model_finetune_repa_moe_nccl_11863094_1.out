Loading cuda/12.4.0
  Unloading conflict: cuda/12.8.1
VERSION: 1.6 (NCCL-only)
MASTER_PORT: 39907
MASTER_ADDR: g020
[2025-09-16 22:06:52,409] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-16 22:06:52,409] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-16 22:06:52,409] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-16 22:06:52,409] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-16 22:06:52,410] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-16 22:06:52,410] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-16 22:06:52,409] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-16 22:06:52,409] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-16 22:06:52,410] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-16 22:06:52,410] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-16 22:06:52,410] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-16 22:06:52,410] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-16 22:06:52,410] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-16 22:06:52,410] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-16 22:06:52,410] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-16 22:06:52,410] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.

Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-09-16 22:07:08,416] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-16 22:07:08,416] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-16 22:07:08,416] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-16 22:07:08,416] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-16 22:07:08,416] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-16 22:07:08,416] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-16 22:07:08,416] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-16 22:07:08,416] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-16 22:07:08,415] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-16 22:07:08,415] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-16 22:07:08,416] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-16 22:07:08,416] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-16 22:07:08,416] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-16 22:07:08,416] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-16 22:07:08,416] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-16 22:07:08,416] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode

⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode


⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode



⚙️  Running in WANDB offline mode
[2025-09-16 22:07:13,837] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-16 22:07:13,837] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-16 22:07:13,837] [INFO] [comm.py:852:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-09-16 22:07:13,837] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-16 22:07:13,837] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-16 22:07:13,838] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-16 22:07:13,838] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-16 22:07:13,838] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-16 22:07:13,838] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-16 22:07:13,837] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-16 22:07:13,837] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-16 22:07:13,838] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-16 22:07:13,838] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-16 22:07:13,838] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-16 22:07:13,838] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-16 22:07:13,838] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-16 22:07:13,838] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-16 22:07:14,340] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-16 22:07:14,343] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-16 22:07:14,344] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-16 22:07:14,346] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-16 22:07:14,348] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-16 22:07:14,350] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-16 22:07:14,351] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-16 22:07:14,353] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-16 22:07:14,355] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-16 22:07:14,356] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-16 22:07:14,358] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-16 22:07:14,360] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
Vision encoder and proj init.

Vision encoder and proj init.

Vision encoder and proj init.

Vision encoder and proj init.

Vision encoder and proj init.

Vision encoder and proj init.

Vision encoder and proj init.

Vision encoder and proj init.

RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Setting up RePaMoE fine-tuning mode...Setting up RePaMoE fine-tuning mode...
Setting up RePaMoE fine-tuning mode...Setting up RePaMoE fine-tuning mode...

  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Setting up RePaMoE fine-tuning mode...Setting up RePaMoE fine-tuning mode...

  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM

  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM

  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
Setting up RePaMoE fine-tuning mode...
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Setting up RePaMoE fine-tuning mode...Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM

  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0  Set initial gated ratio to 1.0

  Total training steps: 5197.0
  Set initial gated ratio to 1.0
  Total training steps: 5197.0  Stage 1 (gated ratio reduction): 2598.0 steps

  Stage 1 (gated ratio reduction): 2598.0 steps  Stage 2 (post-reparam training): 2599.0 steps

  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1
  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Total training steps: 5197.0  Set initial gated ratio to 1.0
  Stage 1 (gated ratio reduction): 2598.0 steps

  Stage 2 (post-reparam training): 2599.0 steps
  Total training steps: 5197.0
  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1  Stage 1 (gated ratio reduction): 2598.0 steps

  Stage 2 (post-reparam training): 2599.0 steps  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]

  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]  Set initial gated ratio to 1.0

  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
Formatting inputs...Skip in lazy mode
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Set initial gated ratio to 1.0
  Total training steps: 5197.0
  Stage 1 (gated ratio reduction): 2598.0 steps
  Stage 2 (post-reparam training): 2599.0 steps
  Gated ratio will be reduced from 1.0 to 0.9 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
No existing process group found, creating a new group named: ep_size_1
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
No existing process group found, creating a new group named: ep_size_1
NCCL version 2.21.5+cuda12.4
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
No existing process group found, creating a new group named: ep_size_1
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
No existing process group found, creating a new group named: ep_size_1
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
NCCL version 2.21.5+cuda12.4
{'loss': 0.9712, 'grad_norm': 0.41389718651771545, 'learning_rate': 0.0, 'epoch': 0.0}
{'loss': 0.9337, 'grad_norm': 0.3456660807132721, 'learning_rate': 1.282051282051282e-07, 'epoch': 0.0}
{'loss': 1.0064, 'grad_norm': 0.40372923016548157, 'learning_rate': 2.564102564102564e-07, 'epoch': 0.0}
{'loss': 1.0524, 'grad_norm': 0.3997986316680908, 'learning_rate': 3.846153846153847e-07, 'epoch': 0.0}
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
Step 4: Updated gated ratio to 0.9998 (progress: 0.2%)
{'loss': 0.9421, 'grad_norm': 0.3677273690700531, 'learning_rate': 5.128205128205128e-07, 'epoch': 0.0}
{'loss': 0.9081, 'grad_norm': 0.38417336344718933, 'learning_rate': 6.41025641025641e-07, 'epoch': 0.0}
{'loss': 1.0421, 'grad_norm': 0.4020051658153534, 'learning_rate': 7.692307692307694e-07, 'epoch': 0.0}
{'loss': 0.9684, 'grad_norm': 0.3704737722873688, 'learning_rate': 8.974358974358975e-07, 'epoch': 0.0}
{'loss': 1.0499, 'grad_norm': 0.36871257424354553, 'learning_rate': 1.0256410256410257e-06, 'epoch': 0.0}
{'loss': 0.9719, 'grad_norm': 0.40167850255966187, 'learning_rate': 1.153846153846154e-06, 'epoch': 0.0}
Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)
Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)
Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)
Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)
Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)
Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)

Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)
Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)
Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)
Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)
Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)
Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)
Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)
Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)
Step 10: Updated gated ratio to 0.9996 (progress: 0.4%)
{'loss': 0.9519, 'grad_norm': 0.37746331095695496, 'learning_rate': 1.282051282051282e-06, 'epoch': 0.0}
{'loss': 0.9869, 'grad_norm': 0.40743520855903625, 'learning_rate': 1.4102564102564104e-06, 'epoch': 0.0}
{'loss': 0.8995, 'grad_norm': 0.322447270154953, 'learning_rate': 1.5384615384615387e-06, 'epoch': 0.0}
{'loss': 0.9569, 'grad_norm': 0.4105580747127533, 'learning_rate': 1.6666666666666667e-06, 'epoch': 0.0}
{'loss': 0.9886, 'grad_norm': 0.3562532961368561, 'learning_rate': 1.794871794871795e-06, 'epoch': 0.0}
Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)
Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)
Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)
Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)
Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)
Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)
Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)
Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)
Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)

Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)
Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)
Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)
Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)
Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)
Step 15: Updated gated ratio to 0.9994 (progress: 0.6%)
{'loss': 0.9968, 'grad_norm': 0.36252400279045105, 'learning_rate': 1.9230769230769234e-06, 'epoch': 0.0}
{'loss': 0.9396, 'grad_norm': 0.3815864324569702, 'learning_rate': 2.0512820512820513e-06, 'epoch': 0.0}
{'loss': 1.0195, 'grad_norm': 0.3936072289943695, 'learning_rate': 2.1794871794871797e-06, 'epoch': 0.0}
{'loss': 1.0451, 'grad_norm': 0.40347689390182495, 'learning_rate': 2.307692307692308e-06, 'epoch': 0.0}
{'loss': 1.0122, 'grad_norm': 0.37359195947647095, 'learning_rate': 2.435897435897436e-06, 'epoch': 0.0}
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
Step 20: Updated gated ratio to 0.9992 (progress: 0.8%)
{'loss': 1.0245, 'grad_norm': 0.3729113042354584, 'learning_rate': 2.564102564102564e-06, 'epoch': 0.0}
{'loss': 1.0305, 'grad_norm': 0.35110872983932495, 'learning_rate': 2.6923076923076923e-06, 'epoch': 0.0}
{'loss': 0.9352, 'grad_norm': 0.4070877730846405, 'learning_rate': 2.8205128205128207e-06, 'epoch': 0.0}
{'loss': 0.9883, 'grad_norm': 0.32944896817207336, 'learning_rate': 2.948717948717949e-06, 'epoch': 0.0}
{'loss': 0.9974, 'grad_norm': 0.4133889377117157, 'learning_rate': 3.0769230769230774e-06, 'epoch': 0.0}
Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)
Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)
Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)
Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)
Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)
Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)
Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)
Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)
Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)

Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)
Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)
Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)
Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)
Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)
Step 25: Updated gated ratio to 0.9990 (progress: 1.0%)
{'loss': 1.0921, 'grad_norm': 0.40624263882637024, 'learning_rate': 3.205128205128206e-06, 'epoch': 0.01}
{'loss': 1.0871, 'grad_norm': 0.36496448516845703, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.01}
{'loss': 0.8487, 'grad_norm': 0.33917751908302307, 'learning_rate': 3.4615384615384617e-06, 'epoch': 0.01}
{'loss': 1.0128, 'grad_norm': 0.3895459473133087, 'learning_rate': 3.58974358974359e-06, 'epoch': 0.01}
{'loss': 1.0636, 'grad_norm': 0.39325928688049316, 'learning_rate': 3.7179487179487184e-06, 'epoch': 0.01}
Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)
Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)
Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)
Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)
Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)
Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)

Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)
Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)
Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)
Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)
Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)
Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)
Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)
Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)
Step 30: Updated gated ratio to 0.9988 (progress: 1.2%)
{'loss': 0.9449, 'grad_norm': 0.3785232901573181, 'learning_rate': 3.846153846153847e-06, 'epoch': 0.01}
{'loss': 0.9782, 'grad_norm': 0.3947598338127136, 'learning_rate': 3.974358974358974e-06, 'epoch': 0.01}
{'loss': 0.9636, 'grad_norm': 0.3150999844074249, 'learning_rate': 4.102564102564103e-06, 'epoch': 0.01}
{'loss': 1.0356, 'grad_norm': 0.39709147810935974, 'learning_rate': 4.230769230769231e-06, 'epoch': 0.01}
{'loss': 0.8837, 'grad_norm': 0.26212939620018005, 'learning_rate': 4.358974358974359e-06, 'epoch': 0.01}
{'loss': 0.8835, 'grad_norm': 0.3770485520362854, 'learning_rate': 4.487179487179488e-06, 'epoch': 0.01}
Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)
Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)
Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)
Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)
Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)
Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)
Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)
Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)

Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)
Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)
Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)
Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)
Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)
Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)
Step 36: Updated gated ratio to 0.9986 (progress: 1.4%)
{'loss': 0.9896, 'grad_norm': 0.394127756357193, 'learning_rate': 4.615384615384616e-06, 'epoch': 0.01}
{'loss': 1.0165, 'grad_norm': 0.4187726676464081, 'learning_rate': 4.743589743589744e-06, 'epoch': 0.01}
{'loss': 0.9778, 'grad_norm': 0.3322708010673523, 'learning_rate': 4.871794871794872e-06, 'epoch': 0.01}
{'loss': 1.0239, 'grad_norm': 0.40231582522392273, 'learning_rate': 5e-06, 'epoch': 0.01}
{'loss': 1.0721, 'grad_norm': 0.3816722333431244, 'learning_rate': 5.128205128205128e-06, 'epoch': 0.01}
Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)
Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)
Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)
Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)
Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)
Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)
Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)
Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)
Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)
Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)
Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)
Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)
Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)
Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)
Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)
Step 41: Updated gated ratio to 0.9984 (progress: 1.6%)
{'loss': 0.8885, 'grad_norm': 0.39773643016815186, 'learning_rate': 5.256410256410257e-06, 'epoch': 0.01}
{'loss': 0.9396, 'grad_norm': 0.40556034445762634, 'learning_rate': 5.384615384615385e-06, 'epoch': 0.01}
{'loss': 1.0316, 'grad_norm': 0.35874074697494507, 'learning_rate': 5.512820512820514e-06, 'epoch': 0.01}
{'loss': 0.999, 'grad_norm': 0.41935497522354126, 'learning_rate': 5.641025641025641e-06, 'epoch': 0.01}
{'loss': 0.9817, 'grad_norm': 0.38946518301963806, 'learning_rate': 5.769230769230769e-06, 'epoch': 0.01}
Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)
Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)
Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)
Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)
Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)
Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)
Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)
Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)

Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)
Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)
Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)
Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)
Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)
Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)
Step 46: Updated gated ratio to 0.9982 (progress: 1.8%)
{'loss': 1.0576, 'grad_norm': 0.4383741021156311, 'learning_rate': 5.897435897435898e-06, 'epoch': 0.01}
{'loss': 0.9773, 'grad_norm': 0.3836148679256439, 'learning_rate': 6.025641025641026e-06, 'epoch': 0.01}
{'loss': 1.004, 'grad_norm': 0.4416984021663666, 'learning_rate': 6.153846153846155e-06, 'epoch': 0.01}
{'loss': 0.9492, 'grad_norm': 0.44677045941352844, 'learning_rate': 6.282051282051282e-06, 'epoch': 0.01}
{'loss': 1.0457, 'grad_norm': 0.43927010893821716, 'learning_rate': 6.410256410256412e-06, 'epoch': 0.01}
Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)
Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)
Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)
Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)
Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)
Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)
Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)

Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)
Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)
Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)
Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)
Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)
Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)
Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)
Step 51: Updated gated ratio to 0.9980 (progress: 2.0%)
{'loss': 1.0494, 'grad_norm': 0.4848203659057617, 'learning_rate': 6.538461538461539e-06, 'epoch': 0.01}
{'loss': 1.0013, 'grad_norm': 0.4305456876754761, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.01}
{'loss': 0.97, 'grad_norm': 0.4867163300514221, 'learning_rate': 6.794871794871796e-06, 'epoch': 0.01}
{'loss': 1.0175, 'grad_norm': 0.41026273369789124, 'learning_rate': 6.923076923076923e-06, 'epoch': 0.01}
{'loss': 0.989, 'grad_norm': 0.4272984266281128, 'learning_rate': 7.051282051282053e-06, 'epoch': 0.01}
Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)
Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)
Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)
Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)
Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)
Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)
Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)
Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)
Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)
Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)


Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)
Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)
Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)
Step 56: Updated gated ratio to 0.9978 (progress: 2.2%)
{'loss': 1.165, 'grad_norm': 0.46989089250564575, 'learning_rate': 7.17948717948718e-06, 'epoch': 0.01}
{'loss': 1.0071, 'grad_norm': 0.4169096052646637, 'learning_rate': 7.307692307692308e-06, 'epoch': 0.01}
{'loss': 1.0153, 'grad_norm': 0.43463563919067383, 'learning_rate': 7.435897435897437e-06, 'epoch': 0.01}
{'loss': 0.997, 'grad_norm': 0.48109716176986694, 'learning_rate': 7.564102564102564e-06, 'epoch': 0.01}
{'loss': 1.0034, 'grad_norm': 0.29734352231025696, 'learning_rate': 7.692307692307694e-06, 'epoch': 0.01}
{'loss': 1.0014, 'grad_norm': 0.38616108894348145, 'learning_rate': 7.820512820512822e-06, 'epoch': 0.01}
Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)
Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)
Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)
Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)
Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)
Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)
Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)


Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)
Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)
Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)
Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)
Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)
Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)
Step 62: Updated gated ratio to 0.9976 (progress: 2.4%)
{'loss': 1.056, 'grad_norm': 0.4522499442100525, 'learning_rate': 7.948717948717949e-06, 'epoch': 0.01}
{'loss': 0.9726, 'grad_norm': 0.4070870578289032, 'learning_rate': 8.076923076923077e-06, 'epoch': 0.01}
{'loss': 1.1403, 'grad_norm': 0.46630096435546875, 'learning_rate': 8.205128205128205e-06, 'epoch': 0.01}
{'loss': 0.959, 'grad_norm': 0.4302593469619751, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.01}
{'loss': 1.0228, 'grad_norm': 0.40755748748779297, 'learning_rate': 8.461538461538462e-06, 'epoch': 0.01}
Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)
Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)
Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)
Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)
Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)
Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)
Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)
Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)
Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)
Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)
Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)
Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)
Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)
Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)
Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)
Step 67: Updated gated ratio to 0.9974 (progress: 2.6%)
{'loss': 1.0086, 'grad_norm': 0.46313294768333435, 'learning_rate': 8.58974358974359e-06, 'epoch': 0.01}
{'loss': 1.0511, 'grad_norm': 0.43703898787498474, 'learning_rate': 8.717948717948719e-06, 'epoch': 0.01}
{'loss': 0.9619, 'grad_norm': 0.46009162068367004, 'learning_rate': 8.846153846153847e-06, 'epoch': 0.01}
{'loss': 1.0368, 'grad_norm': 0.4365774393081665, 'learning_rate': 8.974358974358976e-06, 'epoch': 0.01}
{'loss': 1.065, 'grad_norm': 0.44936075806617737, 'learning_rate': 9.102564102564104e-06, 'epoch': 0.01}
Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)
Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)
Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)
Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)
Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)
Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)
Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)
Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)
Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)
Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)
Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)
Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)
Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)
Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)
Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)
Step 72: Updated gated ratio to 0.9972 (progress: 2.8%)
{'loss': 1.0255, 'grad_norm': 0.41795381903648376, 'learning_rate': 9.230769230769232e-06, 'epoch': 0.01}
{'loss': 1.0253, 'grad_norm': 0.4332466423511505, 'learning_rate': 9.358974358974359e-06, 'epoch': 0.01}
{'loss': 0.9813, 'grad_norm': 0.4330582320690155, 'learning_rate': 9.487179487179487e-06, 'epoch': 0.01}
{'loss': 0.9607, 'grad_norm': 0.42020660638809204, 'learning_rate': 9.615384615384616e-06, 'epoch': 0.01}
{'loss': 0.9981, 'grad_norm': 0.44985321164131165, 'learning_rate': 9.743589743589744e-06, 'epoch': 0.01}
Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)
Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)
Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)
Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)
Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)
Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)
Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)
Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)
Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)
Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)
Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)
Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)
Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)
Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)
Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)
Step 77: Updated gated ratio to 0.9970 (progress: 3.0%)
{'loss': 0.9698, 'grad_norm': 0.36145395040512085, 'learning_rate': 9.871794871794872e-06, 'epoch': 0.02}
{'loss': 1.009, 'grad_norm': 0.43882712721824646, 'learning_rate': 1e-05, 'epoch': 0.02}
{'loss': 1.0217, 'grad_norm': 0.42634862661361694, 'learning_rate': 1.012820512820513e-05, 'epoch': 0.02}
{'loss': 1.0406, 'grad_norm': 0.44929274916648865, 'learning_rate': 1.0256410256410256e-05, 'epoch': 0.02}
{'loss': 0.9418, 'grad_norm': 0.3596830666065216, 'learning_rate': 1.0384615384615386e-05, 'epoch': 0.02}
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
Step 82: Updated gated ratio to 0.9968 (progress: 3.2%)
{'loss': 0.9625, 'grad_norm': 0.4373460114002228, 'learning_rate': 1.0512820512820514e-05, 'epoch': 0.02}
{'loss': 1.0633, 'grad_norm': 0.4252542555332184, 'learning_rate': 1.0641025641025643e-05, 'epoch': 0.02}
{'loss': 1.0064, 'grad_norm': 0.4186912178993225, 'learning_rate': 1.076923076923077e-05, 'epoch': 0.02}
{'loss': 0.954, 'grad_norm': 0.407802939414978, 'learning_rate': 1.0897435897435898e-05, 'epoch': 0.02}
{'loss': 0.9248, 'grad_norm': 0.401694655418396, 'learning_rate': 1.1025641025641028e-05, 'epoch': 0.02}
{'loss': 1.1027, 'grad_norm': 0.4196547567844391, 'learning_rate': 1.1153846153846154e-05, 'epoch': 0.02}
Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)
Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)
Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)
Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)
Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)
Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)
Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)
Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)

Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)
Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)
Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)
Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)
Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)
Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)
Step 88: Updated gated ratio to 0.9966 (progress: 3.4%)
{'loss': 1.0116, 'grad_norm': 0.4592454135417938, 'learning_rate': 1.1282051282051283e-05, 'epoch': 0.02}
{'loss': 0.9846, 'grad_norm': 0.3938075304031372, 'learning_rate': 1.1410256410256411e-05, 'epoch': 0.02}
{'loss': 1.0539, 'grad_norm': 0.4512622654438019, 'learning_rate': 1.1538461538461538e-05, 'epoch': 0.02}
{'loss': 1.1175, 'grad_norm': 0.467301607131958, 'learning_rate': 1.1666666666666668e-05, 'epoch': 0.02}
{'loss': 0.975, 'grad_norm': 0.42794400453567505, 'learning_rate': 1.1794871794871796e-05, 'epoch': 0.02}
Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)
Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)
Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)
Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)
Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)
Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)

Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)
Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)
Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)
Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)
Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)
Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)
Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)
Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)
Step 93: Updated gated ratio to 0.9964 (progress: 3.6%)
{'loss': 1.0033, 'grad_norm': 0.39880290627479553, 'learning_rate': 1.1923076923076925e-05, 'epoch': 0.02}
{'loss': 1.0267, 'grad_norm': 0.3897080421447754, 'learning_rate': 1.2051282051282051e-05, 'epoch': 0.02}
{'loss': 1.0217, 'grad_norm': 0.39981985092163086, 'learning_rate': 1.217948717948718e-05, 'epoch': 0.02}
{'loss': 1.0223, 'grad_norm': 0.36611729860305786, 'learning_rate': 1.230769230769231e-05, 'epoch': 0.02}
{'loss': 1.0022, 'grad_norm': 0.4309163987636566, 'learning_rate': 1.2435897435897436e-05, 'epoch': 0.02}
Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)
Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)
Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)
Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)
Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)
Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)
Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)
Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)

Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)
Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)
Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)
Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)
Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)
Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)
Step 98: Updated gated ratio to 0.9962 (progress: 3.8%)
{'loss': 0.9256, 'grad_norm': 0.44465410709381104, 'learning_rate': 1.2564102564102565e-05, 'epoch': 0.02}
{'loss': 0.9206, 'grad_norm': 0.41883763670921326, 'learning_rate': 1.2692307692307693e-05, 'epoch': 0.02}
{'loss': 1.0408, 'grad_norm': 0.40753206610679626, 'learning_rate': 1.2820512820512823e-05, 'epoch': 0.02}
{'loss': 1.0106, 'grad_norm': 0.35526415705680847, 'learning_rate': 1.294871794871795e-05, 'epoch': 0.02}
{'loss': 1.0541, 'grad_norm': 0.4433387517929077, 'learning_rate': 1.3076923076923078e-05, 'epoch': 0.02}
Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)
Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)
Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)
Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)
Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)
Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)


Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)
Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)
Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)
Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)
Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)
Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)
Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)
Step 103: Updated gated ratio to 0.9960 (progress: 4.0%)
{'loss': 0.9485, 'grad_norm': 0.477984219789505, 'learning_rate': 1.3205128205128207e-05, 'epoch': 0.02}
{'loss': 1.0672, 'grad_norm': 0.4220197796821594, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.02}
{'loss': 1.0241, 'grad_norm': 0.4250194728374481, 'learning_rate': 1.3461538461538463e-05, 'epoch': 0.02}
{'loss': 1.0455, 'grad_norm': 0.3907790184020996, 'learning_rate': 1.3589743589743592e-05, 'epoch': 0.02}
{'loss': 1.0767, 'grad_norm': 0.444903701543808, 'learning_rate': 1.3717948717948718e-05, 'epoch': 0.02}
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
Step 108: Updated gated ratio to 0.9958 (progress: 4.2%)
{'loss': 1.0006, 'grad_norm': 0.43107789754867554, 'learning_rate': 1.3846153846153847e-05, 'epoch': 0.02}
{'loss': 0.992, 'grad_norm': 0.4767668545246124, 'learning_rate': 1.3974358974358975e-05, 'epoch': 0.02}
{'loss': 1.0329, 'grad_norm': 0.44014859199523926, 'learning_rate': 1.4102564102564105e-05, 'epoch': 0.02}
{'loss': 1.0071, 'grad_norm': 0.45107099413871765, 'learning_rate': 1.4230769230769232e-05, 'epoch': 0.02}
{'loss': 0.9911, 'grad_norm': 0.46798595786094666, 'learning_rate': 1.435897435897436e-05, 'epoch': 0.02}
{'loss': 0.9507, 'grad_norm': 0.43323978781700134, 'learning_rate': 1.4487179487179489e-05, 'epoch': 0.02}
Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)
Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)
Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)
Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)
Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)
Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)
Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)
Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)
Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)
Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)
Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)
Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)
Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)
Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)

Step 114: Updated gated ratio to 0.9956 (progress: 4.4%)
{'loss': 0.9529, 'grad_norm': 0.4873094856739044, 'learning_rate': 1.4615384615384615e-05, 'epoch': 0.02}
{'loss': 0.9844, 'grad_norm': 0.73404461145401, 'learning_rate': 1.4743589743589745e-05, 'epoch': 0.02}
{'loss': 1.0161, 'grad_norm': 0.46608787775039673, 'learning_rate': 1.4871794871794874e-05, 'epoch': 0.02}
{'loss': 0.9999, 'grad_norm': 0.44253572821617126, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.02}
{'loss': 0.9201, 'grad_norm': 0.432134747505188, 'learning_rate': 1.5128205128205129e-05, 'epoch': 0.02}
Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)
Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)
Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)
Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)
Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)
Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)
Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)

Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)
Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)
Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)
Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)
Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)

Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)
Step 119: Updated gated ratio to 0.9954 (progress: 4.6%)
{'loss': 1.0385, 'grad_norm': 0.4210757613182068, 'learning_rate': 1.5256410256410257e-05, 'epoch': 0.02}
{'loss': 0.9478, 'grad_norm': 0.3902926445007324, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.02}
{'loss': 1.0085, 'grad_norm': 0.41795477271080017, 'learning_rate': 1.5512820512820516e-05, 'epoch': 0.02}
{'loss': 1.0991, 'grad_norm': 0.42304760217666626, 'learning_rate': 1.5641025641025644e-05, 'epoch': 0.02}
{'loss': 0.976, 'grad_norm': 0.42143216729164124, 'learning_rate': 1.576923076923077e-05, 'epoch': 0.02}
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
Step 124: Updated gated ratio to 0.9952 (progress: 4.8%)
{'loss': 1.0263, 'grad_norm': 0.4406234920024872, 'learning_rate': 1.5897435897435897e-05, 'epoch': 0.02}
{'loss': 0.9991, 'grad_norm': 0.4814096689224243, 'learning_rate': 1.602564102564103e-05, 'epoch': 0.02}
{'loss': 1.0343, 'grad_norm': 0.4238521158695221, 'learning_rate': 1.6153846153846154e-05, 'epoch': 0.02}
{'loss': 1.0897, 'grad_norm': 0.4743797481060028, 'learning_rate': 1.6282051282051282e-05, 'epoch': 0.02}
{'loss': 1.0617, 'grad_norm': 0.4531656503677368, 'learning_rate': 1.641025641025641e-05, 'epoch': 0.02}
Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)
Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)
Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)
Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)

Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)
Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)
Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)
Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)

Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)
Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)
Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)
Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)
Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)
Step 129: Updated gated ratio to 0.9950 (progress: 5.0%)
{'loss': 1.0336, 'grad_norm': 0.4116664230823517, 'learning_rate': 1.653846153846154e-05, 'epoch': 0.03}
{'loss': 0.9439, 'grad_norm': 0.453492134809494, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.03}
Error with image file is truncated (73 bytes not processed)
{'loss': 1.0928, 'grad_norm': 0.48490622639656067, 'learning_rate': 1.6794871794871796e-05, 'epoch': 0.03}
{'loss': 1.006, 'grad_norm': 0.4372979998588562, 'learning_rate': 1.6923076923076924e-05, 'epoch': 0.03}
{'loss': 1.0193, 'grad_norm': 0.35331279039382935, 'learning_rate': 1.7051282051282053e-05, 'epoch': 0.03}
Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)
Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)
Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)
Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)
Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)
Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)

Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)
Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)
Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)
Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)
Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)
Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)
Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)
Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)
Step 134: Updated gated ratio to 0.9948 (progress: 5.2%)
{'loss': 1.0841, 'grad_norm': 0.49039578437805176, 'learning_rate': 1.717948717948718e-05, 'epoch': 0.03}
{'loss': 0.9513, 'grad_norm': 0.42297160625457764, 'learning_rate': 1.730769230769231e-05, 'epoch': 0.03}
{'loss': 1.0729, 'grad_norm': 0.4452645480632782, 'learning_rate': 1.7435897435897438e-05, 'epoch': 0.03}
{'loss': 1.1164, 'grad_norm': 0.4410010874271393, 'learning_rate': 1.7564102564102566e-05, 'epoch': 0.03}
{'loss': 0.9766, 'grad_norm': 0.4220048189163208, 'learning_rate': 1.7692307692307694e-05, 'epoch': 0.03}
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
Step 139: Updated gated ratio to 0.9946 (progress: 5.4%)
{'loss': 0.9302, 'grad_norm': 0.4126376211643219, 'learning_rate': 1.7820512820512823e-05, 'epoch': 0.03}
{'loss': 0.9563, 'grad_norm': 0.41995927691459656, 'learning_rate': 1.794871794871795e-05, 'epoch': 0.03}
{'loss': 1.0296, 'grad_norm': 0.4289492666721344, 'learning_rate': 1.807692307692308e-05, 'epoch': 0.03}
{'loss': 1.0186, 'grad_norm': 0.42607298493385315, 'learning_rate': 1.8205128205128208e-05, 'epoch': 0.03}
{'loss': 1.0276, 'grad_norm': 0.40454065799713135, 'learning_rate': 1.8333333333333333e-05, 'epoch': 0.03}
{'loss': 0.9965, 'grad_norm': 0.4288124442100525, 'learning_rate': 1.8461538461538465e-05, 'epoch': 0.03}
Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)
Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)
Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)
Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)
Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)
Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)
Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)
Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)
Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)
Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)
Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)
Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)
Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)
Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)
Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)
Step 145: Updated gated ratio to 0.9944 (progress: 5.6%)
{'loss': 0.9566, 'grad_norm': 0.43170100450515747, 'learning_rate': 1.8589743589743593e-05, 'epoch': 0.03}
{'loss': 1.0025, 'grad_norm': 0.5125656127929688, 'learning_rate': 1.8717948717948718e-05, 'epoch': 0.03}
{'loss': 1.1105, 'grad_norm': 0.480080246925354, 'learning_rate': 1.8846153846153846e-05, 'epoch': 0.03}
{'loss': 0.9097, 'grad_norm': 0.45654720067977905, 'learning_rate': 1.8974358974358975e-05, 'epoch': 0.03}
{'loss': 0.9762, 'grad_norm': 0.402113139629364, 'learning_rate': 1.9102564102564106e-05, 'epoch': 0.03}
Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)
Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)
Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)
Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)
Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)
Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)

Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)
Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)
Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)
Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)

Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)
Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)
Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)
Step 150: Updated gated ratio to 0.9942 (progress: 5.8%)
{'loss': 1.045, 'grad_norm': 0.44472166895866394, 'learning_rate': 1.923076923076923e-05, 'epoch': 0.03}
{'loss': 1.0209, 'grad_norm': 0.35839757323265076, 'learning_rate': 1.935897435897436e-05, 'epoch': 0.03}
{'loss': 1.0027, 'grad_norm': 0.43277379870414734, 'learning_rate': 1.9487179487179488e-05, 'epoch': 0.03}
{'loss': 1.0881, 'grad_norm': 0.4717496633529663, 'learning_rate': 1.9615384615384617e-05, 'epoch': 0.03}
{'loss': 0.9936, 'grad_norm': 0.43409767746925354, 'learning_rate': 1.9743589743589745e-05, 'epoch': 0.03}
Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)
Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)
Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)
Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)

Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)
Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)
Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)
Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)

Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)
Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)
Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)
Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)
Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)
Step 155: Updated gated ratio to 0.9940 (progress: 6.0%)
{'loss': 1.077, 'grad_norm': 0.41587838530540466, 'learning_rate': 1.9871794871794873e-05, 'epoch': 0.03}
{'loss': 1.0093, 'grad_norm': 0.43755364418029785, 'learning_rate': 2e-05, 'epoch': 0.03}
{'loss': 1.0094, 'grad_norm': 0.4632013738155365, 'learning_rate': 1.9999998058827844e-05, 'epoch': 0.03}
{'loss': 1.0008, 'grad_norm': 0.3973196744918823, 'learning_rate': 1.9999992235312136e-05, 'epoch': 0.03}
{'loss': 0.9322, 'grad_norm': 0.4287348985671997, 'learning_rate': 1.9999982529455127e-05, 'epoch': 0.03}
Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)
Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)
Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)
Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)
Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)

Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)
Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)
Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)
Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)
Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)
Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)
Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)
Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)
Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)
Step 160: Updated gated ratio to 0.9938 (progress: 6.2%)
{'loss': 0.9797, 'grad_norm': 0.43524304032325745, 'learning_rate': 1.9999968941260596e-05, 'epoch': 0.03}
{'loss': 1.0161, 'grad_norm': 0.40924686193466187, 'learning_rate': 1.9999951470733808e-05, 'epoch': 0.03}
{'loss': 1.1018, 'grad_norm': 0.46833521127700806, 'learning_rate': 1.9999930117881548e-05, 'epoch': 0.03}
{'loss': 1.0322, 'grad_norm': 0.4633394479751587, 'learning_rate': 1.9999904882712115e-05, 'epoch': 0.03}
{'loss': 1.0833, 'grad_norm': 0.4383300542831421, 'learning_rate': 1.99998757652353e-05, 'epoch': 0.03}
Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)
Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)
Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)
Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)
Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)
Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)
Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)
Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)
Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)
Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)
Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)
Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)
Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)
Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)
Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)
Step 165: Updated gated ratio to 0.9936 (progress: 6.4%)
{'loss': 1.0159, 'grad_norm': 0.43181267380714417, 'learning_rate': 1.9999842765462403e-05, 'epoch': 0.03}
{'loss': 1.0068, 'grad_norm': 0.4160812199115753, 'learning_rate': 1.999980588340624e-05, 'epoch': 0.03}
{'loss': 1.0918, 'grad_norm': 0.5168874263763428, 'learning_rate': 1.9999765119081132e-05, 'epoch': 0.03}
{'loss': 1.1446, 'grad_norm': 0.457543283700943, 'learning_rate': 1.9999720472502902e-05, 'epoch': 0.03}
{'loss': 1.0116, 'grad_norm': 0.3582324981689453, 'learning_rate': 1.9999671943688885e-05, 'epoch': 0.03}
{'loss': 1.047, 'grad_norm': 0.4461434483528137, 'learning_rate': 1.9999619532657915e-05, 'epoch': 0.03}
Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)
Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)
Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)
Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)
Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)
Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)
Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)
Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)
Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)
Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)
Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)
Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)
Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)

Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)
Step 171: Updated gated ratio to 0.9934 (progress: 6.6%)
{'loss': 0.9715, 'grad_norm': 0.40477660298347473, 'learning_rate': 1.9999563239430352e-05, 'epoch': 0.03}
{'loss': 1.0997, 'grad_norm': 0.44558727741241455, 'learning_rate': 1.9999503064028043e-05, 'epoch': 0.03}
{'loss': 0.9661, 'grad_norm': 0.4206373691558838, 'learning_rate': 1.999943900647435e-05, 'epoch': 0.03}
{'loss': 0.9982, 'grad_norm': 0.4496392607688904, 'learning_rate': 1.9999371066794146e-05, 'epoch': 0.03}
{'loss': 0.952, 'grad_norm': 0.3772055208683014, 'learning_rate': 1.9999299245013805e-05, 'epoch': 0.03}
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
Step 176: Updated gated ratio to 0.9932 (progress: 6.8%)
{'loss': 1.038, 'grad_norm': 0.43819496035575867, 'learning_rate': 1.999922354116121e-05, 'epoch': 0.03}
{'loss': 1.1, 'grad_norm': 0.41046789288520813, 'learning_rate': 1.999914395526575e-05, 'epoch': 0.03}
{'loss': 1.0143, 'grad_norm': 0.44638922810554504, 'learning_rate': 1.9999060487358333e-05, 'epoch': 0.03}
{'loss': 1.0356, 'grad_norm': 0.4424997568130493, 'learning_rate': 1.9998973137471352e-05, 'epoch': 0.03}
{'loss': 1.0193, 'grad_norm': 0.4541681706905365, 'learning_rate': 1.9998881905638727e-05, 'epoch': 0.03}
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
Step 181: Updated gated ratio to 0.9930 (progress: 7.0%)
{'loss': 1.0323, 'grad_norm': 0.4377018213272095, 'learning_rate': 1.9998786791895874e-05, 'epoch': 0.04}
{'loss': 0.9604, 'grad_norm': 0.4309438169002533, 'learning_rate': 1.999868779627972e-05, 'epoch': 0.04}
{'loss': 0.8838, 'grad_norm': 0.4505719542503357, 'learning_rate': 1.9998584918828695e-05, 'epoch': 0.04}
{'loss': 0.9667, 'grad_norm': 0.4575950801372528, 'learning_rate': 1.9998478159582747e-05, 'epoch': 0.04}
{'loss': 1.0592, 'grad_norm': 0.4443136155605316, 'learning_rate': 1.999836751858332e-05, 'epoch': 0.04}
Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)
Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)
Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)
Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)
Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)
Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)
Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)

Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)

Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)
Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)
Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)
Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)
Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)
Step 186: Updated gated ratio to 0.9928 (progress: 7.2%)
{'loss': 0.9805, 'grad_norm': 0.44520023465156555, 'learning_rate': 1.9998252995873367e-05, 'epoch': 0.04}
{'loss': 1.0997, 'grad_norm': 0.45251142978668213, 'learning_rate': 1.999813459149735e-05, 'epoch': 0.04}
{'loss': 1.0614, 'grad_norm': 0.48382556438446045, 'learning_rate': 1.9998012305501243e-05, 'epoch': 0.04}
{'loss': 1.0266, 'grad_norm': 0.4712464213371277, 'learning_rate': 1.999788613793251e-05, 'epoch': 0.04}
{'loss': 1.0749, 'grad_norm': 0.48134055733680725, 'learning_rate': 1.999775608884015e-05, 'epoch': 0.04}
Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)
Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)
Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)
Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)
Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)
Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)
Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)
Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)
Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)
Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)
Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)

Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)
Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)
Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)
Step 191: Updated gated ratio to 0.9926 (progress: 7.4%)
Error with image file is truncated (8 bytes not processed)
{'loss': 0.9531, 'grad_norm': 0.4999736249446869, 'learning_rate': 1.9997622158274635e-05, 'epoch': 0.04}
{'loss': 1.0444, 'grad_norm': 0.4544920325279236, 'learning_rate': 1.9997484346287973e-05, 'epoch': 0.04}
{'loss': 1.0128, 'grad_norm': 0.4524648189544678, 'learning_rate': 1.9997342652933668e-05, 'epoch': 0.04}
{'loss': 1.1007, 'grad_norm': 0.4685767889022827, 'learning_rate': 1.9997197078266723e-05, 'epoch': 0.04}
{'loss': 1.0375, 'grad_norm': 0.42718103528022766, 'learning_rate': 1.999704762234366e-05, 'epoch': 0.04}
{'loss': 1.0006, 'grad_norm': 0.4696931838989258, 'learning_rate': 1.99968942852225e-05, 'epoch': 0.04}
Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)
Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)
Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)
Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)
Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)
Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)
Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)
Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)
Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)
Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)
Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)
Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)
Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)
Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)
Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)
Step 197: Updated gated ratio to 0.9924 (progress: 7.6%)
{'loss': 1.057, 'grad_norm': 0.40513694286346436, 'learning_rate': 1.9996737066962778e-05, 'epoch': 0.04}
{'loss': 1.0933, 'grad_norm': 0.46310606598854065, 'learning_rate': 1.9996575967625525e-05, 'epoch': 0.04}
{'loss': 1.0232, 'grad_norm': 0.4168054163455963, 'learning_rate': 1.999641098727329e-05, 'epoch': 0.04}
{'loss': 1.0494, 'grad_norm': 0.46191713213920593, 'learning_rate': 1.999624212597013e-05, 'epoch': 0.04}
{'loss': 1.0554, 'grad_norm': 0.456855446100235, 'learning_rate': 1.9996069383781587e-05, 'epoch': 0.04}
Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)
Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)
Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)
Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)
Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)
Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)
Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)
Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)
Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)
Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)

Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)
Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)
Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)
Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)
Step 202: Updated gated ratio to 0.9922 (progress: 7.8%)
{'loss': 0.9484, 'grad_norm': 0.3724587857723236, 'learning_rate': 1.9995892760774738e-05, 'epoch': 0.04}
{'loss': 0.9211, 'grad_norm': 0.46088945865631104, 'learning_rate': 1.9995712257018153e-05, 'epoch': 0.04}
{'loss': 0.9952, 'grad_norm': 0.42983195185661316, 'learning_rate': 1.9995527872581903e-05, 'epoch': 0.04}
{'loss': 1.0358, 'grad_norm': 0.4363637864589691, 'learning_rate': 1.9995339607537578e-05, 'epoch': 0.04}
{'loss': 0.9955, 'grad_norm': 0.44706571102142334, 'learning_rate': 1.9995147461958267e-05, 'epoch': 0.04}
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
Step 207: Updated gated ratio to 0.9920 (progress: 8.0%)
{'loss': 1.0696, 'grad_norm': 0.47389915585517883, 'learning_rate': 1.999495143591857e-05, 'epoch': 0.04}
{'loss': 1.0923, 'grad_norm': 0.47851264476776123, 'learning_rate': 1.999475152949459e-05, 'epoch': 0.04}
{'loss': 1.0185, 'grad_norm': 0.4756701588630676, 'learning_rate': 1.9994547742763935e-05, 'epoch': 0.04}
{'loss': 1.0369, 'grad_norm': 0.3831711709499359, 'learning_rate': 1.9994340075805724e-05, 'epoch': 0.04}
{'loss': 1.0097, 'grad_norm': 0.4679936468601227, 'learning_rate': 1.9994128528700583e-05, 'epoch': 0.04}
Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)
Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)
Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)

Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)
Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)
Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)
Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)
Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)

Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)
Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)
Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)
Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)
Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)
Step 212: Updated gated ratio to 0.9918 (progress: 8.2%)
{'loss': 1.0006, 'grad_norm': 0.43132466077804565, 'learning_rate': 1.9993913101530635e-05, 'epoch': 0.04}
{'loss': 1.1211, 'grad_norm': 0.4626607894897461, 'learning_rate': 1.9993693794379525e-05, 'epoch': 0.04}
{'loss': 0.9839, 'grad_norm': 0.4705398380756378, 'learning_rate': 1.9993470607332387e-05, 'epoch': 0.04}
{'loss': 1.0256, 'grad_norm': 0.3674353361129761, 'learning_rate': 1.999324354047588e-05, 'epoch': 0.04}
{'loss': 1.0188, 'grad_norm': 0.44621720910072327, 'learning_rate': 1.9993012593898146e-05, 'epoch': 0.04}
Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)
Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)
Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)
Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)
Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)
Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)
Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)
Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)
Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)
Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)
Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)

Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)
Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)
Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)
Step 217: Updated gated ratio to 0.9916 (progress: 8.4%)
{'loss': 1.0166, 'grad_norm': 0.4362713098526001, 'learning_rate': 1.9992777767688857e-05, 'epoch': 0.04}
{'loss': 1.0063, 'grad_norm': 0.43931737542152405, 'learning_rate': 1.9992539061939175e-05, 'epoch': 0.04}
{'loss': 1.0607, 'grad_norm': 0.48331528902053833, 'learning_rate': 1.999229647674178e-05, 'epoch': 0.04}
{'loss': 1.0224, 'grad_norm': 0.4566577076911926, 'learning_rate': 1.9992050012190845e-05, 'epoch': 0.04}
{'loss': 1.0129, 'grad_norm': 0.41233280301094055, 'learning_rate': 1.9991799668382058e-05, 'epoch': 0.04}
{'loss': 1.072, 'grad_norm': 0.37671250104904175, 'learning_rate': 1.9991545445412614e-05, 'epoch': 0.04}
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
Step 223: Updated gated ratio to 0.9914 (progress: 8.6%)
{'loss': 1.002, 'grad_norm': 0.4292639195919037, 'learning_rate': 1.9991287343381208e-05, 'epoch': 0.04}
{'loss': 1.0904, 'grad_norm': 0.4556616246700287, 'learning_rate': 1.9991025362388044e-05, 'epoch': 0.04}
{'loss': 1.0017, 'grad_norm': 0.36224040389060974, 'learning_rate': 1.9990759502534835e-05, 'epoch': 0.04}
{'loss': 1.0002, 'grad_norm': 0.36790379881858826, 'learning_rate': 1.9990489763924796e-05, 'epoch': 0.04}
{'loss': 0.994, 'grad_norm': 0.42978811264038086, 'learning_rate': 1.9990216146662648e-05, 'epoch': 0.04}
Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)
Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)
Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)
Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)
Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)
Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)
Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)
Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)
Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)
Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)
Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)
Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)
Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)
Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)
Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)
Step 228: Updated gated ratio to 0.9912 (progress: 8.8%)
{'loss': 1.0567, 'grad_norm': 0.46189406514167786, 'learning_rate': 1.9989938650854618e-05, 'epoch': 0.04}
{'loss': 1.0612, 'grad_norm': 0.46514809131622314, 'learning_rate': 1.998965727660844e-05, 'epoch': 0.04}
{'loss': 0.9736, 'grad_norm': 0.3645511269569397, 'learning_rate': 1.9989372024033352e-05, 'epoch': 0.04}
{'loss': 0.9813, 'grad_norm': 0.44641759991645813, 'learning_rate': 1.99890828932401e-05, 'epoch': 0.04}
{'loss': 1.0118, 'grad_norm': 0.3483904302120209, 'learning_rate': 1.9988789884340938e-05, 'epoch': 0.04}
Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)
Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)

Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)
Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)
Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)
Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)
Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)
Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)
Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)
Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)
Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)
Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)
Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)
Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)
Step 233: Updated gated ratio to 0.9910 (progress: 9.0%)
{'loss': 1.0482, 'grad_norm': 0.47410672903060913, 'learning_rate': 1.9988492997449615e-05, 'epoch': 0.05}
{'loss': 0.9889, 'grad_norm': 0.3863827884197235, 'learning_rate': 1.9988192232681398e-05, 'epoch': 0.05}
{'loss': 1.041, 'grad_norm': 0.45395103096961975, 'learning_rate': 1.9987887590153055e-05, 'epoch': 0.05}
{'loss': 1.1016, 'grad_norm': 0.4944867789745331, 'learning_rate': 1.9987579069982856e-05, 'epoch': 0.05}
{'loss': 1.0347, 'grad_norm': 0.3665273189544678, 'learning_rate': 1.9987266672290577e-05, 'epoch': 0.05}
Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)
Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)
Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)
Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)
Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)
Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)
Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)
Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)
Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)
Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)
Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)
Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)
Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)
Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)
Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)
Step 238: Updated gated ratio to 0.9908 (progress: 9.2%)
{'loss': 0.9579, 'grad_norm': 0.4740823805332184, 'learning_rate': 1.9986950397197503e-05, 'epoch': 0.05}
{'loss': 1.0991, 'grad_norm': 0.5052074193954468, 'learning_rate': 1.9986630244826425e-05, 'epoch': 0.05}
{'loss': 1.0538, 'grad_norm': 0.43917757272720337, 'learning_rate': 1.998630621530164e-05, 'epoch': 0.05}
{'loss': 0.9975, 'grad_norm': 0.4684847891330719, 'learning_rate': 1.998597830874894e-05, 'epoch': 0.05}
{'loss': 0.9764, 'grad_norm': 0.3915140926837921, 'learning_rate': 1.9985646525295634e-05, 'epoch': 0.05}
Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)
Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)
Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)
Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)
Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)
Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)
Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)
Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)
Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)
Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)
Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)
Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)
Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)
Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)
Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)
Step 243: Updated gated ratio to 0.9906 (progress: 9.4%)
{'loss': 1.0081, 'grad_norm': 0.3896206021308899, 'learning_rate': 1.998531086507053e-05, 'epoch': 0.05}
{'loss': 0.9814, 'grad_norm': 0.46947234869003296, 'learning_rate': 1.9984971328203945e-05, 'epoch': 0.05}
{'loss': 1.0111, 'grad_norm': 0.4117753207683563, 'learning_rate': 1.9984627914827698e-05, 'epoch': 0.05}
Error with image file is truncated (91 bytes not processed)
{'loss': 0.924, 'grad_norm': 0.4471069574356079, 'learning_rate': 1.9984280625075115e-05, 'epoch': 0.05}
{'loss': 1.0691, 'grad_norm': 0.42237040400505066, 'learning_rate': 1.9983929459081022e-05, 'epoch': 0.05}
{'loss': 1.0797, 'grad_norm': 0.4550693929195404, 'learning_rate': 1.998357441698176e-05, 'epoch': 0.05}
Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)
Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)
Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)
Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)

Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)
Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)
Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)
Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)
Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)
Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)
Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)
Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)
Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)
Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)
Step 249: Updated gated ratio to 0.9904 (progress: 9.6%)
{'loss': 1.0679, 'grad_norm': 0.46458929777145386, 'learning_rate': 1.998321549891516e-05, 'epoch': 0.05}
{'loss': 0.9967, 'grad_norm': 0.41482654213905334, 'learning_rate': 1.9982852705020572e-05, 'epoch': 0.05}
{'loss': 0.9786, 'grad_norm': 0.4683931767940521, 'learning_rate': 1.9982486035438848e-05, 'epoch': 0.05}
{'loss': 1.0708, 'grad_norm': 0.4533332288265228, 'learning_rate': 1.9982115490312334e-05, 'epoch': 0.05}
{'loss': 1.0375, 'grad_norm': 0.43198448419570923, 'learning_rate': 1.9981741069784894e-05, 'epoch': 0.05}
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
Step 254: Updated gated ratio to 0.9902 (progress: 9.8%)
{'loss': 0.9279, 'grad_norm': 0.4419693648815155, 'learning_rate': 1.9981362774001886e-05, 'epoch': 0.05}
{'loss': 1.1352, 'grad_norm': 0.5110899806022644, 'learning_rate': 1.9980980603110185e-05, 'epoch': 0.05}
{'loss': 0.9725, 'grad_norm': 0.4336749017238617, 'learning_rate': 1.9980594557258158e-05, 'epoch': 0.05}
{'loss': 0.9568, 'grad_norm': 0.4384362995624542, 'learning_rate': 1.9980204636595682e-05, 'epoch': 0.05}
{'loss': 1.0406, 'grad_norm': 0.4327908158302307, 'learning_rate': 1.9979810841274135e-05, 'epoch': 0.05}
Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)
Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)
Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)
Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)
Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)
Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)
Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)

Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)
Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)
Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)

Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)
Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)
Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)
Step 259: Updated gated ratio to 0.9900 (progress: 10.0%)
{'loss': 0.9584, 'grad_norm': 0.42392420768737793, 'learning_rate': 1.9979413171446403e-05, 'epoch': 0.05}
{'loss': 1.0317, 'grad_norm': 0.4569608271121979, 'learning_rate': 1.9979011627266884e-05, 'epoch': 0.05}
{'loss': 0.9886, 'grad_norm': 0.4551604688167572, 'learning_rate': 1.997860620889146e-05, 'epoch': 0.05}
{'loss': 0.9835, 'grad_norm': 0.4238359034061432, 'learning_rate': 1.997819691647753e-05, 'epoch': 0.05}
{'loss': 1.005, 'grad_norm': 0.35460326075553894, 'learning_rate': 1.9977783750184e-05, 'epoch': 0.05}
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
Step 264: Updated gated ratio to 0.9898 (progress: 10.2%)
{'loss': 0.9974, 'grad_norm': 0.4305194914340973, 'learning_rate': 1.9977366710171274e-05, 'epoch': 0.05}
{'loss': 0.973, 'grad_norm': 0.4424034059047699, 'learning_rate': 1.9976945796601258e-05, 'epoch': 0.05}
{'loss': 1.0222, 'grad_norm': 0.46645984053611755, 'learning_rate': 1.9976521009637366e-05, 'epoch': 0.05}
{'loss': 1.0226, 'grad_norm': 0.4590684473514557, 'learning_rate': 1.997609234944452e-05, 'epoch': 0.05}
{'loss': 1.0602, 'grad_norm': 0.4162535071372986, 'learning_rate': 1.9975659816189137e-05, 'epoch': 0.05}
Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)
Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)
Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)
Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)
Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)

Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)
Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)
Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)
Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)
Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)
Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)
Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)
Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)
Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)
Step 269: Updated gated ratio to 0.9896 (progress: 10.4%)
{'loss': 1.0855, 'grad_norm': 0.48248541355133057, 'learning_rate': 1.997522341003914e-05, 'epoch': 0.05}
{'loss': 1.0406, 'grad_norm': 0.4139243960380554, 'learning_rate': 1.9974783131163957e-05, 'epoch': 0.05}
{'loss': 1.0702, 'grad_norm': 0.4548746943473816, 'learning_rate': 1.9974338979734523e-05, 'epoch': 0.05}
{'loss': 1.0837, 'grad_norm': 0.4485374093055725, 'learning_rate': 1.997389095592327e-05, 'epoch': 0.05}
{'loss': 0.9915, 'grad_norm': 0.46801358461380005, 'learning_rate': 1.9973439059904133e-05, 'epoch': 0.05}
{'loss': 1.0629, 'grad_norm': 0.4504789113998413, 'learning_rate': 1.9972983291852565e-05, 'epoch': 0.05}
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
Step 275: Updated gated ratio to 0.9894 (progress: 10.6%)
{'loss': 1.0199, 'grad_norm': 0.4454115927219391, 'learning_rate': 1.9972523651945496e-05, 'epoch': 0.05}
{'loss': 1.0805, 'grad_norm': 0.49948617815971375, 'learning_rate': 1.9972060140361384e-05, 'epoch': 0.05}
{'loss': 0.957, 'grad_norm': 0.39572903513908386, 'learning_rate': 1.997159275728018e-05, 'epoch': 0.05}
{'loss': 0.9822, 'grad_norm': 0.36268216371536255, 'learning_rate': 1.9971121502883332e-05, 'epoch': 0.05}
{'loss': 0.9809, 'grad_norm': 0.43080106377601624, 'learning_rate': 1.9970646377353802e-05, 'epoch': 0.05}
Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)
Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)
Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)
Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)
Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)
Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)


Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)
Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)
Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)
Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)
Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)
Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)
Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)
Step 280: Updated gated ratio to 0.9892 (progress: 10.8%)
{'loss': 1.0112, 'grad_norm': 0.41996654868125916, 'learning_rate': 1.997016738087605e-05, 'epoch': 0.05}
{'loss': 0.9863, 'grad_norm': 0.42262303829193115, 'learning_rate': 1.9969684513636035e-05, 'epoch': 0.05}
{'loss': 1.0309, 'grad_norm': 0.4799213111400604, 'learning_rate': 1.9969197775821227e-05, 'epoch': 0.05}
{'loss': 0.9164, 'grad_norm': 0.4311506450176239, 'learning_rate': 1.9968707167620593e-05, 'epoch': 0.05}
{'loss': 1.0304, 'grad_norm': 0.41621482372283936, 'learning_rate': 1.9968212689224603e-05, 'epoch': 0.05}
Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)
Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)
Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)
Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)
Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)
Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)
Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)
Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)
Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)
Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)
Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)
Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)
Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)
Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)
Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)
Step 285: Updated gated ratio to 0.9890 (progress: 11.0%)
{'loss': 1.1193, 'grad_norm': 0.43501415848731995, 'learning_rate': 1.996771434082523e-05, 'epoch': 0.06}
{'loss': 0.9916, 'grad_norm': 0.3796481192111969, 'learning_rate': 1.9967212122615958e-05, 'epoch': 0.06}
{'loss': 1.1124, 'grad_norm': 0.42458996176719666, 'learning_rate': 1.9966706034791752e-05, 'epoch': 0.06}
{'loss': 1.0708, 'grad_norm': 0.46949201822280884, 'learning_rate': 1.9966196077549106e-05, 'epoch': 0.06}
{'loss': 1.0421, 'grad_norm': 0.5074337124824524, 'learning_rate': 1.996568225108599e-05, 'epoch': 0.06}
Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)
Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)
Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)
Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)
Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)
Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)

Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)
Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)
Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)
Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)
Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)
Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)
Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)
Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)
Step 290: Updated gated ratio to 0.9888 (progress: 11.2%)
{'loss': 1.0639, 'grad_norm': 0.4546133279800415, 'learning_rate': 1.99651645556019e-05, 'epoch': 0.06}
{'loss': 0.9246, 'grad_norm': 0.4246006906032562, 'learning_rate': 1.9964642991297817e-05, 'epoch': 0.06}
{'loss': 0.9993, 'grad_norm': 0.43965551257133484, 'learning_rate': 1.996411755837623e-05, 'epoch': 0.06}
{'loss': 0.922, 'grad_norm': 0.4420652985572815, 'learning_rate': 1.9963588257041137e-05, 'epoch': 0.06}
{'loss': 1.0172, 'grad_norm': 0.48647430539131165, 'learning_rate': 1.996305508749802e-05, 'epoch': 0.06}
Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)
Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)
Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)
Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)
Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)
Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)
Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)
Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)

Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)
Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)

Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)
Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)
Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)
Step 295: Updated gated ratio to 0.9886 (progress: 11.4%)
{'loss': 1.043, 'grad_norm': 0.49114179611206055, 'learning_rate': 1.9962518049953887e-05, 'epoch': 0.06}
{'loss': 1.1382, 'grad_norm': 0.48182496428489685, 'learning_rate': 1.9961977144617225e-05, 'epoch': 0.06}
{'loss': 1.0297, 'grad_norm': 0.4669150412082672, 'learning_rate': 1.996143237169803e-05, 'epoch': 0.06}
{'loss': 1.063, 'grad_norm': 0.4792463183403015, 'learning_rate': 1.996088373140781e-05, 'epoch': 0.06}
{'loss': 1.0955, 'grad_norm': 0.39133739471435547, 'learning_rate': 1.9960331223959564e-05, 'epoch': 0.06}
{'loss': 1.0385, 'grad_norm': 0.45581868290901184, 'learning_rate': 1.995977484956779e-05, 'epoch': 0.06}
Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)
Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)
Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)
Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)
Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)

Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)
Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)

Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)
Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)
Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)
Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)
Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)
Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)
Step 301: Updated gated ratio to 0.9884 (progress: 11.6%)
{'loss': 1.0936, 'grad_norm': 0.5182480216026306, 'learning_rate': 1.9959214608448495e-05, 'epoch': 0.06}
{'loss': 1.0048, 'grad_norm': 0.44267740845680237, 'learning_rate': 1.9958650500819183e-05, 'epoch': 0.06}
{'loss': 0.9971, 'grad_norm': 0.4320186376571655, 'learning_rate': 1.995808252689886e-05, 'epoch': 0.06}
{'loss': 1.0068, 'grad_norm': 0.43287956714630127, 'learning_rate': 1.9957510686908034e-05, 'epoch': 0.06}
{'loss': 1.0473, 'grad_norm': 0.44718626141548157, 'learning_rate': 1.9956934981068713e-05, 'epoch': 0.06}
Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)
Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)
Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)
Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)
Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)
Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)
Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)
Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)
Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)
Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)
Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)
Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)
Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)
Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)
Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)
Step 306: Updated gated ratio to 0.9882 (progress: 11.8%)
{'loss': 1.1003, 'grad_norm': 0.4672119915485382, 'learning_rate': 1.9956355409604402e-05, 'epoch': 0.06}
{'loss': 1.1027, 'grad_norm': 0.44217783212661743, 'learning_rate': 1.9955771972740118e-05, 'epoch': 0.06}
{'loss': 1.1906, 'grad_norm': 0.511049747467041, 'learning_rate': 1.9955184670702363e-05, 'epoch': 0.06}
{'loss': 0.9735, 'grad_norm': 1.5444066524505615, 'learning_rate': 1.995459350371915e-05, 'epoch': 0.06}
{'loss': 1.0722, 'grad_norm': 0.46473389863967896, 'learning_rate': 1.9953998472019996e-05, 'epoch': 0.06}
Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)
Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)
Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)
Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)
Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)

Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)
Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)
Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)
Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)
Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)
Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)
Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)
Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)
Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)
Step 311: Updated gated ratio to 0.9880 (progress: 12.0%)
{'loss': 1.0939, 'grad_norm': 0.4781675934791565, 'learning_rate': 1.995339957583591e-05, 'epoch': 0.06}
{'loss': 1.0376, 'grad_norm': 0.3938636779785156, 'learning_rate': 1.9952796815399403e-05, 'epoch': 0.06}
{'loss': 1.0546, 'grad_norm': 0.3791656494140625, 'learning_rate': 1.9952190190944484e-05, 'epoch': 0.06}
{'loss': 0.9765, 'grad_norm': 0.4476751685142517, 'learning_rate': 1.9951579702706668e-05, 'epoch': 0.06}
{'loss': 1.0363, 'grad_norm': 0.41226351261138916, 'learning_rate': 1.9950965350922975e-05, 'epoch': 0.06}
Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)
Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)
Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)
Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)
Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)
Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)
Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)
Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)
Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)
Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)
Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)
Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)
Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)
Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)
Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)
Step 316: Updated gated ratio to 0.9878 (progress: 12.2%)
{'loss': 1.0136, 'grad_norm': 0.41972148418426514, 'learning_rate': 1.9950347135831907e-05, 'epoch': 0.06}
{'loss': 1.047, 'grad_norm': 0.45724931359291077, 'learning_rate': 1.994972505767348e-05, 'epoch': 0.06}
{'loss': 1.0014, 'grad_norm': 0.4234428107738495, 'learning_rate': 1.994909911668921e-05, 'epoch': 0.06}
{'loss': 0.9865, 'grad_norm': 0.42101189494132996, 'learning_rate': 1.99484693131221e-05, 'epoch': 0.06}
{'loss': 1.0384, 'grad_norm': 0.4479565918445587, 'learning_rate': 1.994783564721667e-05, 'epoch': 0.06}
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
Step 321: Updated gated ratio to 0.9876 (progress: 12.4%)
{'loss': 1.0424, 'grad_norm': 0.4162825047969818, 'learning_rate': 1.9947198119218924e-05, 'epoch': 0.06}
{'loss': 1.0438, 'grad_norm': 0.4341857135295868, 'learning_rate': 1.994655672937638e-05, 'epoch': 0.06}
{'loss': 1.0181, 'grad_norm': 0.5099653601646423, 'learning_rate': 1.9945911477938044e-05, 'epoch': 0.06}
{'loss': 1.0455, 'grad_norm': 0.38810035586357117, 'learning_rate': 1.994526236515442e-05, 'epoch': 0.06}
{'loss': 1.0528, 'grad_norm': 0.4200865924358368, 'learning_rate': 1.994460939127753e-05, 'epoch': 0.06}
{'loss': 1.0937, 'grad_norm': 0.4112583100795746, 'learning_rate': 1.9943952556560863e-05, 'epoch': 0.06}
Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)
Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)
Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)
Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)
Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)
Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)
Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)
Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)
Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)
Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)

Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)
Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)
Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)
Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)
Step 327: Updated gated ratio to 0.9874 (progress: 12.6%)
{'loss': 1.0864, 'grad_norm': 0.4635024070739746, 'learning_rate': 1.9943291861259433e-05, 'epoch': 0.06}
{'loss': 1.1042, 'grad_norm': 0.44213974475860596, 'learning_rate': 1.9942627305629747e-05, 'epoch': 0.06}
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
Step 329: Updated gated ratio to 0.9873 (progress: 12.7%)
{'loss': 1.0383, 'grad_norm': 0.4277055859565735, 'learning_rate': 1.9941958889929808e-05, 'epoch': 0.06}
{'loss': 1.0931, 'grad_norm': 0.4470152258872986, 'learning_rate': 1.9941286614419113e-05, 'epoch': 0.06}
{'loss': 0.952, 'grad_norm': 0.41705429553985596, 'learning_rate': 1.994061047935867e-05, 'epoch': 0.06}
{'loss': 0.9737, 'grad_norm': 0.455733984708786, 'learning_rate': 1.9939930485010968e-05, 'epoch': 0.06}
{'loss': 1.1453, 'grad_norm': 0.4591303765773773, 'learning_rate': 1.9939246631640014e-05, 'epoch': 0.06}
Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)
Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)
Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)
Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)

Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)
Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)
Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)

Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)
Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)
Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)
Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)
Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)
Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)
Step 334: Updated gated ratio to 0.9871 (progress: 12.9%)
{'loss': 0.9844, 'grad_norm': 0.412077933549881, 'learning_rate': 1.99385589195113e-05, 'epoch': 0.06}
{'loss': 1.0783, 'grad_norm': 0.46893036365509033, 'learning_rate': 1.9937867348891815e-05, 'epoch': 0.06}
{'loss': 0.9582, 'grad_norm': 0.43332499265670776, 'learning_rate': 1.9937171920050057e-05, 'epoch': 0.06}
{'loss': 1.0875, 'grad_norm': 0.48751750588417053, 'learning_rate': 1.9936472633256012e-05, 'epoch': 0.07}
{'loss': 1.0827, 'grad_norm': 0.437650203704834, 'learning_rate': 1.9935769488781167e-05, 'epoch': 0.07}
{'loss': 1.1125, 'grad_norm': 0.48000797629356384, 'learning_rate': 1.993506248689851e-05, 'epoch': 0.07}
Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)
Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)
Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)

Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)
Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)
Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)
Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)
Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)
Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)
Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)
Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)
Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)
Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)

Step 340: Updated gated ratio to 0.9869 (progress: 13.1%)
{'loss': 1.0219, 'grad_norm': 0.4403000473976135, 'learning_rate': 1.993435162788252e-05, 'epoch': 0.07}
{'loss': 1.07, 'grad_norm': 0.4768434464931488, 'learning_rate': 1.993363691200918e-05, 'epoch': 0.07}
{'loss': 0.9774, 'grad_norm': 0.449462354183197, 'learning_rate': 1.9932918339555965e-05, 'epoch': 0.07}
{'loss': 1.053, 'grad_norm': 0.5082927942276001, 'learning_rate': 1.9932195910801848e-05, 'epoch': 0.07}
{'loss': 1.0165, 'grad_norm': 0.4340040683746338, 'learning_rate': 1.9931469626027305e-05, 'epoch': 0.07}
Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)
Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)
Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)
Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)
Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)
Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)
Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)
Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)
Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)
Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)
Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)
Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)
Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)
Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)
Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)
Step 345: Updated gated ratio to 0.9867 (progress: 13.3%)
Error with image file is truncated (70 bytes not processed)
{'loss': 1.0331, 'grad_norm': 0.4209447205066681, 'learning_rate': 1.9930739485514304e-05, 'epoch': 0.07}
{'loss': 0.9786, 'grad_norm': 0.39687982201576233, 'learning_rate': 1.9930005489546308e-05, 'epoch': 0.07}
{'loss': 1.0067, 'grad_norm': 0.48998844623565674, 'learning_rate': 1.9929267638408277e-05, 'epoch': 0.07}
{'loss': 1.0739, 'grad_norm': 0.42135125398635864, 'learning_rate': 1.9928525932386678e-05, 'epoch': 0.07}
{'loss': 1.0686, 'grad_norm': 0.3628218472003937, 'learning_rate': 1.9927780371769463e-05, 'epoch': 0.07}
Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)
Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)
Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)
Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)
Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)
Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)
Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)
Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)
Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)
Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)
Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)
Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)
Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)

Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)
Step 350: Updated gated ratio to 0.9865 (progress: 13.5%)
{'loss': 1.113, 'grad_norm': 0.43574514985084534, 'learning_rate': 1.9927030956846083e-05, 'epoch': 0.07}
{'loss': 1.0762, 'grad_norm': 0.42895272374153137, 'learning_rate': 1.992627768790749e-05, 'epoch': 0.07}
{'loss': 1.0596, 'grad_norm': 0.44024187326431274, 'learning_rate': 1.9925520565246125e-05, 'epoch': 0.07}
{'loss': 1.0575, 'grad_norm': 0.38377103209495544, 'learning_rate': 1.9924759589155932e-05, 'epoch': 0.07}
{'loss': 1.0783, 'grad_norm': 0.4033851623535156, 'learning_rate': 1.9923994759932344e-05, 'epoch': 0.07}
Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)
Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)
Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)
Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)

Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)
Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)
Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)
Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)
Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)
Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)

Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)
Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)

Step 355: Updated gated ratio to 0.9863 (progress: 13.7%)
{'loss': 1.0499, 'grad_norm': 0.3925071060657501, 'learning_rate': 1.9923226077872296e-05, 'epoch': 0.07}
{'loss': 1.1682, 'grad_norm': 0.4677756428718567, 'learning_rate': 1.9922453543274223e-05, 'epoch': 0.07}
{'loss': 1.0588, 'grad_norm': 0.4496171176433563, 'learning_rate': 1.9921677156438044e-05, 'epoch': 0.07}
{'loss': 1.0511, 'grad_norm': 0.4383632242679596, 'learning_rate': 1.9920896917665178e-05, 'epoch': 0.07}
{'loss': 0.9691, 'grad_norm': 0.37907758355140686, 'learning_rate': 1.992011282725854e-05, 'epoch': 0.07}
Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)
Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)
Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)
Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)
Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)
Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)
Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)
Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)
Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)
Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)

Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)
Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)
Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)
Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)
Step 360: Updated gated ratio to 0.9861 (progress: 13.9%)
WARNING: tokenization mismatch: 0 vs. 517. (ignored)
{'loss': 1.0435, 'grad_norm': 0.4465864896774292, 'learning_rate': 1.9919324885522548e-05, 'epoch': 0.07}
{'loss': 1.0548, 'grad_norm': 0.4611518085002899, 'learning_rate': 1.99185330927631e-05, 'epoch': 0.07}
{'loss': 1.0315, 'grad_norm': 0.40925776958465576, 'learning_rate': 1.99177374492876e-05, 'epoch': 0.07}
{'loss': 1.0537, 'grad_norm': 0.4720209836959839, 'learning_rate': 1.991693795540494e-05, 'epoch': 0.07}
{'loss': 1.1751, 'grad_norm': 0.42590633034706116, 'learning_rate': 1.9916134611425522e-05, 'epoch': 0.07}
{'loss': 1.0477, 'grad_norm': 0.47986119985580444, 'learning_rate': 1.9915327417661226e-05, 'epoch': 0.07}
Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)
Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)
Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)
Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)
Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)
Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)

Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)
Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)
Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)
Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)

Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)
Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)
Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)
Step 366: Updated gated ratio to 0.9859 (progress: 14.1%)
{'loss': 1.1255, 'grad_norm': 0.5147709250450134, 'learning_rate': 1.991451637442543e-05, 'epoch': 0.07}
{'loss': 0.991, 'grad_norm': 0.45508381724357605, 'learning_rate': 1.9913701482033008e-05, 'epoch': 0.07}
{'loss': 1.0808, 'grad_norm': 0.45292890071868896, 'learning_rate': 1.9912882740800336e-05, 'epoch': 0.07}
{'loss': 1.0191, 'grad_norm': 0.37915751338005066, 'learning_rate': 1.9912060151045273e-05, 'epoch': 0.07}
{'loss': 1.0383, 'grad_norm': 0.4511735737323761, 'learning_rate': 1.9911233713087172e-05, 'epoch': 0.07}
Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)
Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)

Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)
Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)
Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)
Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)
Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)

Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)
Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)
Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)
Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)
Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)
Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)
Step 371: Updated gated ratio to 0.9857 (progress: 14.3%)
{'loss': 1.1469, 'grad_norm': 0.477134108543396, 'learning_rate': 1.9910403427246895e-05, 'epoch': 0.07}
{'loss': 1.086, 'grad_norm': 0.444733589887619, 'learning_rate': 1.990956929384678e-05, 'epoch': 0.07}
{'loss': 1.045, 'grad_norm': 0.4141145944595337, 'learning_rate': 1.990873131321067e-05, 'epoch': 0.07}
{'loss': 0.9827, 'grad_norm': 0.43148940801620483, 'learning_rate': 1.9907889485663897e-05, 'epoch': 0.07}
{'loss': 1.0247, 'grad_norm': 0.4291645288467407, 'learning_rate': 1.9907043811533283e-05, 'epoch': 0.07}
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
Step 376: Updated gated ratio to 0.9855 (progress: 14.5%)
{'loss': 1.0625, 'grad_norm': 0.4245534837245941, 'learning_rate': 1.9906194291147155e-05, 'epoch': 0.07}
{'loss': 1.0185, 'grad_norm': 0.44400113821029663, 'learning_rate': 1.9905340924835322e-05, 'epoch': 0.07}
{'loss': 1.0392, 'grad_norm': 0.508801281452179, 'learning_rate': 1.9904483712929094e-05, 'epoch': 0.07}
{'loss': 1.0032, 'grad_norm': 0.3828738331794739, 'learning_rate': 1.9903622655761267e-05, 'epoch': 0.07}
{'loss': 1.0715, 'grad_norm': 0.40809446573257446, 'learning_rate': 1.990275775366613e-05, 'epoch': 0.07}
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
Step 381: Updated gated ratio to 0.9853 (progress: 14.7%)
{'loss': 1.0456, 'grad_norm': 0.45815491676330566, 'learning_rate': 1.9901889006979473e-05, 'epoch': 0.07}
{'loss': 1.0464, 'grad_norm': 0.4380246698856354, 'learning_rate': 1.990101641603857e-05, 'epoch': 0.07}
{'loss': 1.1017, 'grad_norm': 0.45427852869033813, 'learning_rate': 1.9900139981182193e-05, 'epoch': 0.07}
{'loss': 1.0374, 'grad_norm': 0.42013710737228394, 'learning_rate': 1.9899259702750604e-05, 'epoch': 0.07}
{'loss': 0.9555, 'grad_norm': 0.45822054147720337, 'learning_rate': 1.9898375581085555e-05, 'epoch': 0.07}
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
Step 386: Updated gated ratio to 0.9851 (progress: 14.9%)
{'loss': 1.0063, 'grad_norm': 0.4611324369907379, 'learning_rate': 1.9897487616530296e-05, 'epoch': 0.07}
{'loss': 1.0343, 'grad_norm': 0.5014846324920654, 'learning_rate': 1.9896595809429565e-05, 'epoch': 0.07}
{'loss': 0.9958, 'grad_norm': 0.45212674140930176, 'learning_rate': 1.9895700160129593e-05, 'epoch': 0.07}
{'loss': 1.0362, 'grad_norm': 0.42658019065856934, 'learning_rate': 1.9894800668978095e-05, 'epoch': 0.08}
{'loss': 1.1067, 'grad_norm': 0.4733833074569702, 'learning_rate': 1.9893897336324292e-05, 'epoch': 0.08}
Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)
Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)
Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)
Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)
Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)
Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)
Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)
Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)
Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)
Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)
Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)
Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)
Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)
Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)
Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)
Step 391: Updated gated ratio to 0.9849 (progress: 15.1%)
{'loss': 1.1109, 'grad_norm': 0.48326441645622253, 'learning_rate': 1.9892990162518884e-05, 'epoch': 0.08}
{'loss': 1.0587, 'grad_norm': 0.4456924796104431, 'learning_rate': 1.9892079147914072e-05, 'epoch': 0.08}
{'loss': 1.0155, 'grad_norm': 0.43480879068374634, 'learning_rate': 1.9891164292863537e-05, 'epoch': 0.08}
{'loss': 1.0275, 'grad_norm': 0.38178420066833496, 'learning_rate': 1.9890245597722465e-05, 'epoch': 0.08}
{'loss': 1.0464, 'grad_norm': 0.4346759021282196, 'learning_rate': 1.9889323062847516e-05, 'epoch': 0.08}
{'loss': 1.0011, 'grad_norm': 0.46915799379348755, 'learning_rate': 1.988839668859686e-05, 'epoch': 0.08}
Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)
Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)
Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)
Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)
Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)
Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)
Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)
Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)
Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)
Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)

Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)
Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)
Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)
Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)
Step 397: Updated gated ratio to 0.9847 (progress: 15.3%)
{'loss': 0.9644, 'grad_norm': 0.4485468864440918, 'learning_rate': 1.988746647533014e-05, 'epoch': 0.08}
{'loss': 1.0449, 'grad_norm': 0.4402240812778473, 'learning_rate': 1.9886532423408495e-05, 'epoch': 0.08}
{'loss': 1.1013, 'grad_norm': 0.43782052397727966, 'learning_rate': 1.9885594533194564e-05, 'epoch': 0.08}
{'loss': 1.0422, 'grad_norm': 0.4244757890701294, 'learning_rate': 1.9884652805052465e-05, 'epoch': 0.08}
{'loss': 1.0153, 'grad_norm': 0.4104740619659424, 'learning_rate': 1.9883707239347804e-05, 'epoch': 0.08}
Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)
Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)
Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)
Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)
Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)
Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)
Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)
Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)
Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)
Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)
Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)
Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)
Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)
Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)
Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)
Step 402: Updated gated ratio to 0.9845 (progress: 15.5%)
{'loss': 1.019, 'grad_norm': 0.4701026380062103, 'learning_rate': 1.988275783644769e-05, 'epoch': 0.08}
{'loss': 1.0948, 'grad_norm': 0.45126500725746155, 'learning_rate': 1.988180459672071e-05, 'epoch': 0.08}
{'loss': 0.9893, 'grad_norm': 0.4368714988231659, 'learning_rate': 1.988084752053695e-05, 'epoch': 0.08}
{'loss': 1.0873, 'grad_norm': 0.5164222717285156, 'learning_rate': 1.9879886608267967e-05, 'epoch': 0.08}
{'loss': 1.0382, 'grad_norm': 0.44052600860595703, 'learning_rate': 1.9878921860286832e-05, 'epoch': 0.08}
Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)
Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)
Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)
Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)

Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)
Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)
Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)
Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)
Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)
Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)
Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)
Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)
Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)
Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)
Step 407: Updated gated ratio to 0.9843 (progress: 15.7%)
{'loss': 1.016, 'grad_norm': 0.4335276186466217, 'learning_rate': 1.9877953276968088e-05, 'epoch': 0.08}
{'loss': 1.0373, 'grad_norm': 0.49316495656967163, 'learning_rate': 1.9876980858687777e-05, 'epoch': 0.08}
{'loss': 0.9695, 'grad_norm': 0.49357709288597107, 'learning_rate': 1.9876004605823417e-05, 'epoch': 0.08}
{'loss': 1.0783, 'grad_norm': 0.44463783502578735, 'learning_rate': 1.987502451875403e-05, 'epoch': 0.08}
{'loss': 1.093, 'grad_norm': 0.4944148659706116, 'learning_rate': 1.987404059786012e-05, 'epoch': 0.08}
Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)
Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)
Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)
Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)

Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)
Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)
Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)
Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)
Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)
Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)
Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)
Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)
Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)
Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)
Step 412: Updated gated ratio to 0.9841 (progress: 15.9%)
{'loss': 1.0278, 'grad_norm': 0.42263954877853394, 'learning_rate': 1.9873052843523676e-05, 'epoch': 0.08}
{'loss': 1.1138, 'grad_norm': 0.5017216205596924, 'learning_rate': 1.987206125612818e-05, 'epoch': 0.08}
{'loss': 1.0469, 'grad_norm': 0.44271397590637207, 'learning_rate': 1.98710658360586e-05, 'epoch': 0.08}
{'loss': 1.0205, 'grad_norm': 0.4382459819316864, 'learning_rate': 1.987006658370139e-05, 'epoch': 0.08}
{'loss': 1.0784, 'grad_norm': 0.40959522128105164, 'learning_rate': 1.9869063499444495e-05, 'epoch': 0.08}
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
Step 417: Updated gated ratio to 0.9839 (progress: 16.1%)
{'loss': 1.0245, 'grad_norm': 0.41284823417663574, 'learning_rate': 1.9868056583677346e-05, 'epoch': 0.08}
{'loss': 0.9796, 'grad_norm': 0.4320214092731476, 'learning_rate': 1.9867045836790867e-05, 'epoch': 0.08}
