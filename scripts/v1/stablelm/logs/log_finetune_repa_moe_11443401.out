10.141.203.5 slots=4
10.141.203.9 slots=4
MASTER_PORT: 33789
MASTER_ADDR: g002
WORLD_SIZE: 
RANK: 
GLOBAL_RANK: 
LOCAL_RANK: 
NODE_RANK: 
SLURM_NNODES: 1
SLURM_NTASKS: 1
SLURM_PROCID: 0
SLURM_LOCALID: 0
[2025-09-05 13:19:30,745] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-09-05 13:19:54,266] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-05 13:20:00,051] [INFO] [runner.py:610:main] cmd = /home/li309/pct_code/venv/moellava-test2/bin/python -u -m deepspeed.launcher.launch --world_info=eyJnMDAyIjogWzAsIDEsIDIsIDNdfQ== --master_addr=g002 --master_port=33789 --enable_each_rank_log=None moellava/train/train_mem.py --moe_enable True --num_experts 4 --top_k_experts 2 --capacity_factor 1.5 --moe_mode sparse --use_residual False --router_aux_loss_coef 0.01 --train_modules gate_proj up_proj down_proj wg --deepspeed ./scripts/zero2.json --model_name_or_path ./checkpoints/MoE-LLaVA-StableLM-1.6B-4e --version stablelm --data_path /scratch3/li309/data/llava_data/train_json/llava_image_tune_.json /scratch3/li309/data/llava_data/train_json/nlp_tune.json --image_folder /scratch3/li309/data/llava_data/train_data --image_tower openai/clip-vit-large-patch14-336 --image_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./finetuned_checkpoints/MoE-LLaVA-StableLM-1.6B-4e-RePa-2 --num_train_epochs 1 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --gradient_accumulation_steps 1 --eval_strategy no --save_strategy steps --save_steps 4000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 18 --lazy_preprocess True --report_to tensorboard --cache_dir ./cache_dir --report_to wandb --finetune_repa_mode true --repa_gated_ratio 1.0
[2025-09-05 13:20:02,264] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-09-05 13:20:10,706] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-05 13:20:13,715] [INFO] [launch.py:139:main] 0 NCCL_ROOT=/apps/nccl/2.20.5-cu124
[2025-09-05 13:20:13,715] [INFO] [launch.py:139:main] 0 NCCL_ROOT_modshare=/apps/nccl/2.20.5-cu124:1
[2025-09-05 13:20:13,715] [INFO] [launch.py:139:main] 0 NCCL_HOME=/apps/nccl/2.20.5-cu124
[2025-09-05 13:20:13,715] [INFO] [launch.py:146:main] WORLD INFO DICT: {'g002': [0, 1, 2, 3]}
[2025-09-05 13:20:13,715] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-09-05 13:20:13,715] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'g002': [0, 1, 2, 3]})
[2025-09-05 13:20:13,715] [INFO] [launch.py:164:main] dist_world_size=4
[2025-09-05 13:20:13,715] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-09-05 13:20:13,716] [INFO] [launch.py:256:main] process 48007 spawned with command: ['/home/li309/pct_code/venv/moellava-test2/bin/python', '-u', 'moellava/train/train_mem.py', '--local_rank=0', '--moe_enable', 'True', '--num_experts', '4', '--top_k_experts', '2', '--capacity_factor', '1.5', '--moe_mode', 'sparse', '--use_residual', 'False', '--router_aux_loss_coef', '0.01', '--train_modules', 'gate_proj', 'up_proj', 'down_proj', 'wg', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', './checkpoints/MoE-LLaVA-StableLM-1.6B-4e', '--version', 'stablelm', '--data_path', '/scratch3/li309/data/llava_data/train_json/llava_image_tune_.json', '/scratch3/li309/data/llava_data/train_json/nlp_tune.json', '--image_folder', '/scratch3/li309/data/llava_data/train_data', '--image_tower', 'openai/clip-vit-large-patch14-336', '--image_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './finetuned_checkpoints/MoE-LLaVA-StableLM-1.6B-4e-RePa-2', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '8', '--gradient_accumulation_steps', '1', '--eval_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '4000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '18', '--lazy_preprocess', 'True', '--report_to', 'tensorboard', '--cache_dir', './cache_dir', '--report_to', 'wandb', '--finetune_repa_mode', 'true', '--repa_gated_ratio', '1.0']
[2025-09-05 13:20:13,717] [INFO] [launch.py:256:main] process 48008 spawned with command: ['/home/li309/pct_code/venv/moellava-test2/bin/python', '-u', 'moellava/train/train_mem.py', '--local_rank=1', '--moe_enable', 'True', '--num_experts', '4', '--top_k_experts', '2', '--capacity_factor', '1.5', '--moe_mode', 'sparse', '--use_residual', 'False', '--router_aux_loss_coef', '0.01', '--train_modules', 'gate_proj', 'up_proj', 'down_proj', 'wg', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', './checkpoints/MoE-LLaVA-StableLM-1.6B-4e', '--version', 'stablelm', '--data_path', '/scratch3/li309/data/llava_data/train_json/llava_image_tune_.json', '/scratch3/li309/data/llava_data/train_json/nlp_tune.json', '--image_folder', '/scratch3/li309/data/llava_data/train_data', '--image_tower', 'openai/clip-vit-large-patch14-336', '--image_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './finetuned_checkpoints/MoE-LLaVA-StableLM-1.6B-4e-RePa-2', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '8', '--gradient_accumulation_steps', '1', '--eval_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '4000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '18', '--lazy_preprocess', 'True', '--report_to', 'tensorboard', '--cache_dir', './cache_dir', '--report_to', 'wandb', '--finetune_repa_mode', 'true', '--repa_gated_ratio', '1.0']
[2025-09-05 13:20:13,717] [INFO] [launch.py:256:main] process 48009 spawned with command: ['/home/li309/pct_code/venv/moellava-test2/bin/python', '-u', 'moellava/train/train_mem.py', '--local_rank=2', '--moe_enable', 'True', '--num_experts', '4', '--top_k_experts', '2', '--capacity_factor', '1.5', '--moe_mode', 'sparse', '--use_residual', 'False', '--router_aux_loss_coef', '0.01', '--train_modules', 'gate_proj', 'up_proj', 'down_proj', 'wg', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', './checkpoints/MoE-LLaVA-StableLM-1.6B-4e', '--version', 'stablelm', '--data_path', '/scratch3/li309/data/llava_data/train_json/llava_image_tune_.json', '/scratch3/li309/data/llava_data/train_json/nlp_tune.json', '--image_folder', '/scratch3/li309/data/llava_data/train_data', '--image_tower', 'openai/clip-vit-large-patch14-336', '--image_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './finetuned_checkpoints/MoE-LLaVA-StableLM-1.6B-4e-RePa-2', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '8', '--gradient_accumulation_steps', '1', '--eval_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '4000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '18', '--lazy_preprocess', 'True', '--report_to', 'tensorboard', '--cache_dir', './cache_dir', '--report_to', 'wandb', '--finetune_repa_mode', 'true', '--repa_gated_ratio', '1.0']
[2025-09-05 13:20:13,718] [INFO] [launch.py:256:main] process 48010 spawned with command: ['/home/li309/pct_code/venv/moellava-test2/bin/python', '-u', 'moellava/train/train_mem.py', '--local_rank=3', '--moe_enable', 'True', '--num_experts', '4', '--top_k_experts', '2', '--capacity_factor', '1.5', '--moe_mode', 'sparse', '--use_residual', 'False', '--router_aux_loss_coef', '0.01', '--train_modules', 'gate_proj', 'up_proj', 'down_proj', 'wg', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', './checkpoints/MoE-LLaVA-StableLM-1.6B-4e', '--version', 'stablelm', '--data_path', '/scratch3/li309/data/llava_data/train_json/llava_image_tune_.json', '/scratch3/li309/data/llava_data/train_json/nlp_tune.json', '--image_folder', '/scratch3/li309/data/llava_data/train_data', '--image_tower', 'openai/clip-vit-large-patch14-336', '--image_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './finetuned_checkpoints/MoE-LLaVA-StableLM-1.6B-4e-RePa-2', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '8', '--gradient_accumulation_steps', '1', '--eval_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '4000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '18', '--lazy_preprocess', 'True', '--report_to', 'tensorboard', '--cache_dir', './cache_dir', '--report_to', 'wandb', '--finetune_repa_mode', 'true', '--repa_gated_ratio', '1.0']
[2025-09-05 13:20:23,977] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-05 13:20:23,987] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-05 13:20:23,988] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-05 13:20:23,990] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-09-05 13:20:25,699] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-05 13:20:25,699] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-05 13:20:25,699] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-05 13:20:25,700] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode

⚙️  Running in WANDB offline mode
[2025-09-05 13:20:26,247] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-05 13:20:26,247] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-05 13:20:26,247] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-05 13:20:26,247] [INFO] [comm.py:852:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-09-05 13:20:26,247] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-05 13:20:26,862] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 13:20:26,865] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 13:20:26,866] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 13:20:26,868] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 13:20:26,870] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 13:20:26,872] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 13:20:26,874] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 13:20:26,875] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 13:20:26,877] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 13:20:26,879] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 13:20:26,880] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 13:20:26,882] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
Vision encoder and proj init.

RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
Formatting inputs...Skip in lazy mode
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Set initial gated ratio to 1.0
  Total training steps: 20790.0
  Stage 1 (gated ratio reduction): 10395.0 steps
  Stage 2 (post-reparam training): 10395.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 20790.0
  Stage 1 (gated ratio reduction): 10395.0 steps
  Stage 2 (post-reparam training): 10395.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Set initial gated ratio to 1.0
  Total training steps: 20790.0
  Stage 1 (gated ratio reduction): 10395.0 steps
  Stage 2 (post-reparam training): 10395.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Set initial gated ratio to 1.0
  Total training steps: 20790.0
  Stage 1 (gated ratio reduction): 10395.0 steps
  Stage 2 (post-reparam training): 10395.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
WARNING: tokenization mismatch: 0 vs. 1275. (ignored)
{'loss': 1.1232, 'grad_norm': 0.7346362471580505, 'learning_rate': 0.0, 'epoch': 0.0}
{'loss': 0.9433, 'grad_norm': 0.8659026622772217, 'learning_rate': 3.205128205128205e-08, 'epoch': 0.0}
{'loss': 0.9683, 'grad_norm': 0.7754202485084534, 'learning_rate': 6.41025641025641e-08, 'epoch': 0.0}
{'loss': 1.1998, 'grad_norm': 0.8392043113708496, 'learning_rate': 9.615384615384617e-08, 'epoch': 0.0}
{'loss': 1.0247, 'grad_norm': 0.7394837737083435, 'learning_rate': 1.282051282051282e-07, 'epoch': 0.0}
{'loss': 0.975, 'grad_norm': 0.7132079601287842, 'learning_rate': 1.6025641025641025e-07, 'epoch': 0.0}
{'loss': 1.0291, 'grad_norm': 0.7873438000679016, 'learning_rate': 1.9230769230769234e-07, 'epoch': 0.0}
{'loss': 0.8711, 'grad_norm': 0.6497498154640198, 'learning_rate': 2.2435897435897438e-07, 'epoch': 0.0}
{'loss': 0.875, 'grad_norm': 0.851119875907898, 'learning_rate': 2.564102564102564e-07, 'epoch': 0.0}
{'loss': 0.8214, 'grad_norm': 0.6156340837478638, 'learning_rate': 2.884615384615385e-07, 'epoch': 0.0}
{'loss': 1.0782, 'grad_norm': 0.8860834240913391, 'learning_rate': 3.205128205128205e-07, 'epoch': 0.0}
{'loss': 0.8814, 'grad_norm': 0.4093471169471741, 'learning_rate': 3.525641025641026e-07, 'epoch': 0.0}
{'loss': 1.0911, 'grad_norm': 0.7978979349136353, 'learning_rate': 3.846153846153847e-07, 'epoch': 0.0}
{'loss': 0.913, 'grad_norm': 0.7048529982566833, 'learning_rate': 4.1666666666666667e-07, 'epoch': 0.0}
{'loss': 0.9198, 'grad_norm': 0.774105429649353, 'learning_rate': 4.4871794871794876e-07, 'epoch': 0.0}
{'loss': 1.0702, 'grad_norm': 0.6498738527297974, 'learning_rate': 4.807692307692308e-07, 'epoch': 0.0}
{'loss': 1.0816, 'grad_norm': 0.6989814043045044, 'learning_rate': 5.128205128205128e-07, 'epoch': 0.0}
{'loss': 1.2052, 'grad_norm': 0.7771924734115601, 'learning_rate': 5.448717948717949e-07, 'epoch': 0.0}
{'loss': 1.0713, 'grad_norm': 0.7997003197669983, 'learning_rate': 5.76923076923077e-07, 'epoch': 0.0}
{'loss': 0.8722, 'grad_norm': 0.7342482805252075, 'learning_rate': 6.08974358974359e-07, 'epoch': 0.0}
{'loss': 1.0425, 'grad_norm': 0.8811312913894653, 'learning_rate': 6.41025641025641e-07, 'epoch': 0.0}
{'loss': 0.9458, 'grad_norm': 0.44142529368400574, 'learning_rate': 6.730769230769231e-07, 'epoch': 0.0}
{'loss': 0.922, 'grad_norm': 0.7487683296203613, 'learning_rate': 7.051282051282052e-07, 'epoch': 0.0}
{'loss': 0.9782, 'grad_norm': 0.759861409664154, 'learning_rate': 7.371794871794873e-07, 'epoch': 0.0}
{'loss': 1.041, 'grad_norm': 0.8711516857147217, 'learning_rate': 7.692307692307694e-07, 'epoch': 0.0}
{'loss': 0.9464, 'grad_norm': 0.6269745826721191, 'learning_rate': 8.012820512820515e-07, 'epoch': 0.0}
{'loss': 1.1282, 'grad_norm': 0.8873745203018188, 'learning_rate': 8.333333333333333e-07, 'epoch': 0.0}
{'loss': 1.0462, 'grad_norm': 0.7075169086456299, 'learning_rate': 8.653846153846154e-07, 'epoch': 0.0}
{'loss': 1.1498, 'grad_norm': 0.7676811218261719, 'learning_rate': 8.974358974358975e-07, 'epoch': 0.0}
{'loss': 1.0004, 'grad_norm': 0.6403923630714417, 'learning_rate': 9.294871794871796e-07, 'epoch': 0.0}
{'loss': 1.0521, 'grad_norm': 0.7169086933135986, 'learning_rate': 9.615384615384617e-07, 'epoch': 0.0}
{'loss': 1.0076, 'grad_norm': 0.8924190998077393, 'learning_rate': 9.935897435897436e-07, 'epoch': 0.0}
{'loss': 0.9237, 'grad_norm': 0.8179100751876831, 'learning_rate': 1.0256410256410257e-06, 'epoch': 0.0}
{'loss': 1.0307, 'grad_norm': 0.7646263837814331, 'learning_rate': 1.0576923076923078e-06, 'epoch': 0.0}
{'loss': 0.9949, 'grad_norm': 0.7762450575828552, 'learning_rate': 1.0897435897435899e-06, 'epoch': 0.0}
{'loss': 0.956, 'grad_norm': 0.7806118726730347, 'learning_rate': 1.121794871794872e-06, 'epoch': 0.0}
{'loss': 1.0539, 'grad_norm': 0.7563377022743225, 'learning_rate': 1.153846153846154e-06, 'epoch': 0.0}
{'loss': 0.9691, 'grad_norm': 0.898323118686676, 'learning_rate': 1.185897435897436e-06, 'epoch': 0.0}
{'loss': 1.0556, 'grad_norm': 0.7061451077461243, 'learning_rate': 1.217948717948718e-06, 'epoch': 0.0}
{'loss': 0.8383, 'grad_norm': 0.7363886833190918, 'learning_rate': 1.25e-06, 'epoch': 0.0}
{'loss': 0.9606, 'grad_norm': 0.7284542322158813, 'learning_rate': 1.282051282051282e-06, 'epoch': 0.0}
{'loss': 0.9307, 'grad_norm': 0.743604838848114, 'learning_rate': 1.3141025641025643e-06, 'epoch': 0.0}
{'loss': 1.0124, 'grad_norm': 0.6987375617027283, 'learning_rate': 1.3461538461538462e-06, 'epoch': 0.0}
{'loss': 1.0983, 'grad_norm': 0.765491783618927, 'learning_rate': 1.3782051282051285e-06, 'epoch': 0.0}
{'loss': 0.9981, 'grad_norm': 0.7951213121414185, 'learning_rate': 1.4102564102564104e-06, 'epoch': 0.0}
{'loss': 0.9298, 'grad_norm': 0.6788871884346008, 'learning_rate': 1.4423076923076922e-06, 'epoch': 0.0}
{'loss': 0.9379, 'grad_norm': 0.8822317123413086, 'learning_rate': 1.4743589743589745e-06, 'epoch': 0.0}
{'loss': 0.9947, 'grad_norm': 0.48177677392959595, 'learning_rate': 1.5064102564102564e-06, 'epoch': 0.0}
{'loss': 0.9944, 'grad_norm': 0.6927613615989685, 'learning_rate': 1.5384615384615387e-06, 'epoch': 0.0}
{'loss': 0.9616, 'grad_norm': 0.6815250515937805, 'learning_rate': 1.5705128205128206e-06, 'epoch': 0.0}
{'loss': 1.1307, 'grad_norm': 0.7204534411430359, 'learning_rate': 1.602564102564103e-06, 'epoch': 0.0}
{'loss': 0.9948, 'grad_norm': 0.6625851392745972, 'learning_rate': 1.6346153846153848e-06, 'epoch': 0.0}
{'loss': 1.1374, 'grad_norm': 0.7290217280387878, 'learning_rate': 1.6666666666666667e-06, 'epoch': 0.0}
{'loss': 1.0303, 'grad_norm': 0.7579251527786255, 'learning_rate': 1.698717948717949e-06, 'epoch': 0.0}
{'loss': 0.8999, 'grad_norm': 0.8685777187347412, 'learning_rate': 1.7307692307692308e-06, 'epoch': 0.0}
{'loss': 0.957, 'grad_norm': 0.7643397450447083, 'learning_rate': 1.7628205128205131e-06, 'epoch': 0.0}
{'loss': 0.9198, 'grad_norm': 0.47275134921073914, 'learning_rate': 1.794871794871795e-06, 'epoch': 0.0}
{'loss': 1.0055, 'grad_norm': 0.8549197912216187, 'learning_rate': 1.826923076923077e-06, 'epoch': 0.0}
{'loss': 1.143, 'grad_norm': 0.890927255153656, 'learning_rate': 1.8589743589743592e-06, 'epoch': 0.0}
{'loss': 1.1554, 'grad_norm': 0.981715202331543, 'learning_rate': 1.891025641025641e-06, 'epoch': 0.0}
{'loss': 0.9932, 'grad_norm': 0.7205512523651123, 'learning_rate': 1.9230769230769234e-06, 'epoch': 0.0}
{'loss': 0.9646, 'grad_norm': 0.6485276222229004, 'learning_rate': 1.9551282051282055e-06, 'epoch': 0.0}
{'loss': 1.1123, 'grad_norm': 0.9448151588439941, 'learning_rate': 1.987179487179487e-06, 'epoch': 0.0}
{'loss': 1.0471, 'grad_norm': 0.7214096188545227, 'learning_rate': 2.0192307692307692e-06, 'epoch': 0.0}
{'loss': 1.0416, 'grad_norm': 0.7801170349121094, 'learning_rate': 2.0512820512820513e-06, 'epoch': 0.0}
{'loss': 0.9946, 'grad_norm': 0.7549988627433777, 'learning_rate': 2.0833333333333334e-06, 'epoch': 0.0}
{'loss': 1.1126, 'grad_norm': 0.7785344123840332, 'learning_rate': 2.1153846153846155e-06, 'epoch': 0.0}
{'loss': 0.7785, 'grad_norm': 0.4018246829509735, 'learning_rate': 2.1474358974358976e-06, 'epoch': 0.0}
{'loss': 0.9292, 'grad_norm': 0.48090672492980957, 'learning_rate': 2.1794871794871797e-06, 'epoch': 0.0}
{'loss': 0.9867, 'grad_norm': 0.7512015700340271, 'learning_rate': 2.211538461538462e-06, 'epoch': 0.0}
{'loss': 1.0255, 'grad_norm': 0.8009175062179565, 'learning_rate': 2.243589743589744e-06, 'epoch': 0.0}
{'loss': 1.0531, 'grad_norm': 0.9327078461647034, 'learning_rate': 2.275641025641026e-06, 'epoch': 0.0}
{'loss': 0.9766, 'grad_norm': 0.6702020764350891, 'learning_rate': 2.307692307692308e-06, 'epoch': 0.0}
{'loss': 0.9849, 'grad_norm': 0.7965996265411377, 'learning_rate': 2.3397435897435897e-06, 'epoch': 0.0}
{'loss': 0.8483, 'grad_norm': 0.8566994667053223, 'learning_rate': 2.371794871794872e-06, 'epoch': 0.0}
{'loss': 1.0978, 'grad_norm': 0.8708980083465576, 'learning_rate': 2.403846153846154e-06, 'epoch': 0.0}
{'loss': 0.9699, 'grad_norm': 0.6147603392601013, 'learning_rate': 2.435897435897436e-06, 'epoch': 0.0}
{'loss': 0.9739, 'grad_norm': 0.7091526389122009, 'learning_rate': 2.467948717948718e-06, 'epoch': 0.0}
{'loss': 1.1238, 'grad_norm': 0.7687881588935852, 'learning_rate': 2.5e-06, 'epoch': 0.0}
{'loss': 0.9001, 'grad_norm': 0.7248747944831848, 'learning_rate': 2.5320512820512823e-06, 'epoch': 0.0}
{'loss': 1.0512, 'grad_norm': 0.7607538104057312, 'learning_rate': 2.564102564102564e-06, 'epoch': 0.0}
{'loss': 0.8644, 'grad_norm': 0.7051264047622681, 'learning_rate': 2.5961538461538465e-06, 'epoch': 0.0}
{'loss': 1.1046, 'grad_norm': 0.8045806288719177, 'learning_rate': 2.6282051282051286e-06, 'epoch': 0.0}
{'loss': 0.9359, 'grad_norm': 0.7376599907875061, 'learning_rate': 2.6602564102564107e-06, 'epoch': 0.0}
{'loss': 0.8083, 'grad_norm': 0.7155592441558838, 'learning_rate': 2.6923076923076923e-06, 'epoch': 0.0}
{'loss': 1.0368, 'grad_norm': 0.7275007367134094, 'learning_rate': 2.7243589743589744e-06, 'epoch': 0.0}
{'loss': 1.0449, 'grad_norm': 0.4557726979255676, 'learning_rate': 2.756410256410257e-06, 'epoch': 0.0}
{'loss': 1.0853, 'grad_norm': 0.8939875364303589, 'learning_rate': 2.7884615384615386e-06, 'epoch': 0.0}
{'loss': 0.9556, 'grad_norm': 0.8215521574020386, 'learning_rate': 2.8205128205128207e-06, 'epoch': 0.0}
{'loss': 1.1479, 'grad_norm': 0.7019622921943665, 'learning_rate': 2.852564102564103e-06, 'epoch': 0.0}
{'loss': 0.9626, 'grad_norm': 0.6637140512466431, 'learning_rate': 2.8846153846153845e-06, 'epoch': 0.0}
{'loss': 1.1178, 'grad_norm': 0.7813048362731934, 'learning_rate': 2.916666666666667e-06, 'epoch': 0.0}
{'loss': 1.2921, 'grad_norm': 0.8223593235015869, 'learning_rate': 2.948717948717949e-06, 'epoch': 0.0}
{'loss': 1.1224, 'grad_norm': 0.8048459887504578, 'learning_rate': 2.980769230769231e-06, 'epoch': 0.0}
{'loss': 1.0469, 'grad_norm': 0.7704593539237976, 'learning_rate': 3.012820512820513e-06, 'epoch': 0.0}
{'loss': 0.9143, 'grad_norm': 0.7358514666557312, 'learning_rate': 3.044871794871795e-06, 'epoch': 0.0}
{'loss': 1.0027, 'grad_norm': 0.679530143737793, 'learning_rate': 3.0769230769230774e-06, 'epoch': 0.0}
{'loss': 0.971, 'grad_norm': 0.7889167666435242, 'learning_rate': 3.108974358974359e-06, 'epoch': 0.0}
{'loss': 1.0249, 'grad_norm': 0.9319354891777039, 'learning_rate': 3.141025641025641e-06, 'epoch': 0.0}
{'loss': 0.8964, 'grad_norm': 0.7176393866539001, 'learning_rate': 3.1730769230769233e-06, 'epoch': 0.0}
{'loss': 1.0488, 'grad_norm': 0.6984726190567017, 'learning_rate': 3.205128205128206e-06, 'epoch': 0.0}
{'loss': 0.9704, 'grad_norm': 0.8351145386695862, 'learning_rate': 3.2371794871794875e-06, 'epoch': 0.0}
{'loss': 0.9734, 'grad_norm': 0.8174227476119995, 'learning_rate': 3.2692307692307696e-06, 'epoch': 0.0}
{'loss': 1.0345, 'grad_norm': 0.8265982270240784, 'learning_rate': 3.3012820512820517e-06, 'epoch': 0.01}
{'loss': 1.1256, 'grad_norm': 0.6889663338661194, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.01}
{'loss': 0.9738, 'grad_norm': 0.7825217247009277, 'learning_rate': 3.365384615384616e-06, 'epoch': 0.01}
{'loss': 0.9798, 'grad_norm': 0.6217710375785828, 'learning_rate': 3.397435897435898e-06, 'epoch': 0.01}
{'loss': 1.1005, 'grad_norm': 0.7390099167823792, 'learning_rate': 3.4294871794871796e-06, 'epoch': 0.01}
{'loss': 1.1127, 'grad_norm': 0.7850803136825562, 'learning_rate': 3.4615384615384617e-06, 'epoch': 0.01}
{'loss': 1.0588, 'grad_norm': 0.7749985456466675, 'learning_rate': 3.4935897435897438e-06, 'epoch': 0.01}
{'loss': 1.0315, 'grad_norm': 0.7502292394638062, 'learning_rate': 3.5256410256410263e-06, 'epoch': 0.01}
{'loss': 1.0915, 'grad_norm': 0.7702133655548096, 'learning_rate': 3.557692307692308e-06, 'epoch': 0.01}
{'loss': 1.1056, 'grad_norm': 0.8313168287277222, 'learning_rate': 3.58974358974359e-06, 'epoch': 0.01}
{'loss': 0.9084, 'grad_norm': 0.8434048891067505, 'learning_rate': 3.621794871794872e-06, 'epoch': 0.01}
{'loss': 0.9247, 'grad_norm': 0.7908504605293274, 'learning_rate': 3.653846153846154e-06, 'epoch': 0.01}
{'loss': 1.0872, 'grad_norm': 0.8506609201431274, 'learning_rate': 3.6858974358974363e-06, 'epoch': 0.01}
{'loss': 0.9271, 'grad_norm': 0.8779677152633667, 'learning_rate': 3.7179487179487184e-06, 'epoch': 0.01}
{'loss': 0.9397, 'grad_norm': 0.8029981255531311, 'learning_rate': 3.7500000000000005e-06, 'epoch': 0.01}
{'loss': 0.9967, 'grad_norm': 0.9737627506256104, 'learning_rate': 3.782051282051282e-06, 'epoch': 0.01}
{'loss': 1.0525, 'grad_norm': 0.8052775859832764, 'learning_rate': 3.8141025641025643e-06, 'epoch': 0.01}
{'loss': 1.0663, 'grad_norm': 0.9619184136390686, 'learning_rate': 3.846153846153847e-06, 'epoch': 0.01}
{'loss': 1.1056, 'grad_norm': 0.8687289953231812, 'learning_rate': 3.878205128205129e-06, 'epoch': 0.01}
{'loss': 1.1834, 'grad_norm': 1.031633973121643, 'learning_rate': 3.910256410256411e-06, 'epoch': 0.01}
{'loss': 0.9929, 'grad_norm': 0.7526296973228455, 'learning_rate': 3.942307692307692e-06, 'epoch': 0.01}
{'loss': 1.0485, 'grad_norm': 0.7460152506828308, 'learning_rate': 3.974358974358974e-06, 'epoch': 0.01}
{'loss': 0.9647, 'grad_norm': 0.7395356893539429, 'learning_rate': 4.006410256410257e-06, 'epoch': 0.01}
{'loss': 1.1396, 'grad_norm': 0.8237251043319702, 'learning_rate': 4.0384615384615385e-06, 'epoch': 0.01}
{'loss': 1.0403, 'grad_norm': 0.6585218906402588, 'learning_rate': 4.070512820512821e-06, 'epoch': 0.01}
{'loss': 0.8466, 'grad_norm': 0.6934774518013, 'learning_rate': 4.102564102564103e-06, 'epoch': 0.01}
{'loss': 0.8995, 'grad_norm': 0.8392654657363892, 'learning_rate': 4.134615384615385e-06, 'epoch': 0.01}
{'loss': 1.1426, 'grad_norm': 0.8180004358291626, 'learning_rate': 4.166666666666667e-06, 'epoch': 0.01}
{'loss': 1.0261, 'grad_norm': 0.7480025887489319, 'learning_rate': 4.198717948717949e-06, 'epoch': 0.01}
{'loss': 0.8626, 'grad_norm': 0.7264253497123718, 'learning_rate': 4.230769230769231e-06, 'epoch': 0.01}
{'loss': 1.0217, 'grad_norm': 0.7318468689918518, 'learning_rate': 4.262820512820513e-06, 'epoch': 0.01}
{'loss': 0.8719, 'grad_norm': 0.8499716520309448, 'learning_rate': 4.294871794871795e-06, 'epoch': 0.01}
{'loss': 1.0773, 'grad_norm': 0.7335085272789001, 'learning_rate': 4.326923076923077e-06, 'epoch': 0.01}
{'loss': 1.0429, 'grad_norm': 0.8147963285446167, 'learning_rate': 4.358974358974359e-06, 'epoch': 0.01}
{'loss': 1.0968, 'grad_norm': 0.873275876045227, 'learning_rate': 4.3910256410256415e-06, 'epoch': 0.01}
{'loss': 1.0863, 'grad_norm': 1.004019856452942, 'learning_rate': 4.423076923076924e-06, 'epoch': 0.01}
{'loss': 0.9126, 'grad_norm': 0.6985914707183838, 'learning_rate': 4.455128205128206e-06, 'epoch': 0.01}
{'loss': 0.9997, 'grad_norm': 0.735418438911438, 'learning_rate': 4.487179487179488e-06, 'epoch': 0.01}
{'loss': 0.9569, 'grad_norm': 0.7938678860664368, 'learning_rate': 4.51923076923077e-06, 'epoch': 0.01}
{'loss': 0.7035, 'grad_norm': 0.8065444827079773, 'learning_rate': 4.551282051282052e-06, 'epoch': 0.01}
{'loss': 0.8842, 'grad_norm': 0.7138515710830688, 'learning_rate': 4.583333333333333e-06, 'epoch': 0.01}
{'loss': 0.8252, 'grad_norm': 0.6742943525314331, 'learning_rate': 4.615384615384616e-06, 'epoch': 0.01}
{'loss': 1.0032, 'grad_norm': 0.8237826824188232, 'learning_rate': 4.647435897435898e-06, 'epoch': 0.01}
{'loss': 1.0431, 'grad_norm': 0.9215320944786072, 'learning_rate': 4.6794871794871795e-06, 'epoch': 0.01}
{'loss': 0.9466, 'grad_norm': 0.4476214051246643, 'learning_rate': 4.711538461538462e-06, 'epoch': 0.01}
{'loss': 0.9776, 'grad_norm': 0.8903685212135315, 'learning_rate': 4.743589743589744e-06, 'epoch': 0.01}
{'loss': 1.0209, 'grad_norm': 0.7901264429092407, 'learning_rate': 4.775641025641027e-06, 'epoch': 0.01}
{'loss': 1.0222, 'grad_norm': 0.7132570147514343, 'learning_rate': 4.807692307692308e-06, 'epoch': 0.01}
{'loss': 0.9418, 'grad_norm': 0.6806939840316772, 'learning_rate': 4.83974358974359e-06, 'epoch': 0.01}
{'loss': 1.0464, 'grad_norm': 0.973871648311615, 'learning_rate': 4.871794871794872e-06, 'epoch': 0.01}
{'loss': 0.9118, 'grad_norm': 0.8401728272438049, 'learning_rate': 4.903846153846154e-06, 'epoch': 0.01}
{'loss': 0.9861, 'grad_norm': 0.9038891196250916, 'learning_rate': 4.935897435897436e-06, 'epoch': 0.01}
{'loss': 1.0159, 'grad_norm': 0.7400701642036438, 'learning_rate': 4.967948717948718e-06, 'epoch': 0.01}
{'loss': 1.0767, 'grad_norm': 0.8190091252326965, 'learning_rate': 5e-06, 'epoch': 0.01}
{'loss': 1.1085, 'grad_norm': 0.7069677114486694, 'learning_rate': 5.0320512820512825e-06, 'epoch': 0.01}
{'loss': 0.8914, 'grad_norm': 0.7087083458900452, 'learning_rate': 5.064102564102565e-06, 'epoch': 0.01}
{'loss': 1.0957, 'grad_norm': 0.7591313719749451, 'learning_rate': 5.096153846153846e-06, 'epoch': 0.01}
{'loss': 1.0171, 'grad_norm': 0.7454250454902649, 'learning_rate': 5.128205128205128e-06, 'epoch': 0.01}
{'loss': 0.9755, 'grad_norm': 0.8833251595497131, 'learning_rate': 5.160256410256411e-06, 'epoch': 0.01}
{'loss': 0.8498, 'grad_norm': 0.7663891911506653, 'learning_rate': 5.192307692307693e-06, 'epoch': 0.01}
{'loss': 0.9037, 'grad_norm': 0.7033043503761292, 'learning_rate': 5.224358974358975e-06, 'epoch': 0.01}
{'loss': 0.9121, 'grad_norm': 0.7713703513145447, 'learning_rate': 5.256410256410257e-06, 'epoch': 0.01}
{'loss': 0.9282, 'grad_norm': 0.6822992563247681, 'learning_rate': 5.288461538461539e-06, 'epoch': 0.01}
{'loss': 1.1236, 'grad_norm': 0.9158197641372681, 'learning_rate': 5.320512820512821e-06, 'epoch': 0.01}
{'loss': 0.9673, 'grad_norm': 0.912320077419281, 'learning_rate': 5.3525641025641026e-06, 'epoch': 0.01}
{'loss': 0.9392, 'grad_norm': 0.6237982511520386, 'learning_rate': 5.384615384615385e-06, 'epoch': 0.01}
{'loss': 1.1333, 'grad_norm': 0.9801838397979736, 'learning_rate': 5.416666666666667e-06, 'epoch': 0.01}
{'loss': 1.0549, 'grad_norm': 0.7921586632728577, 'learning_rate': 5.448717948717949e-06, 'epoch': 0.01}
{'loss': 1.163, 'grad_norm': 0.9920591711997986, 'learning_rate': 5.480769230769232e-06, 'epoch': 0.01}
{'loss': 1.0877, 'grad_norm': 0.8563170433044434, 'learning_rate': 5.512820512820514e-06, 'epoch': 0.01}
{'loss': 0.9584, 'grad_norm': 0.7141827940940857, 'learning_rate': 5.544871794871796e-06, 'epoch': 0.01}
{'loss': 0.9316, 'grad_norm': 0.8644380569458008, 'learning_rate': 5.576923076923077e-06, 'epoch': 0.01}
{'loss': 0.8808, 'grad_norm': 0.7866358160972595, 'learning_rate': 5.608974358974359e-06, 'epoch': 0.01}
{'loss': 0.949, 'grad_norm': 0.49922212958335876, 'learning_rate': 5.641025641025641e-06, 'epoch': 0.01}
{'loss': 0.7967, 'grad_norm': 0.8113923072814941, 'learning_rate': 5.6730769230769235e-06, 'epoch': 0.01}
{'loss': 0.9921, 'grad_norm': 0.691866934299469, 'learning_rate': 5.705128205128206e-06, 'epoch': 0.01}
{'loss': 0.8945, 'grad_norm': 0.7344111800193787, 'learning_rate': 5.737179487179487e-06, 'epoch': 0.01}
{'loss': 1.0445, 'grad_norm': 0.7513982057571411, 'learning_rate': 5.769230769230769e-06, 'epoch': 0.01}
{'loss': 0.958, 'grad_norm': 0.8165144920349121, 'learning_rate': 5.801282051282052e-06, 'epoch': 0.01}
{'loss': 1.054, 'grad_norm': 0.8655000329017639, 'learning_rate': 5.833333333333334e-06, 'epoch': 0.01}
{'loss': 1.0204, 'grad_norm': 0.7877597212791443, 'learning_rate': 5.865384615384616e-06, 'epoch': 0.01}
{'loss': 1.1234, 'grad_norm': 0.9063160419464111, 'learning_rate': 5.897435897435898e-06, 'epoch': 0.01}
{'loss': 1.0121, 'grad_norm': 0.7494876980781555, 'learning_rate': 5.92948717948718e-06, 'epoch': 0.01}
{'loss': 0.8886, 'grad_norm': 0.44263219833374023, 'learning_rate': 5.961538461538462e-06, 'epoch': 0.01}
{'loss': 1.1022, 'grad_norm': 0.7983574867248535, 'learning_rate': 5.9935897435897436e-06, 'epoch': 0.01}
{'loss': 1.2467, 'grad_norm': 0.9502344131469727, 'learning_rate': 6.025641025641026e-06, 'epoch': 0.01}
{'loss': 1.0064, 'grad_norm': 0.7753276824951172, 'learning_rate': 6.057692307692308e-06, 'epoch': 0.01}
{'loss': 0.9839, 'grad_norm': 0.766548216342926, 'learning_rate': 6.08974358974359e-06, 'epoch': 0.01}
{'loss': 0.9426, 'grad_norm': 0.8420212864875793, 'learning_rate': 6.121794871794873e-06, 'epoch': 0.01}
{'loss': 1.0541, 'grad_norm': 0.797863245010376, 'learning_rate': 6.153846153846155e-06, 'epoch': 0.01}
{'loss': 0.9028, 'grad_norm': 0.9010117650032043, 'learning_rate': 6.185897435897437e-06, 'epoch': 0.01}
{'loss': 0.9884, 'grad_norm': 0.8222707509994507, 'learning_rate': 6.217948717948718e-06, 'epoch': 0.01}
{'loss': 1.0143, 'grad_norm': 0.8961164355278015, 'learning_rate': 6.25e-06, 'epoch': 0.01}
{'loss': 0.9076, 'grad_norm': 0.47705647349357605, 'learning_rate': 6.282051282051282e-06, 'epoch': 0.01}
{'loss': 1.0193, 'grad_norm': 0.8819875717163086, 'learning_rate': 6.3141025641025645e-06, 'epoch': 0.01}
{'loss': 0.8703, 'grad_norm': 0.7489989995956421, 'learning_rate': 6.3461538461538466e-06, 'epoch': 0.01}
{'loss': 1.0701, 'grad_norm': 0.7624578475952148, 'learning_rate': 6.378205128205129e-06, 'epoch': 0.01}
{'loss': 0.9457, 'grad_norm': 0.6983945369720459, 'learning_rate': 6.410256410256412e-06, 'epoch': 0.01}
{'loss': 0.9743, 'grad_norm': 0.7873876094818115, 'learning_rate': 6.442307692307693e-06, 'epoch': 0.01}
{'loss': 1.1037, 'grad_norm': 0.7609715461730957, 'learning_rate': 6.474358974358975e-06, 'epoch': 0.01}
{'loss': 1.0985, 'grad_norm': 0.8119806051254272, 'learning_rate': 6.506410256410257e-06, 'epoch': 0.01}
{'loss': 0.909, 'grad_norm': 0.8164893984794617, 'learning_rate': 6.538461538461539e-06, 'epoch': 0.01}
{'loss': 1.0944, 'grad_norm': 0.982168972492218, 'learning_rate': 6.570512820512821e-06, 'epoch': 0.01}
{'loss': 1.0559, 'grad_norm': 0.7998006343841553, 'learning_rate': 6.602564102564103e-06, 'epoch': 0.01}
{'loss': 1.2434, 'grad_norm': 0.9848833084106445, 'learning_rate': 6.6346153846153846e-06, 'epoch': 0.01}
{'loss': 1.1154, 'grad_norm': 0.6621381640434265, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.01}
{'loss': 1.0017, 'grad_norm': 0.7357891201972961, 'learning_rate': 6.698717948717949e-06, 'epoch': 0.01}
{'loss': 1.016, 'grad_norm': 0.7497349977493286, 'learning_rate': 6.730769230769232e-06, 'epoch': 0.01}
{'loss': 1.0944, 'grad_norm': 0.7606129050254822, 'learning_rate': 6.762820512820514e-06, 'epoch': 0.01}
{'loss': 1.1107, 'grad_norm': 0.9375219345092773, 'learning_rate': 6.794871794871796e-06, 'epoch': 0.01}
{'loss': 1.0487, 'grad_norm': 0.8042964339256287, 'learning_rate': 6.826923076923078e-06, 'epoch': 0.01}
{'loss': 1.0218, 'grad_norm': 0.9479029774665833, 'learning_rate': 6.858974358974359e-06, 'epoch': 0.01}
{'loss': 0.9507, 'grad_norm': 0.7645559906959534, 'learning_rate': 6.891025641025641e-06, 'epoch': 0.01}
{'loss': 1.0113, 'grad_norm': 0.7741028070449829, 'learning_rate': 6.923076923076923e-06, 'epoch': 0.01}
{'loss': 1.1439, 'grad_norm': 0.9212945103645325, 'learning_rate': 6.9551282051282055e-06, 'epoch': 0.01}
{'loss': 0.9589, 'grad_norm': 0.7849764823913574, 'learning_rate': 6.9871794871794876e-06, 'epoch': 0.01}
{'loss': 0.8279, 'grad_norm': 0.5075072646141052, 'learning_rate': 7.01923076923077e-06, 'epoch': 0.01}
{'loss': 1.0613, 'grad_norm': 0.7904084920883179, 'learning_rate': 7.051282051282053e-06, 'epoch': 0.01}
{'loss': 1.0706, 'grad_norm': 0.843697190284729, 'learning_rate': 7.083333333333335e-06, 'epoch': 0.01}
{'loss': 1.091, 'grad_norm': 0.821563184261322, 'learning_rate': 7.115384615384616e-06, 'epoch': 0.01}
{'loss': 1.1722, 'grad_norm': 1.0748943090438843, 'learning_rate': 7.147435897435898e-06, 'epoch': 0.01}
{'loss': 0.9214, 'grad_norm': 0.7759855389595032, 'learning_rate': 7.17948717948718e-06, 'epoch': 0.01}
{'loss': 1.0452, 'grad_norm': 0.9308654069900513, 'learning_rate': 7.211538461538462e-06, 'epoch': 0.01}
{'loss': 0.8874, 'grad_norm': 0.8835304379463196, 'learning_rate': 7.243589743589744e-06, 'epoch': 0.01}
{'loss': 1.0238, 'grad_norm': 0.8811925053596497, 'learning_rate': 7.2756410256410255e-06, 'epoch': 0.01}
{'loss': 1.0857, 'grad_norm': 0.7755162119865417, 'learning_rate': 7.307692307692308e-06, 'epoch': 0.01}
{'loss': 0.9606, 'grad_norm': 0.8194996118545532, 'learning_rate': 7.33974358974359e-06, 'epoch': 0.01}
{'loss': 1.0408, 'grad_norm': 0.8999438285827637, 'learning_rate': 7.371794871794873e-06, 'epoch': 0.01}
{'loss': 0.9896, 'grad_norm': 0.7587051391601562, 'learning_rate': 7.403846153846155e-06, 'epoch': 0.01}
{'loss': 1.145, 'grad_norm': 0.8457808494567871, 'learning_rate': 7.435897435897437e-06, 'epoch': 0.01}
{'loss': 0.9624, 'grad_norm': 0.7727187871932983, 'learning_rate': 7.467948717948719e-06, 'epoch': 0.01}
{'loss': 0.8832, 'grad_norm': 0.48480549454689026, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.01}
{'loss': 0.95, 'grad_norm': 0.7621362805366516, 'learning_rate': 7.532051282051282e-06, 'epoch': 0.01}
{'loss': 1.2396, 'grad_norm': 0.9224541187286377, 'learning_rate': 7.564102564102564e-06, 'epoch': 0.01}
{'loss': 0.9459, 'grad_norm': 0.6709834933280945, 'learning_rate': 7.5961538461538465e-06, 'epoch': 0.01}
{'loss': 0.9563, 'grad_norm': 0.7070239186286926, 'learning_rate': 7.6282051282051286e-06, 'epoch': 0.01}
{'loss': 0.918, 'grad_norm': 0.6882554888725281, 'learning_rate': 7.660256410256411e-06, 'epoch': 0.01}
{'loss': 1.0304, 'grad_norm': 0.8531622886657715, 'learning_rate': 7.692307692307694e-06, 'epoch': 0.01}
{'loss': 0.9174, 'grad_norm': 0.7577370405197144, 'learning_rate': 7.724358974358976e-06, 'epoch': 0.01}
{'loss': 1.0409, 'grad_norm': 0.7460697293281555, 'learning_rate': 7.756410256410258e-06, 'epoch': 0.01}
{'loss': 1.0155, 'grad_norm': 0.8988919854164124, 'learning_rate': 7.78846153846154e-06, 'epoch': 0.01}
{'loss': 1.0153, 'grad_norm': 0.6992008686065674, 'learning_rate': 7.820512820512822e-06, 'epoch': 0.01}
{'loss': 1.0539, 'grad_norm': 0.7838346362113953, 'learning_rate': 7.852564102564102e-06, 'epoch': 0.01}
{'loss': 1.1811, 'grad_norm': 0.9454664587974548, 'learning_rate': 7.884615384615384e-06, 'epoch': 0.01}
{'loss': 0.9714, 'grad_norm': 0.5172556638717651, 'learning_rate': 7.916666666666667e-06, 'epoch': 0.01}
{'loss': 1.1144, 'grad_norm': 0.8930568099021912, 'learning_rate': 7.948717948717949e-06, 'epoch': 0.01}
{'loss': 1.1383, 'grad_norm': 0.9506701827049255, 'learning_rate': 7.980769230769232e-06, 'epoch': 0.01}
{'loss': 1.0635, 'grad_norm': 0.8139315843582153, 'learning_rate': 8.012820512820515e-06, 'epoch': 0.01}
{'loss': 1.1378, 'grad_norm': 0.8582223653793335, 'learning_rate': 8.044871794871797e-06, 'epoch': 0.01}
{'loss': 1.1012, 'grad_norm': 0.8299696445465088, 'learning_rate': 8.076923076923077e-06, 'epoch': 0.01}
{'loss': 1.1113, 'grad_norm': 0.7702494859695435, 'learning_rate': 8.108974358974359e-06, 'epoch': 0.01}
{'loss': 0.93, 'grad_norm': 0.47227931022644043, 'learning_rate': 8.141025641025641e-06, 'epoch': 0.01}
Error with image file is truncated (16 bytes not processed)
{'loss': 1.0705, 'grad_norm': 0.8411458134651184, 'learning_rate': 8.173076923076923e-06, 'epoch': 0.01}
{'loss': 1.098, 'grad_norm': 1.0077557563781738, 'learning_rate': 8.205128205128205e-06, 'epoch': 0.01}
{'loss': 1.136, 'grad_norm': 0.8259098529815674, 'learning_rate': 8.237179487179487e-06, 'epoch': 0.01}
{'loss': 0.9524, 'grad_norm': 0.8352466225624084, 'learning_rate': 8.26923076923077e-06, 'epoch': 0.01}
{'loss': 0.9307, 'grad_norm': 0.7753879427909851, 'learning_rate': 8.301282051282052e-06, 'epoch': 0.01}
{'loss': 1.0245, 'grad_norm': 0.8236362338066101, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.01}
{'loss': 0.9419, 'grad_norm': 0.9135636687278748, 'learning_rate': 8.365384615384616e-06, 'epoch': 0.01}
{'loss': 0.8473, 'grad_norm': 0.4123368561267853, 'learning_rate': 8.397435897435898e-06, 'epoch': 0.01}
{'loss': 1.0609, 'grad_norm': 0.8127539157867432, 'learning_rate': 8.42948717948718e-06, 'epoch': 0.01}
{'loss': 1.1043, 'grad_norm': 0.9380900859832764, 'learning_rate': 8.461538461538462e-06, 'epoch': 0.01}
{'loss': 1.0323, 'grad_norm': 0.8882405757904053, 'learning_rate': 8.493589743589744e-06, 'epoch': 0.01}
{'loss': 1.0163, 'grad_norm': 0.7635160088539124, 'learning_rate': 8.525641025641026e-06, 'epoch': 0.01}
{'loss': 1.0746, 'grad_norm': 0.7444671392440796, 'learning_rate': 8.557692307692308e-06, 'epoch': 0.01}
{'loss': 0.8964, 'grad_norm': 0.7876992225646973, 'learning_rate': 8.58974358974359e-06, 'epoch': 0.01}
{'loss': 1.0101, 'grad_norm': 0.46512678265571594, 'learning_rate': 8.621794871794873e-06, 'epoch': 0.01}
{'loss': 1.2025, 'grad_norm': 0.9825636148452759, 'learning_rate': 8.653846153846155e-06, 'epoch': 0.01}
{'loss': 0.9354, 'grad_norm': 0.7512984275817871, 'learning_rate': 8.685897435897437e-06, 'epoch': 0.01}
{'loss': 0.9889, 'grad_norm': 0.870732843875885, 'learning_rate': 8.717948717948719e-06, 'epoch': 0.01}
{'loss': 1.0803, 'grad_norm': 0.8525665998458862, 'learning_rate': 8.750000000000001e-06, 'epoch': 0.01}
{'loss': 1.0463, 'grad_norm': 0.7714089155197144, 'learning_rate': 8.782051282051283e-06, 'epoch': 0.01}
{'loss': 1.07, 'grad_norm': 0.8968446850776672, 'learning_rate': 8.814102564102565e-06, 'epoch': 0.01}
{'loss': 1.1353, 'grad_norm': 0.7610722184181213, 'learning_rate': 8.846153846153847e-06, 'epoch': 0.01}
{'loss': 0.9093, 'grad_norm': 0.9112856984138489, 'learning_rate': 8.87820512820513e-06, 'epoch': 0.01}
{'loss': 1.249, 'grad_norm': 0.9495800733566284, 'learning_rate': 8.910256410256411e-06, 'epoch': 0.01}
{'loss': 1.1319, 'grad_norm': 0.7562564611434937, 'learning_rate': 8.942307692307693e-06, 'epoch': 0.01}
{'loss': 1.0974, 'grad_norm': 0.784231960773468, 'learning_rate': 8.974358974358976e-06, 'epoch': 0.01}
{'loss': 1.0534, 'grad_norm': 0.8235567808151245, 'learning_rate': 9.006410256410258e-06, 'epoch': 0.01}
{'loss': 0.8727, 'grad_norm': 0.526139497756958, 'learning_rate': 9.03846153846154e-06, 'epoch': 0.01}
{'loss': 0.8703, 'grad_norm': 0.8689786195755005, 'learning_rate': 9.070512820512822e-06, 'epoch': 0.01}
{'loss': 1.207, 'grad_norm': 0.9707593321800232, 'learning_rate': 9.102564102564104e-06, 'epoch': 0.01}
{'loss': 0.8284, 'grad_norm': 0.6910439729690552, 'learning_rate': 9.134615384615384e-06, 'epoch': 0.01}
{'loss': 1.0219, 'grad_norm': 0.7987880110740662, 'learning_rate': 9.166666666666666e-06, 'epoch': 0.01}
{'loss': 1.0263, 'grad_norm': 0.844253659248352, 'learning_rate': 9.198717948717949e-06, 'epoch': 0.01}
{'loss': 0.9337, 'grad_norm': 0.7575427889823914, 'learning_rate': 9.230769230769232e-06, 'epoch': 0.01}
{'loss': 0.991, 'grad_norm': 0.8831285238265991, 'learning_rate': 9.262820512820514e-06, 'epoch': 0.01}
{'loss': 0.9325, 'grad_norm': 0.7976217865943909, 'learning_rate': 9.294871794871796e-06, 'epoch': 0.01}
{'loss': 0.9834, 'grad_norm': 0.8028823733329773, 'learning_rate': 9.326923076923079e-06, 'epoch': 0.01}
{'loss': 0.9357, 'grad_norm': 0.8956277370452881, 'learning_rate': 9.358974358974359e-06, 'epoch': 0.01}
{'loss': 1.0987, 'grad_norm': 0.7172549962997437, 'learning_rate': 9.391025641025641e-06, 'epoch': 0.01}
{'loss': 1.0117, 'grad_norm': 0.7450450658798218, 'learning_rate': 9.423076923076923e-06, 'epoch': 0.01}
{'loss': 0.9258, 'grad_norm': 0.707683801651001, 'learning_rate': 9.455128205128205e-06, 'epoch': 0.01}
{'loss': 0.9609, 'grad_norm': 0.7680329084396362, 'learning_rate': 9.487179487179487e-06, 'epoch': 0.01}
{'loss': 0.9696, 'grad_norm': 0.9448049068450928, 'learning_rate': 9.51923076923077e-06, 'epoch': 0.01}
{'loss': 0.9333, 'grad_norm': 0.8228171467781067, 'learning_rate': 9.551282051282053e-06, 'epoch': 0.01}
{'loss': 0.9335, 'grad_norm': 0.7585817575454712, 'learning_rate': 9.583333333333335e-06, 'epoch': 0.01}
{'loss': 1.0518, 'grad_norm': 0.7978477478027344, 'learning_rate': 9.615384615384616e-06, 'epoch': 0.01}
{'loss': 1.1507, 'grad_norm': 0.8973222374916077, 'learning_rate': 9.647435897435898e-06, 'epoch': 0.01}
{'loss': 1.0107, 'grad_norm': 0.8989363312721252, 'learning_rate': 9.67948717948718e-06, 'epoch': 0.01}
{'loss': 0.9624, 'grad_norm': 0.734792947769165, 'learning_rate': 9.711538461538462e-06, 'epoch': 0.01}
{'loss': 0.944, 'grad_norm': 0.8225606083869934, 'learning_rate': 9.743589743589744e-06, 'epoch': 0.01}
{'loss': 1.0816, 'grad_norm': 0.7883721590042114, 'learning_rate': 9.775641025641026e-06, 'epoch': 0.01}
{'loss': 0.8936, 'grad_norm': 0.7542572021484375, 'learning_rate': 9.807692307692308e-06, 'epoch': 0.01}
{'loss': 0.9404, 'grad_norm': 0.8824356198310852, 'learning_rate': 9.83974358974359e-06, 'epoch': 0.01}
{'loss': 1.0957, 'grad_norm': 0.8125590682029724, 'learning_rate': 9.871794871794872e-06, 'epoch': 0.01}
{'loss': 0.9606, 'grad_norm': 0.8803543448448181, 'learning_rate': 9.903846153846155e-06, 'epoch': 0.01}
{'loss': 0.9698, 'grad_norm': 0.8631675839424133, 'learning_rate': 9.935897435897437e-06, 'epoch': 0.01}
{'loss': 1.0051, 'grad_norm': 0.8177893757820129, 'learning_rate': 9.967948717948719e-06, 'epoch': 0.02}
{'loss': 0.9996, 'grad_norm': 0.8364356756210327, 'learning_rate': 1e-05, 'epoch': 0.02}
{'loss': 0.9504, 'grad_norm': 0.9719263315200806, 'learning_rate': 1.0032051282051283e-05, 'epoch': 0.02}
{'loss': 1.0446, 'grad_norm': 1.1584073305130005, 'learning_rate': 1.0064102564102565e-05, 'epoch': 0.02}
{'loss': 0.9811, 'grad_norm': 0.7879366874694824, 'learning_rate': 1.0096153846153847e-05, 'epoch': 0.02}
{'loss': 0.9875, 'grad_norm': 0.8081228733062744, 'learning_rate': 1.012820512820513e-05, 'epoch': 0.02}
{'loss': 1.0366, 'grad_norm': 0.8022374510765076, 'learning_rate': 1.0160256410256411e-05, 'epoch': 0.02}
{'loss': 1.0871, 'grad_norm': 1.052775263786316, 'learning_rate': 1.0192307692307692e-05, 'epoch': 0.02}
{'loss': 1.1419, 'grad_norm': 0.764898955821991, 'learning_rate': 1.0224358974358974e-05, 'epoch': 0.02}
{'loss': 1.0718, 'grad_norm': 0.7949329614639282, 'learning_rate': 1.0256410256410256e-05, 'epoch': 0.02}
{'loss': 0.9584, 'grad_norm': 0.4867458939552307, 'learning_rate': 1.0288461538461538e-05, 'epoch': 0.02}
{'loss': 0.9483, 'grad_norm': 0.846220850944519, 'learning_rate': 1.0320512820512822e-05, 'epoch': 0.02}
{'loss': 1.0585, 'grad_norm': 0.8912309408187866, 'learning_rate': 1.0352564102564104e-05, 'epoch': 0.02}
{'loss': 0.9156, 'grad_norm': 0.4864983856678009, 'learning_rate': 1.0384615384615386e-05, 'epoch': 0.02}
{'loss': 1.0188, 'grad_norm': 0.7308588027954102, 'learning_rate': 1.0416666666666668e-05, 'epoch': 0.02}
{'loss': 1.1081, 'grad_norm': 0.7854897975921631, 'learning_rate': 1.044871794871795e-05, 'epoch': 0.02}
{'loss': 0.9873, 'grad_norm': 0.7331918478012085, 'learning_rate': 1.0480769230769232e-05, 'epoch': 0.02}
{'loss': 0.8435, 'grad_norm': 0.7226455211639404, 'learning_rate': 1.0512820512820514e-05, 'epoch': 0.02}
{'loss': 0.8944, 'grad_norm': 0.7833594083786011, 'learning_rate': 1.0544871794871796e-05, 'epoch': 0.02}
{'loss': 0.99, 'grad_norm': 0.8805249929428101, 'learning_rate': 1.0576923076923078e-05, 'epoch': 0.02}
{'loss': 1.1412, 'grad_norm': 0.9229205846786499, 'learning_rate': 1.060897435897436e-05, 'epoch': 0.02}
{'loss': 1.1107, 'grad_norm': 0.814499020576477, 'learning_rate': 1.0641025641025643e-05, 'epoch': 0.02}
{'loss': 0.966, 'grad_norm': 0.48906251788139343, 'learning_rate': 1.0673076923076923e-05, 'epoch': 0.02}
{'loss': 0.9764, 'grad_norm': 0.827995240688324, 'learning_rate': 1.0705128205128205e-05, 'epoch': 0.02}
{'loss': 0.9995, 'grad_norm': 0.8386750221252441, 'learning_rate': 1.0737179487179487e-05, 'epoch': 0.02}
{'loss': 1.0185, 'grad_norm': 1.1095935106277466, 'learning_rate': 1.076923076923077e-05, 'epoch': 0.02}
{'loss': 0.9512, 'grad_norm': 0.8906147480010986, 'learning_rate': 1.0801282051282051e-05, 'epoch': 0.02}
{'loss': 1.1996, 'grad_norm': 1.048511028289795, 'learning_rate': 1.0833333333333334e-05, 'epoch': 0.02}
{'loss': 1.031, 'grad_norm': 0.7708719968795776, 'learning_rate': 1.0865384615384616e-05, 'epoch': 0.02}
{'loss': 1.0116, 'grad_norm': 0.6496822237968445, 'learning_rate': 1.0897435897435898e-05, 'epoch': 0.02}
{'loss': 1.084, 'grad_norm': 0.7721797823905945, 'learning_rate': 1.092948717948718e-05, 'epoch': 0.02}
{'loss': 0.9957, 'grad_norm': 0.8507253527641296, 'learning_rate': 1.0961538461538464e-05, 'epoch': 0.02}
{'loss': 1.0779, 'grad_norm': 0.9499549865722656, 'learning_rate': 1.0993589743589746e-05, 'epoch': 0.02}
{'loss': 0.9456, 'grad_norm': 0.8570952415466309, 'learning_rate': 1.1025641025641028e-05, 'epoch': 0.02}
{'loss': 1.0406, 'grad_norm': 0.8394924998283386, 'learning_rate': 1.105769230769231e-05, 'epoch': 0.02}
{'loss': 1.096, 'grad_norm': 0.4862719476222992, 'learning_rate': 1.1089743589743592e-05, 'epoch': 0.02}
{'loss': 1.0166, 'grad_norm': 0.8293138146400452, 'learning_rate': 1.1121794871794872e-05, 'epoch': 0.02}
{'loss': 1.1151, 'grad_norm': 0.840257465839386, 'learning_rate': 1.1153846153846154e-05, 'epoch': 0.02}
{'loss': 1.0845, 'grad_norm': 0.8642345666885376, 'learning_rate': 1.1185897435897437e-05, 'epoch': 0.02}
{'loss': 1.0225, 'grad_norm': 0.8682634830474854, 'learning_rate': 1.1217948717948719e-05, 'epoch': 0.02}
{'loss': 1.0855, 'grad_norm': 0.9250274300575256, 'learning_rate': 1.125e-05, 'epoch': 0.02}
{'loss': 0.9458, 'grad_norm': 0.7386958003044128, 'learning_rate': 1.1282051282051283e-05, 'epoch': 0.02}
{'loss': 0.9552, 'grad_norm': 0.8276582360267639, 'learning_rate': 1.1314102564102565e-05, 'epoch': 0.02}
{'loss': 0.9363, 'grad_norm': 0.8118895888328552, 'learning_rate': 1.1346153846153847e-05, 'epoch': 0.02}
{'loss': 0.8666, 'grad_norm': 0.8555469512939453, 'learning_rate': 1.1378205128205129e-05, 'epoch': 0.02}
{'loss': 1.0049, 'grad_norm': 0.8625882267951965, 'learning_rate': 1.1410256410256411e-05, 'epoch': 0.02}
{'loss': 1.0876, 'grad_norm': 0.9459601640701294, 'learning_rate': 1.1442307692307693e-05, 'epoch': 0.02}
{'loss': 0.8894, 'grad_norm': 0.8911288380622864, 'learning_rate': 1.1474358974358974e-05, 'epoch': 0.02}
{'loss': 1.0178, 'grad_norm': 0.8436288833618164, 'learning_rate': 1.1506410256410256e-05, 'epoch': 0.02}
{'loss': 0.9975, 'grad_norm': 0.8632181286811829, 'learning_rate': 1.1538461538461538e-05, 'epoch': 0.02}
{'loss': 0.9914, 'grad_norm': 0.8386671543121338, 'learning_rate': 1.1570512820512823e-05, 'epoch': 0.02}
{'loss': 0.916, 'grad_norm': 0.7987494468688965, 'learning_rate': 1.1602564102564104e-05, 'epoch': 0.02}
{'loss': 1.0848, 'grad_norm': 0.9129874110221863, 'learning_rate': 1.1634615384615386e-05, 'epoch': 0.02}
{'loss': 0.9304, 'grad_norm': 0.757311224937439, 'learning_rate': 1.1666666666666668e-05, 'epoch': 0.02}
{'loss': 1.0598, 'grad_norm': 0.7840555310249329, 'learning_rate': 1.169871794871795e-05, 'epoch': 0.02}
{'loss': 0.9873, 'grad_norm': 0.7533671855926514, 'learning_rate': 1.1730769230769232e-05, 'epoch': 0.02}
{'loss': 0.9781, 'grad_norm': 0.49173209071159363, 'learning_rate': 1.1762820512820514e-05, 'epoch': 0.02}
{'loss': 1.0282, 'grad_norm': 0.9552844166755676, 'learning_rate': 1.1794871794871796e-05, 'epoch': 0.02}
{'loss': 1.1235, 'grad_norm': 0.8671815395355225, 'learning_rate': 1.1826923076923078e-05, 'epoch': 0.02}
{'loss': 0.9429, 'grad_norm': 0.756068766117096, 'learning_rate': 1.185897435897436e-05, 'epoch': 0.02}
{'loss': 1.1085, 'grad_norm': 0.890811026096344, 'learning_rate': 1.1891025641025643e-05, 'epoch': 0.02}
{'loss': 1.0006, 'grad_norm': 0.7888994812965393, 'learning_rate': 1.1923076923076925e-05, 'epoch': 0.02}
{'loss': 1.0479, 'grad_norm': 0.818525493144989, 'learning_rate': 1.1955128205128205e-05, 'epoch': 0.02}
{'loss': 1.027, 'grad_norm': 0.9109971523284912, 'learning_rate': 1.1987179487179487e-05, 'epoch': 0.02}
{'loss': 1.0086, 'grad_norm': 0.8394908308982849, 'learning_rate': 1.201923076923077e-05, 'epoch': 0.02}
{'loss': 1.0744, 'grad_norm': 0.7425410151481628, 'learning_rate': 1.2051282051282051e-05, 'epoch': 0.02}
{'loss': 0.9139, 'grad_norm': 0.8252086043357849, 'learning_rate': 1.2083333333333333e-05, 'epoch': 0.02}
{'loss': 1.0527, 'grad_norm': 0.5357196927070618, 'learning_rate': 1.2115384615384615e-05, 'epoch': 0.02}
{'loss': 1.0173, 'grad_norm': 0.9491221904754639, 'learning_rate': 1.2147435897435898e-05, 'epoch': 0.02}
{'loss': 1.1595, 'grad_norm': 1.0329887866973877, 'learning_rate': 1.217948717948718e-05, 'epoch': 0.02}
{'loss': 1.0817, 'grad_norm': 0.8277249336242676, 'learning_rate': 1.2211538461538463e-05, 'epoch': 0.02}
{'loss': 1.0756, 'grad_norm': 0.9479571580886841, 'learning_rate': 1.2243589743589746e-05, 'epoch': 0.02}
{'loss': 1.1142, 'grad_norm': 0.9464700222015381, 'learning_rate': 1.2275641025641028e-05, 'epoch': 0.02}
{'loss': 1.0566, 'grad_norm': 0.919857382774353, 'learning_rate': 1.230769230769231e-05, 'epoch': 0.02}
{'loss': 0.9775, 'grad_norm': 1.0369585752487183, 'learning_rate': 1.2339743589743592e-05, 'epoch': 0.02}
{'loss': 1.1184, 'grad_norm': 0.8015099763870239, 'learning_rate': 1.2371794871794874e-05, 'epoch': 0.02}
{'loss': 0.8011, 'grad_norm': 1.2078144550323486, 'learning_rate': 1.2403846153846156e-05, 'epoch': 0.02}
{'loss': 1.1475, 'grad_norm': 1.0703167915344238, 'learning_rate': 1.2435897435897436e-05, 'epoch': 0.02}
{'loss': 0.9531, 'grad_norm': 0.8409735560417175, 'learning_rate': 1.2467948717948719e-05, 'epoch': 0.02}
{'loss': 1.0701, 'grad_norm': 0.8886199593544006, 'learning_rate': 1.25e-05, 'epoch': 0.02}
{'loss': 0.8686, 'grad_norm': 0.7544246912002563, 'learning_rate': 1.2532051282051283e-05, 'epoch': 0.02}
{'loss': 1.0376, 'grad_norm': 0.9822623133659363, 'learning_rate': 1.2564102564102565e-05, 'epoch': 0.02}
