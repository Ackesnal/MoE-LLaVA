10.141.203.5 slots=4
10.141.203.9 slots=4
MASTER_PORT: 33789
MASTER_ADDR: g002
WORLD_SIZE: 
RANK: 
GLOBAL_RANK: 
LOCAL_RANK: 
NODE_RANK: 
SLURM_NNODES: 1
SLURM_NTASKS: 1
SLURM_PROCID: 0
SLURM_LOCALID: 0
[2025-09-05 13:19:30,745] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-09-05 13:19:54,266] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-05 13:20:00,051] [INFO] [runner.py:610:main] cmd = /home/li309/pct_code/venv/moellava-test2/bin/python -u -m deepspeed.launcher.launch --world_info=eyJnMDAyIjogWzAsIDEsIDIsIDNdfQ== --master_addr=g002 --master_port=33789 --enable_each_rank_log=None moellava/train/train_mem.py --moe_enable True --num_experts 4 --top_k_experts 2 --capacity_factor 1.5 --moe_mode sparse --use_residual False --router_aux_loss_coef 0.01 --train_modules gate_proj up_proj down_proj wg --deepspeed ./scripts/zero2.json --model_name_or_path ./checkpoints/MoE-LLaVA-StableLM-1.6B-4e --version stablelm --data_path /scratch3/li309/data/llava_data/train_json/llava_image_tune_.json /scratch3/li309/data/llava_data/train_json/nlp_tune.json --image_folder /scratch3/li309/data/llava_data/train_data --image_tower openai/clip-vit-large-patch14-336 --image_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./finetuned_checkpoints/MoE-LLaVA-StableLM-1.6B-4e-RePa-2 --num_train_epochs 1 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --gradient_accumulation_steps 1 --eval_strategy no --save_strategy steps --save_steps 4000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 18 --lazy_preprocess True --report_to tensorboard --cache_dir ./cache_dir --report_to wandb --finetune_repa_mode true --repa_gated_ratio 1.0
[2025-09-05 13:20:02,264] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-09-05 13:20:10,706] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-05 13:20:13,715] [INFO] [launch.py:139:main] 0 NCCL_ROOT=/apps/nccl/2.20.5-cu124
[2025-09-05 13:20:13,715] [INFO] [launch.py:139:main] 0 NCCL_ROOT_modshare=/apps/nccl/2.20.5-cu124:1
[2025-09-05 13:20:13,715] [INFO] [launch.py:139:main] 0 NCCL_HOME=/apps/nccl/2.20.5-cu124
[2025-09-05 13:20:13,715] [INFO] [launch.py:146:main] WORLD INFO DICT: {'g002': [0, 1, 2, 3]}
[2025-09-05 13:20:13,715] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-09-05 13:20:13,715] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'g002': [0, 1, 2, 3]})
[2025-09-05 13:20:13,715] [INFO] [launch.py:164:main] dist_world_size=4
[2025-09-05 13:20:13,715] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-09-05 13:20:13,716] [INFO] [launch.py:256:main] process 48007 spawned with command: ['/home/li309/pct_code/venv/moellava-test2/bin/python', '-u', 'moellava/train/train_mem.py', '--local_rank=0', '--moe_enable', 'True', '--num_experts', '4', '--top_k_experts', '2', '--capacity_factor', '1.5', '--moe_mode', 'sparse', '--use_residual', 'False', '--router_aux_loss_coef', '0.01', '--train_modules', 'gate_proj', 'up_proj', 'down_proj', 'wg', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', './checkpoints/MoE-LLaVA-StableLM-1.6B-4e', '--version', 'stablelm', '--data_path', '/scratch3/li309/data/llava_data/train_json/llava_image_tune_.json', '/scratch3/li309/data/llava_data/train_json/nlp_tune.json', '--image_folder', '/scratch3/li309/data/llava_data/train_data', '--image_tower', 'openai/clip-vit-large-patch14-336', '--image_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './finetuned_checkpoints/MoE-LLaVA-StableLM-1.6B-4e-RePa-2', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '8', '--gradient_accumulation_steps', '1', '--eval_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '4000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '18', '--lazy_preprocess', 'True', '--report_to', 'tensorboard', '--cache_dir', './cache_dir', '--report_to', 'wandb', '--finetune_repa_mode', 'true', '--repa_gated_ratio', '1.0']
[2025-09-05 13:20:13,717] [INFO] [launch.py:256:main] process 48008 spawned with command: ['/home/li309/pct_code/venv/moellava-test2/bin/python', '-u', 'moellava/train/train_mem.py', '--local_rank=1', '--moe_enable', 'True', '--num_experts', '4', '--top_k_experts', '2', '--capacity_factor', '1.5', '--moe_mode', 'sparse', '--use_residual', 'False', '--router_aux_loss_coef', '0.01', '--train_modules', 'gate_proj', 'up_proj', 'down_proj', 'wg', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', './checkpoints/MoE-LLaVA-StableLM-1.6B-4e', '--version', 'stablelm', '--data_path', '/scratch3/li309/data/llava_data/train_json/llava_image_tune_.json', '/scratch3/li309/data/llava_data/train_json/nlp_tune.json', '--image_folder', '/scratch3/li309/data/llava_data/train_data', '--image_tower', 'openai/clip-vit-large-patch14-336', '--image_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './finetuned_checkpoints/MoE-LLaVA-StableLM-1.6B-4e-RePa-2', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '8', '--gradient_accumulation_steps', '1', '--eval_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '4000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '18', '--lazy_preprocess', 'True', '--report_to', 'tensorboard', '--cache_dir', './cache_dir', '--report_to', 'wandb', '--finetune_repa_mode', 'true', '--repa_gated_ratio', '1.0']
[2025-09-05 13:20:13,717] [INFO] [launch.py:256:main] process 48009 spawned with command: ['/home/li309/pct_code/venv/moellava-test2/bin/python', '-u', 'moellava/train/train_mem.py', '--local_rank=2', '--moe_enable', 'True', '--num_experts', '4', '--top_k_experts', '2', '--capacity_factor', '1.5', '--moe_mode', 'sparse', '--use_residual', 'False', '--router_aux_loss_coef', '0.01', '--train_modules', 'gate_proj', 'up_proj', 'down_proj', 'wg', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', './checkpoints/MoE-LLaVA-StableLM-1.6B-4e', '--version', 'stablelm', '--data_path', '/scratch3/li309/data/llava_data/train_json/llava_image_tune_.json', '/scratch3/li309/data/llava_data/train_json/nlp_tune.json', '--image_folder', '/scratch3/li309/data/llava_data/train_data', '--image_tower', 'openai/clip-vit-large-patch14-336', '--image_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './finetuned_checkpoints/MoE-LLaVA-StableLM-1.6B-4e-RePa-2', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '8', '--gradient_accumulation_steps', '1', '--eval_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '4000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '18', '--lazy_preprocess', 'True', '--report_to', 'tensorboard', '--cache_dir', './cache_dir', '--report_to', 'wandb', '--finetune_repa_mode', 'true', '--repa_gated_ratio', '1.0']
[2025-09-05 13:20:13,718] [INFO] [launch.py:256:main] process 48010 spawned with command: ['/home/li309/pct_code/venv/moellava-test2/bin/python', '-u', 'moellava/train/train_mem.py', '--local_rank=3', '--moe_enable', 'True', '--num_experts', '4', '--top_k_experts', '2', '--capacity_factor', '1.5', '--moe_mode', 'sparse', '--use_residual', 'False', '--router_aux_loss_coef', '0.01', '--train_modules', 'gate_proj', 'up_proj', 'down_proj', 'wg', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', './checkpoints/MoE-LLaVA-StableLM-1.6B-4e', '--version', 'stablelm', '--data_path', '/scratch3/li309/data/llava_data/train_json/llava_image_tune_.json', '/scratch3/li309/data/llava_data/train_json/nlp_tune.json', '--image_folder', '/scratch3/li309/data/llava_data/train_data', '--image_tower', 'openai/clip-vit-large-patch14-336', '--image_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', './finetuned_checkpoints/MoE-LLaVA-StableLM-1.6B-4e-RePa-2', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '8', '--gradient_accumulation_steps', '1', '--eval_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '4000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '18', '--lazy_preprocess', 'True', '--report_to', 'tensorboard', '--cache_dir', './cache_dir', '--report_to', 'wandb', '--finetune_repa_mode', 'true', '--repa_gated_ratio', '1.0']
[2025-09-05 13:20:23,977] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-05 13:20:23,987] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-05 13:20:23,988] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-05 13:20:23,990] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/li309/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-09-05 13:20:25,699] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-05 13:20:25,699] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-05 13:20:25,699] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-05 13:20:25,700] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode

⚙️  Running in WANDB offline mode
[2025-09-05 13:20:26,247] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-05 13:20:26,247] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-05 13:20:26,247] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-05 13:20:26,247] [INFO] [comm.py:852:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-09-05 13:20:26,247] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-09-05 13:20:26,862] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 13:20:26,865] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 13:20:26,866] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 13:20:26,868] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 13:20:26,870] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 13:20:26,872] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 13:20:26,874] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 13:20:26,875] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 13:20:26,877] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 13:20:26,879] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 13:20:26,880] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
[2025-09-05 13:20:26,882] [INFO] [logging.py:107:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1
Vision encoder and proj init.

RePaMoELLaVAStablelmForCausalLM(
  (model): MoELLaVAStablelmModel(
    (embed_tokens): Embedding(100352, 2048, padding_idx=100280)
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (1): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (2): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (3): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (4): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (5): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (6): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (7): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (8): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (9): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (10): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (11): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (12): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (13): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (14): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (15): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (16): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (17): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (18): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (19): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (20): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (21): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (22): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): RePaMoE(
          (deepspeed_moe): MOELayer(
            (gate): TopKGate(
              (wg): Linear(in_features=2048, out_features=4, bias=False)
            )
            (experts): Experts(
              (deepspeed_experts): ModuleList(
                (0-3): 4 x RePaMLP(
                  (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                  (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                  (act_fn): SiLU()
                )
              )
            )
          )
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (23): DecoderLayer(
        (self_attn): Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (mlp): MLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (image_tower): CLIPVisionTower(
      (image_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): build_projector(
      (image_spatial_proj): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (video_patch_proj): Identity()
      (video_spatial_proj): Identity()
      (video_temproal_proj): Identity()
      (video_global_proj): Identity()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=100352, bias=False)
)
Formatting inputs...Skip in lazy mode
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Set initial gated ratio to 1.0
  Total training steps: 20790.0
  Stage 1 (gated ratio reduction): 10395.0 steps
  Stage 2 (post-reparam training): 10395.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
  Set initial gated ratio to 1.0
  Total training steps: 20790.0
  Stage 1 (gated ratio reduction): 10395.0 steps
  Stage 2 (post-reparam training): 10395.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Set initial gated ratio to 1.0
  Total training steps: 20790.0
  Stage 1 (gated ratio reduction): 10395.0 steps
  Stage 2 (post-reparam training): 10395.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
Setting up RePaMoE fine-tuning mode...
  Detected RePaMoE model: RePaMoELLaVAStablelmForCausalLM
  Frozen 699 non-MoE parameters, kept 156 MoE parameters trainable
  Set initial gated ratio to 1.0
  Total training steps: 20790.0
  Stage 1 (gated ratio reduction): 10395.0 steps
  Stage 2 (post-reparam training): 10395.0 steps
  Gated ratio will be reduced from 1.0 to 1.0 linearly over stage 1
  MoE layers: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
Skipping empty parameter group: no_decay_parameters
✓ DeepSpeed MoE requirements satisfied: 10 MoE groups found
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
No existing process group found, creating a new group named: ep_size_1
WARNING: tokenization mismatch: 0 vs. 1275. (ignored)
{'loss': 1.1232, 'grad_norm': 0.7346362471580505, 'learning_rate': 0.0, 'epoch': 0.0}
{'loss': 0.9433, 'grad_norm': 0.8659026622772217, 'learning_rate': 3.205128205128205e-08, 'epoch': 0.0}
{'loss': 0.9683, 'grad_norm': 0.7754202485084534, 'learning_rate': 6.41025641025641e-08, 'epoch': 0.0}
{'loss': 1.1998, 'grad_norm': 0.8392043113708496, 'learning_rate': 9.615384615384617e-08, 'epoch': 0.0}
{'loss': 1.0247, 'grad_norm': 0.7394837737083435, 'learning_rate': 1.282051282051282e-07, 'epoch': 0.0}
{'loss': 0.975, 'grad_norm': 0.7132079601287842, 'learning_rate': 1.6025641025641025e-07, 'epoch': 0.0}
{'loss': 1.0291, 'grad_norm': 0.7873438000679016, 'learning_rate': 1.9230769230769234e-07, 'epoch': 0.0}
{'loss': 0.8711, 'grad_norm': 0.6497498154640198, 'learning_rate': 2.2435897435897438e-07, 'epoch': 0.0}
{'loss': 0.875, 'grad_norm': 0.851119875907898, 'learning_rate': 2.564102564102564e-07, 'epoch': 0.0}
{'loss': 0.8214, 'grad_norm': 0.6156340837478638, 'learning_rate': 2.884615384615385e-07, 'epoch': 0.0}
{'loss': 1.0782, 'grad_norm': 0.8860834240913391, 'learning_rate': 3.205128205128205e-07, 'epoch': 0.0}
{'loss': 0.8814, 'grad_norm': 0.4093471169471741, 'learning_rate': 3.525641025641026e-07, 'epoch': 0.0}
{'loss': 1.0911, 'grad_norm': 0.7978979349136353, 'learning_rate': 3.846153846153847e-07, 'epoch': 0.0}
{'loss': 0.913, 'grad_norm': 0.7048529982566833, 'learning_rate': 4.1666666666666667e-07, 'epoch': 0.0}
{'loss': 0.9198, 'grad_norm': 0.774105429649353, 'learning_rate': 4.4871794871794876e-07, 'epoch': 0.0}
{'loss': 1.0702, 'grad_norm': 0.6498738527297974, 'learning_rate': 4.807692307692308e-07, 'epoch': 0.0}
{'loss': 1.0816, 'grad_norm': 0.6989814043045044, 'learning_rate': 5.128205128205128e-07, 'epoch': 0.0}
{'loss': 1.2052, 'grad_norm': 0.7771924734115601, 'learning_rate': 5.448717948717949e-07, 'epoch': 0.0}
{'loss': 1.0713, 'grad_norm': 0.7997003197669983, 'learning_rate': 5.76923076923077e-07, 'epoch': 0.0}
{'loss': 0.8722, 'grad_norm': 0.7342482805252075, 'learning_rate': 6.08974358974359e-07, 'epoch': 0.0}
{'loss': 1.0425, 'grad_norm': 0.8811312913894653, 'learning_rate': 6.41025641025641e-07, 'epoch': 0.0}
{'loss': 0.9458, 'grad_norm': 0.44142529368400574, 'learning_rate': 6.730769230769231e-07, 'epoch': 0.0}
{'loss': 0.922, 'grad_norm': 0.7487683296203613, 'learning_rate': 7.051282051282052e-07, 'epoch': 0.0}
{'loss': 0.9782, 'grad_norm': 0.759861409664154, 'learning_rate': 7.371794871794873e-07, 'epoch': 0.0}
{'loss': 1.041, 'grad_norm': 0.8711516857147217, 'learning_rate': 7.692307692307694e-07, 'epoch': 0.0}
{'loss': 0.9464, 'grad_norm': 0.6269745826721191, 'learning_rate': 8.012820512820515e-07, 'epoch': 0.0}
{'loss': 1.1282, 'grad_norm': 0.8873745203018188, 'learning_rate': 8.333333333333333e-07, 'epoch': 0.0}
{'loss': 1.0462, 'grad_norm': 0.7075169086456299, 'learning_rate': 8.653846153846154e-07, 'epoch': 0.0}
{'loss': 1.1498, 'grad_norm': 0.7676811218261719, 'learning_rate': 8.974358974358975e-07, 'epoch': 0.0}
{'loss': 1.0004, 'grad_norm': 0.6403923630714417, 'learning_rate': 9.294871794871796e-07, 'epoch': 0.0}
{'loss': 1.0521, 'grad_norm': 0.7169086933135986, 'learning_rate': 9.615384615384617e-07, 'epoch': 0.0}
{'loss': 1.0076, 'grad_norm': 0.8924190998077393, 'learning_rate': 9.935897435897436e-07, 'epoch': 0.0}
{'loss': 0.9237, 'grad_norm': 0.8179100751876831, 'learning_rate': 1.0256410256410257e-06, 'epoch': 0.0}
{'loss': 1.0307, 'grad_norm': 0.7646263837814331, 'learning_rate': 1.0576923076923078e-06, 'epoch': 0.0}
{'loss': 0.9949, 'grad_norm': 0.7762450575828552, 'learning_rate': 1.0897435897435899e-06, 'epoch': 0.0}
{'loss': 0.956, 'grad_norm': 0.7806118726730347, 'learning_rate': 1.121794871794872e-06, 'epoch': 0.0}
{'loss': 1.0539, 'grad_norm': 0.7563377022743225, 'learning_rate': 1.153846153846154e-06, 'epoch': 0.0}
{'loss': 0.9691, 'grad_norm': 0.898323118686676, 'learning_rate': 1.185897435897436e-06, 'epoch': 0.0}
{'loss': 1.0556, 'grad_norm': 0.7061451077461243, 'learning_rate': 1.217948717948718e-06, 'epoch': 0.0}
{'loss': 0.8383, 'grad_norm': 0.7363886833190918, 'learning_rate': 1.25e-06, 'epoch': 0.0}
{'loss': 0.9606, 'grad_norm': 0.7284542322158813, 'learning_rate': 1.282051282051282e-06, 'epoch': 0.0}
{'loss': 0.9307, 'grad_norm': 0.743604838848114, 'learning_rate': 1.3141025641025643e-06, 'epoch': 0.0}
{'loss': 1.0124, 'grad_norm': 0.6987375617027283, 'learning_rate': 1.3461538461538462e-06, 'epoch': 0.0}
{'loss': 1.0983, 'grad_norm': 0.765491783618927, 'learning_rate': 1.3782051282051285e-06, 'epoch': 0.0}
{'loss': 0.9981, 'grad_norm': 0.7951213121414185, 'learning_rate': 1.4102564102564104e-06, 'epoch': 0.0}
{'loss': 0.9298, 'grad_norm': 0.6788871884346008, 'learning_rate': 1.4423076923076922e-06, 'epoch': 0.0}
{'loss': 0.9379, 'grad_norm': 0.8822317123413086, 'learning_rate': 1.4743589743589745e-06, 'epoch': 0.0}
{'loss': 0.9947, 'grad_norm': 0.48177677392959595, 'learning_rate': 1.5064102564102564e-06, 'epoch': 0.0}
{'loss': 0.9944, 'grad_norm': 0.6927613615989685, 'learning_rate': 1.5384615384615387e-06, 'epoch': 0.0}
{'loss': 0.9616, 'grad_norm': 0.6815250515937805, 'learning_rate': 1.5705128205128206e-06, 'epoch': 0.0}
{'loss': 1.1307, 'grad_norm': 0.7204534411430359, 'learning_rate': 1.602564102564103e-06, 'epoch': 0.0}
{'loss': 0.9948, 'grad_norm': 0.6625851392745972, 'learning_rate': 1.6346153846153848e-06, 'epoch': 0.0}
{'loss': 1.1374, 'grad_norm': 0.7290217280387878, 'learning_rate': 1.6666666666666667e-06, 'epoch': 0.0}
{'loss': 1.0303, 'grad_norm': 0.7579251527786255, 'learning_rate': 1.698717948717949e-06, 'epoch': 0.0}
{'loss': 0.8999, 'grad_norm': 0.8685777187347412, 'learning_rate': 1.7307692307692308e-06, 'epoch': 0.0}
{'loss': 0.957, 'grad_norm': 0.7643397450447083, 'learning_rate': 1.7628205128205131e-06, 'epoch': 0.0}
{'loss': 0.9198, 'grad_norm': 0.47275134921073914, 'learning_rate': 1.794871794871795e-06, 'epoch': 0.0}
{'loss': 1.0055, 'grad_norm': 0.8549197912216187, 'learning_rate': 1.826923076923077e-06, 'epoch': 0.0}
{'loss': 1.143, 'grad_norm': 0.890927255153656, 'learning_rate': 1.8589743589743592e-06, 'epoch': 0.0}
{'loss': 1.1554, 'grad_norm': 0.981715202331543, 'learning_rate': 1.891025641025641e-06, 'epoch': 0.0}
{'loss': 0.9932, 'grad_norm': 0.7205512523651123, 'learning_rate': 1.9230769230769234e-06, 'epoch': 0.0}
{'loss': 0.9646, 'grad_norm': 0.6485276222229004, 'learning_rate': 1.9551282051282055e-06, 'epoch': 0.0}
{'loss': 1.1123, 'grad_norm': 0.9448151588439941, 'learning_rate': 1.987179487179487e-06, 'epoch': 0.0}
{'loss': 1.0471, 'grad_norm': 0.7214096188545227, 'learning_rate': 2.0192307692307692e-06, 'epoch': 0.0}
{'loss': 1.0416, 'grad_norm': 0.7801170349121094, 'learning_rate': 2.0512820512820513e-06, 'epoch': 0.0}
{'loss': 0.9946, 'grad_norm': 0.7549988627433777, 'learning_rate': 2.0833333333333334e-06, 'epoch': 0.0}
{'loss': 1.1126, 'grad_norm': 0.7785344123840332, 'learning_rate': 2.1153846153846155e-06, 'epoch': 0.0}
{'loss': 0.7785, 'grad_norm': 0.4018246829509735, 'learning_rate': 2.1474358974358976e-06, 'epoch': 0.0}
{'loss': 0.9292, 'grad_norm': 0.48090672492980957, 'learning_rate': 2.1794871794871797e-06, 'epoch': 0.0}
{'loss': 0.9867, 'grad_norm': 0.7512015700340271, 'learning_rate': 2.211538461538462e-06, 'epoch': 0.0}
{'loss': 1.0255, 'grad_norm': 0.8009175062179565, 'learning_rate': 2.243589743589744e-06, 'epoch': 0.0}
{'loss': 1.0531, 'grad_norm': 0.9327078461647034, 'learning_rate': 2.275641025641026e-06, 'epoch': 0.0}
{'loss': 0.9766, 'grad_norm': 0.6702020764350891, 'learning_rate': 2.307692307692308e-06, 'epoch': 0.0}
{'loss': 0.9849, 'grad_norm': 0.7965996265411377, 'learning_rate': 2.3397435897435897e-06, 'epoch': 0.0}
{'loss': 0.8483, 'grad_norm': 0.8566994667053223, 'learning_rate': 2.371794871794872e-06, 'epoch': 0.0}
{'loss': 1.0978, 'grad_norm': 0.8708980083465576, 'learning_rate': 2.403846153846154e-06, 'epoch': 0.0}
{'loss': 0.9699, 'grad_norm': 0.6147603392601013, 'learning_rate': 2.435897435897436e-06, 'epoch': 0.0}
{'loss': 0.9739, 'grad_norm': 0.7091526389122009, 'learning_rate': 2.467948717948718e-06, 'epoch': 0.0}
{'loss': 1.1238, 'grad_norm': 0.7687881588935852, 'learning_rate': 2.5e-06, 'epoch': 0.0}
{'loss': 0.9001, 'grad_norm': 0.7248747944831848, 'learning_rate': 2.5320512820512823e-06, 'epoch': 0.0}
{'loss': 1.0512, 'grad_norm': 0.7607538104057312, 'learning_rate': 2.564102564102564e-06, 'epoch': 0.0}
{'loss': 0.8644, 'grad_norm': 0.7051264047622681, 'learning_rate': 2.5961538461538465e-06, 'epoch': 0.0}
{'loss': 1.1046, 'grad_norm': 0.8045806288719177, 'learning_rate': 2.6282051282051286e-06, 'epoch': 0.0}
{'loss': 0.9359, 'grad_norm': 0.7376599907875061, 'learning_rate': 2.6602564102564107e-06, 'epoch': 0.0}
{'loss': 0.8083, 'grad_norm': 0.7155592441558838, 'learning_rate': 2.6923076923076923e-06, 'epoch': 0.0}
{'loss': 1.0368, 'grad_norm': 0.7275007367134094, 'learning_rate': 2.7243589743589744e-06, 'epoch': 0.0}
{'loss': 1.0449, 'grad_norm': 0.4557726979255676, 'learning_rate': 2.756410256410257e-06, 'epoch': 0.0}
{'loss': 1.0853, 'grad_norm': 0.8939875364303589, 'learning_rate': 2.7884615384615386e-06, 'epoch': 0.0}
{'loss': 0.9556, 'grad_norm': 0.8215521574020386, 'learning_rate': 2.8205128205128207e-06, 'epoch': 0.0}
{'loss': 1.1479, 'grad_norm': 0.7019622921943665, 'learning_rate': 2.852564102564103e-06, 'epoch': 0.0}
{'loss': 0.9626, 'grad_norm': 0.6637140512466431, 'learning_rate': 2.8846153846153845e-06, 'epoch': 0.0}
{'loss': 1.1178, 'grad_norm': 0.7813048362731934, 'learning_rate': 2.916666666666667e-06, 'epoch': 0.0}
{'loss': 1.2921, 'grad_norm': 0.8223593235015869, 'learning_rate': 2.948717948717949e-06, 'epoch': 0.0}
{'loss': 1.1224, 'grad_norm': 0.8048459887504578, 'learning_rate': 2.980769230769231e-06, 'epoch': 0.0}
{'loss': 1.0469, 'grad_norm': 0.7704593539237976, 'learning_rate': 3.012820512820513e-06, 'epoch': 0.0}
{'loss': 0.9143, 'grad_norm': 0.7358514666557312, 'learning_rate': 3.044871794871795e-06, 'epoch': 0.0}
{'loss': 1.0027, 'grad_norm': 0.679530143737793, 'learning_rate': 3.0769230769230774e-06, 'epoch': 0.0}
{'loss': 0.971, 'grad_norm': 0.7889167666435242, 'learning_rate': 3.108974358974359e-06, 'epoch': 0.0}
{'loss': 1.0249, 'grad_norm': 0.9319354891777039, 'learning_rate': 3.141025641025641e-06, 'epoch': 0.0}
{'loss': 0.8964, 'grad_norm': 0.7176393866539001, 'learning_rate': 3.1730769230769233e-06, 'epoch': 0.0}
{'loss': 1.0488, 'grad_norm': 0.6984726190567017, 'learning_rate': 3.205128205128206e-06, 'epoch': 0.0}
{'loss': 0.9704, 'grad_norm': 0.8351145386695862, 'learning_rate': 3.2371794871794875e-06, 'epoch': 0.0}
{'loss': 0.9734, 'grad_norm': 0.8174227476119995, 'learning_rate': 3.2692307692307696e-06, 'epoch': 0.0}
{'loss': 1.0345, 'grad_norm': 0.8265982270240784, 'learning_rate': 3.3012820512820517e-06, 'epoch': 0.01}
{'loss': 1.1256, 'grad_norm': 0.6889663338661194, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.01}
{'loss': 0.9738, 'grad_norm': 0.7825217247009277, 'learning_rate': 3.365384615384616e-06, 'epoch': 0.01}
{'loss': 0.9798, 'grad_norm': 0.6217710375785828, 'learning_rate': 3.397435897435898e-06, 'epoch': 0.01}
{'loss': 1.1005, 'grad_norm': 0.7390099167823792, 'learning_rate': 3.4294871794871796e-06, 'epoch': 0.01}
{'loss': 1.1127, 'grad_norm': 0.7850803136825562, 'learning_rate': 3.4615384615384617e-06, 'epoch': 0.01}
{'loss': 1.0588, 'grad_norm': 0.7749985456466675, 'learning_rate': 3.4935897435897438e-06, 'epoch': 0.01}
{'loss': 1.0315, 'grad_norm': 0.7502292394638062, 'learning_rate': 3.5256410256410263e-06, 'epoch': 0.01}
{'loss': 1.0915, 'grad_norm': 0.7702133655548096, 'learning_rate': 3.557692307692308e-06, 'epoch': 0.01}
{'loss': 1.1056, 'grad_norm': 0.8313168287277222, 'learning_rate': 3.58974358974359e-06, 'epoch': 0.01}
{'loss': 0.9084, 'grad_norm': 0.8434048891067505, 'learning_rate': 3.621794871794872e-06, 'epoch': 0.01}
{'loss': 0.9247, 'grad_norm': 0.7908504605293274, 'learning_rate': 3.653846153846154e-06, 'epoch': 0.01}
{'loss': 1.0872, 'grad_norm': 0.8506609201431274, 'learning_rate': 3.6858974358974363e-06, 'epoch': 0.01}
{'loss': 0.9271, 'grad_norm': 0.8779677152633667, 'learning_rate': 3.7179487179487184e-06, 'epoch': 0.01}
{'loss': 0.9397, 'grad_norm': 0.8029981255531311, 'learning_rate': 3.7500000000000005e-06, 'epoch': 0.01}
