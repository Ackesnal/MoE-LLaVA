Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:23<00:23, 23.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 11.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.66s/it]
Some weights of RePaMoELLaVAStablelmForCausalLM were not initialized from the model checkpoint at ./checkpoints/MoE-LLaVA-StableLM-1.6B-4e and are newly initialized: ['model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:23<00:23, 23.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 11.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.66s/it]
Some weights of RePaMoELLaVAStablelmForCausalLM were not initialized from the model checkpoint at ./checkpoints/MoE-LLaVA-StableLM-1.6B-4e and are newly initialized: ['model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:23<00:23, 23.69s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 11.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.66s/it]
Some weights of RePaMoELLaVAStablelmForCausalLM were not initialized from the model checkpoint at ./checkpoints/MoE-LLaVA-StableLM-1.6B-4e and are newly initialized: ['model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:23<00:23, 23.71s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 11.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.66s/it]
Some weights of RePaMoELLaVAStablelmForCausalLM were not initialized from the model checkpoint at ./checkpoints/MoE-LLaVA-StableLM-1.6B-4e and are newly initialized: ['model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.0.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.10.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.12.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.14.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.16.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.18.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.2.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.20.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.22.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.4.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.6.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask', 'model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.0.mask', 'model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.1.mask', 'model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.2.mask', 'model.layers.8.mlp.deepspeed_moe.experts.deepspeed_experts.3.mask']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA/moellava/train/llava_trainer.py:187: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `LLaVATrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
/datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA/moellava/train/llava_trainer.py:187: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `LLaVATrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
/datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA/moellava/train/llava_trainer.py:187: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `LLaVATrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
/datasets/work/d61-pct/work/yang_work/moe/MoE-LLaVA/moellava/train/llava_trainer.py:187: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `LLaVATrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 100280}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 100280}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 100280}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 100280}.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
[g008:24860] OPAL ERROR: Unreachable in file pmix3x_client.c at line 111
--------------------------------------------------------------------------
The application appears to have been direct launched using "srun",
but OMPI was not built with SLURM's PMI support and therefore cannot
execute. There are several options for building PMI support under
SLURM, depending upon the SLURM version you are using:

  version 16.05 or later: you can use SLURM's PMIx support. This
  requires that you configure and build SLURM --with-pmix.

  Versions earlier than 16.05: you must use either SLURM's PMI-1 or
  PMI-2 support. SLURM builds PMI-1 by default, or you can manually
  install PMI-2. You must then build Open MPI using --with-pmi pointing
  to the SLURM PMI library location.

Please configure as appropriate and try again.
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[g008:24860] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
[g003:14432] OPAL ERROR: Unreachable in file pmix3x_client.c at line 111
[g009:46405] OPAL ERROR: Unreachable in file pmix3x_client.c at line 111
[g002:54264] OPAL ERROR: Unreachable in file pmix3x_client.c at line 111
--------------------------------------------------------------------------
The application appears to have been direct launched using "srun",
but OMPI was not built with SLURM's PMI support and therefore cannot
execute. There are several options for building PMI support under
SLURM, depending upon the SLURM version you are using:

  version 16.05 or later: you can use SLURM's PMIx support. This
  requires that you configure and build SLURM --with-pmix.

  Versions earlier than 16.05: you must use either SLURM's PMI-1 or
  PMI-2 support. SLURM builds PMI-1 by default, or you can manually
  install PMI-2. You must then build Open MPI using --with-pmi pointing
  to the SLURM PMI library location.

Please configure as appropriate and try again.
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[g003:14432] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
--------------------------------------------------------------------------
The application appears to have been direct launched using "srun",
but OMPI was not built with SLURM's PMI support and therefore cannot
execute. There are several options for building PMI support under
SLURM, depending upon the SLURM version you are using:

  version 16.05 or later: you can use SLURM's PMIx support. This
  requires that you configure and build SLURM --with-pmix.

  Versions earlier than 16.05: you must use either SLURM's PMI-1 or
  PMI-2 support. SLURM builds PMI-1 by default, or you can manually
  install PMI-2. You must then build Open MPI using --with-pmi pointing
  to the SLURM PMI library location.

Please configure as appropriate and try again.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
The application appears to have been direct launched using "srun",
but OMPI was not built with SLURM's PMI support and therefore cannot
execute. There are several options for building PMI support under
SLURM, depending upon the SLURM version you are using:

  version 16.05 or later: you can use SLURM's PMIx support. This
  requires that you configure and build SLURM --with-pmix.

  Versions earlier than 16.05: you must use either SLURM's PMI-1 or
  PMI-2 support. SLURM builds PMI-1 by default, or you can manually
  install PMI-2. You must then build Open MPI using --with-pmi pointing
  to the SLURM PMI library location.

Please configure as appropriate and try again.
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[g002:54264] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
[g009:46405] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
srun: error: g008: task 2: Exited with exit code 1
srun: error: g009: task 3: Exited with exit code 1
srun: error: g003: task 1: Exited with exit code 1
srun: error: g002: task 0: Exited with exit code 1
